I0903 22:03:17.129941      22 test_context.go:410] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-476854905
I0903 22:03:17.129986      22 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0903 22:03:17.130124      22 e2e.go:124] Starting e2e run "09a98756-81a2-4385-89d2-7b73a2c32160" on Ginkgo node 1
{"msg":"Test Suite starting","total":277,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1599170595 - Will randomize all specs
Will run 277 of 4992 specs

Sep  3 22:03:17.143: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
E0903 22:03:17.145921      22 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp 127.0.0.1:8099: connect: connection refused
Sep  3 22:03:17.148: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  3 22:03:17.163: INFO: Unschedulable nodes:
Sep  3 22:03:17.163: INFO: -> karbon-test1186mm-f1ba59-k8s-master-0 Ready=true Network=false Taints=[{node.kubernetes.io/master  NoSchedule <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Sep  3 22:03:17.163: INFO: -> karbon-test1186mm-f1ba59-k8s-master-1 Ready=true Network=false Taints=[{node.kubernetes.io/master  NoSchedule <nil>}] NonblockingTaints:node-role.kubernetes.io/master
Sep  3 22:03:17.163: INFO: ================================
Sep  3 22:03:47.172: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  3 22:03:47.202: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  3 22:03:47.202: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Sep  3 22:03:47.202: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  3 22:03:47.209: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Sep  3 22:03:47.209: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-ds' (0 seconds elapsed)
Sep  3 22:03:47.209: INFO: e2e test version: v1.18.6
Sep  3 22:03:47.210: INFO: kube-apiserver version: v1.18.6
Sep  3 22:03:47.210: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:03:47.215: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:03:47.215: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-probe
Sep  3 22:03:47.255: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-19d8182b-67d1-4dbc-aff1-4661d26e6822 in namespace container-probe-1048
Sep  3 22:03:55.291: INFO: Started pod liveness-19d8182b-67d1-4dbc-aff1-4661d26e6822 in namespace container-probe-1048
STEP: checking the pod's current state and verifying that restartCount is present
Sep  3 22:03:55.295: INFO: Initial restart count of pod liveness-19d8182b-67d1-4dbc-aff1-4661d26e6822 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:07:55.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1048" for this suite.

• [SLOW TEST:248.621 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":277,"completed":1,"skipped":20,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:07:55.837: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-78445752-9ebd-46d1-bd66-31dd97338266
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:08:01.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9477" for this suite.

• [SLOW TEST:6.130 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":2,"skipped":25,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:08:01.967: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:08:06.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8669" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":277,"completed":3,"skipped":32,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:08:06.043: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-0840fb41-0515-4c78-96cb-aca176e3104d
STEP: Creating a pod to test consume secrets
Sep  3 22:08:06.095: INFO: Waiting up to 5m0s for pod "pod-secrets-50a60ba9-4f42-48fb-a1a8-85bcb8ce0fbd" in namespace "secrets-2067" to be "Succeeded or Failed"
Sep  3 22:08:06.099: INFO: Pod "pod-secrets-50a60ba9-4f42-48fb-a1a8-85bcb8ce0fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917957ms
Sep  3 22:08:08.104: INFO: Pod "pod-secrets-50a60ba9-4f42-48fb-a1a8-85bcb8ce0fbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008842754s
STEP: Saw pod success
Sep  3 22:08:08.104: INFO: Pod "pod-secrets-50a60ba9-4f42-48fb-a1a8-85bcb8ce0fbd" satisfied condition "Succeeded or Failed"
Sep  3 22:08:08.107: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-secrets-50a60ba9-4f42-48fb-a1a8-85bcb8ce0fbd container secret-volume-test: <nil>
STEP: delete the pod
Sep  3 22:08:08.133: INFO: Waiting for pod pod-secrets-50a60ba9-4f42-48fb-a1a8-85bcb8ce0fbd to disappear
Sep  3 22:08:08.137: INFO: Pod pod-secrets-50a60ba9-4f42-48fb-a1a8-85bcb8ce0fbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:08:08.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2067" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":4,"skipped":43,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:08:08.151: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a372acea-9a7c-4f00-9631-5740ade67348
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a372acea-9a7c-4f00-9631-5740ade67348
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:08:12.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9468" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":5,"skipped":52,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:08:12.255: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:08:12.303: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d08772db-ef2a-4211-8ceb-b41fb9d6d068" in namespace "downward-api-1813" to be "Succeeded or Failed"
Sep  3 22:08:12.305: INFO: Pod "downwardapi-volume-d08772db-ef2a-4211-8ceb-b41fb9d6d068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.830963ms
Sep  3 22:08:14.309: INFO: Pod "downwardapi-volume-d08772db-ef2a-4211-8ceb-b41fb9d6d068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006708941s
STEP: Saw pod success
Sep  3 22:08:14.309: INFO: Pod "downwardapi-volume-d08772db-ef2a-4211-8ceb-b41fb9d6d068" satisfied condition "Succeeded or Failed"
Sep  3 22:08:14.312: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-d08772db-ef2a-4211-8ceb-b41fb9d6d068 container client-container: <nil>
STEP: delete the pod
Sep  3 22:08:14.333: INFO: Waiting for pod downwardapi-volume-d08772db-ef2a-4211-8ceb-b41fb9d6d068 to disappear
Sep  3 22:08:14.336: INFO: Pod downwardapi-volume-d08772db-ef2a-4211-8ceb-b41fb9d6d068 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:08:14.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1813" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":6,"skipped":65,"failed":0}

------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:08:14.346: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-b1ea2d39-e5b1-447f-ba4a-dd4baf0bd59d
STEP: Creating secret with name s-test-opt-upd-32340cd7-e042-4f7a-b7e7-bc1171a70037
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b1ea2d39-e5b1-447f-ba4a-dd4baf0bd59d
STEP: Updating secret s-test-opt-upd-32340cd7-e042-4f7a-b7e7-bc1171a70037
STEP: Creating secret with name s-test-opt-create-45018818-efe3-4819-b8dd-56a7d2990f8a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:09:28.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6396" for this suite.

• [SLOW TEST:74.499 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":7,"skipped":65,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:09:28.846: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-2pvd
STEP: Creating a pod to test atomic-volume-subpath
Sep  3 22:09:28.896: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2pvd" in namespace "subpath-4892" to be "Succeeded or Failed"
Sep  3 22:09:28.900: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.756958ms
Sep  3 22:09:30.904: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008088085s
Sep  3 22:09:32.908: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Running", Reason="", readiness=true. Elapsed: 4.011880222s
Sep  3 22:09:34.911: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Running", Reason="", readiness=true. Elapsed: 6.015527873s
Sep  3 22:09:36.916: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Running", Reason="", readiness=true. Elapsed: 8.020266746s
Sep  3 22:09:38.920: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Running", Reason="", readiness=true. Elapsed: 10.024065071s
Sep  3 22:09:40.925: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Running", Reason="", readiness=true. Elapsed: 12.028790981s
Sep  3 22:09:42.929: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Running", Reason="", readiness=true. Elapsed: 14.033071666s
Sep  3 22:09:44.934: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Running", Reason="", readiness=true. Elapsed: 16.037895238s
Sep  3 22:09:46.939: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Running", Reason="", readiness=true. Elapsed: 18.043638063s
Sep  3 22:09:48.944: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Running", Reason="", readiness=true. Elapsed: 20.04853656s
Sep  3 22:09:50.948: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Running", Reason="", readiness=true. Elapsed: 22.052497542s
Sep  3 22:09:52.952: INFO: Pod "pod-subpath-test-projected-2pvd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.056127798s
STEP: Saw pod success
Sep  3 22:09:52.952: INFO: Pod "pod-subpath-test-projected-2pvd" satisfied condition "Succeeded or Failed"
Sep  3 22:09:52.954: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-1 pod pod-subpath-test-projected-2pvd container test-container-subpath-projected-2pvd: <nil>
STEP: delete the pod
Sep  3 22:09:52.981: INFO: Waiting for pod pod-subpath-test-projected-2pvd to disappear
Sep  3 22:09:52.983: INFO: Pod pod-subpath-test-projected-2pvd no longer exists
STEP: Deleting pod pod-subpath-test-projected-2pvd
Sep  3 22:09:52.983: INFO: Deleting pod "pod-subpath-test-projected-2pvd" in namespace "subpath-4892"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:09:52.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4892" for this suite.

• [SLOW TEST:24.149 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":277,"completed":8,"skipped":102,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:09:52.995: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep  3 22:09:53.027: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  3 22:09:53.037: INFO: Waiting for terminating namespaces to be deleted...
Sep  3 22:09:53.041: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-master-0 before test
Sep  3 22:09:53.063: INFO: kube-flannel-ds-q7xlv from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.063: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:09:53.063: INFO: fluent-bit-6wlnp from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.063: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:09:53.063: INFO: node-exporter-rvmxt from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:09:53.063: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:09:53.063: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-722pc from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.063: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:09:53.063: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:09:53.063: INFO: csi-node-ntnx-plugin-xhqx9 from ntnx-system started at 2020-09-03 22:03:17 +0000 UTC (3 container statuses recorded)
Sep  3 22:09:53.063: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:09:53.063: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:09:53.063: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:09:53.063: INFO: kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-0 from kube-system started at 2020-09-03 21:51:20 +0000 UTC (3 container statuses recorded)
Sep  3 22:09:53.063: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  3 22:09:53.063: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  3 22:09:53.063: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  3 22:09:53.063: INFO: kube-proxy-ds-rtvz4 from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.063: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:09:53.063: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-master-1 before test
Sep  3 22:09:53.080: INFO: csi-node-ntnx-plugin-wgm9j from ntnx-system started at 2020-09-03 22:03:21 +0000 UTC (3 container statuses recorded)
Sep  3 22:09:53.080: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:09:53.080: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:09:53.080: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:09:53.080: INFO: kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-1 from kube-system started at 2020-09-03 21:52:32 +0000 UTC (3 container statuses recorded)
Sep  3 22:09:53.080: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  3 22:09:53.080: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  3 22:09:53.080: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  3 22:09:53.080: INFO: kube-proxy-ds-8vwrs from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.080: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:09:53.080: INFO: kube-flannel-ds-8qrb7 from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.080: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:09:53.080: INFO: kube-dns-75bfbbdcb6-v6vf9 from kube-system started at 2020-09-03 21:55:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:09:53.080: INFO: 	Container dnsmasq ready: true, restart count 0
Sep  3 22:09:53.080: INFO: 	Container kubedns ready: true, restart count 0
Sep  3 22:09:53.080: INFO: 	Container sidecar ready: true, restart count 0
Sep  3 22:09:53.080: INFO: fluent-bit-cpczk from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.080: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:09:53.080: INFO: node-exporter-4wwlp from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.080: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:09:53.080: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:09:53.080: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-2qnxc from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.080: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:09:53.080: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:09:53.080: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-0 before test
Sep  3 22:09:53.090: INFO: kube-proxy-ds-cbqh9 from kube-system started at 2020-09-03 21:55:15 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:09:53.090: INFO: prometheus-operator-8c5f5b4-9r8jx from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  3 22:09:53.090: INFO: alertmanager-main-0 from ntnx-system started at 2020-09-03 21:59:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container alertmanager ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 	Container config-reloader ready: true, restart count 0
Sep  3 22:09:53.090: INFO: prometheus-k8s-0 from ntnx-system started at 2020-09-03 21:59:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container prometheus ready: true, restart count 1
Sep  3 22:09:53.090: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  3 22:09:53.090: INFO: sonobuoy from sonobuoy started at 2020-09-03 22:02:43 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  3 22:09:53.090: INFO: kibana-logging-58dfc7bc95-bk4wt from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container kibana-logging ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 	Container nginxhttp ready: true, restart count 0
Sep  3 22:09:53.090: INFO: kube-state-metrics-7b754ff76b-vpckb from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (4 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container addon-resizer ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  3 22:09:53.090: INFO: node-exporter-fl4nm from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:09:53.090: INFO: kube-flannel-ds-m9clk from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:09:53.090: INFO: csi-node-ntnx-plugin-fxlcg from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (3 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:09:53.090: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-tcdxz from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:09:53.090: INFO: fluent-bit-25t4p from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.090: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:09:53.090: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-1 before test
Sep  3 22:09:53.098: INFO: fluent-bit-ngkvm from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.098: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:09:53.098: INFO: alertmanager-main-1 from ntnx-system started at 2020-09-03 21:59:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.098: INFO: 	Container alertmanager ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 	Container config-reloader ready: true, restart count 0
Sep  3 22:09:53.098: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-gdgws from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.098: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:09:53.098: INFO: kube-proxy-ds-w7kvw from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.098: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:09:53.098: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (4 container statuses recorded)
Sep  3 22:09:53.098: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 	Container csi-resizer ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Sep  3 22:09:53.098: INFO: csi-node-ntnx-plugin-qqhtx from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (3 container statuses recorded)
Sep  3 22:09:53.098: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:09:53.098: INFO: node-exporter-kzwh4 from ntnx-system started at 2020-09-03 21:59:12 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:09:53.098: INFO: prometheus-k8s-1 from ntnx-system started at 2020-09-03 21:59:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:09:53.098: INFO: 	Container prometheus ready: true, restart count 1
Sep  3 22:09:53.098: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  3 22:09:53.098: INFO: sonobuoy-e2e-job-38c4ed60912f46a6 from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.098: INFO: 	Container e2e ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:09:53.098: INFO: kube-flannel-ds-4mqbc from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.098: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:09:53.098: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-2 before test
Sep  3 22:09:53.115: INFO: fluent-bit-wsm97 from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.115: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:09:53.115: INFO: kubernetes-events-printer-5767b7d649-8m9sl from ntnx-system started at 2020-09-03 21:56:21 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.115: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Sep  3 22:09:53.115: INFO: elasticsearch-logging-0 from ntnx-system started at 2020-09-03 21:56:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.115: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Sep  3 22:09:53.115: INFO: node-exporter-wdljm from ntnx-system started at 2020-09-03 21:59:12 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.115: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:09:53.115: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:09:53.115: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-f4pr7 from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:09:53.115: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:09:53.115: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:09:53.115: INFO: kube-proxy-ds-rsdbq from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.115: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:09:53.115: INFO: kube-flannel-ds-4vfnt from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:09:53.115: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:09:53.115: INFO: csi-node-ntnx-plugin-l9tvt from ntnx-system started at 2020-09-03 21:55:53 +0000 UTC (3 container statuses recorded)
Sep  3 22:09:53.115: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:09:53.115: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:09:53.115: INFO: 	Container liveness-probe ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e449cb48-c20f-4ec4-a858-87b8967014ea 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-e449cb48-c20f-4ec4-a858-87b8967014ea off the node karbon-test1186mm-f1ba59-k8s-worker-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e449cb48-c20f-4ec4-a858-87b8967014ea
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:14:59.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1365" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:306.232 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":277,"completed":9,"skipped":102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:14:59.227: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  3 22:14:59.275: INFO: Waiting up to 5m0s for pod "pod-84d7b35b-4b49-47e2-9c2c-d7dae21b4d2f" in namespace "emptydir-763" to be "Succeeded or Failed"
Sep  3 22:14:59.279: INFO: Pod "pod-84d7b35b-4b49-47e2-9c2c-d7dae21b4d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.478855ms
Sep  3 22:15:01.284: INFO: Pod "pod-84d7b35b-4b49-47e2-9c2c-d7dae21b4d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008287601s
Sep  3 22:15:03.287: INFO: Pod "pod-84d7b35b-4b49-47e2-9c2c-d7dae21b4d2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012134584s
STEP: Saw pod success
Sep  3 22:15:03.287: INFO: Pod "pod-84d7b35b-4b49-47e2-9c2c-d7dae21b4d2f" satisfied condition "Succeeded or Failed"
Sep  3 22:15:03.290: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-84d7b35b-4b49-47e2-9c2c-d7dae21b4d2f container test-container: <nil>
STEP: delete the pod
Sep  3 22:15:03.325: INFO: Waiting for pod pod-84d7b35b-4b49-47e2-9c2c-d7dae21b4d2f to disappear
Sep  3 22:15:03.330: INFO: Pod pod-84d7b35b-4b49-47e2-9c2c-d7dae21b4d2f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:15:03.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-763" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":10,"skipped":124,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:15:03.344: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:15:03.382: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:15:09.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8957" for this suite.

• [SLOW TEST:6.359 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":277,"completed":11,"skipped":127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:15:09.704: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep  3 22:15:15.779: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0903 22:15:15.778959      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:15:15.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-256" for this suite.

• [SLOW TEST:6.087 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":277,"completed":12,"skipped":169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:15:15.791: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  3 22:15:15.842: INFO: Waiting up to 5m0s for pod "pod-0c97c253-8572-40f8-b30e-fc194c9d2e78" in namespace "emptydir-4095" to be "Succeeded or Failed"
Sep  3 22:15:15.848: INFO: Pod "pod-0c97c253-8572-40f8-b30e-fc194c9d2e78": Phase="Pending", Reason="", readiness=false. Elapsed: 6.307821ms
Sep  3 22:15:17.852: INFO: Pod "pod-0c97c253-8572-40f8-b30e-fc194c9d2e78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010009718s
STEP: Saw pod success
Sep  3 22:15:17.852: INFO: Pod "pod-0c97c253-8572-40f8-b30e-fc194c9d2e78" satisfied condition "Succeeded or Failed"
Sep  3 22:15:17.854: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-0c97c253-8572-40f8-b30e-fc194c9d2e78 container test-container: <nil>
STEP: delete the pod
Sep  3 22:15:17.874: INFO: Waiting for pod pod-0c97c253-8572-40f8-b30e-fc194c9d2e78 to disappear
Sep  3 22:15:17.876: INFO: Pod pod-0c97c253-8572-40f8-b30e-fc194c9d2e78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:15:17.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4095" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":13,"skipped":195,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:15:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep  3 22:15:17.926: INFO: Created pod &Pod{ObjectMeta:{dns-7451  dns-7451 /api/v1/namespaces/dns-7451/pods/dns-7451 3e3b4706-8221-4eb2-b391-010201e51bdc 6205 0 2020-09-03 22:15:17 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-09-03 22:15:17 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 67 111 110 102 105 103 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 115 101 114 118 101 114 115 34 58 123 125 44 34 102 58 115 101 97 114 99 104 101 115 34 58 123 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-48xqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-48xqc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-48xqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:15:17.929: INFO: The status of Pod dns-7451 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:15:19.933: INFO: The status of Pod dns-7451 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep  3 22:15:19.934: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7451 PodName:dns-7451 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:15:19.934: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Verifying customized DNS server is configured on pod...
Sep  3 22:15:20.079: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7451 PodName:dns-7451 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:15:20.079: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:15:20.220: INFO: Deleting pod dns-7451...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:15:20.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7451" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":277,"completed":14,"skipped":197,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:15:20.248: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-mc5x
STEP: Creating a pod to test atomic-volume-subpath
Sep  3 22:15:20.306: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mc5x" in namespace "subpath-9276" to be "Succeeded or Failed"
Sep  3 22:15:20.309: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.874572ms
Sep  3 22:15:22.313: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006622068s
Sep  3 22:15:24.317: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Running", Reason="", readiness=true. Elapsed: 4.01015516s
Sep  3 22:15:26.321: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Running", Reason="", readiness=true. Elapsed: 6.014646501s
Sep  3 22:15:28.325: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Running", Reason="", readiness=true. Elapsed: 8.018644874s
Sep  3 22:15:30.329: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Running", Reason="", readiness=true. Elapsed: 10.022309039s
Sep  3 22:15:32.332: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Running", Reason="", readiness=true. Elapsed: 12.025833204s
Sep  3 22:15:34.336: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Running", Reason="", readiness=true. Elapsed: 14.029750911s
Sep  3 22:15:36.340: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Running", Reason="", readiness=true. Elapsed: 16.033878732s
Sep  3 22:15:38.345: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Running", Reason="", readiness=true. Elapsed: 18.038170969s
Sep  3 22:15:40.348: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Running", Reason="", readiness=true. Elapsed: 20.041930153s
Sep  3 22:15:42.353: INFO: Pod "pod-subpath-test-downwardapi-mc5x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.046490259s
STEP: Saw pod success
Sep  3 22:15:42.353: INFO: Pod "pod-subpath-test-downwardapi-mc5x" satisfied condition "Succeeded or Failed"
Sep  3 22:15:42.356: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-subpath-test-downwardapi-mc5x container test-container-subpath-downwardapi-mc5x: <nil>
STEP: delete the pod
Sep  3 22:15:42.379: INFO: Waiting for pod pod-subpath-test-downwardapi-mc5x to disappear
Sep  3 22:15:42.381: INFO: Pod pod-subpath-test-downwardapi-mc5x no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mc5x
Sep  3 22:15:42.381: INFO: Deleting pod "pod-subpath-test-downwardapi-mc5x" in namespace "subpath-9276"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:15:42.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9276" for this suite.

• [SLOW TEST:22.147 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":277,"completed":15,"skipped":224,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:15:42.395: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-520c05e6-5335-43a5-9396-3f11c57a17a6
STEP: Creating a pod to test consume secrets
Sep  3 22:15:42.474: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-becb711f-c2f0-4439-aa4a-0d31ee415a0c" in namespace "projected-3978" to be "Succeeded or Failed"
Sep  3 22:15:42.477: INFO: Pod "pod-projected-secrets-becb711f-c2f0-4439-aa4a-0d31ee415a0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596391ms
Sep  3 22:15:44.482: INFO: Pod "pod-projected-secrets-becb711f-c2f0-4439-aa4a-0d31ee415a0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007541728s
STEP: Saw pod success
Sep  3 22:15:44.482: INFO: Pod "pod-projected-secrets-becb711f-c2f0-4439-aa4a-0d31ee415a0c" satisfied condition "Succeeded or Failed"
Sep  3 22:15:44.485: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-secrets-becb711f-c2f0-4439-aa4a-0d31ee415a0c container secret-volume-test: <nil>
STEP: delete the pod
Sep  3 22:15:44.519: INFO: Waiting for pod pod-projected-secrets-becb711f-c2f0-4439-aa4a-0d31ee415a0c to disappear
Sep  3 22:15:44.522: INFO: Pod pod-projected-secrets-becb711f-c2f0-4439-aa4a-0d31ee415a0c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:15:44.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3978" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":16,"skipped":234,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:15:44.535: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Sep  3 22:15:44.576: INFO: namespace kubectl-6303
Sep  3 22:15:44.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-6303'
Sep  3 22:15:45.025: INFO: stderr: ""
Sep  3 22:15:45.026: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep  3 22:15:46.029: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:15:46.029: INFO: Found 0 / 1
Sep  3 22:15:47.030: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:15:47.030: INFO: Found 0 / 1
Sep  3 22:15:48.031: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:15:48.031: INFO: Found 1 / 1
Sep  3 22:15:48.031: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  3 22:15:48.033: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:15:48.033: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  3 22:15:48.033: INFO: wait on agnhost-master startup in kubectl-6303 
Sep  3 22:15:48.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 logs agnhost-master-xlgvb agnhost-master --namespace=kubectl-6303'
Sep  3 22:15:48.147: INFO: stderr: ""
Sep  3 22:15:48.147: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep  3 22:15:48.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6303'
Sep  3 22:15:48.258: INFO: stderr: ""
Sep  3 22:15:48.258: INFO: stdout: "service/rm2 exposed\n"
Sep  3 22:15:48.262: INFO: Service rm2 in namespace kubectl-6303 found.
STEP: exposing service
Sep  3 22:15:50.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6303'
Sep  3 22:15:50.375: INFO: stderr: ""
Sep  3 22:15:50.375: INFO: stdout: "service/rm3 exposed\n"
Sep  3 22:15:50.380: INFO: Service rm3 in namespace kubectl-6303 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:15:52.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6303" for this suite.

• [SLOW TEST:7.862 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1119
    should create services for rc  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":277,"completed":17,"skipped":235,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:15:52.397: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  3 22:15:56.480: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  3 22:15:56.483: INFO: Pod pod-with-prestop-http-hook still exists
Sep  3 22:15:58.483: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  3 22:15:58.490: INFO: Pod pod-with-prestop-http-hook still exists
Sep  3 22:16:00.483: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  3 22:16:00.488: INFO: Pod pod-with-prestop-http-hook still exists
Sep  3 22:16:02.483: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  3 22:16:02.488: INFO: Pod pod-with-prestop-http-hook still exists
Sep  3 22:16:04.483: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  3 22:16:04.488: INFO: Pod pod-with-prestop-http-hook still exists
Sep  3 22:16:06.483: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  3 22:16:06.487: INFO: Pod pod-with-prestop-http-hook still exists
Sep  3 22:16:08.483: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  3 22:16:08.486: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:16:08.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1803" for this suite.

• [SLOW TEST:16.106 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":277,"completed":18,"skipped":239,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:16:08.504: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:16:09.308: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:16:12.331: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Sep  3 22:16:12.358: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:16:12.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8392" for this suite.
STEP: Destroying namespace "webhook-8392-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":277,"completed":19,"skipped":293,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:16:12.622: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-eeed7d2e-1a42-488b-8b6b-a446d5ba34ca
STEP: Creating a pod to test consume secrets
Sep  3 22:16:12.679: INFO: Waiting up to 5m0s for pod "pod-secrets-14546deb-a4dc-48ef-9300-4911c8f8f54a" in namespace "secrets-9683" to be "Succeeded or Failed"
Sep  3 22:16:12.681: INFO: Pod "pod-secrets-14546deb-a4dc-48ef-9300-4911c8f8f54a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.854795ms
Sep  3 22:16:14.686: INFO: Pod "pod-secrets-14546deb-a4dc-48ef-9300-4911c8f8f54a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007253588s
Sep  3 22:16:16.691: INFO: Pod "pod-secrets-14546deb-a4dc-48ef-9300-4911c8f8f54a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01221081s
STEP: Saw pod success
Sep  3 22:16:16.691: INFO: Pod "pod-secrets-14546deb-a4dc-48ef-9300-4911c8f8f54a" satisfied condition "Succeeded or Failed"
Sep  3 22:16:16.694: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-secrets-14546deb-a4dc-48ef-9300-4911c8f8f54a container secret-volume-test: <nil>
STEP: delete the pod
Sep  3 22:16:16.721: INFO: Waiting for pod pod-secrets-14546deb-a4dc-48ef-9300-4911c8f8f54a to disappear
Sep  3 22:16:16.724: INFO: Pod pod-secrets-14546deb-a4dc-48ef-9300-4911c8f8f54a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:16:16.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9683" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":20,"skipped":334,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:16:16.735: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:16:16.765: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Creating first CR 
Sep  3 22:16:17.338: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-03T22:16:17Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-03T22:16:17Z]] name:name1 resourceVersion:6753 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:09612633-f204-4732-8280-2ddc1d3eff0e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep  3 22:16:27.344: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-03T22:16:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-03T22:16:27Z]] name:name2 resourceVersion:6813 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:177bd1cb-7338-42a9-99fd-1bc46c84283e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep  3 22:16:37.352: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-03T22:16:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-03T22:16:37Z]] name:name1 resourceVersion:6844 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:09612633-f204-4732-8280-2ddc1d3eff0e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep  3 22:16:47.359: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-03T22:16:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-03T22:16:47Z]] name:name2 resourceVersion:6876 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:177bd1cb-7338-42a9-99fd-1bc46c84283e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep  3 22:16:57.370: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-03T22:16:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-03T22:16:37Z]] name:name1 resourceVersion:6906 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:09612633-f204-4732-8280-2ddc1d3eff0e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep  3 22:17:07.380: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-03T22:16:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-03T22:16:47Z]] name:name2 resourceVersion:6936 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:177bd1cb-7338-42a9-99fd-1bc46c84283e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:17:17.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6034" for this suite.

• [SLOW TEST:61.170 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":277,"completed":21,"skipped":344,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:17:17.906: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3988.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3988.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3988.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3988.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  3 22:17:29.976: INFO: DNS probes using dns-test-39262eb2-3723-477e-a062-265f045e8744 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3988.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3988.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3988.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3988.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  3 22:17:34.035: INFO: DNS probes using dns-test-d467985d-a4cd-4487-90a9-66930f97b1b0 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3988.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3988.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3988.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3988.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  3 22:17:38.106: INFO: DNS probes using dns-test-df5f4a0d-159d-41f4-a6a1-22b62c46d2bf succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:17:38.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3988" for this suite.

• [SLOW TEST:20.244 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":277,"completed":22,"skipped":375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:17:38.151: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Sep  3 22:17:38.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-5599 -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  3 22:17:38.279: INFO: stderr: ""
Sep  3 22:17:38.279: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Sep  3 22:17:38.279: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  3 22:17:38.279: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5599" to be "running and ready, or succeeded"
Sep  3 22:17:38.282: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.953972ms
Sep  3 22:17:40.287: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007442548s
Sep  3 22:17:40.287: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  3 22:17:40.287: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep  3 22:17:40.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 logs logs-generator logs-generator --namespace=kubectl-5599'
Sep  3 22:17:40.383: INFO: stderr: ""
Sep  3 22:17:40.383: INFO: stdout: "I0903 22:17:39.309356       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/54pf 222\nI0903 22:17:39.509660       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/ffkl 502\nI0903 22:17:39.709532       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/qzcb 521\nI0903 22:17:39.909609       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/mwr 202\nI0903 22:17:40.109660       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/lmv 225\nI0903 22:17:40.309582       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/sqx 354\n"
STEP: limiting log lines
Sep  3 22:17:40.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 logs logs-generator logs-generator --namespace=kubectl-5599 --tail=1'
Sep  3 22:17:40.480: INFO: stderr: ""
Sep  3 22:17:40.480: INFO: stdout: "I0903 22:17:40.309582       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/sqx 354\n"
Sep  3 22:17:40.480: INFO: got output "I0903 22:17:40.309582       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/sqx 354\n"
STEP: limiting log bytes
Sep  3 22:17:40.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 logs logs-generator logs-generator --namespace=kubectl-5599 --limit-bytes=1'
Sep  3 22:17:40.569: INFO: stderr: ""
Sep  3 22:17:40.569: INFO: stdout: "I"
Sep  3 22:17:40.569: INFO: got output "I"
STEP: exposing timestamps
Sep  3 22:17:40.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 logs logs-generator logs-generator --namespace=kubectl-5599 --tail=1 --timestamps'
Sep  3 22:17:40.663: INFO: stderr: ""
Sep  3 22:17:40.663: INFO: stdout: "2020-09-03T22:17:40.509621093Z I0903 22:17:40.509471       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/rvs 460\n"
Sep  3 22:17:40.663: INFO: got output "2020-09-03T22:17:40.509621093Z I0903 22:17:40.509471       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/rvs 460\n"
STEP: restricting to a time range
Sep  3 22:17:43.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 logs logs-generator logs-generator --namespace=kubectl-5599 --since=1s'
Sep  3 22:17:43.257: INFO: stderr: ""
Sep  3 22:17:43.257: INFO: stdout: "I0903 22:17:42.309517       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/5lp 297\nI0903 22:17:42.509540       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/q5w 393\nI0903 22:17:42.709528       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/xjqz 372\nI0903 22:17:42.909532       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/r4l7 568\nI0903 22:17:43.109571       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/n5gd 277\n"
Sep  3 22:17:43.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 logs logs-generator logs-generator --namespace=kubectl-5599 --since=24h'
Sep  3 22:17:43.373: INFO: stderr: ""
Sep  3 22:17:43.373: INFO: stdout: "I0903 22:17:39.309356       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/54pf 222\nI0903 22:17:39.509660       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/ffkl 502\nI0903 22:17:39.709532       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/qzcb 521\nI0903 22:17:39.909609       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/mwr 202\nI0903 22:17:40.109660       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/lmv 225\nI0903 22:17:40.309582       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/sqx 354\nI0903 22:17:40.509471       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/rvs 460\nI0903 22:17:40.709506       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/fg7v 502\nI0903 22:17:40.909545       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/wtx 446\nI0903 22:17:41.109514       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/b7qj 599\nI0903 22:17:41.309492       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/lhw 320\nI0903 22:17:41.509514       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/8ls 460\nI0903 22:17:41.709504       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/5rr 495\nI0903 22:17:41.909531       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/j9w 268\nI0903 22:17:42.109519       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/68l 551\nI0903 22:17:42.309517       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/5lp 297\nI0903 22:17:42.509540       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/q5w 393\nI0903 22:17:42.709528       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/xjqz 372\nI0903 22:17:42.909532       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/r4l7 568\nI0903 22:17:43.109571       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/n5gd 277\nI0903 22:17:43.309574       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/nx9l 354\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Sep  3 22:17:43.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete pod logs-generator --namespace=kubectl-5599'
Sep  3 22:17:45.912: INFO: stderr: ""
Sep  3 22:17:45.912: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:17:45.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5599" for this suite.

• [SLOW TEST:7.770 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1284
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":277,"completed":23,"skipped":450,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:17:45.921: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep  3 22:17:45.952: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep  3 22:17:57.911: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:18:01.457: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:18:13.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9082" for this suite.

• [SLOW TEST:27.893 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":277,"completed":24,"skipped":471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:18:13.815: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-9536/secret-test-b4408fac-8375-43ae-ad5b-a516b4f99a36
STEP: Creating a pod to test consume secrets
Sep  3 22:18:13.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-225a0fc8-d366-4740-9c56-fb6d84e06d0d" in namespace "secrets-9536" to be "Succeeded or Failed"
Sep  3 22:18:13.870: INFO: Pod "pod-configmaps-225a0fc8-d366-4740-9c56-fb6d84e06d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.756632ms
Sep  3 22:18:15.874: INFO: Pod "pod-configmaps-225a0fc8-d366-4740-9c56-fb6d84e06d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007053596s
STEP: Saw pod success
Sep  3 22:18:15.874: INFO: Pod "pod-configmaps-225a0fc8-d366-4740-9c56-fb6d84e06d0d" satisfied condition "Succeeded or Failed"
Sep  3 22:18:15.876: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-configmaps-225a0fc8-d366-4740-9c56-fb6d84e06d0d container env-test: <nil>
STEP: delete the pod
Sep  3 22:18:15.899: INFO: Waiting for pod pod-configmaps-225a0fc8-d366-4740-9c56-fb6d84e06d0d to disappear
Sep  3 22:18:15.901: INFO: Pod pod-configmaps-225a0fc8-d366-4740-9c56-fb6d84e06d0d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:18:15.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9536" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":25,"skipped":496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:18:15.910: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep  3 22:18:17.004: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0903 22:18:17.004024      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:18:17.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-410" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":277,"completed":26,"skipped":526,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:18:17.017: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Sep  3 22:18:17.580: INFO: created pod pod-service-account-defaultsa
Sep  3 22:18:17.580: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  3 22:18:17.591: INFO: created pod pod-service-account-mountsa
Sep  3 22:18:17.592: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  3 22:18:17.602: INFO: created pod pod-service-account-nomountsa
Sep  3 22:18:17.602: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  3 22:18:17.610: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  3 22:18:17.610: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  3 22:18:17.620: INFO: created pod pod-service-account-mountsa-mountspec
Sep  3 22:18:17.620: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  3 22:18:17.630: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  3 22:18:17.630: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  3 22:18:17.647: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  3 22:18:17.647: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  3 22:18:17.658: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  3 22:18:17.658: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  3 22:18:17.670: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  3 22:18:17.670: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:18:17.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8275" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":277,"completed":27,"skipped":539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:18:17.688: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:18:17.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9181" for this suite.
STEP: Destroying namespace "nspatchtest-3142fc79-8476-43e5-9f66-d7475304ce02-3123" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":277,"completed":28,"skipped":573,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:18:17.794: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:18:24.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4118" for this suite.

• [SLOW TEST:7.064 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":277,"completed":29,"skipped":580,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:18:24.857: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:18:26.925: INFO: Waiting up to 5m0s for pod "client-envvars-ce6674d0-1711-4b21-a222-32f7d8cb03a1" in namespace "pods-5990" to be "Succeeded or Failed"
Sep  3 22:18:26.929: INFO: Pod "client-envvars-ce6674d0-1711-4b21-a222-32f7d8cb03a1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.598783ms
Sep  3 22:18:28.933: INFO: Pod "client-envvars-ce6674d0-1711-4b21-a222-32f7d8cb03a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00783512s
STEP: Saw pod success
Sep  3 22:18:28.933: INFO: Pod "client-envvars-ce6674d0-1711-4b21-a222-32f7d8cb03a1" satisfied condition "Succeeded or Failed"
Sep  3 22:18:28.937: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod client-envvars-ce6674d0-1711-4b21-a222-32f7d8cb03a1 container env3cont: <nil>
STEP: delete the pod
Sep  3 22:18:28.959: INFO: Waiting for pod client-envvars-ce6674d0-1711-4b21-a222-32f7d8cb03a1 to disappear
Sep  3 22:18:28.961: INFO: Pod client-envvars-ce6674d0-1711-4b21-a222-32f7d8cb03a1 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:18:28.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5990" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":277,"completed":30,"skipped":581,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:18:28.970: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-ks4m
STEP: Creating a pod to test atomic-volume-subpath
Sep  3 22:18:29.022: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ks4m" in namespace "subpath-4735" to be "Succeeded or Failed"
Sep  3 22:18:29.031: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Pending", Reason="", readiness=false. Elapsed: 9.005598ms
Sep  3 22:18:31.035: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Running", Reason="", readiness=true. Elapsed: 2.012715965s
Sep  3 22:18:33.039: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Running", Reason="", readiness=true. Elapsed: 4.017537705s
Sep  3 22:18:35.043: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Running", Reason="", readiness=true. Elapsed: 6.021150432s
Sep  3 22:18:37.047: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Running", Reason="", readiness=true. Elapsed: 8.025392804s
Sep  3 22:18:39.051: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Running", Reason="", readiness=true. Elapsed: 10.029519566s
Sep  3 22:18:41.056: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Running", Reason="", readiness=true. Elapsed: 12.033788829s
Sep  3 22:18:43.059: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Running", Reason="", readiness=true. Elapsed: 14.037482395s
Sep  3 22:18:45.063: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Running", Reason="", readiness=true. Elapsed: 16.040831299s
Sep  3 22:18:47.066: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Running", Reason="", readiness=true. Elapsed: 18.044376129s
Sep  3 22:18:49.070: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Running", Reason="", readiness=true. Elapsed: 20.048090055s
Sep  3 22:18:51.074: INFO: Pod "pod-subpath-test-secret-ks4m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.052442785s
STEP: Saw pod success
Sep  3 22:18:51.074: INFO: Pod "pod-subpath-test-secret-ks4m" satisfied condition "Succeeded or Failed"
Sep  3 22:18:51.077: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-subpath-test-secret-ks4m container test-container-subpath-secret-ks4m: <nil>
STEP: delete the pod
Sep  3 22:18:51.099: INFO: Waiting for pod pod-subpath-test-secret-ks4m to disappear
Sep  3 22:18:51.101: INFO: Pod pod-subpath-test-secret-ks4m no longer exists
STEP: Deleting pod pod-subpath-test-secret-ks4m
Sep  3 22:18:51.101: INFO: Deleting pod "pod-subpath-test-secret-ks4m" in namespace "subpath-4735"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:18:51.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4735" for this suite.

• [SLOW TEST:22.145 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":277,"completed":31,"skipped":582,"failed":0}
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:18:51.114: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:19:07.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-384" for this suite.

• [SLOW TEST:16.144 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":277,"completed":32,"skipped":582,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:19:07.259: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:19:07.293: INFO: Creating deployment "webserver-deployment"
Sep  3 22:19:07.300: INFO: Waiting for observed generation 1
Sep  3 22:19:09.306: INFO: Waiting for all required pods to come up
Sep  3 22:19:09.310: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep  3 22:19:21.318: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  3 22:19:21.324: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  3 22:19:21.332: INFO: Updating deployment webserver-deployment
Sep  3 22:19:21.332: INFO: Waiting for observed generation 2
Sep  3 22:19:23.342: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  3 22:19:23.345: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  3 22:19:23.347: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  3 22:19:23.353: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  3 22:19:23.353: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  3 22:19:23.355: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  3 22:19:23.359: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  3 22:19:23.359: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  3 22:19:23.369: INFO: Updating deployment webserver-deployment
Sep  3 22:19:23.369: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  3 22:19:23.378: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  3 22:19:23.383: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep  3 22:19:23.407: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5955 /apis/apps/v1/namespaces/deployment-5955/deployments/webserver-deployment 0526d088-1e83-461b-bde3-944e3a16db93 8168 3 2020-09-03 22:19:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001c6e2b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2020-09-03 22:19:21 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-09-03 22:19:23 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep  3 22:19:23.418: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-5955 /apis/apps/v1/namespaces/deployment-5955/replicasets/webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 8163 3 2020-09-03 22:19:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 0526d088-1e83-461b-bde3-944e3a16db93 0xc0043c40a7 0xc0043c40a8}] []  [{kube-controller-manager Update apps/v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 53 50 54 100 48 56 56 45 49 101 56 51 45 52 54 49 98 45 98 100 101 51 45 57 52 52 101 51 97 49 54 100 98 57 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0043c4128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 22:19:23.418: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  3 22:19:23.418: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-5955 /apis/apps/v1/namespaces/deployment-5955/replicasets/webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 8159 3 2020-09-03 22:19:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 0526d088-1e83-461b-bde3-944e3a16db93 0xc0043c4187 0xc0043c4188}] []  [{kube-controller-manager Update apps/v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 53 50 54 100 48 56 56 45 49 101 56 51 45 52 54 49 98 45 98 100 101 51 45 57 52 52 101 51 97 49 54 100 98 57 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0043c41f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep  3 22:19:23.456: INFO: Pod "webserver-deployment-6676bcd6d4-25mwb" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-25mwb webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-25mwb 45d3f3cc-a320-4c85-b8cd-9516d907e20d 8102 0 2020-09-03 22:19:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c4750 0xc0043c4751}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:,StartTime:2020-09-03 22:19:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.456: INFO: Pod "webserver-deployment-6676bcd6d4-4r5kp" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-4r5kp webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-4r5kp 7842f68b-1830-4a76-86ff-18f93f2c6de3 8190 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c48e7 0xc0043c48e8}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.456: INFO: Pod "webserver-deployment-6676bcd6d4-5qntg" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-5qntg webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-5qntg 3127797c-2982-4395-9611-6c3d4ccadb0e 8186 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c49f7 0xc0043c49f8}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.457: INFO: Pod "webserver-deployment-6676bcd6d4-8tjmr" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-8tjmr webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-8tjmr b91b3d57-83e2-42e8-9bf8-08989d0bc087 8181 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c4b20 0xc0043c4b21}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.457: INFO: Pod "webserver-deployment-6676bcd6d4-9lgt7" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-9lgt7 webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-9lgt7 86f86bf3-574a-4c6e-a226-5dafb7fb5722 8180 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c4c50 0xc0043c4c51}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.457: INFO: Pod "webserver-deployment-6676bcd6d4-q9krx" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-q9krx webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-q9krx 2e861da7-df04-4b3f-b962-5a2ab09985fe 8187 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c4d80 0xc0043c4d81}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.457: INFO: Pod "webserver-deployment-6676bcd6d4-qsx84" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-qsx84 webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-qsx84 bdcbbc8b-34ca-4b7d-8fbb-8c4d89734a61 8112 0 2020-09-03 22:19:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c4e97 0xc0043c4e98}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.112,PodIP:,StartTime:2020-09-03 22:19:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.458: INFO: Pod "webserver-deployment-6676bcd6d4-rc9mg" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-rc9mg webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-rc9mg 794a4b67-3d1a-40f4-a962-c9f454cee97c 8134 0 2020-09-03 22:19:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c5037 0xc0043c5038}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:,StartTime:2020-09-03 22:19:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.458: INFO: Pod "webserver-deployment-6676bcd6d4-rnw4w" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-rnw4w webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-rnw4w 5bd1eca2-f75e-404f-8925-cd4a07cd915b 8191 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c51e7 0xc0043c51e8}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.458: INFO: Pod "webserver-deployment-6676bcd6d4-sfbsv" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-sfbsv webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-sfbsv 7b46167f-d1d4-4b49-b6dc-fbc741fa3638 8105 0 2020-09-03 22:19:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c5307 0xc0043c5308}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.122,PodIP:,StartTime:2020-09-03 22:19:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.458: INFO: Pod "webserver-deployment-6676bcd6d4-x552d" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-x552d webserver-deployment-6676bcd6d4- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-6676bcd6d4-x552d a517f675-c3e7-4408-8d50-fed419afbda0 8132 0 2020-09-03 22:19:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e3f82766-566b-4596-b759-51ab05e14b73 0xc0043c54a7 0xc0043c54a8}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 102 56 50 55 54 54 45 53 54 54 98 45 52 53 57 54 45 98 55 53 57 45 53 49 97 98 48 53 101 49 52 98 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:,StartTime:2020-09-03 22:19:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.459: INFO: Pod "webserver-deployment-84855cf797-24b9v" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-24b9v webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-24b9v 9d25d107-3382-437c-89ee-3b02a7182e3f 8182 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc0043c5647 0xc0043c5648}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.459: INFO: Pod "webserver-deployment-84855cf797-26ggv" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-26ggv webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-26ggv b0c4af8e-5f15-4880-903c-721c31a301ce 8188 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc0043c5760 0xc0043c5761}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.459: INFO: Pod "webserver-deployment-84855cf797-2w7xt" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-2w7xt webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-2w7xt 6257b592-95e4-4d96-b220-9c00a0cbdf20 8050 0 2020-09-03 22:19:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc0043c5867 0xc0043c5868}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:17 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 50 46 53 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:172.20.2.55,StartTime:2020-09-03 22:19:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 22:19:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://59e0abd6c12f093834edb7b505fe2248795b9dd195c95a40c18ef206b894ec90,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.2.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.459: INFO: Pod "webserver-deployment-84855cf797-4wbtj" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-4wbtj webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-4wbtj e43330bc-2a7b-415c-9425-7fecfd4bd208 8044 0 2020-09-03 22:19:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc0043c5a00 0xc0043c5a01}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:17 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 52 46 50 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.112,PodIP:172.20.4.20,StartTime:2020-09-03 22:19:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 22:19:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9eb3e64f78deec881531a764fb2bbc42960375390edd9e3c3de31d5cff0597bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.4.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.460: INFO: Pod "webserver-deployment-84855cf797-4wwkz" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-4wwkz webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-4wwkz 368dc1dd-9248-4671-ba08-d50ed2a6cb2a 7996 0 2020-09-03 22:19:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc0043c5b90 0xc0043c5b91}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 50 46 53 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:172.20.2.52,StartTime:2020-09-03 22:19:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 22:19:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1b873473b03ee19f1d0302b5be779bfc8b692be3c205183f2bc2a114ee86e4b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.2.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.460: INFO: Pod "webserver-deployment-84855cf797-57s9p" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-57s9p webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-57s9p c104fd69-f19f-4ca8-81d7-0a3e3a8c3ecd 8193 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc0043c5d20 0xc0043c5d21}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.460: INFO: Pod "webserver-deployment-84855cf797-5nd2f" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-5nd2f webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-5nd2f c8841f26-8966-416b-b2b8-f8e3230f10b7 8185 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc0043c5e27 0xc0043c5e28}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.460: INFO: Pod "webserver-deployment-84855cf797-6p6xx" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-6p6xx webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-6p6xx a30cec85-f84d-499a-a2c8-02df3732f9bb 8023 0 2020-09-03 22:19:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc0043c5f40 0xc0043c5f41}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:16 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 52 46 49 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.112,PodIP:172.20.4.19,StartTime:2020-09-03 22:19:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 22:19:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e8691af463e115d9cdce2e9e140c14aff00e9558e9fc94ae5cc97b60af9a1aab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.4.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.461: INFO: Pod "webserver-deployment-84855cf797-9s9cs" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-9s9cs webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-9s9cs 8f4ceaf2-ada1-4375-8b3a-1555800f422a 8173 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc004416310 0xc004416311}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.461: INFO: Pod "webserver-deployment-84855cf797-k7tqf" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-k7tqf webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-k7tqf be3bfaf6-4207-4d48-a4ad-e7ea28563c6c 8005 0 2020-09-03 22:19:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc004416430 0xc004416431}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 52 46 49 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.112,PodIP:172.20.4.18,StartTime:2020-09-03 22:19:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 22:19:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://bcd24993058a08dcca42827d745d67526b6feca6880f37f187fc3f0a8b8e7975,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.4.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.461: INFO: Pod "webserver-deployment-84855cf797-l8nvw" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-l8nvw webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-l8nvw de071fdd-ae23-4ec1-ad41-07cd1f3af3d5 8192 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc0044165c0 0xc0044165c1}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.461: INFO: Pod "webserver-deployment-84855cf797-lfrrw" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-lfrrw webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-lfrrw a39b91de-7523-4964-8ed3-3a57d44e1746 7983 0 2020-09-03 22:19:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc0044166e0 0xc0044166e1}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 52 46 49 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.112,PodIP:172.20.4.17,StartTime:2020-09-03 22:19:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 22:19:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9b7e5f0cd7ca7f76e6bae32049db449b85e98b7ae0f3c298c5adbb7680930d04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.4.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.461: INFO: Pod "webserver-deployment-84855cf797-m5gzw" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-m5gzw webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-m5gzw 0d303177-2485-47a5-b618-6f49ca06da36 8035 0 2020-09-03 22:19:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc004416870 0xc004416871}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:16 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 50 46 53 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:172.20.2.54,StartTime:2020-09-03 22:19:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 22:19:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://cda8905866392039fce22a31a889b8b3d4a0e48f3d42a6878c59c369681f8d6d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.2.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.461: INFO: Pod "webserver-deployment-84855cf797-nsd2p" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-nsd2p webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-nsd2p fc391e7e-2704-4bf4-93c6-8e6ce1d0b600 8178 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc004416a00 0xc004416a01}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.462: INFO: Pod "webserver-deployment-84855cf797-sbvlg" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-sbvlg webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-sbvlg 1cb2899a-ac90-4128-9a85-2446b7bf6c5f 8183 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc004416b07 0xc004416b08}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.122,PodIP:,StartTime:2020-09-03 22:19:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.462: INFO: Pod "webserver-deployment-84855cf797-snqkm" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-snqkm webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-snqkm e01099ad-b85e-4388-af42-78d9a0e131f0 8177 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc004416c87 0xc004416c88}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.462: INFO: Pod "webserver-deployment-84855cf797-szs82" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-szs82 webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-szs82 3185e4a0-ff64-49e8-82ea-0eddb03eda04 8194 0 2020-09-03 22:19:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc004416d87 0xc004416d88}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:23 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:19:23.462: INFO: Pod "webserver-deployment-84855cf797-v2sb2" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-v2sb2 webserver-deployment-84855cf797- deployment-5955 /api/v1/namespaces/deployment-5955/pods/webserver-deployment-84855cf797-v2sb2 9da67f8c-7fc9-41ab-b843-64c71b9b67eb 8015 0 2020-09-03 22:19:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 c5826204-87d1-454b-ae36-c4a8132d9611 0xc004416e87 0xc004416e88}] []  [{kube-controller-manager Update v1 2020-09-03 22:19:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 53 56 50 54 50 48 52 45 56 55 100 49 45 52 53 52 98 45 97 101 51 54 45 99 52 97 56 49 51 50 100 57 54 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:19:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 50 46 53 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-krz7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-krz7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-krz7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:19:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:172.20.2.53,StartTime:2020-09-03 22:19:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 22:19:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://718b7d29ae9c3563b1f6e48b1cdf23c0e4a592bf2b02a0cfba2fc07eb08aa2bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.2.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:19:23.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5955" for this suite.

• [SLOW TEST:16.269 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":277,"completed":33,"skipped":594,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:19:23.528: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:19:23.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5307db86-7941-41e5-bf3f-608ea6775416" in namespace "downward-api-1858" to be "Succeeded or Failed"
Sep  3 22:19:23.611: INFO: Pod "downwardapi-volume-5307db86-7941-41e5-bf3f-608ea6775416": Phase="Pending", Reason="", readiness=false. Elapsed: 5.898744ms
Sep  3 22:19:25.615: INFO: Pod "downwardapi-volume-5307db86-7941-41e5-bf3f-608ea6775416": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010361108s
Sep  3 22:19:27.619: INFO: Pod "downwardapi-volume-5307db86-7941-41e5-bf3f-608ea6775416": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013846773s
STEP: Saw pod success
Sep  3 22:19:27.619: INFO: Pod "downwardapi-volume-5307db86-7941-41e5-bf3f-608ea6775416" satisfied condition "Succeeded or Failed"
Sep  3 22:19:27.622: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-5307db86-7941-41e5-bf3f-608ea6775416 container client-container: <nil>
STEP: delete the pod
Sep  3 22:19:27.685: INFO: Waiting for pod downwardapi-volume-5307db86-7941-41e5-bf3f-608ea6775416 to disappear
Sep  3 22:19:27.687: INFO: Pod downwardapi-volume-5307db86-7941-41e5-bf3f-608ea6775416 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:19:27.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1858" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":34,"skipped":597,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:19:27.698: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep  3 22:19:27.736: INFO: PodSpec: initContainers in spec.initContainers
Sep  3 22:20:13.977: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2cd04a11-056b-4b4d-8fb9-4fa6e7436e1e", GenerateName:"", Namespace:"init-container-1356", SelfLink:"/api/v1/namespaces/init-container-1356/pods/pod-init-2cd04a11-056b-4b4d-8fb9-4fa6e7436e1e", UID:"2bcdad70-5c1d-44fc-90e2-06ea1a1f64ab", ResourceVersion:"8781", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63734768367, loc:(*time.Location)(0x7b51220)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"736184582"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0028ef4a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0028ef4c0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0028ef4e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0028ef500)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-vwlb8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0031c2680), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vwlb8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vwlb8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vwlb8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001889d58), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"karbon-test1186mm-f1ba59-k8s-worker-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002343dc0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001889dd0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001889df0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001889df8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001889dfc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768367, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768367, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768367, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768367, loc:(*time.Location)(0x7b51220)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.45.43.119", PodIP:"172.20.2.68", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.20.2.68"}}, StartTime:(*v1.Time)(0xc0028ef520), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002343ea0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002343f10)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://376970973a66d5ed1f9e9bc7545a911e727eb36cf236dbf95cf9ddf10bf428b3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0028ef560), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0028ef540), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc001889e7f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:20:13.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1356" for this suite.

• [SLOW TEST:46.289 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":277,"completed":35,"skipped":608,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:20:13.988: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Sep  3 22:20:14.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-8549'
Sep  3 22:20:14.264: INFO: stderr: ""
Sep  3 22:20:14.264: INFO: stdout: "pod/pause created\n"
Sep  3 22:20:14.264: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  3 22:20:14.264: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8549" to be "running and ready"
Sep  3 22:20:14.269: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.458182ms
Sep  3 22:20:16.274: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.00996129s
Sep  3 22:20:16.274: INFO: Pod "pause" satisfied condition "running and ready"
Sep  3 22:20:16.274: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Sep  3 22:20:16.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 label pods pause testing-label=testing-label-value --namespace=kubectl-8549'
Sep  3 22:20:16.365: INFO: stderr: ""
Sep  3 22:20:16.365: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep  3 22:20:16.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pod pause -L testing-label --namespace=kubectl-8549'
Sep  3 22:20:16.445: INFO: stderr: ""
Sep  3 22:20:16.445: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep  3 22:20:16.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 label pods pause testing-label- --namespace=kubectl-8549'
Sep  3 22:20:16.541: INFO: stderr: ""
Sep  3 22:20:16.541: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep  3 22:20:16.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pod pause -L testing-label --namespace=kubectl-8549'
Sep  3 22:20:16.631: INFO: stderr: ""
Sep  3 22:20:16.631: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Sep  3 22:20:16.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete --grace-period=0 --force -f - --namespace=kubectl-8549'
Sep  3 22:20:16.724: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 22:20:16.724: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  3 22:20:16.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get rc,svc -l name=pause --no-headers --namespace=kubectl-8549'
Sep  3 22:20:16.833: INFO: stderr: "No resources found in kubectl-8549 namespace.\n"
Sep  3 22:20:16.833: INFO: stdout: ""
Sep  3 22:20:16.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -l name=pause --namespace=kubectl-8549 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  3 22:20:16.921: INFO: stderr: ""
Sep  3 22:20:16.921: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:20:16.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8549" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":277,"completed":36,"skipped":627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:20:16.937: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep  3 22:20:19.502: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8835 pod-service-account-3e43b9a2-14e6-4c62-9e86-9bf400c35330 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep  3 22:20:19.713: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8835 pod-service-account-3e43b9a2-14e6-4c62-9e86-9bf400c35330 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep  3 22:20:19.921: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8835 pod-service-account-3e43b9a2-14e6-4c62-9e86-9bf400c35330 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:20:20.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8835" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":277,"completed":37,"skipped":661,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:20:20.142: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-87d0b4ac-42f7-4196-9bed-457643a0925c in namespace container-probe-4065
Sep  3 22:20:22.190: INFO: Started pod liveness-87d0b4ac-42f7-4196-9bed-457643a0925c in namespace container-probe-4065
STEP: checking the pod's current state and verifying that restartCount is present
Sep  3 22:20:22.195: INFO: Initial restart count of pod liveness-87d0b4ac-42f7-4196-9bed-457643a0925c is 0
Sep  3 22:20:40.234: INFO: Restart count of pod container-probe-4065/liveness-87d0b4ac-42f7-4196-9bed-457643a0925c is now 1 (18.039257853s elapsed)
Sep  3 22:21:00.274: INFO: Restart count of pod container-probe-4065/liveness-87d0b4ac-42f7-4196-9bed-457643a0925c is now 2 (38.079081734s elapsed)
Sep  3 22:21:20.314: INFO: Restart count of pod container-probe-4065/liveness-87d0b4ac-42f7-4196-9bed-457643a0925c is now 3 (58.118394356s elapsed)
Sep  3 22:21:40.355: INFO: Restart count of pod container-probe-4065/liveness-87d0b4ac-42f7-4196-9bed-457643a0925c is now 4 (1m18.160109839s elapsed)
Sep  3 22:22:52.506: INFO: Restart count of pod container-probe-4065/liveness-87d0b4ac-42f7-4196-9bed-457643a0925c is now 5 (2m30.311195881s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:22:52.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4065" for this suite.

• [SLOW TEST:152.386 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":277,"completed":38,"skipped":682,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:22:52.528: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep  3 22:22:52.565: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:23:07.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7970" for this suite.

• [SLOW TEST:14.585 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":277,"completed":39,"skipped":695,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:23:07.114: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-3e20e48a-6f0f-41a7-bec3-4e6ba8aa084c in namespace container-probe-4909
Sep  3 22:23:09.163: INFO: Started pod test-webserver-3e20e48a-6f0f-41a7-bec3-4e6ba8aa084c in namespace container-probe-4909
STEP: checking the pod's current state and verifying that restartCount is present
Sep  3 22:23:09.167: INFO: Initial restart count of pod test-webserver-3e20e48a-6f0f-41a7-bec3-4e6ba8aa084c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:27:09.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4909" for this suite.

• [SLOW TEST:242.569 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":40,"skipped":762,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:27:09.683: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Sep  3 22:27:09.716: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-476854905 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:27:09.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3246" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":277,"completed":41,"skipped":767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:27:09.808: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-8cc60ee1-b8a1-42d2-bf74-6a48feb64958
STEP: Creating a pod to test consume configMaps
Sep  3 22:27:09.852: INFO: Waiting up to 5m0s for pod "pod-configmaps-6a7688e0-048d-48d6-9a13-fba9bf294f29" in namespace "configmap-398" to be "Succeeded or Failed"
Sep  3 22:27:09.855: INFO: Pod "pod-configmaps-6a7688e0-048d-48d6-9a13-fba9bf294f29": Phase="Pending", Reason="", readiness=false. Elapsed: 3.436741ms
Sep  3 22:27:11.859: INFO: Pod "pod-configmaps-6a7688e0-048d-48d6-9a13-fba9bf294f29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007075268s
STEP: Saw pod success
Sep  3 22:27:11.859: INFO: Pod "pod-configmaps-6a7688e0-048d-48d6-9a13-fba9bf294f29" satisfied condition "Succeeded or Failed"
Sep  3 22:27:11.862: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-configmaps-6a7688e0-048d-48d6-9a13-fba9bf294f29 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:27:11.890: INFO: Waiting for pod pod-configmaps-6a7688e0-048d-48d6-9a13-fba9bf294f29 to disappear
Sep  3 22:27:11.893: INFO: Pod pod-configmaps-6a7688e0-048d-48d6-9a13-fba9bf294f29 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:27:11.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-398" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":42,"skipped":801,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:27:11.904: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:27:13.159: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  3 22:27:15.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768833, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768833, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768833, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768833, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:27:18.186: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:27:18.189: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8881-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:27:19.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7806" for this suite.
STEP: Destroying namespace "webhook-7806-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.534 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":277,"completed":43,"skipped":811,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:27:19.438: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:27:30.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8633" for this suite.

• [SLOW TEST:11.088 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":277,"completed":44,"skipped":811,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:27:30.526: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:27:30.558: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  3 22:27:34.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2380 create -f -'
Sep  3 22:27:34.637: INFO: stderr: ""
Sep  3 22:27:34.637: INFO: stdout: "e2e-test-crd-publish-openapi-667-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  3 22:27:34.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2380 delete e2e-test-crd-publish-openapi-667-crds test-cr'
Sep  3 22:27:34.775: INFO: stderr: ""
Sep  3 22:27:34.775: INFO: stdout: "e2e-test-crd-publish-openapi-667-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  3 22:27:34.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2380 apply -f -'
Sep  3 22:27:35.005: INFO: stderr: ""
Sep  3 22:27:35.005: INFO: stdout: "e2e-test-crd-publish-openapi-667-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  3 22:27:35.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2380 delete e2e-test-crd-publish-openapi-667-crds test-cr'
Sep  3 22:27:35.091: INFO: stderr: ""
Sep  3 22:27:35.091: INFO: stdout: "e2e-test-crd-publish-openapi-667-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep  3 22:27:35.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 explain e2e-test-crd-publish-openapi-667-crds'
Sep  3 22:27:35.336: INFO: stderr: ""
Sep  3 22:27:35.336: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-667-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:27:38.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2380" for this suite.

• [SLOW TEST:8.401 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":277,"completed":45,"skipped":825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:27:38.928: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:27:38.966: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e759dc2d-90ec-4204-b7c9-4c2d81b5d255" in namespace "projected-4831" to be "Succeeded or Failed"
Sep  3 22:27:38.969: INFO: Pod "downwardapi-volume-e759dc2d-90ec-4204-b7c9-4c2d81b5d255": Phase="Pending", Reason="", readiness=false. Elapsed: 2.806809ms
Sep  3 22:27:40.973: INFO: Pod "downwardapi-volume-e759dc2d-90ec-4204-b7c9-4c2d81b5d255": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007393646s
STEP: Saw pod success
Sep  3 22:27:40.974: INFO: Pod "downwardapi-volume-e759dc2d-90ec-4204-b7c9-4c2d81b5d255" satisfied condition "Succeeded or Failed"
Sep  3 22:27:40.976: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-e759dc2d-90ec-4204-b7c9-4c2d81b5d255 container client-container: <nil>
STEP: delete the pod
Sep  3 22:27:41.000: INFO: Waiting for pod downwardapi-volume-e759dc2d-90ec-4204-b7c9-4c2d81b5d255 to disappear
Sep  3 22:27:41.004: INFO: Pod downwardapi-volume-e759dc2d-90ec-4204-b7c9-4c2d81b5d255 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:27:41.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4831" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":46,"skipped":859,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:27:41.016: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:27:41.047: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2115
I0903 22:27:41.059446      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2115, replica count: 1
I0903 22:27:42.109949      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0903 22:27:43.110261      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 22:27:43.223: INFO: Created: latency-svc-6h82h
Sep  3 22:27:43.231: INFO: Got endpoints: latency-svc-6h82h [21.053802ms]
Sep  3 22:27:43.245: INFO: Created: latency-svc-lvggd
Sep  3 22:27:43.253: INFO: Got endpoints: latency-svc-lvggd [21.348815ms]
Sep  3 22:27:43.261: INFO: Created: latency-svc-qj7tn
Sep  3 22:27:43.265: INFO: Created: latency-svc-dbxkn
Sep  3 22:27:43.270: INFO: Got endpoints: latency-svc-qj7tn [38.391342ms]
Sep  3 22:27:43.278: INFO: Got endpoints: latency-svc-dbxkn [45.514794ms]
Sep  3 22:27:43.281: INFO: Created: latency-svc-lfpxs
Sep  3 22:27:43.293: INFO: Got endpoints: latency-svc-lfpxs [60.913596ms]
Sep  3 22:27:43.297: INFO: Created: latency-svc-v48f6
Sep  3 22:27:43.305: INFO: Got endpoints: latency-svc-v48f6 [72.466124ms]
Sep  3 22:27:43.308: INFO: Created: latency-svc-4pf5n
Sep  3 22:27:43.317: INFO: Got endpoints: latency-svc-4pf5n [84.388277ms]
Sep  3 22:27:43.320: INFO: Created: latency-svc-hm452
Sep  3 22:27:43.327: INFO: Got endpoints: latency-svc-hm452 [94.056086ms]
Sep  3 22:27:43.342: INFO: Created: latency-svc-2slch
Sep  3 22:27:43.349: INFO: Got endpoints: latency-svc-2slch [116.500529ms]
Sep  3 22:27:43.351: INFO: Created: latency-svc-zs2t8
Sep  3 22:27:43.360: INFO: Got endpoints: latency-svc-zs2t8 [127.170048ms]
Sep  3 22:27:43.363: INFO: Created: latency-svc-zdwm2
Sep  3 22:27:43.375: INFO: Got endpoints: latency-svc-zdwm2 [141.891524ms]
Sep  3 22:27:43.377: INFO: Created: latency-svc-jhpp2
Sep  3 22:27:43.385: INFO: Got endpoints: latency-svc-jhpp2 [152.522023ms]
Sep  3 22:27:43.387: INFO: Created: latency-svc-8v788
Sep  3 22:27:43.396: INFO: Got endpoints: latency-svc-8v788 [163.318554ms]
Sep  3 22:27:43.399: INFO: Created: latency-svc-ddjqt
Sep  3 22:27:43.406: INFO: Got endpoints: latency-svc-ddjqt [172.773544ms]
Sep  3 22:27:43.409: INFO: Created: latency-svc-gc9fd
Sep  3 22:27:43.416: INFO: Got endpoints: latency-svc-gc9fd [182.410586ms]
Sep  3 22:27:43.421: INFO: Created: latency-svc-frwvx
Sep  3 22:27:43.432: INFO: Got endpoints: latency-svc-frwvx [199.398385ms]
Sep  3 22:27:43.439: INFO: Created: latency-svc-4mp2h
Sep  3 22:27:43.446: INFO: Got endpoints: latency-svc-4mp2h [193.299755ms]
Sep  3 22:27:43.451: INFO: Created: latency-svc-wb76x
Sep  3 22:27:43.460: INFO: Created: latency-svc-d6kqc
Sep  3 22:27:43.460: INFO: Got endpoints: latency-svc-wb76x [190.443658ms]
Sep  3 22:27:43.471: INFO: Got endpoints: latency-svc-d6kqc [193.716691ms]
Sep  3 22:27:43.472: INFO: Created: latency-svc-7q4g5
Sep  3 22:27:43.481: INFO: Got endpoints: latency-svc-7q4g5 [188.33542ms]
Sep  3 22:27:43.484: INFO: Created: latency-svc-r8f5j
Sep  3 22:27:43.492: INFO: Got endpoints: latency-svc-r8f5j [186.637157ms]
Sep  3 22:27:43.493: INFO: Created: latency-svc-h2bxr
Sep  3 22:27:43.500: INFO: Got endpoints: latency-svc-h2bxr [183.594537ms]
Sep  3 22:27:43.504: INFO: Created: latency-svc-g27rv
Sep  3 22:27:43.511: INFO: Got endpoints: latency-svc-g27rv [184.319268ms]
Sep  3 22:27:43.519: INFO: Created: latency-svc-fkcsg
Sep  3 22:27:43.529: INFO: Got endpoints: latency-svc-fkcsg [179.49815ms]
Sep  3 22:27:43.534: INFO: Created: latency-svc-bzlgr
Sep  3 22:27:43.547: INFO: Got endpoints: latency-svc-bzlgr [186.285661ms]
Sep  3 22:27:43.550: INFO: Created: latency-svc-nkppr
Sep  3 22:27:43.557: INFO: Got endpoints: latency-svc-nkppr [181.870264ms]
Sep  3 22:27:43.564: INFO: Created: latency-svc-56tk9
Sep  3 22:27:43.570: INFO: Got endpoints: latency-svc-56tk9 [184.246158ms]
Sep  3 22:27:43.573: INFO: Created: latency-svc-2g4jh
Sep  3 22:27:43.580: INFO: Got endpoints: latency-svc-2g4jh [183.779071ms]
Sep  3 22:27:43.583: INFO: Created: latency-svc-bq28x
Sep  3 22:27:43.590: INFO: Got endpoints: latency-svc-bq28x [184.376213ms]
Sep  3 22:27:43.597: INFO: Created: latency-svc-gxw27
Sep  3 22:27:43.602: INFO: Got endpoints: latency-svc-gxw27 [185.889255ms]
Sep  3 22:27:43.604: INFO: Created: latency-svc-bvxlk
Sep  3 22:27:43.614: INFO: Got endpoints: latency-svc-bvxlk [181.24494ms]
Sep  3 22:27:43.616: INFO: Created: latency-svc-xbq2s
Sep  3 22:27:43.627: INFO: Got endpoints: latency-svc-xbq2s [180.29786ms]
Sep  3 22:27:43.629: INFO: Created: latency-svc-86rtm
Sep  3 22:27:43.639: INFO: Got endpoints: latency-svc-86rtm [179.122921ms]
Sep  3 22:27:43.644: INFO: Created: latency-svc-h6cn4
Sep  3 22:27:43.654: INFO: Got endpoints: latency-svc-h6cn4 [182.20811ms]
Sep  3 22:27:43.655: INFO: Created: latency-svc-wgbf9
Sep  3 22:27:43.659: INFO: Got endpoints: latency-svc-wgbf9 [177.887109ms]
Sep  3 22:27:43.664: INFO: Created: latency-svc-m7jtf
Sep  3 22:27:43.671: INFO: Got endpoints: latency-svc-m7jtf [178.958598ms]
Sep  3 22:27:43.673: INFO: Created: latency-svc-ctrjd
Sep  3 22:27:43.682: INFO: Got endpoints: latency-svc-ctrjd [182.029083ms]
Sep  3 22:27:43.685: INFO: Created: latency-svc-7jlzt
Sep  3 22:27:43.699: INFO: Got endpoints: latency-svc-7jlzt [187.280209ms]
Sep  3 22:27:43.703: INFO: Created: latency-svc-pxvzt
Sep  3 22:27:43.708: INFO: Got endpoints: latency-svc-pxvzt [179.248804ms]
Sep  3 22:27:43.711: INFO: Created: latency-svc-lbfsx
Sep  3 22:27:43.722: INFO: Created: latency-svc-mlhr5
Sep  3 22:27:43.731: INFO: Got endpoints: latency-svc-lbfsx [184.240847ms]
Sep  3 22:27:43.735: INFO: Created: latency-svc-lvlc6
Sep  3 22:27:43.743: INFO: Created: latency-svc-xtbv7
Sep  3 22:27:43.754: INFO: Created: latency-svc-vn8bt
Sep  3 22:27:43.762: INFO: Created: latency-svc-44jxb
Sep  3 22:27:43.772: INFO: Created: latency-svc-67t29
Sep  3 22:27:43.781: INFO: Got endpoints: latency-svc-mlhr5 [223.904533ms]
Sep  3 22:27:43.783: INFO: Created: latency-svc-rkcp9
Sep  3 22:27:43.795: INFO: Created: latency-svc-922dl
Sep  3 22:27:43.802: INFO: Created: latency-svc-sck8l
Sep  3 22:27:43.810: INFO: Created: latency-svc-p54qp
Sep  3 22:27:43.823: INFO: Created: latency-svc-xpql2
Sep  3 22:27:43.834: INFO: Got endpoints: latency-svc-lvlc6 [264.599419ms]
Sep  3 22:27:43.837: INFO: Created: latency-svc-krbl2
Sep  3 22:27:43.859: INFO: Created: latency-svc-trw5w
Sep  3 22:27:43.868: INFO: Created: latency-svc-dt92l
Sep  3 22:27:43.877: INFO: Created: latency-svc-m2mt7
Sep  3 22:27:43.884: INFO: Got endpoints: latency-svc-xtbv7 [304.017006ms]
Sep  3 22:27:43.887: INFO: Created: latency-svc-h5p84
Sep  3 22:27:43.897: INFO: Created: latency-svc-2w95g
Sep  3 22:27:43.906: INFO: Created: latency-svc-2n4w7
Sep  3 22:27:43.929: INFO: Got endpoints: latency-svc-vn8bt [338.873577ms]
Sep  3 22:27:43.945: INFO: Created: latency-svc-bqr52
Sep  3 22:27:43.981: INFO: Got endpoints: latency-svc-44jxb [379.784901ms]
Sep  3 22:27:43.993: INFO: Created: latency-svc-n967h
Sep  3 22:27:44.035: INFO: Got endpoints: latency-svc-67t29 [421.21875ms]
Sep  3 22:27:44.056: INFO: Created: latency-svc-tch2s
Sep  3 22:27:44.079: INFO: Got endpoints: latency-svc-rkcp9 [452.143433ms]
Sep  3 22:27:44.091: INFO: Created: latency-svc-xs465
Sep  3 22:27:44.132: INFO: Got endpoints: latency-svc-922dl [492.551793ms]
Sep  3 22:27:44.146: INFO: Created: latency-svc-vmrlh
Sep  3 22:27:44.181: INFO: Got endpoints: latency-svc-sck8l [527.183518ms]
Sep  3 22:27:44.193: INFO: Created: latency-svc-2xzlr
Sep  3 22:27:44.230: INFO: Got endpoints: latency-svc-p54qp [571.261364ms]
Sep  3 22:27:44.244: INFO: Created: latency-svc-n2cd7
Sep  3 22:27:44.281: INFO: Got endpoints: latency-svc-xpql2 [610.740234ms]
Sep  3 22:27:44.295: INFO: Created: latency-svc-t4jmf
Sep  3 22:27:44.338: INFO: Got endpoints: latency-svc-krbl2 [655.526394ms]
Sep  3 22:27:44.353: INFO: Created: latency-svc-l88s9
Sep  3 22:27:44.381: INFO: Got endpoints: latency-svc-trw5w [682.217067ms]
Sep  3 22:27:44.392: INFO: Created: latency-svc-tnx7b
Sep  3 22:27:44.433: INFO: Got endpoints: latency-svc-dt92l [725.331149ms]
Sep  3 22:27:44.446: INFO: Created: latency-svc-wxs8b
Sep  3 22:27:44.481: INFO: Got endpoints: latency-svc-m2mt7 [749.573812ms]
Sep  3 22:27:44.493: INFO: Created: latency-svc-dzr5v
Sep  3 22:27:44.529: INFO: Got endpoints: latency-svc-h5p84 [747.56178ms]
Sep  3 22:27:44.543: INFO: Created: latency-svc-qjr4t
Sep  3 22:27:44.582: INFO: Got endpoints: latency-svc-2w95g [747.17037ms]
Sep  3 22:27:44.594: INFO: Created: latency-svc-pr5v8
Sep  3 22:27:44.630: INFO: Got endpoints: latency-svc-2n4w7 [745.646674ms]
Sep  3 22:27:44.641: INFO: Created: latency-svc-jjznn
Sep  3 22:27:44.681: INFO: Got endpoints: latency-svc-bqr52 [751.898686ms]
Sep  3 22:27:44.694: INFO: Created: latency-svc-fxcw4
Sep  3 22:27:44.731: INFO: Got endpoints: latency-svc-n967h [749.442482ms]
Sep  3 22:27:44.748: INFO: Created: latency-svc-tn94d
Sep  3 22:27:44.781: INFO: Got endpoints: latency-svc-tch2s [746.030247ms]
Sep  3 22:27:44.793: INFO: Created: latency-svc-j42pq
Sep  3 22:27:44.832: INFO: Got endpoints: latency-svc-xs465 [752.452715ms]
Sep  3 22:27:44.845: INFO: Created: latency-svc-t2m2r
Sep  3 22:27:44.882: INFO: Got endpoints: latency-svc-vmrlh [749.586426ms]
Sep  3 22:27:44.897: INFO: Created: latency-svc-tqsdl
Sep  3 22:27:44.929: INFO: Got endpoints: latency-svc-2xzlr [748.544156ms]
Sep  3 22:27:44.941: INFO: Created: latency-svc-5rtkf
Sep  3 22:27:44.982: INFO: Got endpoints: latency-svc-n2cd7 [751.730386ms]
Sep  3 22:27:44.995: INFO: Created: latency-svc-rz5lh
Sep  3 22:27:45.031: INFO: Got endpoints: latency-svc-t4jmf [749.926462ms]
Sep  3 22:27:45.046: INFO: Created: latency-svc-jq86h
Sep  3 22:27:45.079: INFO: Got endpoints: latency-svc-l88s9 [741.250283ms]
Sep  3 22:27:45.091: INFO: Created: latency-svc-6db2p
Sep  3 22:27:45.130: INFO: Got endpoints: latency-svc-tnx7b [749.496853ms]
Sep  3 22:27:45.142: INFO: Created: latency-svc-rxg9b
Sep  3 22:27:45.181: INFO: Got endpoints: latency-svc-wxs8b [747.885513ms]
Sep  3 22:27:45.194: INFO: Created: latency-svc-96wcn
Sep  3 22:27:45.233: INFO: Got endpoints: latency-svc-dzr5v [752.483105ms]
Sep  3 22:27:45.245: INFO: Created: latency-svc-6kc2k
Sep  3 22:27:45.281: INFO: Got endpoints: latency-svc-qjr4t [752.520161ms]
Sep  3 22:27:45.313: INFO: Created: latency-svc-p9jqw
Sep  3 22:27:45.329: INFO: Got endpoints: latency-svc-pr5v8 [747.644922ms]
Sep  3 22:27:45.345: INFO: Created: latency-svc-vq2pv
Sep  3 22:27:45.381: INFO: Got endpoints: latency-svc-jjznn [751.362279ms]
Sep  3 22:27:45.392: INFO: Created: latency-svc-lmsgc
Sep  3 22:27:45.432: INFO: Got endpoints: latency-svc-fxcw4 [750.941465ms]
Sep  3 22:27:45.447: INFO: Created: latency-svc-8vl64
Sep  3 22:27:45.482: INFO: Got endpoints: latency-svc-tn94d [750.779846ms]
Sep  3 22:27:45.497: INFO: Created: latency-svc-hc2wh
Sep  3 22:27:45.531: INFO: Got endpoints: latency-svc-j42pq [749.595681ms]
Sep  3 22:27:45.544: INFO: Created: latency-svc-kmngm
Sep  3 22:27:45.579: INFO: Got endpoints: latency-svc-t2m2r [747.625658ms]
Sep  3 22:27:45.592: INFO: Created: latency-svc-8bw2b
Sep  3 22:27:45.633: INFO: Got endpoints: latency-svc-tqsdl [751.638567ms]
Sep  3 22:27:45.647: INFO: Created: latency-svc-flf8m
Sep  3 22:27:45.680: INFO: Got endpoints: latency-svc-5rtkf [750.910221ms]
Sep  3 22:27:45.693: INFO: Created: latency-svc-pjw67
Sep  3 22:27:45.734: INFO: Got endpoints: latency-svc-rz5lh [751.325533ms]
Sep  3 22:27:45.747: INFO: Created: latency-svc-h6s5v
Sep  3 22:27:45.782: INFO: Got endpoints: latency-svc-jq86h [750.366719ms]
Sep  3 22:27:45.794: INFO: Created: latency-svc-gb4hb
Sep  3 22:27:45.829: INFO: Got endpoints: latency-svc-6db2p [750.037926ms]
Sep  3 22:27:45.845: INFO: Created: latency-svc-crwk5
Sep  3 22:27:45.882: INFO: Got endpoints: latency-svc-rxg9b [751.50517ms]
Sep  3 22:27:45.893: INFO: Created: latency-svc-q6nnk
Sep  3 22:27:45.934: INFO: Got endpoints: latency-svc-96wcn [752.755563ms]
Sep  3 22:27:45.949: INFO: Created: latency-svc-kw7hl
Sep  3 22:27:45.980: INFO: Got endpoints: latency-svc-6kc2k [747.1243ms]
Sep  3 22:27:45.991: INFO: Created: latency-svc-jt8tp
Sep  3 22:27:46.046: INFO: Got endpoints: latency-svc-p9jqw [764.239606ms]
Sep  3 22:27:46.059: INFO: Created: latency-svc-b4zz2
Sep  3 22:27:46.083: INFO: Got endpoints: latency-svc-vq2pv [753.097887ms]
Sep  3 22:27:46.095: INFO: Created: latency-svc-gbfqs
Sep  3 22:27:46.134: INFO: Got endpoints: latency-svc-lmsgc [753.186831ms]
Sep  3 22:27:46.150: INFO: Created: latency-svc-hwhtj
Sep  3 22:27:46.182: INFO: Got endpoints: latency-svc-8vl64 [749.620596ms]
Sep  3 22:27:46.194: INFO: Created: latency-svc-frfm7
Sep  3 22:27:46.230: INFO: Got endpoints: latency-svc-hc2wh [748.621479ms]
Sep  3 22:27:46.245: INFO: Created: latency-svc-mggmk
Sep  3 22:27:46.283: INFO: Got endpoints: latency-svc-kmngm [752.654293ms]
Sep  3 22:27:46.301: INFO: Created: latency-svc-8cqbj
Sep  3 22:27:46.335: INFO: Got endpoints: latency-svc-8bw2b [755.343919ms]
Sep  3 22:27:46.350: INFO: Created: latency-svc-c95g9
Sep  3 22:27:46.384: INFO: Got endpoints: latency-svc-flf8m [750.970306ms]
Sep  3 22:27:46.400: INFO: Created: latency-svc-bb9mt
Sep  3 22:27:46.432: INFO: Got endpoints: latency-svc-pjw67 [751.180774ms]
Sep  3 22:27:46.446: INFO: Created: latency-svc-84p54
Sep  3 22:27:46.481: INFO: Got endpoints: latency-svc-h6s5v [747.429912ms]
Sep  3 22:27:46.493: INFO: Created: latency-svc-hdnz8
Sep  3 22:27:46.533: INFO: Got endpoints: latency-svc-gb4hb [750.864283ms]
Sep  3 22:27:46.546: INFO: Created: latency-svc-qnwk4
Sep  3 22:27:46.581: INFO: Got endpoints: latency-svc-crwk5 [752.046554ms]
Sep  3 22:27:46.595: INFO: Created: latency-svc-ww5mg
Sep  3 22:27:46.632: INFO: Got endpoints: latency-svc-q6nnk [749.594884ms]
Sep  3 22:27:46.646: INFO: Created: latency-svc-swrfx
Sep  3 22:27:46.682: INFO: Got endpoints: latency-svc-kw7hl [747.563467ms]
Sep  3 22:27:46.693: INFO: Created: latency-svc-dwjm6
Sep  3 22:27:46.731: INFO: Got endpoints: latency-svc-jt8tp [750.588082ms]
Sep  3 22:27:46.745: INFO: Created: latency-svc-v258n
Sep  3 22:27:46.782: INFO: Got endpoints: latency-svc-b4zz2 [735.813021ms]
Sep  3 22:27:46.795: INFO: Created: latency-svc-bfvlb
Sep  3 22:27:46.830: INFO: Got endpoints: latency-svc-gbfqs [747.344922ms]
Sep  3 22:27:46.844: INFO: Created: latency-svc-jncmq
Sep  3 22:27:46.882: INFO: Got endpoints: latency-svc-hwhtj [747.540801ms]
Sep  3 22:27:46.894: INFO: Created: latency-svc-sf8hb
Sep  3 22:27:46.930: INFO: Got endpoints: latency-svc-frfm7 [748.585389ms]
Sep  3 22:27:46.946: INFO: Created: latency-svc-b8krd
Sep  3 22:27:46.982: INFO: Got endpoints: latency-svc-mggmk [751.731553ms]
Sep  3 22:27:46.995: INFO: Created: latency-svc-t49zs
Sep  3 22:27:47.032: INFO: Got endpoints: latency-svc-8cqbj [748.186382ms]
Sep  3 22:27:47.050: INFO: Created: latency-svc-nzfsd
Sep  3 22:27:47.081: INFO: Got endpoints: latency-svc-c95g9 [745.986387ms]
Sep  3 22:27:47.092: INFO: Created: latency-svc-km8ts
Sep  3 22:27:47.130: INFO: Got endpoints: latency-svc-bb9mt [745.64767ms]
Sep  3 22:27:47.143: INFO: Created: latency-svc-qwqjh
Sep  3 22:27:47.180: INFO: Got endpoints: latency-svc-84p54 [748.517583ms]
Sep  3 22:27:47.192: INFO: Created: latency-svc-f7gn5
Sep  3 22:27:47.230: INFO: Got endpoints: latency-svc-hdnz8 [748.673636ms]
Sep  3 22:27:47.243: INFO: Created: latency-svc-x2s78
Sep  3 22:27:47.281: INFO: Got endpoints: latency-svc-qnwk4 [748.146977ms]
Sep  3 22:27:47.293: INFO: Created: latency-svc-kqzhr
Sep  3 22:27:47.331: INFO: Got endpoints: latency-svc-ww5mg [749.720895ms]
Sep  3 22:27:47.343: INFO: Created: latency-svc-bvzz4
Sep  3 22:27:47.380: INFO: Got endpoints: latency-svc-swrfx [748.127103ms]
Sep  3 22:27:47.393: INFO: Created: latency-svc-6pvxr
Sep  3 22:27:47.435: INFO: Got endpoints: latency-svc-dwjm6 [753.184909ms]
Sep  3 22:27:47.449: INFO: Created: latency-svc-z6qmn
Sep  3 22:27:47.479: INFO: Got endpoints: latency-svc-v258n [748.120184ms]
Sep  3 22:27:47.492: INFO: Created: latency-svc-2glzq
Sep  3 22:27:47.531: INFO: Got endpoints: latency-svc-bfvlb [748.957235ms]
Sep  3 22:27:47.545: INFO: Created: latency-svc-428qh
Sep  3 22:27:47.580: INFO: Got endpoints: latency-svc-jncmq [749.852446ms]
Sep  3 22:27:47.595: INFO: Created: latency-svc-lmf8k
Sep  3 22:27:47.632: INFO: Got endpoints: latency-svc-sf8hb [750.595291ms]
Sep  3 22:27:47.646: INFO: Created: latency-svc-lrpmg
Sep  3 22:27:47.683: INFO: Got endpoints: latency-svc-b8krd [752.567228ms]
Sep  3 22:27:47.694: INFO: Created: latency-svc-ggfg4
Sep  3 22:27:47.736: INFO: Got endpoints: latency-svc-t49zs [753.47547ms]
Sep  3 22:27:47.751: INFO: Created: latency-svc-xp8mp
Sep  3 22:27:47.782: INFO: Got endpoints: latency-svc-nzfsd [749.791173ms]
Sep  3 22:27:47.798: INFO: Created: latency-svc-jb6nm
Sep  3 22:27:47.829: INFO: Got endpoints: latency-svc-km8ts [748.556941ms]
Sep  3 22:27:47.842: INFO: Created: latency-svc-7krv5
Sep  3 22:27:47.881: INFO: Got endpoints: latency-svc-qwqjh [750.815187ms]
Sep  3 22:27:47.893: INFO: Created: latency-svc-p5fn5
Sep  3 22:27:47.930: INFO: Got endpoints: latency-svc-f7gn5 [749.947249ms]
Sep  3 22:27:47.942: INFO: Created: latency-svc-z6wsw
Sep  3 22:27:47.982: INFO: Got endpoints: latency-svc-x2s78 [751.972007ms]
Sep  3 22:27:47.993: INFO: Created: latency-svc-vhhww
Sep  3 22:27:48.031: INFO: Got endpoints: latency-svc-kqzhr [750.545834ms]
Sep  3 22:27:48.045: INFO: Created: latency-svc-g2nwb
Sep  3 22:27:48.080: INFO: Got endpoints: latency-svc-bvzz4 [748.508306ms]
Sep  3 22:27:48.094: INFO: Created: latency-svc-b2mdg
Sep  3 22:27:48.129: INFO: Got endpoints: latency-svc-6pvxr [748.906383ms]
Sep  3 22:27:48.142: INFO: Created: latency-svc-v8mbl
Sep  3 22:27:48.181: INFO: Got endpoints: latency-svc-z6qmn [746.255742ms]
Sep  3 22:27:48.198: INFO: Created: latency-svc-ccktp
Sep  3 22:27:48.233: INFO: Got endpoints: latency-svc-2glzq [753.270188ms]
Sep  3 22:27:48.248: INFO: Created: latency-svc-k2z6f
Sep  3 22:27:48.283: INFO: Got endpoints: latency-svc-428qh [752.367577ms]
Sep  3 22:27:48.295: INFO: Created: latency-svc-mw97m
Sep  3 22:27:48.361: INFO: Got endpoints: latency-svc-lmf8k [781.230033ms]
Sep  3 22:27:48.384: INFO: Got endpoints: latency-svc-lrpmg [751.300439ms]
Sep  3 22:27:48.386: INFO: Created: latency-svc-2fpb5
Sep  3 22:27:48.399: INFO: Created: latency-svc-mcm7r
Sep  3 22:27:48.431: INFO: Got endpoints: latency-svc-ggfg4 [747.802511ms]
Sep  3 22:27:48.443: INFO: Created: latency-svc-chvqc
Sep  3 22:27:48.482: INFO: Got endpoints: latency-svc-xp8mp [745.837378ms]
Sep  3 22:27:48.495: INFO: Created: latency-svc-48qq5
Sep  3 22:27:48.531: INFO: Got endpoints: latency-svc-jb6nm [749.478551ms]
Sep  3 22:27:48.543: INFO: Created: latency-svc-vmhqt
Sep  3 22:27:48.580: INFO: Got endpoints: latency-svc-7krv5 [750.999377ms]
Sep  3 22:27:48.592: INFO: Created: latency-svc-dzr8d
Sep  3 22:27:48.634: INFO: Got endpoints: latency-svc-p5fn5 [752.810032ms]
Sep  3 22:27:48.647: INFO: Created: latency-svc-5bw96
Sep  3 22:27:48.680: INFO: Got endpoints: latency-svc-z6wsw [749.897901ms]
Sep  3 22:27:48.694: INFO: Created: latency-svc-d4lch
Sep  3 22:27:48.729: INFO: Got endpoints: latency-svc-vhhww [747.275755ms]
Sep  3 22:27:48.742: INFO: Created: latency-svc-8c4l8
Sep  3 22:27:48.780: INFO: Got endpoints: latency-svc-g2nwb [748.590261ms]
Sep  3 22:27:48.792: INFO: Created: latency-svc-wr7rh
Sep  3 22:27:48.832: INFO: Got endpoints: latency-svc-b2mdg [752.070732ms]
Sep  3 22:27:48.844: INFO: Created: latency-svc-mr629
Sep  3 22:27:48.881: INFO: Got endpoints: latency-svc-v8mbl [752.07921ms]
Sep  3 22:27:48.893: INFO: Created: latency-svc-dlkkv
Sep  3 22:27:48.929: INFO: Got endpoints: latency-svc-ccktp [747.553817ms]
Sep  3 22:27:48.944: INFO: Created: latency-svc-6crsv
Sep  3 22:27:48.982: INFO: Got endpoints: latency-svc-k2z6f [749.167161ms]
Sep  3 22:27:48.993: INFO: Created: latency-svc-b7th5
Sep  3 22:27:49.033: INFO: Got endpoints: latency-svc-mw97m [750.048441ms]
Sep  3 22:27:49.048: INFO: Created: latency-svc-blq8s
Sep  3 22:27:49.082: INFO: Got endpoints: latency-svc-2fpb5 [720.62114ms]
Sep  3 22:27:49.094: INFO: Created: latency-svc-4rpsp
Sep  3 22:27:49.130: INFO: Got endpoints: latency-svc-mcm7r [746.292548ms]
Sep  3 22:27:49.146: INFO: Created: latency-svc-2qgtk
Sep  3 22:27:49.179: INFO: Got endpoints: latency-svc-chvqc [747.670334ms]
Sep  3 22:27:49.191: INFO: Created: latency-svc-k7b9j
Sep  3 22:27:49.231: INFO: Got endpoints: latency-svc-48qq5 [749.391423ms]
Sep  3 22:27:49.242: INFO: Created: latency-svc-q6cd2
Sep  3 22:27:49.282: INFO: Got endpoints: latency-svc-vmhqt [750.558802ms]
Sep  3 22:27:49.295: INFO: Created: latency-svc-5dsmk
Sep  3 22:27:49.335: INFO: Got endpoints: latency-svc-dzr8d [754.801884ms]
Sep  3 22:27:49.347: INFO: Created: latency-svc-4wtlt
Sep  3 22:27:49.381: INFO: Got endpoints: latency-svc-5bw96 [747.24249ms]
Sep  3 22:27:49.394: INFO: Created: latency-svc-gxkpd
Sep  3 22:27:49.431: INFO: Got endpoints: latency-svc-d4lch [751.054332ms]
Sep  3 22:27:49.447: INFO: Created: latency-svc-8kv4h
Sep  3 22:27:49.483: INFO: Got endpoints: latency-svc-8c4l8 [753.783292ms]
Sep  3 22:27:49.493: INFO: Created: latency-svc-rjqr2
Sep  3 22:27:49.533: INFO: Got endpoints: latency-svc-wr7rh [753.091121ms]
Sep  3 22:27:49.548: INFO: Created: latency-svc-4rghg
Sep  3 22:27:49.581: INFO: Got endpoints: latency-svc-mr629 [749.105416ms]
Sep  3 22:27:49.595: INFO: Created: latency-svc-ns5cc
Sep  3 22:27:49.630: INFO: Got endpoints: latency-svc-dlkkv [748.785585ms]
Sep  3 22:27:49.645: INFO: Created: latency-svc-jdh2r
Sep  3 22:27:49.682: INFO: Got endpoints: latency-svc-6crsv [752.831647ms]
Sep  3 22:27:49.694: INFO: Created: latency-svc-j6dvj
Sep  3 22:27:49.732: INFO: Got endpoints: latency-svc-b7th5 [750.323849ms]
Sep  3 22:27:49.744: INFO: Created: latency-svc-2fgxs
Sep  3 22:27:49.781: INFO: Got endpoints: latency-svc-blq8s [747.979443ms]
Sep  3 22:27:49.792: INFO: Created: latency-svc-78z27
Sep  3 22:27:49.831: INFO: Got endpoints: latency-svc-4rpsp [749.33348ms]
Sep  3 22:27:49.845: INFO: Created: latency-svc-qqxml
Sep  3 22:27:49.879: INFO: Got endpoints: latency-svc-2qgtk [748.270247ms]
Sep  3 22:27:49.893: INFO: Created: latency-svc-nrmvj
Sep  3 22:27:49.932: INFO: Got endpoints: latency-svc-k7b9j [753.596551ms]
Sep  3 22:27:49.949: INFO: Created: latency-svc-j22wz
Sep  3 22:27:49.980: INFO: Got endpoints: latency-svc-q6cd2 [749.157907ms]
Sep  3 22:27:49.991: INFO: Created: latency-svc-2mlm9
Sep  3 22:27:50.031: INFO: Got endpoints: latency-svc-5dsmk [749.321904ms]
Sep  3 22:27:50.044: INFO: Created: latency-svc-cbmx6
Sep  3 22:27:50.082: INFO: Got endpoints: latency-svc-4wtlt [747.00483ms]
Sep  3 22:27:50.094: INFO: Created: latency-svc-6ll8d
Sep  3 22:27:50.131: INFO: Got endpoints: latency-svc-gxkpd [750.005996ms]
Sep  3 22:27:50.145: INFO: Created: latency-svc-29stb
Sep  3 22:27:50.180: INFO: Got endpoints: latency-svc-8kv4h [749.030985ms]
Sep  3 22:27:50.193: INFO: Created: latency-svc-j6b2g
Sep  3 22:27:50.231: INFO: Got endpoints: latency-svc-rjqr2 [747.94416ms]
Sep  3 22:27:50.244: INFO: Created: latency-svc-5cn24
Sep  3 22:27:50.281: INFO: Got endpoints: latency-svc-4rghg [747.767439ms]
Sep  3 22:27:50.294: INFO: Created: latency-svc-kpkrn
Sep  3 22:27:50.363: INFO: Got endpoints: latency-svc-ns5cc [781.445344ms]
Sep  3 22:27:50.381: INFO: Got endpoints: latency-svc-jdh2r [751.03411ms]
Sep  3 22:27:50.382: INFO: Created: latency-svc-c5rfm
Sep  3 22:27:50.393: INFO: Created: latency-svc-tgrnk
Sep  3 22:27:50.430: INFO: Got endpoints: latency-svc-j6dvj [748.253646ms]
Sep  3 22:27:50.445: INFO: Created: latency-svc-5ccd7
Sep  3 22:27:50.481: INFO: Got endpoints: latency-svc-2fgxs [748.371076ms]
Sep  3 22:27:50.493: INFO: Created: latency-svc-7krcd
Sep  3 22:27:50.532: INFO: Got endpoints: latency-svc-78z27 [750.76427ms]
Sep  3 22:27:50.544: INFO: Created: latency-svc-b6s66
Sep  3 22:27:50.582: INFO: Got endpoints: latency-svc-qqxml [750.349745ms]
Sep  3 22:27:50.593: INFO: Created: latency-svc-zxjgj
Sep  3 22:27:50.630: INFO: Got endpoints: latency-svc-nrmvj [751.691435ms]
Sep  3 22:27:50.643: INFO: Created: latency-svc-djvjq
Sep  3 22:27:50.679: INFO: Got endpoints: latency-svc-j22wz [746.978452ms]
Sep  3 22:27:50.690: INFO: Created: latency-svc-76rqv
Sep  3 22:27:50.732: INFO: Got endpoints: latency-svc-2mlm9 [751.283904ms]
Sep  3 22:27:50.745: INFO: Created: latency-svc-bv4hd
Sep  3 22:27:50.781: INFO: Got endpoints: latency-svc-cbmx6 [750.056077ms]
Sep  3 22:27:50.795: INFO: Created: latency-svc-5s8gw
Sep  3 22:27:50.832: INFO: Got endpoints: latency-svc-6ll8d [750.13168ms]
Sep  3 22:27:50.844: INFO: Created: latency-svc-5vhqd
Sep  3 22:27:50.883: INFO: Got endpoints: latency-svc-29stb [752.311939ms]
Sep  3 22:27:50.895: INFO: Created: latency-svc-lqwgs
Sep  3 22:27:50.930: INFO: Got endpoints: latency-svc-j6b2g [750.08251ms]
Sep  3 22:27:50.944: INFO: Created: latency-svc-f2hjl
Sep  3 22:27:50.979: INFO: Got endpoints: latency-svc-5cn24 [748.080369ms]
Sep  3 22:27:50.991: INFO: Created: latency-svc-s2462
Sep  3 22:27:51.031: INFO: Got endpoints: latency-svc-kpkrn [749.509663ms]
Sep  3 22:27:51.043: INFO: Created: latency-svc-plgzm
Sep  3 22:27:51.082: INFO: Got endpoints: latency-svc-c5rfm [719.190572ms]
Sep  3 22:27:51.131: INFO: Got endpoints: latency-svc-tgrnk [750.24068ms]
Sep  3 22:27:51.181: INFO: Got endpoints: latency-svc-5ccd7 [751.341163ms]
Sep  3 22:27:51.233: INFO: Got endpoints: latency-svc-7krcd [752.328893ms]
Sep  3 22:27:51.279: INFO: Got endpoints: latency-svc-b6s66 [746.821541ms]
Sep  3 22:27:51.336: INFO: Got endpoints: latency-svc-zxjgj [754.620089ms]
Sep  3 22:27:51.379: INFO: Got endpoints: latency-svc-djvjq [748.866878ms]
Sep  3 22:27:51.432: INFO: Got endpoints: latency-svc-76rqv [752.871611ms]
Sep  3 22:27:51.481: INFO: Got endpoints: latency-svc-bv4hd [749.260218ms]
Sep  3 22:27:51.530: INFO: Got endpoints: latency-svc-5s8gw [748.335335ms]
Sep  3 22:27:51.581: INFO: Got endpoints: latency-svc-5vhqd [748.525716ms]
Sep  3 22:27:51.631: INFO: Got endpoints: latency-svc-lqwgs [747.852707ms]
Sep  3 22:27:51.681: INFO: Got endpoints: latency-svc-f2hjl [750.723683ms]
Sep  3 22:27:51.729: INFO: Got endpoints: latency-svc-s2462 [750.282497ms]
Sep  3 22:27:51.781: INFO: Got endpoints: latency-svc-plgzm [750.788233ms]
Sep  3 22:27:51.782: INFO: Latencies: [21.348815ms 38.391342ms 45.514794ms 60.913596ms 72.466124ms 84.388277ms 94.056086ms 116.500529ms 127.170048ms 141.891524ms 152.522023ms 163.318554ms 172.773544ms 177.887109ms 178.958598ms 179.122921ms 179.248804ms 179.49815ms 180.29786ms 181.24494ms 181.870264ms 182.029083ms 182.20811ms 182.410586ms 183.594537ms 183.779071ms 184.240847ms 184.246158ms 184.319268ms 184.376213ms 185.889255ms 186.285661ms 186.637157ms 187.280209ms 188.33542ms 190.443658ms 193.299755ms 193.716691ms 199.398385ms 223.904533ms 264.599419ms 304.017006ms 338.873577ms 379.784901ms 421.21875ms 452.143433ms 492.551793ms 527.183518ms 571.261364ms 610.740234ms 655.526394ms 682.217067ms 719.190572ms 720.62114ms 725.331149ms 735.813021ms 741.250283ms 745.646674ms 745.64767ms 745.837378ms 745.986387ms 746.030247ms 746.255742ms 746.292548ms 746.821541ms 746.978452ms 747.00483ms 747.1243ms 747.17037ms 747.24249ms 747.275755ms 747.344922ms 747.429912ms 747.540801ms 747.553817ms 747.56178ms 747.563467ms 747.625658ms 747.644922ms 747.670334ms 747.767439ms 747.802511ms 747.852707ms 747.885513ms 747.94416ms 747.979443ms 748.080369ms 748.120184ms 748.127103ms 748.146977ms 748.186382ms 748.253646ms 748.270247ms 748.335335ms 748.371076ms 748.508306ms 748.517583ms 748.525716ms 748.544156ms 748.556941ms 748.585389ms 748.590261ms 748.621479ms 748.673636ms 748.785585ms 748.866878ms 748.906383ms 748.957235ms 749.030985ms 749.105416ms 749.157907ms 749.167161ms 749.260218ms 749.321904ms 749.33348ms 749.391423ms 749.442482ms 749.478551ms 749.496853ms 749.509663ms 749.573812ms 749.586426ms 749.594884ms 749.595681ms 749.620596ms 749.720895ms 749.791173ms 749.852446ms 749.897901ms 749.926462ms 749.947249ms 750.005996ms 750.037926ms 750.048441ms 750.056077ms 750.08251ms 750.13168ms 750.24068ms 750.282497ms 750.323849ms 750.349745ms 750.366719ms 750.545834ms 750.558802ms 750.588082ms 750.595291ms 750.723683ms 750.76427ms 750.779846ms 750.788233ms 750.815187ms 750.864283ms 750.910221ms 750.941465ms 750.970306ms 750.999377ms 751.03411ms 751.054332ms 751.180774ms 751.283904ms 751.300439ms 751.325533ms 751.341163ms 751.362279ms 751.50517ms 751.638567ms 751.691435ms 751.730386ms 751.731553ms 751.898686ms 751.972007ms 752.046554ms 752.070732ms 752.07921ms 752.311939ms 752.328893ms 752.367577ms 752.452715ms 752.483105ms 752.520161ms 752.567228ms 752.654293ms 752.755563ms 752.810032ms 752.831647ms 752.871611ms 753.091121ms 753.097887ms 753.184909ms 753.186831ms 753.270188ms 753.47547ms 753.596551ms 753.783292ms 754.620089ms 754.801884ms 755.343919ms 764.239606ms 781.230033ms 781.445344ms]
Sep  3 22:27:51.782: INFO: 50 %ile: 748.585389ms
Sep  3 22:27:51.782: INFO: 90 %ile: 752.567228ms
Sep  3 22:27:51.782: INFO: 99 %ile: 781.230033ms
Sep  3 22:27:51.782: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:27:51.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2115" for this suite.

• [SLOW TEST:10.777 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":277,"completed":47,"skipped":879,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:27:51.793: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  3 22:27:51.836: INFO: Waiting up to 5m0s for pod "pod-ce3ba601-7e30-481f-b006-37e89d27217e" in namespace "emptydir-8994" to be "Succeeded or Failed"
Sep  3 22:27:51.839: INFO: Pod "pod-ce3ba601-7e30-481f-b006-37e89d27217e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.864289ms
Sep  3 22:27:53.842: INFO: Pod "pod-ce3ba601-7e30-481f-b006-37e89d27217e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006557943s
STEP: Saw pod success
Sep  3 22:27:53.842: INFO: Pod "pod-ce3ba601-7e30-481f-b006-37e89d27217e" satisfied condition "Succeeded or Failed"
Sep  3 22:27:53.844: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-ce3ba601-7e30-481f-b006-37e89d27217e container test-container: <nil>
STEP: delete the pod
Sep  3 22:27:53.863: INFO: Waiting for pod pod-ce3ba601-7e30-481f-b006-37e89d27217e to disappear
Sep  3 22:27:53.866: INFO: Pod pod-ce3ba601-7e30-481f-b006-37e89d27217e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:27:53.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8994" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":48,"skipped":880,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:27:53.879: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-5620
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5620
STEP: Creating statefulset with conflicting port in namespace statefulset-5620
STEP: Waiting until pod test-pod will start running in namespace statefulset-5620
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5620
Sep  3 22:27:57.945: INFO: Observed stateful pod in namespace: statefulset-5620, name: ss-0, uid: e85d1dc8-4f18-4e31-87e9-f32ebb4a8712, status phase: Pending. Waiting for statefulset controller to delete.
Sep  3 22:27:58.345: INFO: Observed stateful pod in namespace: statefulset-5620, name: ss-0, uid: e85d1dc8-4f18-4e31-87e9-f32ebb4a8712, status phase: Failed. Waiting for statefulset controller to delete.
Sep  3 22:27:58.366: INFO: Observed stateful pod in namespace: statefulset-5620, name: ss-0, uid: e85d1dc8-4f18-4e31-87e9-f32ebb4a8712, status phase: Failed. Waiting for statefulset controller to delete.
Sep  3 22:27:58.406: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5620
STEP: Removing pod with conflicting port in namespace statefulset-5620
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5620 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep  3 22:28:02.447: INFO: Deleting all statefulset in ns statefulset-5620
Sep  3 22:28:02.449: INFO: Scaling statefulset ss to 0
Sep  3 22:28:12.475: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 22:28:12.478: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:28:12.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5620" for this suite.

• [SLOW TEST:18.628 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":277,"completed":49,"skipped":886,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:28:12.507: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep  3 22:28:12.538: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  3 22:28:12.549: INFO: Waiting for terminating namespaces to be deleted...
Sep  3 22:28:12.551: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-master-0 before test
Sep  3 22:28:12.568: INFO: fluent-bit-6wlnp from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.568: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:28:12.568: INFO: csi-node-ntnx-plugin-xhqx9 from ntnx-system started at 2020-09-03 22:03:17 +0000 UTC (3 container statuses recorded)
Sep  3 22:28:12.568: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:28:12.568: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:28:12.568: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:28:12.568: INFO: kube-proxy-ds-rtvz4 from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.568: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:28:12.568: INFO: kube-flannel-ds-q7xlv from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.568: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:28:12.568: INFO: node-exporter-rvmxt from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.568: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:28:12.568: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:28:12.568: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-722pc from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.568: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:28:12.568: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:28:12.568: INFO: kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-0 from kube-system started at 2020-09-03 21:51:20 +0000 UTC (3 container statuses recorded)
Sep  3 22:28:12.568: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  3 22:28:12.568: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  3 22:28:12.568: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  3 22:28:12.568: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-master-1 before test
Sep  3 22:28:12.589: INFO: kube-flannel-ds-8qrb7 from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.589: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:28:12.589: INFO: fluent-bit-cpczk from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.589: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:28:12.589: INFO: node-exporter-4wwlp from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.589: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:28:12.589: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:28:12.589: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-2qnxc from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.589: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:28:12.589: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:28:12.589: INFO: csi-node-ntnx-plugin-wgm9j from ntnx-system started at 2020-09-03 22:03:21 +0000 UTC (3 container statuses recorded)
Sep  3 22:28:12.589: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:28:12.589: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:28:12.589: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:28:12.589: INFO: kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-1 from kube-system started at 2020-09-03 21:52:32 +0000 UTC (3 container statuses recorded)
Sep  3 22:28:12.589: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  3 22:28:12.589: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  3 22:28:12.589: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  3 22:28:12.589: INFO: kube-proxy-ds-8vwrs from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.589: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:28:12.589: INFO: kube-dns-75bfbbdcb6-v6vf9 from kube-system started at 2020-09-03 21:55:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:28:12.589: INFO: 	Container dnsmasq ready: true, restart count 0
Sep  3 22:28:12.589: INFO: 	Container kubedns ready: true, restart count 0
Sep  3 22:28:12.589: INFO: 	Container sidecar ready: true, restart count 0
Sep  3 22:28:12.589: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-0 before test
Sep  3 22:28:12.601: INFO: kube-proxy-ds-cbqh9 from kube-system started at 2020-09-03 21:55:15 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:28:12.601: INFO: prometheus-operator-8c5f5b4-9r8jx from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  3 22:28:12.601: INFO: kibana-logging-58dfc7bc95-bk4wt from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container kibana-logging ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 	Container nginxhttp ready: true, restart count 0
Sep  3 22:28:12.601: INFO: kube-state-metrics-7b754ff76b-vpckb from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (4 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container addon-resizer ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  3 22:28:12.601: INFO: node-exporter-fl4nm from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:28:12.601: INFO: alertmanager-main-0 from ntnx-system started at 2020-09-03 21:59:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container alertmanager ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 	Container config-reloader ready: true, restart count 0
Sep  3 22:28:12.601: INFO: prometheus-k8s-0 from ntnx-system started at 2020-09-03 21:59:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container prometheus ready: true, restart count 1
Sep  3 22:28:12.601: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  3 22:28:12.601: INFO: sonobuoy from sonobuoy started at 2020-09-03 22:02:43 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  3 22:28:12.601: INFO: kube-flannel-ds-m9clk from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:28:12.601: INFO: csi-node-ntnx-plugin-fxlcg from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (3 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:28:12.601: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-tcdxz from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:28:12.601: INFO: fluent-bit-25t4p from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.601: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:28:12.601: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-1 before test
Sep  3 22:28:12.621: INFO: fluent-bit-ngkvm from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.621: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:28:12.621: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-gdgws from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.621: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:28:12.621: INFO: csi-node-ntnx-plugin-qqhtx from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (3 container statuses recorded)
Sep  3 22:28:12.621: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:28:12.621: INFO: node-exporter-kzwh4 from ntnx-system started at 2020-09-03 21:59:12 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.621: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:28:12.621: INFO: prometheus-k8s-1 from ntnx-system started at 2020-09-03 21:59:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:28:12.621: INFO: 	Container prometheus ready: true, restart count 1
Sep  3 22:28:12.621: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  3 22:28:12.621: INFO: sonobuoy-e2e-job-38c4ed60912f46a6 from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.621: INFO: 	Container e2e ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:28:12.621: INFO: kube-proxy-ds-w7kvw from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.621: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:28:12.621: INFO: alertmanager-main-1 from ntnx-system started at 2020-09-03 21:59:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.621: INFO: 	Container alertmanager ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 	Container config-reloader ready: true, restart count 0
Sep  3 22:28:12.621: INFO: kube-flannel-ds-4mqbc from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.621: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:28:12.621: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (4 container statuses recorded)
Sep  3 22:28:12.621: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 	Container csi-resizer ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Sep  3 22:28:12.621: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-2 before test
Sep  3 22:28:12.639: INFO: kubernetes-events-printer-5767b7d649-8m9sl from ntnx-system started at 2020-09-03 21:56:21 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.639: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Sep  3 22:28:12.639: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-f4pr7 from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.639: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:28:12.639: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:28:12.639: INFO: kube-flannel-ds-4vfnt from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.639: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:28:12.639: INFO: node-exporter-wdljm from ntnx-system started at 2020-09-03 21:59:12 +0000 UTC (2 container statuses recorded)
Sep  3 22:28:12.639: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:28:12.639: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:28:12.639: INFO: kube-proxy-ds-rsdbq from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.639: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:28:12.639: INFO: csi-node-ntnx-plugin-l9tvt from ntnx-system started at 2020-09-03 21:55:53 +0000 UTC (3 container statuses recorded)
Sep  3 22:28:12.639: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:28:12.639: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:28:12.639: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:28:12.639: INFO: elasticsearch-logging-0 from ntnx-system started at 2020-09-03 21:56:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.639: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Sep  3 22:28:12.639: INFO: fluent-bit-wsm97 from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:28:12.639: INFO: 	Container fluent-bit ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node karbon-test1186mm-f1ba59-k8s-master-0
STEP: verifying the node has the label node karbon-test1186mm-f1ba59-k8s-master-1
STEP: verifying the node has the label node karbon-test1186mm-f1ba59-k8s-worker-0
STEP: verifying the node has the label node karbon-test1186mm-f1ba59-k8s-worker-1
STEP: verifying the node has the label node karbon-test1186mm-f1ba59-k8s-worker-2
Sep  3 22:28:12.732: INFO: Pod kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-0 requesting resource cpu=300m on Node karbon-test1186mm-f1ba59-k8s-master-0
Sep  3 22:28:12.732: INFO: Pod kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-1 requesting resource cpu=300m on Node karbon-test1186mm-f1ba59-k8s-master-1
Sep  3 22:28:12.732: INFO: Pod kube-dns-75bfbbdcb6-v6vf9 requesting resource cpu=260m on Node karbon-test1186mm-f1ba59-k8s-master-1
Sep  3 22:28:12.732: INFO: Pod kube-flannel-ds-4mqbc requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.732: INFO: Pod kube-flannel-ds-4vfnt requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-2
Sep  3 22:28:12.732: INFO: Pod kube-flannel-ds-8qrb7 requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-master-1
Sep  3 22:28:12.732: INFO: Pod kube-flannel-ds-m9clk requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod kube-flannel-ds-q7xlv requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-master-0
Sep  3 22:28:12.732: INFO: Pod kube-proxy-ds-8vwrs requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-master-1
Sep  3 22:28:12.732: INFO: Pod kube-proxy-ds-cbqh9 requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod kube-proxy-ds-rsdbq requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-2
Sep  3 22:28:12.732: INFO: Pod kube-proxy-ds-rtvz4 requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-master-0
Sep  3 22:28:12.732: INFO: Pod kube-proxy-ds-w7kvw requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.732: INFO: Pod alertmanager-main-0 requesting resource cpu=200m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod alertmanager-main-1 requesting resource cpu=200m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.732: INFO: Pod csi-node-ntnx-plugin-fxlcg requesting resource cpu=200m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod csi-node-ntnx-plugin-l9tvt requesting resource cpu=200m on Node karbon-test1186mm-f1ba59-k8s-worker-2
Sep  3 22:28:12.732: INFO: Pod csi-node-ntnx-plugin-qqhtx requesting resource cpu=200m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.732: INFO: Pod csi-node-ntnx-plugin-wgm9j requesting resource cpu=200m on Node karbon-test1186mm-f1ba59-k8s-master-1
Sep  3 22:28:12.732: INFO: Pod csi-node-ntnx-plugin-xhqx9 requesting resource cpu=200m on Node karbon-test1186mm-f1ba59-k8s-master-0
Sep  3 22:28:12.732: INFO: Pod csi-provisioner-ntnx-plugin-0 requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.732: INFO: Pod elasticsearch-logging-0 requesting resource cpu=500m on Node karbon-test1186mm-f1ba59-k8s-worker-2
Sep  3 22:28:12.732: INFO: Pod fluent-bit-25t4p requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod fluent-bit-6wlnp requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-master-0
Sep  3 22:28:12.732: INFO: Pod fluent-bit-cpczk requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-master-1
Sep  3 22:28:12.732: INFO: Pod fluent-bit-ngkvm requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.732: INFO: Pod fluent-bit-wsm97 requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-2
Sep  3 22:28:12.732: INFO: Pod kibana-logging-58dfc7bc95-bk4wt requesting resource cpu=200m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod kube-state-metrics-7b754ff76b-vpckb requesting resource cpu=150m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod kubernetes-events-printer-5767b7d649-8m9sl requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-2
Sep  3 22:28:12.732: INFO: Pod node-exporter-4wwlp requesting resource cpu=112m on Node karbon-test1186mm-f1ba59-k8s-master-1
Sep  3 22:28:12.732: INFO: Pod node-exporter-fl4nm requesting resource cpu=112m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod node-exporter-kzwh4 requesting resource cpu=112m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.732: INFO: Pod node-exporter-rvmxt requesting resource cpu=112m on Node karbon-test1186mm-f1ba59-k8s-master-0
Sep  3 22:28:12.732: INFO: Pod node-exporter-wdljm requesting resource cpu=112m on Node karbon-test1186mm-f1ba59-k8s-worker-2
Sep  3 22:28:12.732: INFO: Pod prometheus-k8s-0 requesting resource cpu=400m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod prometheus-k8s-1 requesting resource cpu=400m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.732: INFO: Pod prometheus-operator-8c5f5b4-9r8jx requesting resource cpu=100m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod sonobuoy requesting resource cpu=0m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.732: INFO: Pod sonobuoy-e2e-job-38c4ed60912f46a6 requesting resource cpu=0m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.732: INFO: Pod sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-2qnxc requesting resource cpu=0m on Node karbon-test1186mm-f1ba59-k8s-master-1
Sep  3 22:28:12.732: INFO: Pod sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-722pc requesting resource cpu=0m on Node karbon-test1186mm-f1ba59-k8s-master-0
Sep  3 22:28:12.732: INFO: Pod sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-f4pr7 requesting resource cpu=0m on Node karbon-test1186mm-f1ba59-k8s-worker-2
Sep  3 22:28:12.732: INFO: Pod sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-gdgws requesting resource cpu=0m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.732: INFO: Pod sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-tcdxz requesting resource cpu=0m on Node karbon-test1186mm-f1ba59-k8s-worker-0
STEP: Starting Pods to consume most of the cluster CPU.
Sep  3 22:28:12.732: INFO: Creating a pod which consumes cpu=2161m on Node karbon-test1186mm-f1ba59-k8s-master-0
Sep  3 22:28:12.742: INFO: Creating a pod which consumes cpu=1979m on Node karbon-test1186mm-f1ba59-k8s-master-1
Sep  3 22:28:12.750: INFO: Creating a pod which consumes cpu=4436m on Node karbon-test1186mm-f1ba59-k8s-worker-0
Sep  3 22:28:12.762: INFO: Creating a pod which consumes cpu=4681m on Node karbon-test1186mm-f1ba59-k8s-worker-1
Sep  3 22:28:12.770: INFO: Creating a pod which consumes cpu=4751m on Node karbon-test1186mm-f1ba59-k8s-worker-2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b303796-c82b-4f9c-b437-5f40a6d7aa2e.1631668bab7c4ad5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6998/filler-pod-5b303796-c82b-4f9c-b437-5f40a6d7aa2e to karbon-test1186mm-f1ba59-k8s-master-1]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-5b303796-c82b-4f9c-b437-5f40a6d7aa2e.1631668bbeaeedfe], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-6998.svc.cluster.local svc.cluster.local cluster.local dev.eng.nutanix.com eng.nutanix.com corp.nutanix.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b303796-c82b-4f9c-b437-5f40a6d7aa2e.1631668bd5ce1f43], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b303796-c82b-4f9c-b437-5f40a6d7aa2e.1631668c24784e30], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b303796-c82b-4f9c-b437-5f40a6d7aa2e.1631668c29633b75], Reason = [Created], Message = [Created container filler-pod-5b303796-c82b-4f9c-b437-5f40a6d7aa2e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b303796-c82b-4f9c-b437-5f40a6d7aa2e.1631668c31cd1f0b], Reason = [Started], Message = [Started container filler-pod-5b303796-c82b-4f9c-b437-5f40a6d7aa2e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84da134c-6983-45da-97ee-b05b50e518dc.1631668baccb48f5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6998/filler-pod-84da134c-6983-45da-97ee-b05b50e518dc to karbon-test1186mm-f1ba59-k8s-worker-1]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-84da134c-6983-45da-97ee-b05b50e518dc.1631668bc065ea1c], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-6998.svc.cluster.local svc.cluster.local cluster.local dev.eng.nutanix.com eng.nutanix.com corp.nutanix.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84da134c-6983-45da-97ee-b05b50e518dc.1631668bdb603d86], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84da134c-6983-45da-97ee-b05b50e518dc.1631668c285f03b6], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84da134c-6983-45da-97ee-b05b50e518dc.1631668c2cd8d669], Reason = [Created], Message = [Created container filler-pod-84da134c-6983-45da-97ee-b05b50e518dc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84da134c-6983-45da-97ee-b05b50e518dc.1631668c35df0103], Reason = [Started], Message = [Started container filler-pod-84da134c-6983-45da-97ee-b05b50e518dc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b628e64d-6d73-4e6c-ba3d-f77f7da2d2d0.1631668bab0744f6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6998/filler-pod-b628e64d-6d73-4e6c-ba3d-f77f7da2d2d0 to karbon-test1186mm-f1ba59-k8s-master-0]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-b628e64d-6d73-4e6c-ba3d-f77f7da2d2d0.1631668bbe0f7ddd], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-6998.svc.cluster.local svc.cluster.local cluster.local dev.eng.nutanix.com eng.nutanix.com corp.nutanix.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b628e64d-6d73-4e6c-ba3d-f77f7da2d2d0.1631668bd54851f6], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b628e64d-6d73-4e6c-ba3d-f77f7da2d2d0.1631668c230818fa], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b628e64d-6d73-4e6c-ba3d-f77f7da2d2d0.1631668c282ed7da], Reason = [Created], Message = [Created container filler-pod-b628e64d-6d73-4e6c-ba3d-f77f7da2d2d0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b628e64d-6d73-4e6c-ba3d-f77f7da2d2d0.1631668c30db02ae], Reason = [Started], Message = [Started container filler-pod-b628e64d-6d73-4e6c-ba3d-f77f7da2d2d0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca317021-e4ea-4dce-b3fe-9ea1ce8c31f7.1631668bac61af4d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6998/filler-pod-ca317021-e4ea-4dce-b3fe-9ea1ce8c31f7 to karbon-test1186mm-f1ba59-k8s-worker-0]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-ca317021-e4ea-4dce-b3fe-9ea1ce8c31f7.1631668bc0614efd], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-6998.svc.cluster.local svc.cluster.local cluster.local dev.eng.nutanix.com eng.nutanix.com corp.nutanix.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca317021-e4ea-4dce-b3fe-9ea1ce8c31f7.1631668bdc3237d9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca317021-e4ea-4dce-b3fe-9ea1ce8c31f7.1631668be2822989], Reason = [Created], Message = [Created container filler-pod-ca317021-e4ea-4dce-b3fe-9ea1ce8c31f7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca317021-e4ea-4dce-b3fe-9ea1ce8c31f7.1631668bea57e099], Reason = [Started], Message = [Started container filler-pod-ca317021-e4ea-4dce-b3fe-9ea1ce8c31f7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f059000a-1a0f-4736-a5cb-3ee786af6413.1631668bad9963a4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6998/filler-pod-f059000a-1a0f-4736-a5cb-3ee786af6413 to karbon-test1186mm-f1ba59-k8s-worker-2]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-f059000a-1a0f-4736-a5cb-3ee786af6413.1631668bc1ab1e73], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-6998.svc.cluster.local svc.cluster.local cluster.local dev.eng.nutanix.com eng.nutanix.com corp.nutanix.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f059000a-1a0f-4736-a5cb-3ee786af6413.1631668bda79bdf6], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f059000a-1a0f-4736-a5cb-3ee786af6413.1631668c28e9241f], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f059000a-1a0f-4736-a5cb-3ee786af6413.1631668c2de53889], Reason = [Created], Message = [Created container filler-pod-f059000a-1a0f-4736-a5cb-3ee786af6413]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f059000a-1a0f-4736-a5cb-3ee786af6413.1631668c360d0fe2], Reason = [Started], Message = [Started container filler-pod-f059000a-1a0f-4736-a5cb-3ee786af6413]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1631668c9db0fa3e], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1631668c9e66834a], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node karbon-test1186mm-f1ba59-k8s-master-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node karbon-test1186mm-f1ba59-k8s-master-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node karbon-test1186mm-f1ba59-k8s-worker-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node karbon-test1186mm-f1ba59-k8s-worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node karbon-test1186mm-f1ba59-k8s-worker-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:28:17.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6998" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:5.396 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":277,"completed":50,"skipped":891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:28:17.904: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:28:17.932: INFO: Creating deployment "test-recreate-deployment"
Sep  3 22:28:17.937: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  3 22:28:17.943: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  3 22:28:19.957: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  3 22:28:19.959: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:28:21.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:28:23.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734768897, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:28:25.964: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  3 22:28:25.975: INFO: Updating deployment test-recreate-deployment
Sep  3 22:28:25.975: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep  3 22:28:26.104: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7536 /apis/apps/v1/namespaces/deployment-7536/deployments/test-recreate-deployment 653aae46-cb55-4b3d-8341-fe3c1c3aa0b6 12698 2 2020-09-03 22:28:17 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-09-03 22:28:25 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-03 22:28:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003330408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-09-03 22:28:26 +0000 UTC,LastTransitionTime:2020-09-03 22:28:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2020-09-03 22:28:26 +0000 UTC,LastTransitionTime:2020-09-03 22:28:17 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  3 22:28:26.108: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-7536 /apis/apps/v1/namespaces/deployment-7536/replicasets/test-recreate-deployment-d5667d9c7 7d7c9db4-152a-4226-bd6d-ed5132fe8f9d 12697 1 2020-09-03 22:28:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 653aae46-cb55-4b3d-8341-fe3c1c3aa0b6 0xc0033309d0 0xc0033309d1}] []  [{kube-controller-manager Update apps/v1 2020-09-03 22:28:26 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 53 51 97 97 101 52 54 45 99 98 53 53 45 52 98 51 100 45 56 51 52 49 45 102 101 51 99 49 99 51 97 97 48 98 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003330a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 22:28:26.108: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  3 22:28:26.108: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-7536 /apis/apps/v1/namespaces/deployment-7536/replicasets/test-recreate-deployment-74d98b5f7c 74b1069f-8f15-45fc-aae9-dfcfd39c88fa 12687 2 2020-09-03 22:28:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 653aae46-cb55-4b3d-8341-fe3c1c3aa0b6 0xc0033308d7 0xc0033308d8}] []  [{kube-controller-manager Update apps/v1 2020-09-03 22:28:26 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 53 51 97 97 101 52 54 45 99 98 53 53 45 52 98 51 100 45 56 51 52 49 45 102 101 51 99 49 99 51 97 97 48 98 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003330968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 22:28:26.111: INFO: Pod "test-recreate-deployment-d5667d9c7-22hlw" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-22hlw test-recreate-deployment-d5667d9c7- deployment-7536 /api/v1/namespaces/deployment-7536/pods/test-recreate-deployment-d5667d9c7-22hlw f1054981-a148-4904-9575-7ec576347289 12699 0 2020-09-03 22:28:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 7d7c9db4-152a-4226-bd6d-ed5132fe8f9d 0xc003330f30 0xc003330f31}] []  [{kube-controller-manager Update v1 2020-09-03 22:28:26 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 100 55 99 57 100 98 52 45 49 53 50 97 45 52 50 50 54 45 98 100 54 100 45 101 100 53 49 51 50 102 101 56 102 57 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:28:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v2z4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v2z4v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v2z4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:28:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:28:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:28:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:28:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:,StartTime:2020-09-03 22:28:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:28:26.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7536" for this suite.

• [SLOW TEST:8.218 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":51,"skipped":916,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:28:26.122: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-e34833af-c055-4826-a443-b2b401e71b48
STEP: Creating a pod to test consume configMaps
Sep  3 22:28:26.168: INFO: Waiting up to 5m0s for pod "pod-configmaps-e070576a-8100-4ae2-a47e-1ff912559fd4" in namespace "configmap-4180" to be "Succeeded or Failed"
Sep  3 22:28:26.173: INFO: Pod "pod-configmaps-e070576a-8100-4ae2-a47e-1ff912559fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.3216ms
Sep  3 22:28:28.176: INFO: Pod "pod-configmaps-e070576a-8100-4ae2-a47e-1ff912559fd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007887052s
Sep  3 22:28:30.180: INFO: Pod "pod-configmaps-e070576a-8100-4ae2-a47e-1ff912559fd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011623902s
STEP: Saw pod success
Sep  3 22:28:30.180: INFO: Pod "pod-configmaps-e070576a-8100-4ae2-a47e-1ff912559fd4" satisfied condition "Succeeded or Failed"
Sep  3 22:28:30.183: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-configmaps-e070576a-8100-4ae2-a47e-1ff912559fd4 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:28:30.205: INFO: Waiting for pod pod-configmaps-e070576a-8100-4ae2-a47e-1ff912559fd4 to disappear
Sep  3 22:28:30.207: INFO: Pod pod-configmaps-e070576a-8100-4ae2-a47e-1ff912559fd4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:28:30.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4180" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":52,"skipped":932,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:28:30.216: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-40e4d970-9f1e-477d-b29b-c6206bfde5e6
STEP: Creating a pod to test consume secrets
Sep  3 22:28:30.265: INFO: Waiting up to 5m0s for pod "pod-secrets-ca881fc6-0170-46a9-b2ba-79c7d0733c6d" in namespace "secrets-4968" to be "Succeeded or Failed"
Sep  3 22:28:30.269: INFO: Pod "pod-secrets-ca881fc6-0170-46a9-b2ba-79c7d0733c6d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.926985ms
Sep  3 22:28:32.274: INFO: Pod "pod-secrets-ca881fc6-0170-46a9-b2ba-79c7d0733c6d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008929015s
Sep  3 22:28:34.278: INFO: Pod "pod-secrets-ca881fc6-0170-46a9-b2ba-79c7d0733c6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013334297s
STEP: Saw pod success
Sep  3 22:28:34.278: INFO: Pod "pod-secrets-ca881fc6-0170-46a9-b2ba-79c7d0733c6d" satisfied condition "Succeeded or Failed"
Sep  3 22:28:34.281: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-secrets-ca881fc6-0170-46a9-b2ba-79c7d0733c6d container secret-volume-test: <nil>
STEP: delete the pod
Sep  3 22:28:34.305: INFO: Waiting for pod pod-secrets-ca881fc6-0170-46a9-b2ba-79c7d0733c6d to disappear
Sep  3 22:28:34.307: INFO: Pod pod-secrets-ca881fc6-0170-46a9-b2ba-79c7d0733c6d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:28:34.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4968" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":53,"skipped":942,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:28:34.315: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-296d8b5d-3cd8-4d27-a475-0be6d9a7d9ed
STEP: Creating a pod to test consume configMaps
Sep  3 22:28:34.366: INFO: Waiting up to 5m0s for pod "pod-configmaps-81e830cd-1170-4a7b-9d24-4a14d1cc5718" in namespace "configmap-740" to be "Succeeded or Failed"
Sep  3 22:28:34.369: INFO: Pod "pod-configmaps-81e830cd-1170-4a7b-9d24-4a14d1cc5718": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680973ms
Sep  3 22:28:36.373: INFO: Pod "pod-configmaps-81e830cd-1170-4a7b-9d24-4a14d1cc5718": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006824624s
STEP: Saw pod success
Sep  3 22:28:36.373: INFO: Pod "pod-configmaps-81e830cd-1170-4a7b-9d24-4a14d1cc5718" satisfied condition "Succeeded or Failed"
Sep  3 22:28:36.376: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-configmaps-81e830cd-1170-4a7b-9d24-4a14d1cc5718 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:28:36.401: INFO: Waiting for pod pod-configmaps-81e830cd-1170-4a7b-9d24-4a14d1cc5718 to disappear
Sep  3 22:28:36.404: INFO: Pod pod-configmaps-81e830cd-1170-4a7b-9d24-4a14d1cc5718 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:28:36.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-740" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":54,"skipped":944,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:28:36.414: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  3 22:28:36.485: INFO: Number of nodes with available pods: 0
Sep  3 22:28:36.485: INFO: Node karbon-test1186mm-f1ba59-k8s-master-0 is running more than one daemon pod
Sep  3 22:28:37.493: INFO: Number of nodes with available pods: 0
Sep  3 22:28:37.493: INFO: Node karbon-test1186mm-f1ba59-k8s-master-0 is running more than one daemon pod
Sep  3 22:28:38.495: INFO: Number of nodes with available pods: 4
Sep  3 22:28:38.495: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-0 is running more than one daemon pod
Sep  3 22:28:39.493: INFO: Number of nodes with available pods: 5
Sep  3 22:28:39.493: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep  3 22:28:39.525: INFO: Number of nodes with available pods: 4
Sep  3 22:28:39.525: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-0 is running more than one daemon pod
Sep  3 22:28:40.534: INFO: Number of nodes with available pods: 4
Sep  3 22:28:40.534: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-0 is running more than one daemon pod
Sep  3 22:28:41.533: INFO: Number of nodes with available pods: 5
Sep  3 22:28:41.533: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2534, will wait for the garbage collector to delete the pods
Sep  3 22:28:41.599: INFO: Deleting DaemonSet.extensions daemon-set took: 7.064117ms
Sep  3 22:28:41.999: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.2169ms
Sep  3 22:28:52.903: INFO: Number of nodes with available pods: 0
Sep  3 22:28:52.903: INFO: Number of running nodes: 0, number of available pods: 0
Sep  3 22:28:52.906: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2534/daemonsets","resourceVersion":"13028"},"items":null}

Sep  3 22:28:52.909: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2534/pods","resourceVersion":"13028"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:28:52.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2534" for this suite.

• [SLOW TEST:16.520 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":277,"completed":55,"skipped":948,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:28:52.934: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  3 22:28:59.009: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  3 22:28:59.012: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  3 22:29:01.012: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  3 22:29:01.017: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  3 22:29:03.012: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  3 22:29:03.016: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  3 22:29:05.012: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  3 22:29:05.016: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  3 22:29:07.012: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  3 22:29:07.018: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  3 22:29:09.012: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  3 22:29:09.016: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:29:09.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3992" for this suite.

• [SLOW TEST:16.114 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":277,"completed":56,"skipped":965,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:29:09.048: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:29:09.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-2676'
Sep  3 22:29:09.359: INFO: stderr: ""
Sep  3 22:29:09.359: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Sep  3 22:29:09.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-2676'
Sep  3 22:29:09.608: INFO: stderr: ""
Sep  3 22:29:09.608: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep  3 22:29:10.612: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:29:10.612: INFO: Found 0 / 1
Sep  3 22:29:11.613: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:29:11.613: INFO: Found 1 / 1
Sep  3 22:29:11.613: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  3 22:29:11.616: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:29:11.616: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  3 22:29:11.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 describe pod agnhost-master-qxmwt --namespace=kubectl-2676'
Sep  3 22:29:11.712: INFO: stderr: ""
Sep  3 22:29:11.712: INFO: stdout: "Name:         agnhost-master-qxmwt\nNamespace:    kubectl-2676\nPriority:     0\nNode:         karbon-test1186mm-f1ba59-k8s-worker-0/10.45.43.119\nStart Time:   Thu, 03 Sep 2020 22:29:09 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           172.20.2.89\nIPs:\n  IP:           172.20.2.89\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://8752547bb46ed244b41d99b8c4f42a43c21c9b08847a40b65f011ba685cd8968\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 03 Sep 2020 22:29:10 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2b8j (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-n2b8j:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-n2b8j\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason            Age              From                                            Message\n  ----     ------            ----             ----                                            -------\n  Normal   Scheduled         <unknown>        default-scheduler                               Successfully assigned kubectl-2676/agnhost-master-qxmwt to karbon-test1186mm-f1ba59-k8s-worker-0\n  Normal   Pulled            1s               kubelet, karbon-test1186mm-f1ba59-k8s-worker-0  Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal   Created           1s               kubelet, karbon-test1186mm-f1ba59-k8s-worker-0  Created container agnhost-master\n  Normal   Started           1s               kubelet, karbon-test1186mm-f1ba59-k8s-worker-0  Started container agnhost-master\n  Warning  DNSConfigForming  0s (x3 over 2s)  kubelet, karbon-test1186mm-f1ba59-k8s-worker-0  Search Line limits were exceeded, some search paths have been omitted, the applied search line is: kubectl-2676.svc.cluster.local svc.cluster.local cluster.local dev.eng.nutanix.com eng.nutanix.com corp.nutanix.com\n"
Sep  3 22:29:11.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 describe rc agnhost-master --namespace=kubectl-2676'
Sep  3 22:29:11.819: INFO: stderr: ""
Sep  3 22:29:11.819: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-2676\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-master-qxmwt\n"
Sep  3 22:29:11.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 describe service agnhost-master --namespace=kubectl-2676'
Sep  3 22:29:11.908: INFO: stderr: ""
Sep  3 22:29:11.908: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-2676\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                172.19.135.252\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.20.2.89:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  3 22:29:11.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 describe node karbon-test1186mm-f1ba59-k8s-master-0'
Sep  3 22:29:12.040: INFO: stderr: ""
Sep  3 22:29:12.040: INFO: stdout: "Name:               karbon-test1186mm-f1ba59-k8s-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=karbon-test1186mm-f1ba59-k8s-master-0\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=master\n                    node.kubernetes.io/master=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"com.nutanix.csi\":\"karbon-test1186mm-f1ba59-k8s-master-0\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"ca:f5:4d:19:5c:ae\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.45.43.123\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 03 Sep 2020 21:51:33 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  karbon-test1186mm-f1ba59-k8s-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 03 Sep 2020 22:29:04 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 03 Sep 2020 22:28:24 +0000   Thu, 03 Sep 2020 21:51:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 03 Sep 2020 22:28:24 +0000   Thu, 03 Sep 2020 21:51:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 03 Sep 2020 22:28:24 +0000   Thu, 03 Sep 2020 21:51:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 03 Sep 2020 22:28:24 +0000   Thu, 03 Sep 2020 21:51:43 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.45.43.123\n  Hostname:    karbon-test1186mm-f1ba59-k8s-master-0\nCapacity:\n  cpu:                4\n  ephemeral-storage:  123723328Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3843988Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  123723328Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3434388Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 8d5147e831e64411bd5ce70e59b01a1f\n  System UUID:                E37CCB17-8E7F-4B01-B7BA-E4A9FE2E9524\n  Boot ID:                    c6ca165d-b53d-420e-99d1-8847986249b2\n  Kernel Version:             3.10.0-1127.13.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.9.8\n  Kubelet Version:            v1.18.6\n  Kube-Proxy Version:         v1.18.6\nPodCIDR:                      172.20.0.0/24\nPodCIDRs:                     172.20.0.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-0       300m (7%)     0 (0%)      0 (0%)           0 (0%)         36m\n  kube-system                 kube-flannel-ds-q7xlv                                      100m (2%)     500m (12%)  50Mi (1%)        50Mi (1%)      33m\n  kube-system                 kube-proxy-ds-rtvz4                                        100m (2%)     100m (2%)   70Mi (2%)        70Mi (2%)      33m\n  ntnx-system                 csi-node-ntnx-plugin-xhqx9                                 200m (5%)     200m (5%)   400Mi (11%)      400Mi (11%)    25m\n  ntnx-system                 fluent-bit-6wlnp                                           100m (2%)     100m (2%)   100Mi (2%)       100Mi (2%)     32m\n  ntnx-system                 node-exporter-rvmxt                                        112m (2%)     600m (15%)  200Mi (5%)       220Mi (6%)     30m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-722pc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                912m (22%)   1500m (37%)\n  memory             820Mi (24%)  840Mi (25%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From                                               Message\n  ----    ------                   ----               ----                                               -------\n  Normal  NodeHasNoDiskPressure    37m (x7 over 37m)  kubelet, karbon-test1186mm-f1ba59-k8s-master-0     Node karbon-test1186mm-f1ba59-k8s-master-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     37m (x7 over 37m)  kubelet, karbon-test1186mm-f1ba59-k8s-master-0     Node karbon-test1186mm-f1ba59-k8s-master-0 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  37m (x8 over 37m)  kubelet, karbon-test1186mm-f1ba59-k8s-master-0     Node karbon-test1186mm-f1ba59-k8s-master-0 status is now: NodeHasSufficientMemory\n  Normal  Starting                 33m                kube-proxy, karbon-test1186mm-f1ba59-k8s-master-0  Starting kube-proxy.\n"
Sep  3 22:29:12.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 describe namespace kubectl-2676'
Sep  3 22:29:12.132: INFO: stderr: ""
Sep  3 22:29:12.132: INFO: stdout: "Name:         kubectl-2676\nLabels:       e2e-framework=kubectl\n              e2e-run=09a98756-81a2-4385-89d2-7b73a2c32160\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:29:12.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2676" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":277,"completed":57,"skipped":983,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:29:12.143: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-64d43ba3-2b9a-438b-8326-428cd8f6fea7
STEP: Creating a pod to test consume configMaps
Sep  3 22:29:12.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-af1e764e-7859-4285-b90a-e195463290d3" in namespace "configmap-8803" to be "Succeeded or Failed"
Sep  3 22:29:12.204: INFO: Pod "pod-configmaps-af1e764e-7859-4285-b90a-e195463290d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.744814ms
Sep  3 22:29:14.207: INFO: Pod "pod-configmaps-af1e764e-7859-4285-b90a-e195463290d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00586962s
STEP: Saw pod success
Sep  3 22:29:14.207: INFO: Pod "pod-configmaps-af1e764e-7859-4285-b90a-e195463290d3" satisfied condition "Succeeded or Failed"
Sep  3 22:29:14.210: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-configmaps-af1e764e-7859-4285-b90a-e195463290d3 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:29:14.230: INFO: Waiting for pod pod-configmaps-af1e764e-7859-4285-b90a-e195463290d3 to disappear
Sep  3 22:29:14.232: INFO: Pod pod-configmaps-af1e764e-7859-4285-b90a-e195463290d3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:29:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8803" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":58,"skipped":989,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:29:14.245: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:29:14.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31000bd2-2889-402d-9862-19445ef28380" in namespace "projected-44" to be "Succeeded or Failed"
Sep  3 22:29:14.295: INFO: Pod "downwardapi-volume-31000bd2-2889-402d-9862-19445ef28380": Phase="Pending", Reason="", readiness=false. Elapsed: 7.566795ms
Sep  3 22:29:16.299: INFO: Pod "downwardapi-volume-31000bd2-2889-402d-9862-19445ef28380": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011626244s
STEP: Saw pod success
Sep  3 22:29:16.299: INFO: Pod "downwardapi-volume-31000bd2-2889-402d-9862-19445ef28380" satisfied condition "Succeeded or Failed"
Sep  3 22:29:16.302: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-31000bd2-2889-402d-9862-19445ef28380 container client-container: <nil>
STEP: delete the pod
Sep  3 22:29:16.325: INFO: Waiting for pod downwardapi-volume-31000bd2-2889-402d-9862-19445ef28380 to disappear
Sep  3 22:29:16.327: INFO: Pod downwardapi-volume-31000bd2-2889-402d-9862-19445ef28380 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:29:16.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-44" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":59,"skipped":990,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:29:16.341: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-34dcb39f-e2b2-486a-898a-d75380edeb04 in namespace container-probe-1682
Sep  3 22:29:18.393: INFO: Started pod liveness-34dcb39f-e2b2-486a-898a-d75380edeb04 in namespace container-probe-1682
STEP: checking the pod's current state and verifying that restartCount is present
Sep  3 22:29:18.397: INFO: Initial restart count of pod liveness-34dcb39f-e2b2-486a-898a-d75380edeb04 is 0
Sep  3 22:29:40.447: INFO: Restart count of pod container-probe-1682/liveness-34dcb39f-e2b2-486a-898a-d75380edeb04 is now 1 (22.049771979s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:29:40.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1682" for this suite.

• [SLOW TEST:24.130 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":60,"skipped":1009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:29:40.471: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep  3 22:29:40.516: INFO: Waiting up to 5m0s for pod "downward-api-0ae65eac-a8c5-416c-881d-dc01bb12e651" in namespace "downward-api-4078" to be "Succeeded or Failed"
Sep  3 22:29:40.526: INFO: Pod "downward-api-0ae65eac-a8c5-416c-881d-dc01bb12e651": Phase="Pending", Reason="", readiness=false. Elapsed: 9.283875ms
Sep  3 22:29:42.529: INFO: Pod "downward-api-0ae65eac-a8c5-416c-881d-dc01bb12e651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012565599s
STEP: Saw pod success
Sep  3 22:29:42.529: INFO: Pod "downward-api-0ae65eac-a8c5-416c-881d-dc01bb12e651" satisfied condition "Succeeded or Failed"
Sep  3 22:29:42.531: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downward-api-0ae65eac-a8c5-416c-881d-dc01bb12e651 container dapi-container: <nil>
STEP: delete the pod
Sep  3 22:29:42.557: INFO: Waiting for pod downward-api-0ae65eac-a8c5-416c-881d-dc01bb12e651 to disappear
Sep  3 22:29:42.559: INFO: Pod downward-api-0ae65eac-a8c5-416c-881d-dc01bb12e651 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:29:42.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4078" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":277,"completed":61,"skipped":1082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:29:42.570: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-5338bcff-a98f-482f-b354-2a536d64b40f
STEP: Creating configMap with name cm-test-opt-upd-d53fcf8d-58d8-4eb5-b80a-4128d0d043a2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5338bcff-a98f-482f-b354-2a536d64b40f
STEP: Updating configmap cm-test-opt-upd-d53fcf8d-58d8-4eb5-b80a-4128d0d043a2
STEP: Creating configMap with name cm-test-opt-create-9941f1cb-c583-4fc6-a6dd-0fe1aa73d6aa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:29:46.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1310" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":62,"skipped":1104,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:29:46.722: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep  3 22:29:46.770: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8725 /api/v1/namespaces/watch-8725/configmaps/e2e-watch-test-label-changed d14b4038-360e-4eda-8bb2-e6af7aee4504 13495 0 2020-09-03 22:29:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-03 22:29:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:29:46.770: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8725 /api/v1/namespaces/watch-8725/configmaps/e2e-watch-test-label-changed d14b4038-360e-4eda-8bb2-e6af7aee4504 13496 0 2020-09-03 22:29:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-03 22:29:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:29:46.770: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8725 /api/v1/namespaces/watch-8725/configmaps/e2e-watch-test-label-changed d14b4038-360e-4eda-8bb2-e6af7aee4504 13497 0 2020-09-03 22:29:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-03 22:29:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep  3 22:29:56.802: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8725 /api/v1/namespaces/watch-8725/configmaps/e2e-watch-test-label-changed d14b4038-360e-4eda-8bb2-e6af7aee4504 13557 0 2020-09-03 22:29:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-03 22:29:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:29:56.802: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8725 /api/v1/namespaces/watch-8725/configmaps/e2e-watch-test-label-changed d14b4038-360e-4eda-8bb2-e6af7aee4504 13558 0 2020-09-03 22:29:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-03 22:29:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:29:56.802: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8725 /api/v1/namespaces/watch-8725/configmaps/e2e-watch-test-label-changed d14b4038-360e-4eda-8bb2-e6af7aee4504 13559 0 2020-09-03 22:29:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-03 22:29:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:29:56.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8725" for this suite.

• [SLOW TEST:10.090 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":277,"completed":63,"skipped":1110,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:29:56.812: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep  3 22:29:56.858: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6296 /api/v1/namespaces/watch-6296/configmaps/e2e-watch-test-watch-closed b8895649-f77f-428e-9c6a-fb260f85a60c 13565 0 2020-09-03 22:29:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-03 22:29:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:29:56.859: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6296 /api/v1/namespaces/watch-6296/configmaps/e2e-watch-test-watch-closed b8895649-f77f-428e-9c6a-fb260f85a60c 13566 0 2020-09-03 22:29:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-03 22:29:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep  3 22:29:56.876: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6296 /api/v1/namespaces/watch-6296/configmaps/e2e-watch-test-watch-closed b8895649-f77f-428e-9c6a-fb260f85a60c 13567 0 2020-09-03 22:29:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-03 22:29:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:29:56.877: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6296 /api/v1/namespaces/watch-6296/configmaps/e2e-watch-test-watch-closed b8895649-f77f-428e-9c6a-fb260f85a60c 13568 0 2020-09-03 22:29:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-03 22:29:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:29:56.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6296" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":277,"completed":64,"skipped":1121,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:29:56.886: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-f1304665-7947-43f4-84f3-01d9503f3d33
STEP: Creating a pod to test consume configMaps
Sep  3 22:29:56.928: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-95fdfe0c-4417-4e85-8158-3d913f66e7a5" in namespace "projected-8684" to be "Succeeded or Failed"
Sep  3 22:29:56.931: INFO: Pod "pod-projected-configmaps-95fdfe0c-4417-4e85-8158-3d913f66e7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.521095ms
Sep  3 22:29:58.935: INFO: Pod "pod-projected-configmaps-95fdfe0c-4417-4e85-8158-3d913f66e7a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006475893s
STEP: Saw pod success
Sep  3 22:29:58.935: INFO: Pod "pod-projected-configmaps-95fdfe0c-4417-4e85-8158-3d913f66e7a5" satisfied condition "Succeeded or Failed"
Sep  3 22:29:58.939: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-configmaps-95fdfe0c-4417-4e85-8158-3d913f66e7a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:29:58.965: INFO: Waiting for pod pod-projected-configmaps-95fdfe0c-4417-4e85-8158-3d913f66e7a5 to disappear
Sep  3 22:29:58.968: INFO: Pod pod-projected-configmaps-95fdfe0c-4417-4e85-8158-3d913f66e7a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:29:58.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8684" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":65,"skipped":1126,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:29:58.981: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep  3 22:29:59.306: INFO: Pod name wrapped-volume-race-eafa74c8-53f1-4c15-a9a5-0a808d49b8dd: Found 0 pods out of 5
Sep  3 22:30:04.314: INFO: Pod name wrapped-volume-race-eafa74c8-53f1-4c15-a9a5-0a808d49b8dd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-eafa74c8-53f1-4c15-a9a5-0a808d49b8dd in namespace emptydir-wrapper-6548, will wait for the garbage collector to delete the pods
Sep  3 22:30:14.398: INFO: Deleting ReplicationController wrapped-volume-race-eafa74c8-53f1-4c15-a9a5-0a808d49b8dd took: 10.568382ms
Sep  3 22:30:14.799: INFO: Terminating ReplicationController wrapped-volume-race-eafa74c8-53f1-4c15-a9a5-0a808d49b8dd pods took: 400.266124ms
STEP: Creating RC which spawns configmap-volume pods
Sep  3 22:30:18.018: INFO: Pod name wrapped-volume-race-1b3c1f66-1ae6-4d7a-8617-c8627898f809: Found 0 pods out of 5
Sep  3 22:30:23.027: INFO: Pod name wrapped-volume-race-1b3c1f66-1ae6-4d7a-8617-c8627898f809: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1b3c1f66-1ae6-4d7a-8617-c8627898f809 in namespace emptydir-wrapper-6548, will wait for the garbage collector to delete the pods
Sep  3 22:30:33.113: INFO: Deleting ReplicationController wrapped-volume-race-1b3c1f66-1ae6-4d7a-8617-c8627898f809 took: 10.413613ms
Sep  3 22:30:33.213: INFO: Terminating ReplicationController wrapped-volume-race-1b3c1f66-1ae6-4d7a-8617-c8627898f809 pods took: 100.247567ms
STEP: Creating RC which spawns configmap-volume pods
Sep  3 22:30:42.931: INFO: Pod name wrapped-volume-race-192b9c95-cb60-4a2a-92cc-d4e9c00aeee5: Found 0 pods out of 5
Sep  3 22:30:47.937: INFO: Pod name wrapped-volume-race-192b9c95-cb60-4a2a-92cc-d4e9c00aeee5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-192b9c95-cb60-4a2a-92cc-d4e9c00aeee5 in namespace emptydir-wrapper-6548, will wait for the garbage collector to delete the pods
Sep  3 22:30:58.023: INFO: Deleting ReplicationController wrapped-volume-race-192b9c95-cb60-4a2a-92cc-d4e9c00aeee5 took: 11.960659ms
Sep  3 22:30:58.424: INFO: Terminating ReplicationController wrapped-volume-race-192b9c95-cb60-4a2a-92cc-d4e9c00aeee5 pods took: 400.280914ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:12.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6548" for this suite.

• [SLOW TEST:73.382 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":277,"completed":66,"skipped":1139,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:12.364: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:31:13.052: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  3 22:31:15.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769073, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769073, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769073, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769073, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:31:18.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep  3 22:31:22.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 attach --namespace=webhook-6165 to-be-attached-pod -i -c=container1'
Sep  3 22:31:22.204: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:22.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6165" for this suite.
STEP: Destroying namespace "webhook-6165-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.917 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":277,"completed":67,"skipped":1140,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:22.281: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:22.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9726" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":277,"completed":68,"skipped":1183,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:22.372: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-07ba9002-d2bc-4678-b307-3c01363c01d1
STEP: Creating a pod to test consume configMaps
Sep  3 22:31:22.415: INFO: Waiting up to 5m0s for pod "pod-configmaps-bfb8e036-f10a-40ec-95e2-0302102da8b7" in namespace "configmap-7838" to be "Succeeded or Failed"
Sep  3 22:31:22.418: INFO: Pod "pod-configmaps-bfb8e036-f10a-40ec-95e2-0302102da8b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.898123ms
Sep  3 22:31:24.423: INFO: Pod "pod-configmaps-bfb8e036-f10a-40ec-95e2-0302102da8b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007632415s
STEP: Saw pod success
Sep  3 22:31:24.423: INFO: Pod "pod-configmaps-bfb8e036-f10a-40ec-95e2-0302102da8b7" satisfied condition "Succeeded or Failed"
Sep  3 22:31:24.426: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-configmaps-bfb8e036-f10a-40ec-95e2-0302102da8b7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:31:24.448: INFO: Waiting for pod pod-configmaps-bfb8e036-f10a-40ec-95e2-0302102da8b7 to disappear
Sep  3 22:31:24.451: INFO: Pod pod-configmaps-bfb8e036-f10a-40ec-95e2-0302102da8b7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:24.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7838" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":69,"skipped":1204,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:24.462: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:31:24.489: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:26.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7101" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":277,"completed":70,"skipped":1213,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:26.539: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Sep  3 22:31:26.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-9306'
Sep  3 22:31:26.782: INFO: stderr: ""
Sep  3 22:31:26.782: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep  3 22:31:27.787: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:31:27.787: INFO: Found 0 / 1
Sep  3 22:31:28.788: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:31:28.788: INFO: Found 1 / 1
Sep  3 22:31:28.788: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep  3 22:31:28.791: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:31:28.791: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  3 22:31:28.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 patch pod agnhost-master-kmdn5 --namespace=kubectl-9306 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  3 22:31:28.876: INFO: stderr: ""
Sep  3 22:31:28.876: INFO: stdout: "pod/agnhost-master-kmdn5 patched\n"
STEP: checking annotations
Sep  3 22:31:28.880: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  3 22:31:28.880: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:28.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9306" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":277,"completed":71,"skipped":1224,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:28.891: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:31:28.919: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:33.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5134" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":277,"completed":72,"skipped":1225,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:33.055: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep  3 22:31:33.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1915'
Sep  3 22:31:33.197: INFO: stderr: ""
Sep  3 22:31:33.197: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Sep  3 22:31:33.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete pods e2e-test-httpd-pod --namespace=kubectl-1915'
Sep  3 22:31:47.100: INFO: stderr: ""
Sep  3 22:31:47.100: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:47.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1915" for this suite.

• [SLOW TEST:14.062 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":277,"completed":73,"skipped":1232,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:47.117: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:31:47.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4560f906-6642-4549-b199-6b7fd770f783" in namespace "projected-4935" to be "Succeeded or Failed"
Sep  3 22:31:47.163: INFO: Pod "downwardapi-volume-4560f906-6642-4549-b199-6b7fd770f783": Phase="Pending", Reason="", readiness=false. Elapsed: 3.185748ms
Sep  3 22:31:49.166: INFO: Pod "downwardapi-volume-4560f906-6642-4549-b199-6b7fd770f783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006671434s
STEP: Saw pod success
Sep  3 22:31:49.166: INFO: Pod "downwardapi-volume-4560f906-6642-4549-b199-6b7fd770f783" satisfied condition "Succeeded or Failed"
Sep  3 22:31:49.168: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-4560f906-6642-4549-b199-6b7fd770f783 container client-container: <nil>
STEP: delete the pod
Sep  3 22:31:49.189: INFO: Waiting for pod downwardapi-volume-4560f906-6642-4549-b199-6b7fd770f783 to disappear
Sep  3 22:31:49.191: INFO: Pod downwardapi-volume-4560f906-6642-4549-b199-6b7fd770f783 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:49.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4935" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":74,"skipped":1243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:49.201: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:31:49.236: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35c23ed4-838b-43a7-adfa-34c8fc66aea3" in namespace "downward-api-4480" to be "Succeeded or Failed"
Sep  3 22:31:49.239: INFO: Pod "downwardapi-volume-35c23ed4-838b-43a7-adfa-34c8fc66aea3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611779ms
Sep  3 22:31:51.242: INFO: Pod "downwardapi-volume-35c23ed4-838b-43a7-adfa-34c8fc66aea3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00572614s
STEP: Saw pod success
Sep  3 22:31:51.242: INFO: Pod "downwardapi-volume-35c23ed4-838b-43a7-adfa-34c8fc66aea3" satisfied condition "Succeeded or Failed"
Sep  3 22:31:51.244: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-35c23ed4-838b-43a7-adfa-34c8fc66aea3 container client-container: <nil>
STEP: delete the pod
Sep  3 22:31:51.265: INFO: Waiting for pod downwardapi-volume-35c23ed4-838b-43a7-adfa-34c8fc66aea3 to disappear
Sep  3 22:31:51.268: INFO: Pod downwardapi-volume-35c23ed4-838b-43a7-adfa-34c8fc66aea3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:51.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4480" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":75,"skipped":1306,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:51.277: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Sep  3 22:31:51.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-5687'
Sep  3 22:31:51.485: INFO: stderr: ""
Sep  3 22:31:51.485: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  3 22:31:51.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5687'
Sep  3 22:31:51.573: INFO: stderr: ""
Sep  3 22:31:51.573: INFO: stdout: "update-demo-nautilus-2wgs2 update-demo-nautilus-6fmmq "
Sep  3 22:31:51.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-2wgs2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5687'
Sep  3 22:31:51.649: INFO: stderr: ""
Sep  3 22:31:51.649: INFO: stdout: ""
Sep  3 22:31:51.649: INFO: update-demo-nautilus-2wgs2 is created but not running
Sep  3 22:31:56.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5687'
Sep  3 22:31:56.742: INFO: stderr: ""
Sep  3 22:31:56.742: INFO: stdout: "update-demo-nautilus-2wgs2 update-demo-nautilus-6fmmq "
Sep  3 22:31:56.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-2wgs2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5687'
Sep  3 22:31:56.829: INFO: stderr: ""
Sep  3 22:31:56.829: INFO: stdout: "true"
Sep  3 22:31:56.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-2wgs2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5687'
Sep  3 22:31:56.911: INFO: stderr: ""
Sep  3 22:31:56.911: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  3 22:31:56.911: INFO: validating pod update-demo-nautilus-2wgs2
Sep  3 22:31:56.916: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 22:31:56.916: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 22:31:56.916: INFO: update-demo-nautilus-2wgs2 is verified up and running
Sep  3 22:31:56.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-6fmmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5687'
Sep  3 22:31:56.997: INFO: stderr: ""
Sep  3 22:31:56.997: INFO: stdout: "true"
Sep  3 22:31:56.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-6fmmq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5687'
Sep  3 22:31:57.083: INFO: stderr: ""
Sep  3 22:31:57.083: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  3 22:31:57.083: INFO: validating pod update-demo-nautilus-6fmmq
Sep  3 22:31:57.089: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 22:31:57.089: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 22:31:57.089: INFO: update-demo-nautilus-6fmmq is verified up and running
STEP: using delete to clean up resources
Sep  3 22:31:57.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete --grace-period=0 --force -f - --namespace=kubectl-5687'
Sep  3 22:31:57.181: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 22:31:57.181: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  3 22:31:57.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5687'
Sep  3 22:31:57.274: INFO: stderr: "No resources found in kubectl-5687 namespace.\n"
Sep  3 22:31:57.274: INFO: stdout: ""
Sep  3 22:31:57.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -l name=update-demo --namespace=kubectl-5687 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  3 22:31:57.364: INFO: stderr: ""
Sep  3 22:31:57.364: INFO: stdout: "update-demo-nautilus-2wgs2\nupdate-demo-nautilus-6fmmq\n"
Sep  3 22:31:57.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5687'
Sep  3 22:31:57.958: INFO: stderr: "No resources found in kubectl-5687 namespace.\n"
Sep  3 22:31:57.958: INFO: stdout: ""
Sep  3 22:31:57.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -l name=update-demo --namespace=kubectl-5687 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  3 22:31:58.044: INFO: stderr: ""
Sep  3 22:31:58.044: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:31:58.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5687" for this suite.

• [SLOW TEST:6.780 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":277,"completed":76,"skipped":1317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:31:58.057: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Sep  3 22:31:58.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-3918'
Sep  3 22:31:58.277: INFO: stderr: ""
Sep  3 22:31:58.277: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  3 22:31:58.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3918'
Sep  3 22:31:58.380: INFO: stderr: ""
Sep  3 22:31:58.381: INFO: stdout: "update-demo-nautilus-9tj8c update-demo-nautilus-ldzw9 "
Sep  3 22:31:58.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-9tj8c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:31:58.461: INFO: stderr: ""
Sep  3 22:31:58.461: INFO: stdout: ""
Sep  3 22:31:58.461: INFO: update-demo-nautilus-9tj8c is created but not running
Sep  3 22:32:03.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3918'
Sep  3 22:32:03.549: INFO: stderr: ""
Sep  3 22:32:03.549: INFO: stdout: "update-demo-nautilus-9tj8c update-demo-nautilus-ldzw9 "
Sep  3 22:32:03.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-9tj8c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:03.638: INFO: stderr: ""
Sep  3 22:32:03.638: INFO: stdout: "true"
Sep  3 22:32:03.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-9tj8c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:03.719: INFO: stderr: ""
Sep  3 22:32:03.720: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  3 22:32:03.720: INFO: validating pod update-demo-nautilus-9tj8c
Sep  3 22:32:03.726: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 22:32:03.726: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 22:32:03.726: INFO: update-demo-nautilus-9tj8c is verified up and running
Sep  3 22:32:03.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-ldzw9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:03.802: INFO: stderr: ""
Sep  3 22:32:03.802: INFO: stdout: "true"
Sep  3 22:32:03.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-ldzw9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:03.884: INFO: stderr: ""
Sep  3 22:32:03.884: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  3 22:32:03.884: INFO: validating pod update-demo-nautilus-ldzw9
Sep  3 22:32:03.890: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 22:32:03.890: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 22:32:03.890: INFO: update-demo-nautilus-ldzw9 is verified up and running
STEP: scaling down the replication controller
Sep  3 22:32:03.892: INFO: scanned /root for discovery docs: <nil>
Sep  3 22:32:03.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3918'
Sep  3 22:32:05.007: INFO: stderr: ""
Sep  3 22:32:05.007: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  3 22:32:05.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3918'
Sep  3 22:32:05.092: INFO: stderr: ""
Sep  3 22:32:05.092: INFO: stdout: "update-demo-nautilus-9tj8c update-demo-nautilus-ldzw9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  3 22:32:10.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3918'
Sep  3 22:32:10.177: INFO: stderr: ""
Sep  3 22:32:10.177: INFO: stdout: "update-demo-nautilus-9tj8c update-demo-nautilus-ldzw9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  3 22:32:15.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3918'
Sep  3 22:32:15.252: INFO: stderr: ""
Sep  3 22:32:15.252: INFO: stdout: "update-demo-nautilus-ldzw9 "
Sep  3 22:32:15.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-ldzw9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:15.328: INFO: stderr: ""
Sep  3 22:32:15.328: INFO: stdout: "true"
Sep  3 22:32:15.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-ldzw9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:15.401: INFO: stderr: ""
Sep  3 22:32:15.401: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  3 22:32:15.401: INFO: validating pod update-demo-nautilus-ldzw9
Sep  3 22:32:15.406: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 22:32:15.406: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 22:32:15.406: INFO: update-demo-nautilus-ldzw9 is verified up and running
STEP: scaling up the replication controller
Sep  3 22:32:15.407: INFO: scanned /root for discovery docs: <nil>
Sep  3 22:32:15.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3918'
Sep  3 22:32:16.510: INFO: stderr: ""
Sep  3 22:32:16.510: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  3 22:32:16.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3918'
Sep  3 22:32:16.589: INFO: stderr: ""
Sep  3 22:32:16.589: INFO: stdout: "update-demo-nautilus-ldzw9 update-demo-nautilus-wkdnq "
Sep  3 22:32:16.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-ldzw9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:16.674: INFO: stderr: ""
Sep  3 22:32:16.674: INFO: stdout: "true"
Sep  3 22:32:16.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-ldzw9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:16.747: INFO: stderr: ""
Sep  3 22:32:16.748: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  3 22:32:16.748: INFO: validating pod update-demo-nautilus-ldzw9
Sep  3 22:32:16.760: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 22:32:16.760: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 22:32:16.760: INFO: update-demo-nautilus-ldzw9 is verified up and running
Sep  3 22:32:16.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-wkdnq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:16.844: INFO: stderr: ""
Sep  3 22:32:16.845: INFO: stdout: ""
Sep  3 22:32:16.845: INFO: update-demo-nautilus-wkdnq is created but not running
Sep  3 22:32:21.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3918'
Sep  3 22:32:21.919: INFO: stderr: ""
Sep  3 22:32:21.919: INFO: stdout: "update-demo-nautilus-ldzw9 update-demo-nautilus-wkdnq "
Sep  3 22:32:21.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-ldzw9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:21.995: INFO: stderr: ""
Sep  3 22:32:21.995: INFO: stdout: "true"
Sep  3 22:32:21.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-ldzw9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:22.073: INFO: stderr: ""
Sep  3 22:32:22.073: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  3 22:32:22.073: INFO: validating pod update-demo-nautilus-ldzw9
Sep  3 22:32:22.077: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 22:32:22.077: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 22:32:22.077: INFO: update-demo-nautilus-ldzw9 is verified up and running
Sep  3 22:32:22.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-wkdnq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:22.151: INFO: stderr: ""
Sep  3 22:32:22.151: INFO: stdout: "true"
Sep  3 22:32:22.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods update-demo-nautilus-wkdnq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3918'
Sep  3 22:32:22.223: INFO: stderr: ""
Sep  3 22:32:22.223: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  3 22:32:22.223: INFO: validating pod update-demo-nautilus-wkdnq
Sep  3 22:32:22.227: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  3 22:32:22.227: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  3 22:32:22.227: INFO: update-demo-nautilus-wkdnq is verified up and running
STEP: using delete to clean up resources
Sep  3 22:32:22.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete --grace-period=0 --force -f - --namespace=kubectl-3918'
Sep  3 22:32:22.312: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 22:32:22.312: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  3 22:32:22.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3918'
Sep  3 22:32:22.396: INFO: stderr: "No resources found in kubectl-3918 namespace.\n"
Sep  3 22:32:22.396: INFO: stdout: ""
Sep  3 22:32:22.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -l name=update-demo --namespace=kubectl-3918 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  3 22:32:22.474: INFO: stderr: ""
Sep  3 22:32:22.474: INFO: stdout: "update-demo-nautilus-ldzw9\nupdate-demo-nautilus-wkdnq\n"
Sep  3 22:32:22.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3918'
Sep  3 22:32:23.077: INFO: stderr: "No resources found in kubectl-3918 namespace.\n"
Sep  3 22:32:23.077: INFO: stdout: ""
Sep  3 22:32:23.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pods -l name=update-demo --namespace=kubectl-3918 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  3 22:32:23.151: INFO: stderr: ""
Sep  3 22:32:23.151: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:32:23.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3918" for this suite.

• [SLOW TEST:25.103 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":277,"completed":77,"skipped":1341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:32:23.161: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  3 22:32:26.217: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:32:26.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2769" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":78,"skipped":1381,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:32:26.245: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:32:26.960: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:32:29.982: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:32:40.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4535" for this suite.
STEP: Destroying namespace "webhook-4535-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.943 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":277,"completed":79,"skipped":1396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:32:40.188: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-8056
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8056 to expose endpoints map[]
Sep  3 22:32:40.235: INFO: successfully validated that service endpoint-test2 in namespace services-8056 exposes endpoints map[] (5.89161ms elapsed)
STEP: Creating pod pod1 in namespace services-8056
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8056 to expose endpoints map[pod1:[80]]
Sep  3 22:32:42.267: INFO: successfully validated that service endpoint-test2 in namespace services-8056 exposes endpoints map[pod1:[80]] (2.019959692s elapsed)
STEP: Creating pod pod2 in namespace services-8056
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8056 to expose endpoints map[pod1:[80] pod2:[80]]
Sep  3 22:32:46.322: INFO: Unexpected endpoints: found map[c1bd2c13-8940-4a4f-aa87-0de530ecfa17:[80]], expected map[pod1:[80] pod2:[80]] (4.048938899s elapsed, will retry)
Sep  3 22:32:51.364: INFO: Unexpected endpoints: found map[c1bd2c13-8940-4a4f-aa87-0de530ecfa17:[80]], expected map[pod1:[80] pod2:[80]] (9.090777749s elapsed, will retry)
Sep  3 22:32:55.401: INFO: successfully validated that service endpoint-test2 in namespace services-8056 exposes endpoints map[pod1:[80] pod2:[80]] (13.127366906s elapsed)
STEP: Deleting pod pod1 in namespace services-8056
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8056 to expose endpoints map[pod2:[80]]
Sep  3 22:32:56.419: INFO: successfully validated that service endpoint-test2 in namespace services-8056 exposes endpoints map[pod2:[80]] (1.011598681s elapsed)
STEP: Deleting pod pod2 in namespace services-8056
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8056 to expose endpoints map[]
Sep  3 22:32:57.434: INFO: successfully validated that service endpoint-test2 in namespace services-8056 exposes endpoints map[] (1.006447963s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:32:57.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8056" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:17.281 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":277,"completed":80,"skipped":1419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:32:57.469: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep  3 22:32:57.502: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Sep  3 22:32:57.880: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  3 22:32:59.923: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:33:01.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:33:03.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:33:05.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:33:07.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:33:09.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:33:11.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769177, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:33:14.656: INFO: Waited 720.74806ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:33:15.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7828" for this suite.

• [SLOW TEST:17.830 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":277,"completed":81,"skipped":1443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:33:15.299: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-2490/configmap-test-067734f1-ffb1-49b9-aa88-10b488d9eaa5
STEP: Creating a pod to test consume configMaps
Sep  3 22:33:15.353: INFO: Waiting up to 5m0s for pod "pod-configmaps-41daa86d-ac8f-419d-b237-a8bce1a2bf69" in namespace "configmap-2490" to be "Succeeded or Failed"
Sep  3 22:33:15.357: INFO: Pod "pod-configmaps-41daa86d-ac8f-419d-b237-a8bce1a2bf69": Phase="Pending", Reason="", readiness=false. Elapsed: 3.286875ms
Sep  3 22:33:17.360: INFO: Pod "pod-configmaps-41daa86d-ac8f-419d-b237-a8bce1a2bf69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006827729s
STEP: Saw pod success
Sep  3 22:33:17.360: INFO: Pod "pod-configmaps-41daa86d-ac8f-419d-b237-a8bce1a2bf69" satisfied condition "Succeeded or Failed"
Sep  3 22:33:17.363: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-configmaps-41daa86d-ac8f-419d-b237-a8bce1a2bf69 container env-test: <nil>
STEP: delete the pod
Sep  3 22:33:17.389: INFO: Waiting for pod pod-configmaps-41daa86d-ac8f-419d-b237-a8bce1a2bf69 to disappear
Sep  3 22:33:17.391: INFO: Pod pod-configmaps-41daa86d-ac8f-419d-b237-a8bce1a2bf69 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:33:17.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2490" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":277,"completed":82,"skipped":1470,"failed":0}

------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:33:17.401: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep  3 22:33:17.433: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:33:20.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4026" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":277,"completed":83,"skipped":1470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:33:20.492: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1532
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1532
I0903 22:33:20.567058      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1532, replica count: 2
Sep  3 22:33:23.617: INFO: Creating new exec pod
I0903 22:33:23.617586      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 22:33:28.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-1532 execpod4qxlr -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep  3 22:33:28.857: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  3 22:33:28.857: INFO: stdout: ""
Sep  3 22:33:28.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-1532 execpod4qxlr -- /bin/sh -x -c nc -zv -t -w 2 172.19.96.190 80'
Sep  3 22:33:29.072: INFO: stderr: "+ nc -zv -t -w 2 172.19.96.190 80\nConnection to 172.19.96.190 80 port [tcp/http] succeeded!\n"
Sep  3 22:33:29.072: INFO: stdout: ""
Sep  3 22:33:29.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-1532 execpod4qxlr -- /bin/sh -x -c nc -zv -t -w 2 10.45.43.112 30452'
Sep  3 22:33:29.280: INFO: stderr: "+ nc -zv -t -w 2 10.45.43.112 30452\nConnection to 10.45.43.112 30452 port [tcp/30452] succeeded!\n"
Sep  3 22:33:29.280: INFO: stdout: ""
Sep  3 22:33:29.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-1532 execpod4qxlr -- /bin/sh -x -c nc -zv -t -w 2 10.45.43.111 30452'
Sep  3 22:33:29.498: INFO: stderr: "+ nc -zv -t -w 2 10.45.43.111 30452\nConnection to 10.45.43.111 30452 port [tcp/30452] succeeded!\n"
Sep  3 22:33:29.499: INFO: stdout: ""
Sep  3 22:33:29.499: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:33:29.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1532" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.047 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":277,"completed":84,"skipped":1496,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:33:29.539: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-0d585159-08fe-4c9a-bfc2-2e0c7ea28ea8
STEP: Creating a pod to test consume secrets
Sep  3 22:33:29.589: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-75e08fe2-80a0-40b2-9147-d7424a7f93ff" in namespace "projected-8264" to be "Succeeded or Failed"
Sep  3 22:33:29.592: INFO: Pod "pod-projected-secrets-75e08fe2-80a0-40b2-9147-d7424a7f93ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433242ms
Sep  3 22:33:31.595: INFO: Pod "pod-projected-secrets-75e08fe2-80a0-40b2-9147-d7424a7f93ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00616386s
STEP: Saw pod success
Sep  3 22:33:31.595: INFO: Pod "pod-projected-secrets-75e08fe2-80a0-40b2-9147-d7424a7f93ff" satisfied condition "Succeeded or Failed"
Sep  3 22:33:31.598: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-secrets-75e08fe2-80a0-40b2-9147-d7424a7f93ff container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  3 22:33:31.619: INFO: Waiting for pod pod-projected-secrets-75e08fe2-80a0-40b2-9147-d7424a7f93ff to disappear
Sep  3 22:33:31.622: INFO: Pod pod-projected-secrets-75e08fe2-80a0-40b2-9147-d7424a7f93ff no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:33:31.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8264" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":85,"skipped":1511,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:33:31.633: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:33:32.895: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  3 22:33:34.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769212, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769212, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769212, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769212, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:33:37.923: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:33:37.927: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9285-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:33:39.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6519" for this suite.
STEP: Destroying namespace "webhook-6519-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.580 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":277,"completed":86,"skipped":1530,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:33:39.214: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Sep  3 22:33:39.258: INFO: Waiting up to 5m0s for pod "client-containers-65977f37-0b6f-47c7-b5c6-7ad6b933274c" in namespace "containers-3003" to be "Succeeded or Failed"
Sep  3 22:33:39.261: INFO: Pod "client-containers-65977f37-0b6f-47c7-b5c6-7ad6b933274c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.874634ms
Sep  3 22:33:41.265: INFO: Pod "client-containers-65977f37-0b6f-47c7-b5c6-7ad6b933274c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006368383s
STEP: Saw pod success
Sep  3 22:33:41.265: INFO: Pod "client-containers-65977f37-0b6f-47c7-b5c6-7ad6b933274c" satisfied condition "Succeeded or Failed"
Sep  3 22:33:41.267: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod client-containers-65977f37-0b6f-47c7-b5c6-7ad6b933274c container test-container: <nil>
STEP: delete the pod
Sep  3 22:33:41.290: INFO: Waiting for pod client-containers-65977f37-0b6f-47c7-b5c6-7ad6b933274c to disappear
Sep  3 22:33:41.293: INFO: Pod client-containers-65977f37-0b6f-47c7-b5c6-7ad6b933274c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:33:41.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3003" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":277,"completed":87,"skipped":1550,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:33:41.303: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-4762
STEP: creating replication controller nodeport-test in namespace services-4762
I0903 22:33:41.364145      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-4762, replica count: 2
Sep  3 22:33:44.414: INFO: Creating new exec pod
I0903 22:33:44.414783      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 22:33:49.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-4762 execpod6lfgf -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep  3 22:33:49.634: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  3 22:33:49.634: INFO: stdout: ""
Sep  3 22:33:49.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-4762 execpod6lfgf -- /bin/sh -x -c nc -zv -t -w 2 172.19.167.140 80'
Sep  3 22:33:49.844: INFO: stderr: "+ nc -zv -t -w 2 172.19.167.140 80\nConnection to 172.19.167.140 80 port [tcp/http] succeeded!\n"
Sep  3 22:33:49.844: INFO: stdout: ""
Sep  3 22:33:49.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-4762 execpod6lfgf -- /bin/sh -x -c nc -zv -t -w 2 10.45.43.111 30421'
Sep  3 22:33:50.049: INFO: stderr: "+ nc -zv -t -w 2 10.45.43.111 30421\nConnection to 10.45.43.111 30421 port [tcp/30421] succeeded!\n"
Sep  3 22:33:50.049: INFO: stdout: ""
Sep  3 22:33:50.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-4762 execpod6lfgf -- /bin/sh -x -c nc -zv -t -w 2 10.45.43.119 30421'
Sep  3 22:33:50.261: INFO: stderr: "+ nc -zv -t -w 2 10.45.43.119 30421\nConnection to 10.45.43.119 30421 port [tcp/30421] succeeded!\n"
Sep  3 22:33:50.261: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:33:50.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4762" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:8.967 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":277,"completed":88,"skipped":1556,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:33:50.270: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:33:51.030: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep  3 22:33:53.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769231, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769231, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769231, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769231, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:33:56.055: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:33:56.059: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:33:57.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9826" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.119 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":277,"completed":89,"skipped":1586,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:33:57.389: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep  3 22:33:59.970: INFO: Successfully updated pod "annotationupdate22169ecc-9560-43ba-8a1f-9d67ba0f1043"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:34:01.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1307" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":90,"skipped":1589,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:34:01.998: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:34:02.032: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep  3 22:34:04.072: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:34:04.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2095" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":277,"completed":91,"skipped":1590,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:34:04.092: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:34:04.135: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  3 22:34:09.138: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  3 22:34:09.138: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  3 22:34:11.143: INFO: Creating deployment "test-rollover-deployment"
Sep  3 22:34:11.151: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  3 22:34:13.157: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  3 22:34:13.161: INFO: Ensure that both replica sets have 1 created replica
Sep  3 22:34:13.167: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  3 22:34:13.175: INFO: Updating deployment test-rollover-deployment
Sep  3 22:34:13.175: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  3 22:34:15.182: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  3 22:34:15.188: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  3 22:34:15.193: INFO: all replica sets need to contain the pod-template-hash label
Sep  3 22:34:15.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769254, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:34:17.200: INFO: all replica sets need to contain the pod-template-hash label
Sep  3 22:34:17.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769254, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:34:19.201: INFO: all replica sets need to contain the pod-template-hash label
Sep  3 22:34:19.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769254, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:34:21.201: INFO: all replica sets need to contain the pod-template-hash label
Sep  3 22:34:21.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769254, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:34:23.201: INFO: all replica sets need to contain the pod-template-hash label
Sep  3 22:34:23.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769254, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769251, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  3 22:34:25.201: INFO: 
Sep  3 22:34:25.201: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep  3 22:34:25.210: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-777 /apis/apps/v1/namespaces/deployment-777/deployments/test-rollover-deployment 32b88c15-64f6-4bcb-bb92-46b74dc72c21 16656 2 2020-09-03 22:34:11 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-09-03 22:34:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-03 22:34:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031fc4d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-09-03 22:34:11 +0000 UTC,LastTransitionTime:2020-09-03 22:34:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2020-09-03 22:34:24 +0000 UTC,LastTransitionTime:2020-09-03 22:34:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  3 22:34:25.213: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-777 /apis/apps/v1/namespaces/deployment-777/replicasets/test-rollover-deployment-84f7f6f64b f001ea61-8817-4257-b79e-34d4d280bcef 16646 2 2020-09-03 22:34:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 32b88c15-64f6-4bcb-bb92-46b74dc72c21 0xc0031fcb47 0xc0031fcb48}] []  [{kube-controller-manager Update apps/v1 2020-09-03 22:34:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 50 98 56 56 99 49 53 45 54 52 102 54 45 52 98 99 98 45 98 98 57 50 45 52 54 98 55 52 100 99 55 50 99 50 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031fcbd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  3 22:34:25.213: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  3 22:34:25.213: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-777 /apis/apps/v1/namespaces/deployment-777/replicasets/test-rollover-controller 4a8a4a00-7df4-4510-ae56-0036fb7580e7 16655 2 2020-09-03 22:34:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 32b88c15-64f6-4bcb-bb92-46b74dc72c21 0xc0031fc937 0xc0031fc938}] []  [{e2e.test Update apps/v1 2020-09-03 22:34:04 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-03 22:34:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 50 98 56 56 99 49 53 45 54 52 102 54 45 52 98 99 98 45 98 98 57 50 45 52 54 98 55 52 100 99 55 50 99 50 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0031fc9d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 22:34:25.214: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-777 /apis/apps/v1/namespaces/deployment-777/replicasets/test-rollover-deployment-5686c4cfd5 32a5b492-1f6c-4fdb-8d53-5df5aea1e8f0 16593 2 2020-09-03 22:34:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 32b88c15-64f6-4bcb-bb92-46b74dc72c21 0xc0031fca47 0xc0031fca48}] []  [{kube-controller-manager Update apps/v1 2020-09-03 22:34:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 50 98 56 56 99 49 53 45 54 52 102 54 45 52 98 99 98 45 98 98 57 50 45 52 54 98 55 52 100 99 55 50 99 50 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 114 101 100 105 115 45 115 108 97 118 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031fcad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 22:34:25.217: INFO: Pod "test-rollover-deployment-84f7f6f64b-hsxjb" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-hsxjb test-rollover-deployment-84f7f6f64b- deployment-777 /api/v1/namespaces/deployment-777/pods/test-rollover-deployment-84f7f6f64b-hsxjb d1f92c6c-f57a-407f-ad2f-b6e3dc88f31a 16610 0 2020-09-03 22:34:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b f001ea61-8817-4257-b79e-34d4d280bcef 0xc0031fd1a7 0xc0031fd1a8}] []  [{kube-controller-manager Update v1 2020-09-03 22:34:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 48 48 49 101 97 54 49 45 56 56 49 55 45 52 50 53 55 45 98 55 57 101 45 51 52 100 52 100 50 56 48 98 99 101 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:34:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 50 46 49 50 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nskp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nskp8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nskp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:34:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:34:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:172.20.2.127,StartTime:2020-09-03 22:34:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 22:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://b9bca11b6ca2f95565a2f29476d421076849e152207fd9503a55bf7d34eb9370,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.2.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:34:25.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-777" for this suite.

• [SLOW TEST:21.133 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":277,"completed":92,"skipped":1605,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:34:25.226: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-f24375e4-835c-4829-8980-c37379d28811
STEP: Creating a pod to test consume secrets
Sep  3 22:34:25.268: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b8b660dd-3e7a-4d8c-bff1-a4e93fc291d7" in namespace "projected-8045" to be "Succeeded or Failed"
Sep  3 22:34:25.271: INFO: Pod "pod-projected-secrets-b8b660dd-3e7a-4d8c-bff1-a4e93fc291d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.078391ms
Sep  3 22:34:27.275: INFO: Pod "pod-projected-secrets-b8b660dd-3e7a-4d8c-bff1-a4e93fc291d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006390316s
STEP: Saw pod success
Sep  3 22:34:27.275: INFO: Pod "pod-projected-secrets-b8b660dd-3e7a-4d8c-bff1-a4e93fc291d7" satisfied condition "Succeeded or Failed"
Sep  3 22:34:27.277: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-secrets-b8b660dd-3e7a-4d8c-bff1-a4e93fc291d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  3 22:34:27.298: INFO: Waiting for pod pod-projected-secrets-b8b660dd-3e7a-4d8c-bff1-a4e93fc291d7 to disappear
Sep  3 22:34:27.301: INFO: Pod pod-projected-secrets-b8b660dd-3e7a-4d8c-bff1-a4e93fc291d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:34:27.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8045" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":93,"skipped":1613,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:34:27.311: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep  3 22:34:27.353: INFO: Waiting up to 5m0s for pod "downward-api-fe46e834-0f85-414e-bf4f-0142dc3a8d4e" in namespace "downward-api-6061" to be "Succeeded or Failed"
Sep  3 22:34:27.356: INFO: Pod "downward-api-fe46e834-0f85-414e-bf4f-0142dc3a8d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.681607ms
Sep  3 22:34:29.360: INFO: Pod "downward-api-fe46e834-0f85-414e-bf4f-0142dc3a8d4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006914128s
STEP: Saw pod success
Sep  3 22:34:29.360: INFO: Pod "downward-api-fe46e834-0f85-414e-bf4f-0142dc3a8d4e" satisfied condition "Succeeded or Failed"
Sep  3 22:34:29.363: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downward-api-fe46e834-0f85-414e-bf4f-0142dc3a8d4e container dapi-container: <nil>
STEP: delete the pod
Sep  3 22:34:29.386: INFO: Waiting for pod downward-api-fe46e834-0f85-414e-bf4f-0142dc3a8d4e to disappear
Sep  3 22:34:29.388: INFO: Pod downward-api-fe46e834-0f85-414e-bf4f-0142dc3a8d4e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:34:29.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6061" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":277,"completed":94,"skipped":1616,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:34:29.406: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:34:45.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6643" for this suite.

• [SLOW TEST:16.139 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":277,"completed":95,"skipped":1620,"failed":0}
SSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:34:45.545: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep  3 22:34:45.579: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep  3 22:34:45.586: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  3 22:34:45.586: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep  3 22:34:45.594: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  3 22:34:45.594: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep  3 22:34:45.605: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  3 22:34:45.605: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep  3 22:34:52.655: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:34:52.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6924" for this suite.

• [SLOW TEST:7.130 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":277,"completed":96,"skipped":1623,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:34:52.676: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:34:52.755: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d949fb13-d737-46de-8e68-feb38c4b58be" in namespace "downward-api-2672" to be "Succeeded or Failed"
Sep  3 22:34:52.764: INFO: Pod "downwardapi-volume-d949fb13-d737-46de-8e68-feb38c4b58be": Phase="Pending", Reason="", readiness=false. Elapsed: 9.675311ms
Sep  3 22:34:54.768: INFO: Pod "downwardapi-volume-d949fb13-d737-46de-8e68-feb38c4b58be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013056769s
STEP: Saw pod success
Sep  3 22:34:54.768: INFO: Pod "downwardapi-volume-d949fb13-d737-46de-8e68-feb38c4b58be" satisfied condition "Succeeded or Failed"
Sep  3 22:34:54.769: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-d949fb13-d737-46de-8e68-feb38c4b58be container client-container: <nil>
STEP: delete the pod
Sep  3 22:34:54.793: INFO: Waiting for pod downwardapi-volume-d949fb13-d737-46de-8e68-feb38c4b58be to disappear
Sep  3 22:34:54.795: INFO: Pod downwardapi-volume-d949fb13-d737-46de-8e68-feb38c4b58be no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:34:54.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2672" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":97,"skipped":1632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:34:54.805: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:34:55.511: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:34:58.532: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:34:58.536: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:34:59.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7682" for this suite.
STEP: Destroying namespace "webhook-7682-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":277,"completed":98,"skipped":1707,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:34:59.752: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:35:00.779: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  3 22:35:02.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769300, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769300, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769300, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769300, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:35:05.803: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:35:17.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7754" for this suite.
STEP: Destroying namespace "webhook-7754-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.272 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":277,"completed":99,"skipped":1723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:35:18.025: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-a53cf40f-a841-44e3-bf5b-25405bdb2f9a
STEP: Creating a pod to test consume configMaps
Sep  3 22:35:18.072: INFO: Waiting up to 5m0s for pod "pod-configmaps-42753c97-964d-4aec-a0e8-a7257a73cb4b" in namespace "configmap-2650" to be "Succeeded or Failed"
Sep  3 22:35:18.077: INFO: Pod "pod-configmaps-42753c97-964d-4aec-a0e8-a7257a73cb4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.335798ms
Sep  3 22:35:20.082: INFO: Pod "pod-configmaps-42753c97-964d-4aec-a0e8-a7257a73cb4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009801685s
Sep  3 22:35:22.086: INFO: Pod "pod-configmaps-42753c97-964d-4aec-a0e8-a7257a73cb4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013729599s
STEP: Saw pod success
Sep  3 22:35:22.086: INFO: Pod "pod-configmaps-42753c97-964d-4aec-a0e8-a7257a73cb4b" satisfied condition "Succeeded or Failed"
Sep  3 22:35:22.088: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-configmaps-42753c97-964d-4aec-a0e8-a7257a73cb4b container configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:35:22.110: INFO: Waiting for pod pod-configmaps-42753c97-964d-4aec-a0e8-a7257a73cb4b to disappear
Sep  3 22:35:22.113: INFO: Pod pod-configmaps-42753c97-964d-4aec-a0e8-a7257a73cb4b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:35:22.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2650" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":100,"skipped":1774,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:35:22.122: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:35:22.173: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1541db1a-9132-42d8-8854-ba5ca3075097" in namespace "projected-4460" to be "Succeeded or Failed"
Sep  3 22:35:22.176: INFO: Pod "downwardapi-volume-1541db1a-9132-42d8-8854-ba5ca3075097": Phase="Pending", Reason="", readiness=false. Elapsed: 3.43888ms
Sep  3 22:35:24.180: INFO: Pod "downwardapi-volume-1541db1a-9132-42d8-8854-ba5ca3075097": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007163172s
STEP: Saw pod success
Sep  3 22:35:24.180: INFO: Pod "downwardapi-volume-1541db1a-9132-42d8-8854-ba5ca3075097" satisfied condition "Succeeded or Failed"
Sep  3 22:35:24.182: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-1541db1a-9132-42d8-8854-ba5ca3075097 container client-container: <nil>
STEP: delete the pod
Sep  3 22:35:24.203: INFO: Waiting for pod downwardapi-volume-1541db1a-9132-42d8-8854-ba5ca3075097 to disappear
Sep  3 22:35:24.205: INFO: Pod downwardapi-volume-1541db1a-9132-42d8-8854-ba5ca3075097 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:35:24.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4460" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":101,"skipped":1776,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:35:24.217: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep  3 22:35:28.773: INFO: Successfully updated pod "adopt-release-4xtjn"
STEP: Checking that the Job readopts the Pod
Sep  3 22:35:28.773: INFO: Waiting up to 15m0s for pod "adopt-release-4xtjn" in namespace "job-1694" to be "adopted"
Sep  3 22:35:28.776: INFO: Pod "adopt-release-4xtjn": Phase="Running", Reason="", readiness=true. Elapsed: 3.020345ms
Sep  3 22:35:30.780: INFO: Pod "adopt-release-4xtjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.007220017s
Sep  3 22:35:30.780: INFO: Pod "adopt-release-4xtjn" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep  3 22:35:31.293: INFO: Successfully updated pod "adopt-release-4xtjn"
STEP: Checking that the Job releases the Pod
Sep  3 22:35:31.293: INFO: Waiting up to 15m0s for pod "adopt-release-4xtjn" in namespace "job-1694" to be "released"
Sep  3 22:35:31.296: INFO: Pod "adopt-release-4xtjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.864969ms
Sep  3 22:35:33.299: INFO: Pod "adopt-release-4xtjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.005666826s
Sep  3 22:35:33.299: INFO: Pod "adopt-release-4xtjn" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:35:33.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1694" for this suite.

• [SLOW TEST:9.090 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":277,"completed":102,"skipped":1787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:35:33.307: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:35:33.344: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1096705-8cd3-44e0-b2ab-aac8c2562933" in namespace "downward-api-8771" to be "Succeeded or Failed"
Sep  3 22:35:33.346: INFO: Pod "downwardapi-volume-d1096705-8cd3-44e0-b2ab-aac8c2562933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.306799ms
Sep  3 22:35:35.350: INFO: Pod "downwardapi-volume-d1096705-8cd3-44e0-b2ab-aac8c2562933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006226525s
STEP: Saw pod success
Sep  3 22:35:35.350: INFO: Pod "downwardapi-volume-d1096705-8cd3-44e0-b2ab-aac8c2562933" satisfied condition "Succeeded or Failed"
Sep  3 22:35:35.353: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-1 pod downwardapi-volume-d1096705-8cd3-44e0-b2ab-aac8c2562933 container client-container: <nil>
STEP: delete the pod
Sep  3 22:35:35.383: INFO: Waiting for pod downwardapi-volume-d1096705-8cd3-44e0-b2ab-aac8c2562933 to disappear
Sep  3 22:35:35.385: INFO: Pod downwardapi-volume-d1096705-8cd3-44e0-b2ab-aac8c2562933 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:35:35.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8771" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":103,"skipped":1836,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:35:35.394: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-4111
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-4111
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4111
Sep  3 22:35:35.439: INFO: Found 0 stateful pods, waiting for 1
Sep  3 22:35:45.443: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep  3 22:35:45.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-4111 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 22:35:45.661: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 22:35:45.661: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 22:35:45.661: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 22:35:45.665: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  3 22:35:55.669: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 22:35:55.669: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 22:35:55.686: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Sep  3 22:35:55.686: INFO: ss-0  karbon-test1186mm-f1ba59-k8s-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:35 +0000 UTC  }]
Sep  3 22:35:55.687: INFO: 
Sep  3 22:35:55.687: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  3 22:35:56.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995117553s
Sep  3 22:35:57.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988942146s
Sep  3 22:35:58.701: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984926641s
Sep  3 22:35:59.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980702656s
Sep  3 22:36:00.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977243087s
Sep  3 22:36:01.715: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972019569s
Sep  3 22:36:02.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966841448s
Sep  3 22:36:03.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962612175s
Sep  3 22:36:04.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.893521ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4111
Sep  3 22:36:05.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-4111 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 22:36:05.938: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 22:36:05.938: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 22:36:05.938: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 22:36:05.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-4111 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 22:36:06.160: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  3 22:36:06.160: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 22:36:06.160: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 22:36:06.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-4111 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 22:36:06.366: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  3 22:36:06.366: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 22:36:06.366: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 22:36:06.370: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 22:36:06.370: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 22:36:06.370: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep  3 22:36:06.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-4111 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 22:36:06.592: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 22:36:06.592: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 22:36:06.592: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 22:36:06.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-4111 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 22:36:06.811: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 22:36:06.811: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 22:36:06.812: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 22:36:06.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-4111 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 22:36:07.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 22:36:07.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 22:36:07.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 22:36:07.024: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 22:36:07.027: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  3 22:36:17.037: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 22:36:17.037: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 22:36:17.037: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 22:36:17.052: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Sep  3 22:36:17.052: INFO: ss-0  karbon-test1186mm-f1ba59-k8s-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:35 +0000 UTC  }]
Sep  3 22:36:17.052: INFO: ss-1  karbon-test1186mm-f1ba59-k8s-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:55 +0000 UTC  }]
Sep  3 22:36:17.052: INFO: ss-2  karbon-test1186mm-f1ba59-k8s-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:55 +0000 UTC  }]
Sep  3 22:36:17.052: INFO: 
Sep  3 22:36:17.052: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  3 22:36:18.056: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Sep  3 22:36:18.056: INFO: ss-0  karbon-test1186mm-f1ba59-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:35 +0000 UTC  }]
Sep  3 22:36:18.056: INFO: ss-1  karbon-test1186mm-f1ba59-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:55 +0000 UTC  }]
Sep  3 22:36:18.056: INFO: ss-2  karbon-test1186mm-f1ba59-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:55 +0000 UTC  }]
Sep  3 22:36:18.056: INFO: 
Sep  3 22:36:18.056: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  3 22:36:19.061: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Sep  3 22:36:19.061: INFO: ss-0  karbon-test1186mm-f1ba59-k8s-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:35 +0000 UTC  }]
Sep  3 22:36:19.061: INFO: ss-1  karbon-test1186mm-f1ba59-k8s-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:36:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-03 22:35:55 +0000 UTC  }]
Sep  3 22:36:19.061: INFO: 
Sep  3 22:36:19.061: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  3 22:36:20.065: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986972554s
Sep  3 22:36:21.069: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.983451407s
Sep  3 22:36:22.072: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.979237385s
Sep  3 22:36:23.076: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.975634599s
Sep  3 22:36:24.080: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.971916428s
Sep  3 22:36:25.085: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.967734781s
Sep  3 22:36:26.089: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.576365ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4111
Sep  3 22:36:27.094: INFO: Scaling statefulset ss to 0
Sep  3 22:36:27.103: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep  3 22:36:27.105: INFO: Deleting all statefulset in ns statefulset-4111
Sep  3 22:36:27.107: INFO: Scaling statefulset ss to 0
Sep  3 22:36:27.115: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 22:36:27.117: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:36:27.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4111" for this suite.

• [SLOW TEST:51.749 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":277,"completed":104,"skipped":1856,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:36:27.143: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1639.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1639.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1639.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1639.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1639.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1639.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  3 22:36:29.239: INFO: DNS probes using dns-1639/dns-test-8e2eac13-4cbe-41e6-abff-995e53589f6f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:36:29.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1639" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":277,"completed":105,"skipped":1869,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:36:29.286: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep  3 22:36:29.348: INFO: Waiting up to 5m0s for pod "downward-api-82cbf236-9913-4799-a8e8-957d3c881489" in namespace "downward-api-1292" to be "Succeeded or Failed"
Sep  3 22:36:29.354: INFO: Pod "downward-api-82cbf236-9913-4799-a8e8-957d3c881489": Phase="Pending", Reason="", readiness=false. Elapsed: 6.414729ms
Sep  3 22:36:31.358: INFO: Pod "downward-api-82cbf236-9913-4799-a8e8-957d3c881489": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010164404s
STEP: Saw pod success
Sep  3 22:36:31.358: INFO: Pod "downward-api-82cbf236-9913-4799-a8e8-957d3c881489" satisfied condition "Succeeded or Failed"
Sep  3 22:36:31.363: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downward-api-82cbf236-9913-4799-a8e8-957d3c881489 container dapi-container: <nil>
STEP: delete the pod
Sep  3 22:36:31.385: INFO: Waiting for pod downward-api-82cbf236-9913-4799-a8e8-957d3c881489 to disappear
Sep  3 22:36:31.389: INFO: Pod downward-api-82cbf236-9913-4799-a8e8-957d3c881489 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:36:31.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1292" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":277,"completed":106,"skipped":1889,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:36:31.402: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:36:31.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4021192-cdda-40ce-9cdc-6e087b0f8e73" in namespace "projected-8248" to be "Succeeded or Failed"
Sep  3 22:36:31.451: INFO: Pod "downwardapi-volume-e4021192-cdda-40ce-9cdc-6e087b0f8e73": Phase="Pending", Reason="", readiness=false. Elapsed: 3.630105ms
Sep  3 22:36:33.456: INFO: Pod "downwardapi-volume-e4021192-cdda-40ce-9cdc-6e087b0f8e73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007930625s
STEP: Saw pod success
Sep  3 22:36:33.456: INFO: Pod "downwardapi-volume-e4021192-cdda-40ce-9cdc-6e087b0f8e73" satisfied condition "Succeeded or Failed"
Sep  3 22:36:33.459: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-e4021192-cdda-40ce-9cdc-6e087b0f8e73 container client-container: <nil>
STEP: delete the pod
Sep  3 22:36:33.484: INFO: Waiting for pod downwardapi-volume-e4021192-cdda-40ce-9cdc-6e087b0f8e73 to disappear
Sep  3 22:36:33.487: INFO: Pod downwardapi-volume-e4021192-cdda-40ce-9cdc-6e087b0f8e73 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:36:33.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8248" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":107,"skipped":1895,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:36:33.500: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  3 22:36:33.534: INFO: Waiting up to 5m0s for pod "pod-90da9236-2a94-406e-888c-94c4911cfb11" in namespace "emptydir-7312" to be "Succeeded or Failed"
Sep  3 22:36:33.537: INFO: Pod "pod-90da9236-2a94-406e-888c-94c4911cfb11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.566567ms
Sep  3 22:36:35.541: INFO: Pod "pod-90da9236-2a94-406e-888c-94c4911cfb11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006841245s
STEP: Saw pod success
Sep  3 22:36:35.541: INFO: Pod "pod-90da9236-2a94-406e-888c-94c4911cfb11" satisfied condition "Succeeded or Failed"
Sep  3 22:36:35.543: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-90da9236-2a94-406e-888c-94c4911cfb11 container test-container: <nil>
STEP: delete the pod
Sep  3 22:36:35.565: INFO: Waiting for pod pod-90da9236-2a94-406e-888c-94c4911cfb11 to disappear
Sep  3 22:36:35.570: INFO: Pod pod-90da9236-2a94-406e-888c-94c4911cfb11 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:36:35.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7312" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":108,"skipped":1900,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:36:35.579: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-a9004b64-8dc1-42f9-b370-a1e35e99918d
STEP: Creating a pod to test consume configMaps
Sep  3 22:36:35.624: INFO: Waiting up to 5m0s for pod "pod-configmaps-02b29955-b6a0-43bc-8ec2-77917e67debb" in namespace "configmap-4710" to be "Succeeded or Failed"
Sep  3 22:36:35.627: INFO: Pod "pod-configmaps-02b29955-b6a0-43bc-8ec2-77917e67debb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.809578ms
Sep  3 22:36:37.631: INFO: Pod "pod-configmaps-02b29955-b6a0-43bc-8ec2-77917e67debb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007607047s
STEP: Saw pod success
Sep  3 22:36:37.632: INFO: Pod "pod-configmaps-02b29955-b6a0-43bc-8ec2-77917e67debb" satisfied condition "Succeeded or Failed"
Sep  3 22:36:37.634: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-configmaps-02b29955-b6a0-43bc-8ec2-77917e67debb container configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:36:37.654: INFO: Waiting for pod pod-configmaps-02b29955-b6a0-43bc-8ec2-77917e67debb to disappear
Sep  3 22:36:37.656: INFO: Pod pod-configmaps-02b29955-b6a0-43bc-8ec2-77917e67debb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:36:37.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4710" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":109,"skipped":1903,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:36:37.666: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:36:38.502: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  3 22:36:40.511: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769398, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769398, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769398, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769398, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:36:43.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:36:43.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9926" for this suite.
STEP: Destroying namespace "webhook-9926-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.109 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":277,"completed":110,"skipped":1903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:36:43.777: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:36:54.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5278" for this suite.

• [SLOW TEST:11.083 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":277,"completed":111,"skipped":1990,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:36:54.859: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  3 22:36:59.433: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4e52d872-b67d-4ce0-a419-1d39cb177c7f"
Sep  3 22:36:59.433: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4e52d872-b67d-4ce0-a419-1d39cb177c7f" in namespace "pods-2831" to be "terminated due to deadline exceeded"
Sep  3 22:36:59.435: INFO: Pod "pod-update-activedeadlineseconds-4e52d872-b67d-4ce0-a419-1d39cb177c7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.319771ms
Sep  3 22:37:01.439: INFO: Pod "pod-update-activedeadlineseconds-4e52d872-b67d-4ce0-a419-1d39cb177c7f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.006299509s
Sep  3 22:37:01.439: INFO: Pod "pod-update-activedeadlineseconds-4e52d872-b67d-4ce0-a419-1d39cb177c7f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:37:01.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2831" for this suite.

• [SLOW TEST:6.595 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":277,"completed":112,"skipped":1994,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:37:01.455: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep  3 22:37:01.496: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-a 3ad7837c-a6c1-4449-9b07-9bccfdf95cbb 18198 0 2020-09-03 22:37:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:37:01.496: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-a 3ad7837c-a6c1-4449-9b07-9bccfdf95cbb 18198 0 2020-09-03 22:37:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep  3 22:37:11.506: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-a 3ad7837c-a6c1-4449-9b07-9bccfdf95cbb 18243 0 2020-09-03 22:37:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:37:11.506: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-a 3ad7837c-a6c1-4449-9b07-9bccfdf95cbb 18243 0 2020-09-03 22:37:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep  3 22:37:21.516: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-a 3ad7837c-a6c1-4449-9b07-9bccfdf95cbb 18271 0 2020-09-03 22:37:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:37:21.516: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-a 3ad7837c-a6c1-4449-9b07-9bccfdf95cbb 18271 0 2020-09-03 22:37:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep  3 22:37:31.526: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-a 3ad7837c-a6c1-4449-9b07-9bccfdf95cbb 18299 0 2020-09-03 22:37:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:37:31.526: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-a 3ad7837c-a6c1-4449-9b07-9bccfdf95cbb 18299 0 2020-09-03 22:37:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep  3 22:37:41.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-b 1ebc0dff-f56a-4584-a6d5-5c051b8f683b 18327 0 2020-09-03 22:37:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:37:41.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-b 1ebc0dff-f56a-4584-a6d5-5c051b8f683b 18327 0 2020-09-03 22:37:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep  3 22:37:51.544: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-b 1ebc0dff-f56a-4584-a6d5-5c051b8f683b 18356 0 2020-09-03 22:37:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:37:51.544: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9985 /api/v1/namespaces/watch-9985/configmaps/e2e-watch-test-configmap-b 1ebc0dff-f56a-4584-a6d5-5c051b8f683b 18356 0 2020-09-03 22:37:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-03 22:37:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:01.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9985" for this suite.

• [SLOW TEST:60.103 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":277,"completed":113,"skipped":2000,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:01.557: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:38:01.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe12386b-03c7-4e1a-bb54-f9569aab91a0" in namespace "downward-api-8672" to be "Succeeded or Failed"
Sep  3 22:38:01.609: INFO: Pod "downwardapi-volume-fe12386b-03c7-4e1a-bb54-f9569aab91a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.809334ms
Sep  3 22:38:03.613: INFO: Pod "downwardapi-volume-fe12386b-03c7-4e1a-bb54-f9569aab91a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00762709s
STEP: Saw pod success
Sep  3 22:38:03.613: INFO: Pod "downwardapi-volume-fe12386b-03c7-4e1a-bb54-f9569aab91a0" satisfied condition "Succeeded or Failed"
Sep  3 22:38:03.616: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-fe12386b-03c7-4e1a-bb54-f9569aab91a0 container client-container: <nil>
STEP: delete the pod
Sep  3 22:38:03.638: INFO: Waiting for pod downwardapi-volume-fe12386b-03c7-4e1a-bb54-f9569aab91a0 to disappear
Sep  3 22:38:03.640: INFO: Pod downwardapi-volume-fe12386b-03c7-4e1a-bb54-f9569aab91a0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:03.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8672" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":277,"completed":114,"skipped":2012,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:03.652: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-332.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-332.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  3 22:38:07.733: INFO: DNS probes using dns-332/dns-test-95af4f52-189f-441e-a76d-21fcf1468ce3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:07.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-332" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":277,"completed":115,"skipped":2061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:07.754: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-cf411c9b-5ad2-44a2-bba7-0656c1946911
STEP: Creating a pod to test consume configMaps
Sep  3 22:38:07.815: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e0fe48e-abcc-4fc7-9121-d55b52a51c26" in namespace "projected-9414" to be "Succeeded or Failed"
Sep  3 22:38:07.818: INFO: Pod "pod-projected-configmaps-2e0fe48e-abcc-4fc7-9121-d55b52a51c26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238054ms
Sep  3 22:38:09.822: INFO: Pod "pod-projected-configmaps-2e0fe48e-abcc-4fc7-9121-d55b52a51c26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00667383s
STEP: Saw pod success
Sep  3 22:38:09.822: INFO: Pod "pod-projected-configmaps-2e0fe48e-abcc-4fc7-9121-d55b52a51c26" satisfied condition "Succeeded or Failed"
Sep  3 22:38:09.826: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-configmaps-2e0fe48e-abcc-4fc7-9121-d55b52a51c26 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:38:09.852: INFO: Waiting for pod pod-projected-configmaps-2e0fe48e-abcc-4fc7-9121-d55b52a51c26 to disappear
Sep  3 22:38:09.855: INFO: Pod pod-projected-configmaps-2e0fe48e-abcc-4fc7-9121-d55b52a51c26 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:09.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9414" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":116,"skipped":2124,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:09.864: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:09.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8890" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":277,"completed":117,"skipped":2130,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:09.942: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep  3 22:38:12.514: INFO: Successfully updated pod "labelsupdatedc50b426-640b-47a4-ab5d-5e382a5dc284"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:16.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5825" for this suite.

• [SLOW TEST:6.609 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":118,"skipped":2145,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:38:16.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-884ca495-70e9-4f59-8643-7359fbbae867" in namespace "downward-api-2090" to be "Succeeded or Failed"
Sep  3 22:38:16.594: INFO: Pod "downwardapi-volume-884ca495-70e9-4f59-8643-7359fbbae867": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349151ms
Sep  3 22:38:18.598: INFO: Pod "downwardapi-volume-884ca495-70e9-4f59-8643-7359fbbae867": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006416126s
STEP: Saw pod success
Sep  3 22:38:18.598: INFO: Pod "downwardapi-volume-884ca495-70e9-4f59-8643-7359fbbae867" satisfied condition "Succeeded or Failed"
Sep  3 22:38:18.601: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-884ca495-70e9-4f59-8643-7359fbbae867 container client-container: <nil>
STEP: delete the pod
Sep  3 22:38:18.622: INFO: Waiting for pod downwardapi-volume-884ca495-70e9-4f59-8643-7359fbbae867 to disappear
Sep  3 22:38:18.624: INFO: Pod downwardapi-volume-884ca495-70e9-4f59-8643-7359fbbae867 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:18.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2090" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":119,"skipped":2149,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:18.633: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep  3 22:38:18.676: INFO: Waiting up to 5m0s for pod "downward-api-0c123601-a1db-4ec2-a10a-2fb06fb15eea" in namespace "downward-api-9503" to be "Succeeded or Failed"
Sep  3 22:38:18.679: INFO: Pod "downward-api-0c123601-a1db-4ec2-a10a-2fb06fb15eea": Phase="Pending", Reason="", readiness=false. Elapsed: 3.450095ms
Sep  3 22:38:20.684: INFO: Pod "downward-api-0c123601-a1db-4ec2-a10a-2fb06fb15eea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00812573s
STEP: Saw pod success
Sep  3 22:38:20.684: INFO: Pod "downward-api-0c123601-a1db-4ec2-a10a-2fb06fb15eea" satisfied condition "Succeeded or Failed"
Sep  3 22:38:20.688: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downward-api-0c123601-a1db-4ec2-a10a-2fb06fb15eea container dapi-container: <nil>
STEP: delete the pod
Sep  3 22:38:20.710: INFO: Waiting for pod downward-api-0c123601-a1db-4ec2-a10a-2fb06fb15eea to disappear
Sep  3 22:38:20.713: INFO: Pod downward-api-0c123601-a1db-4ec2-a10a-2fb06fb15eea no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:20.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9503" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":277,"completed":120,"skipped":2154,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:20.727: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:38:21.493: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  3 22:38:23.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769501, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769501, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769501, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769501, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:38:26.521: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:38:26.525: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7451-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:27.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-975" for this suite.
STEP: Destroying namespace "webhook-975-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.931 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":277,"completed":121,"skipped":2156,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:27.658: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:38:27.716: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"53b0cc9b-8743-4b04-baec-c7263c6acb72", Controller:(*bool)(0xc003330c5a), BlockOwnerDeletion:(*bool)(0xc003330c5b)}}
Sep  3 22:38:27.725: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9696be09-b9f9-4e62-a9db-ca6970cf3dce", Controller:(*bool)(0xc003330e56), BlockOwnerDeletion:(*bool)(0xc003330e57)}}
Sep  3 22:38:27.738: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a9c860b2-f2e5-4db2-adfc-2af995d2dfbe", Controller:(*bool)(0xc0032bd566), BlockOwnerDeletion:(*bool)(0xc0032bd567)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:32.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1395" for this suite.

• [SLOW TEST:5.105 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":277,"completed":122,"skipped":2160,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:32.764: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep  3 22:38:35.834: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:36.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3144" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":277,"completed":123,"skipped":2178,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:36.864: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:38:36.901: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  3 22:38:40.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2249 create -f -'
Sep  3 22:38:40.998: INFO: stderr: ""
Sep  3 22:38:40.998: INFO: stdout: "e2e-test-crd-publish-openapi-2175-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  3 22:38:40.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2249 delete e2e-test-crd-publish-openapi-2175-crds test-cr'
Sep  3 22:38:41.086: INFO: stderr: ""
Sep  3 22:38:41.086: INFO: stdout: "e2e-test-crd-publish-openapi-2175-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  3 22:38:41.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2249 apply -f -'
Sep  3 22:38:41.250: INFO: stderr: ""
Sep  3 22:38:41.250: INFO: stdout: "e2e-test-crd-publish-openapi-2175-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  3 22:38:41.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2249 delete e2e-test-crd-publish-openapi-2175-crds test-cr'
Sep  3 22:38:41.337: INFO: stderr: ""
Sep  3 22:38:41.337: INFO: stdout: "e2e-test-crd-publish-openapi-2175-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep  3 22:38:41.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 explain e2e-test-crd-publish-openapi-2175-crds'
Sep  3 22:38:41.481: INFO: stderr: ""
Sep  3 22:38:41.481: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2175-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:45.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2249" for this suite.

• [SLOW TEST:8.183 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":277,"completed":124,"skipped":2188,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:45.047: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep  3 22:38:45.112: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3513 /api/v1/namespaces/watch-3513/configmaps/e2e-watch-test-resource-version 02c0458e-a64d-41a0-8a94-778cfa4a05ce 18942 0 2020-09-03 22:38:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-09-03 22:38:45 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  3 22:38:45.112: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3513 /api/v1/namespaces/watch-3513/configmaps/e2e-watch-test-resource-version 02c0458e-a64d-41a0-8a94-778cfa4a05ce 18943 0 2020-09-03 22:38:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-09-03 22:38:45 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:45.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3513" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":277,"completed":125,"skipped":2214,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:45.124: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-8738/configmap-test-15977e43-bfa4-474c-a523-afeb8418a448
STEP: Creating a pod to test consume configMaps
Sep  3 22:38:45.169: INFO: Waiting up to 5m0s for pod "pod-configmaps-c0937ced-8226-4968-8e0c-f3273dcd961f" in namespace "configmap-8738" to be "Succeeded or Failed"
Sep  3 22:38:45.172: INFO: Pod "pod-configmaps-c0937ced-8226-4968-8e0c-f3273dcd961f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.589516ms
Sep  3 22:38:47.175: INFO: Pod "pod-configmaps-c0937ced-8226-4968-8e0c-f3273dcd961f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005474346s
STEP: Saw pod success
Sep  3 22:38:47.175: INFO: Pod "pod-configmaps-c0937ced-8226-4968-8e0c-f3273dcd961f" satisfied condition "Succeeded or Failed"
Sep  3 22:38:47.176: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-configmaps-c0937ced-8226-4968-8e0c-f3273dcd961f container env-test: <nil>
STEP: delete the pod
Sep  3 22:38:47.195: INFO: Waiting for pod pod-configmaps-c0937ced-8226-4968-8e0c-f3273dcd961f to disappear
Sep  3 22:38:47.197: INFO: Pod pod-configmaps-c0937ced-8226-4968-8e0c-f3273dcd961f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:47.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8738" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":126,"skipped":2231,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:47.206: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:38:47.778: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:38:50.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:50.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6130" for this suite.
STEP: Destroying namespace "webhook-6130-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":277,"completed":127,"skipped":2237,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:50.900: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:157
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:50.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2797" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":277,"completed":128,"skipped":2241,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:50.958: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep  3 22:38:51.000: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  3 22:38:56.005: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:38:57.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8974" for this suite.

• [SLOW TEST:6.075 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":277,"completed":129,"skipped":2256,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:38:57.034: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep  3 22:38:59.652: INFO: Successfully updated pod "labelsupdatecec68a29-b1ca-4134-a09c-24efc07471d3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:39:01.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8451" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":130,"skipped":2272,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:39:01.678: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Sep  3 22:39:01.720: INFO: Waiting up to 5m0s for pod "var-expansion-ff0115e8-a941-4f46-931c-9640414bda60" in namespace "var-expansion-9966" to be "Succeeded or Failed"
Sep  3 22:39:01.724: INFO: Pod "var-expansion-ff0115e8-a941-4f46-931c-9640414bda60": Phase="Pending", Reason="", readiness=false. Elapsed: 3.702746ms
Sep  3 22:39:03.729: INFO: Pod "var-expansion-ff0115e8-a941-4f46-931c-9640414bda60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008415853s
STEP: Saw pod success
Sep  3 22:39:03.729: INFO: Pod "var-expansion-ff0115e8-a941-4f46-931c-9640414bda60" satisfied condition "Succeeded or Failed"
Sep  3 22:39:03.733: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod var-expansion-ff0115e8-a941-4f46-931c-9640414bda60 container dapi-container: <nil>
STEP: delete the pod
Sep  3 22:39:03.755: INFO: Waiting for pod var-expansion-ff0115e8-a941-4f46-931c-9640414bda60 to disappear
Sep  3 22:39:03.758: INFO: Pod var-expansion-ff0115e8-a941-4f46-931c-9640414bda60 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:39:03.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9966" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":277,"completed":131,"skipped":2292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:39:03.769: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:39:03.810: INFO: Creating ReplicaSet my-hostname-basic-803f4b3b-4b2d-4d5a-8a8f-27d045770bd1
Sep  3 22:39:03.822: INFO: Pod name my-hostname-basic-803f4b3b-4b2d-4d5a-8a8f-27d045770bd1: Found 0 pods out of 1
Sep  3 22:39:08.825: INFO: Pod name my-hostname-basic-803f4b3b-4b2d-4d5a-8a8f-27d045770bd1: Found 1 pods out of 1
Sep  3 22:39:08.825: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-803f4b3b-4b2d-4d5a-8a8f-27d045770bd1" is running
Sep  3 22:39:08.828: INFO: Pod "my-hostname-basic-803f4b3b-4b2d-4d5a-8a8f-27d045770bd1-5v996" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-03 22:39:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-03 22:39:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-03 22:39:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-03 22:39:03 +0000 UTC Reason: Message:}])
Sep  3 22:39:08.828: INFO: Trying to dial the pod
Sep  3 22:39:13.839: INFO: Controller my-hostname-basic-803f4b3b-4b2d-4d5a-8a8f-27d045770bd1: Got expected result from replica 1 [my-hostname-basic-803f4b3b-4b2d-4d5a-8a8f-27d045770bd1-5v996]: "my-hostname-basic-803f4b3b-4b2d-4d5a-8a8f-27d045770bd1-5v996", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:39:13.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3208" for this suite.

• [SLOW TEST:10.080 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":132,"skipped":2333,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:39:13.849: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-940d1994-e25b-494e-bb8e-96298a5b4e5d
STEP: Creating a pod to test consume secrets
Sep  3 22:39:13.889: INFO: Waiting up to 5m0s for pod "pod-secrets-60b39edb-7793-4ed1-9d7c-ff194a7478be" in namespace "secrets-6761" to be "Succeeded or Failed"
Sep  3 22:39:13.891: INFO: Pod "pod-secrets-60b39edb-7793-4ed1-9d7c-ff194a7478be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.328815ms
Sep  3 22:39:15.895: INFO: Pod "pod-secrets-60b39edb-7793-4ed1-9d7c-ff194a7478be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006120549s
STEP: Saw pod success
Sep  3 22:39:15.895: INFO: Pod "pod-secrets-60b39edb-7793-4ed1-9d7c-ff194a7478be" satisfied condition "Succeeded or Failed"
Sep  3 22:39:15.897: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-secrets-60b39edb-7793-4ed1-9d7c-ff194a7478be container secret-volume-test: <nil>
STEP: delete the pod
Sep  3 22:39:15.921: INFO: Waiting for pod pod-secrets-60b39edb-7793-4ed1-9d7c-ff194a7478be to disappear
Sep  3 22:39:15.926: INFO: Pod pod-secrets-60b39edb-7793-4ed1-9d7c-ff194a7478be no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:39:15.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6761" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":133,"skipped":2333,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:39:15.939: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Sep  3 22:39:16.004: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1190" to be "Succeeded or Failed"
Sep  3 22:39:16.007: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.101749ms
Sep  3 22:39:18.012: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007944423s
STEP: Saw pod success
Sep  3 22:39:18.012: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Sep  3 22:39:18.016: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep  3 22:39:18.041: INFO: Waiting for pod pod-host-path-test to disappear
Sep  3 22:39:18.044: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:39:18.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1190" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":134,"skipped":2350,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:39:18.055: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:39:18.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8414" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":277,"completed":135,"skipped":2365,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:39:18.123: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep  3 22:39:22.189: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4351 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:39:22.189: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:39:22.325: INFO: Exec stderr: ""
Sep  3 22:39:22.325: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4351 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:39:22.325: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:39:22.438: INFO: Exec stderr: ""
Sep  3 22:39:22.438: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4351 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:39:22.438: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:39:22.551: INFO: Exec stderr: ""
Sep  3 22:39:22.551: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4351 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:39:22.551: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:39:22.669: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep  3 22:39:22.669: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4351 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:39:22.669: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:39:22.785: INFO: Exec stderr: ""
Sep  3 22:39:22.785: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4351 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:39:22.785: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:39:22.916: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep  3 22:39:22.916: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4351 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:39:22.916: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:39:23.031: INFO: Exec stderr: ""
Sep  3 22:39:23.031: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4351 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:39:23.031: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:39:23.159: INFO: Exec stderr: ""
Sep  3 22:39:23.159: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4351 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:39:23.159: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:39:23.282: INFO: Exec stderr: ""
Sep  3 22:39:23.282: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4351 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 22:39:23.282: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:39:23.397: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:39:23.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4351" for this suite.

• [SLOW TEST:5.283 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":136,"skipped":2381,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:39:23.406: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-f4a3f834-8167-452c-87a9-45cf3ff9202c
STEP: Creating secret with name s-test-opt-upd-49587542-b11e-4c1c-8e2f-50b36cd5d91b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f4a3f834-8167-452c-87a9-45cf3ff9202c
STEP: Updating secret s-test-opt-upd-49587542-b11e-4c1c-8e2f-50b36cd5d91b
STEP: Creating secret with name s-test-opt-create-c0aae6a4-31d3-42fb-a499-f14f71d7c033
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:39:27.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6658" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":137,"skipped":2396,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:39:27.559: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:40:27.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5279" for this suite.

• [SLOW TEST:60.063 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":277,"completed":138,"skipped":2403,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:40:27.622: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:40:27.657: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 22:40:29.660: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = false)
Sep  3 22:40:31.661: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = false)
Sep  3 22:40:33.661: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = false)
Sep  3 22:40:35.661: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = false)
Sep  3 22:40:37.662: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = false)
Sep  3 22:40:39.661: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = false)
Sep  3 22:40:41.661: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = false)
Sep  3 22:40:43.661: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = false)
Sep  3 22:40:45.660: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = false)
Sep  3 22:40:47.661: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = false)
Sep  3 22:40:49.660: INFO: The status of Pod test-webserver-336f49fc-150d-4e98-ad5e-91ae78294ac6 is Running (Ready = true)
Sep  3 22:40:49.663: INFO: Container started at 2020-09-03 22:40:28 +0000 UTC, pod became ready at 2020-09-03 22:40:48 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:40:49.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2374" for this suite.

• [SLOW TEST:22.050 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":277,"completed":139,"skipped":2416,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:40:49.672: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  3 22:40:49.718: INFO: Waiting up to 5m0s for pod "pod-b913cdfc-62db-4235-88c3-2e8a9d7affb7" in namespace "emptydir-4389" to be "Succeeded or Failed"
Sep  3 22:40:49.722: INFO: Pod "pod-b913cdfc-62db-4235-88c3-2e8a9d7affb7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.403014ms
Sep  3 22:40:51.726: INFO: Pod "pod-b913cdfc-62db-4235-88c3-2e8a9d7affb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00749202s
Sep  3 22:40:53.731: INFO: Pod "pod-b913cdfc-62db-4235-88c3-2e8a9d7affb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012408166s
STEP: Saw pod success
Sep  3 22:40:53.731: INFO: Pod "pod-b913cdfc-62db-4235-88c3-2e8a9d7affb7" satisfied condition "Succeeded or Failed"
Sep  3 22:40:53.733: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-b913cdfc-62db-4235-88c3-2e8a9d7affb7 container test-container: <nil>
STEP: delete the pod
Sep  3 22:40:53.756: INFO: Waiting for pod pod-b913cdfc-62db-4235-88c3-2e8a9d7affb7 to disappear
Sep  3 22:40:53.759: INFO: Pod pod-b913cdfc-62db-4235-88c3-2e8a9d7affb7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:40:53.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4389" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":140,"skipped":2418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:40:53.769: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Sep  3 22:40:53.802: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:41:13.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1040" for this suite.

• [SLOW TEST:19.911 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":277,"completed":141,"skipped":2444,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:41:13.680: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Sep  3 22:41:13.713: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Sep  3 22:41:13.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-5663'
Sep  3 22:41:13.940: INFO: stderr: ""
Sep  3 22:41:13.940: INFO: stdout: "service/agnhost-slave created\n"
Sep  3 22:41:13.940: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Sep  3 22:41:13.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-5663'
Sep  3 22:41:14.164: INFO: stderr: ""
Sep  3 22:41:14.164: INFO: stdout: "service/agnhost-master created\n"
Sep  3 22:41:14.164: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  3 22:41:14.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-5663'
Sep  3 22:41:14.395: INFO: stderr: ""
Sep  3 22:41:14.395: INFO: stdout: "service/frontend created\n"
Sep  3 22:41:14.395: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  3 22:41:14.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-5663'
Sep  3 22:41:14.560: INFO: stderr: ""
Sep  3 22:41:14.560: INFO: stdout: "deployment.apps/frontend created\n"
Sep  3 22:41:14.560: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  3 22:41:14.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-5663'
Sep  3 22:41:14.798: INFO: stderr: ""
Sep  3 22:41:14.798: INFO: stdout: "deployment.apps/agnhost-master created\n"
Sep  3 22:41:14.798: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  3 22:41:14.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 create -f - --namespace=kubectl-5663'
Sep  3 22:41:14.992: INFO: stderr: ""
Sep  3 22:41:14.992: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Sep  3 22:41:14.993: INFO: Waiting for all frontend pods to be Running.
Sep  3 22:41:20.043: INFO: Waiting for frontend to serve content.
Sep  3 22:41:20.052: INFO: Trying to add a new entry to the guestbook.
Sep  3 22:41:20.063: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep  3 22:41:20.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete --grace-period=0 --force -f - --namespace=kubectl-5663'
Sep  3 22:41:20.179: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 22:41:20.179: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep  3 22:41:20.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete --grace-period=0 --force -f - --namespace=kubectl-5663'
Sep  3 22:41:20.273: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 22:41:20.273: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  3 22:41:20.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete --grace-period=0 --force -f - --namespace=kubectl-5663'
Sep  3 22:41:20.383: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 22:41:20.383: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  3 22:41:20.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete --grace-period=0 --force -f - --namespace=kubectl-5663'
Sep  3 22:41:20.469: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 22:41:20.470: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  3 22:41:20.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete --grace-period=0 --force -f - --namespace=kubectl-5663'
Sep  3 22:41:20.562: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 22:41:20.562: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  3 22:41:20.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete --grace-period=0 --force -f - --namespace=kubectl-5663'
Sep  3 22:41:20.655: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  3 22:41:20.655: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:41:20.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5663" for this suite.

• [SLOW TEST:6.993 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:310
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":277,"completed":142,"skipped":2457,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:41:20.674: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:41:33.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3509" for this suite.

• [SLOW TEST:13.118 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":277,"completed":143,"skipped":2458,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:41:33.792: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  3 22:41:33.862: INFO: Number of nodes with available pods: 0
Sep  3 22:41:33.862: INFO: Node karbon-test1186mm-f1ba59-k8s-master-0 is running more than one daemon pod
Sep  3 22:41:34.871: INFO: Number of nodes with available pods: 0
Sep  3 22:41:34.871: INFO: Node karbon-test1186mm-f1ba59-k8s-master-0 is running more than one daemon pod
Sep  3 22:41:35.871: INFO: Number of nodes with available pods: 5
Sep  3 22:41:35.871: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep  3 22:41:35.890: INFO: Number of nodes with available pods: 4
Sep  3 22:41:35.890: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-0 is running more than one daemon pod
Sep  3 22:41:36.898: INFO: Number of nodes with available pods: 4
Sep  3 22:41:36.898: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-0 is running more than one daemon pod
Sep  3 22:41:37.899: INFO: Number of nodes with available pods: 4
Sep  3 22:41:37.899: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-0 is running more than one daemon pod
Sep  3 22:41:38.899: INFO: Number of nodes with available pods: 4
Sep  3 22:41:38.899: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-0 is running more than one daemon pod
Sep  3 22:41:39.900: INFO: Number of nodes with available pods: 4
Sep  3 22:41:39.900: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-0 is running more than one daemon pod
Sep  3 22:41:40.899: INFO: Number of nodes with available pods: 4
Sep  3 22:41:40.899: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-0 is running more than one daemon pod
Sep  3 22:41:41.900: INFO: Number of nodes with available pods: 4
Sep  3 22:41:41.900: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-0 is running more than one daemon pod
Sep  3 22:41:42.900: INFO: Number of nodes with available pods: 5
Sep  3 22:41:42.900: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-616, will wait for the garbage collector to delete the pods
Sep  3 22:41:42.971: INFO: Deleting DaemonSet.extensions daemon-set took: 14.834451ms
Sep  3 22:41:43.371: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.239222ms
Sep  3 22:41:57.174: INFO: Number of nodes with available pods: 0
Sep  3 22:41:57.174: INFO: Number of running nodes: 0, number of available pods: 0
Sep  3 22:41:57.177: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-616/daemonsets","resourceVersion":"20501"},"items":null}

Sep  3 22:41:57.179: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-616/pods","resourceVersion":"20501"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:41:57.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-616" for this suite.

• [SLOW TEST:23.412 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":277,"completed":144,"skipped":2466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:41:57.204: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4054
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4054
I0903 22:41:57.276638      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4054, replica count: 2
Sep  3 22:42:00.327: INFO: Creating new exec pod
I0903 22:42:00.327161      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 22:42:03.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-4054 execpodrrrxn -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep  3 22:42:03.571: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  3 22:42:03.572: INFO: stdout: ""
Sep  3 22:42:03.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-4054 execpodrrrxn -- /bin/sh -x -c nc -zv -t -w 2 172.19.117.129 80'
Sep  3 22:42:03.783: INFO: stderr: "+ nc -zv -t -w 2 172.19.117.129 80\nConnection to 172.19.117.129 80 port [tcp/http] succeeded!\n"
Sep  3 22:42:03.783: INFO: stdout: ""
Sep  3 22:42:03.783: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:03.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4054" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:6.621 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":277,"completed":145,"skipped":2489,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:03.826: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  3 22:42:05.890: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:05.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4916" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":277,"completed":146,"skipped":2510,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:05.914: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:22.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7948" for this suite.

• [SLOW TEST:17.085 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":277,"completed":147,"skipped":2525,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:23.000: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Sep  3 22:42:23.044: INFO: Waiting up to 5m0s for pod "client-containers-78c8eb23-8567-4fc2-bd9d-cabd90a417ef" in namespace "containers-5466" to be "Succeeded or Failed"
Sep  3 22:42:23.047: INFO: Pod "client-containers-78c8eb23-8567-4fc2-bd9d-cabd90a417ef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.044115ms
Sep  3 22:42:25.051: INFO: Pod "client-containers-78c8eb23-8567-4fc2-bd9d-cabd90a417ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006756876s
Sep  3 22:42:27.055: INFO: Pod "client-containers-78c8eb23-8567-4fc2-bd9d-cabd90a417ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011096314s
STEP: Saw pod success
Sep  3 22:42:27.055: INFO: Pod "client-containers-78c8eb23-8567-4fc2-bd9d-cabd90a417ef" satisfied condition "Succeeded or Failed"
Sep  3 22:42:27.058: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod client-containers-78c8eb23-8567-4fc2-bd9d-cabd90a417ef container test-container: <nil>
STEP: delete the pod
Sep  3 22:42:27.091: INFO: Waiting for pod client-containers-78c8eb23-8567-4fc2-bd9d-cabd90a417ef to disappear
Sep  3 22:42:27.094: INFO: Pod client-containers-78c8eb23-8567-4fc2-bd9d-cabd90a417ef no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:27.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5466" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":277,"completed":148,"skipped":2534,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:27.109: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  3 22:42:27.151: INFO: Waiting up to 5m0s for pod "pod-6b3db4b8-bb66-4f4f-8088-840a635447dd" in namespace "emptydir-7028" to be "Succeeded or Failed"
Sep  3 22:42:27.154: INFO: Pod "pod-6b3db4b8-bb66-4f4f-8088-840a635447dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595453ms
Sep  3 22:42:29.157: INFO: Pod "pod-6b3db4b8-bb66-4f4f-8088-840a635447dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005855506s
Sep  3 22:42:31.161: INFO: Pod "pod-6b3db4b8-bb66-4f4f-8088-840a635447dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009848024s
STEP: Saw pod success
Sep  3 22:42:31.161: INFO: Pod "pod-6b3db4b8-bb66-4f4f-8088-840a635447dd" satisfied condition "Succeeded or Failed"
Sep  3 22:42:31.164: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-6b3db4b8-bb66-4f4f-8088-840a635447dd container test-container: <nil>
STEP: delete the pod
Sep  3 22:42:31.186: INFO: Waiting for pod pod-6b3db4b8-bb66-4f4f-8088-840a635447dd to disappear
Sep  3 22:42:31.188: INFO: Pod pod-6b3db4b8-bb66-4f4f-8088-840a635447dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:31.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7028" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":149,"skipped":2534,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:31.197: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-39.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-39.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-39.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-39.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-39.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-39.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  3 22:42:35.272: INFO: DNS probes using dns-39/dns-test-7f3154c6-0db2-4bde-985c-efc306a2a4bc succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:35.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-39" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":277,"completed":150,"skipped":2535,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:35.292: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:42:36.234: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  3 22:42:38.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769756, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769756, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769756, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734769756, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:42:41.261: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep  3 22:42:41.282: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:41.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-855" for this suite.
STEP: Destroying namespace "webhook-855-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.102 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":277,"completed":151,"skipped":2562,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:41.395: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:52.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5815" for this suite.

• [SLOW TEST:11.097 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":277,"completed":152,"skipped":2564,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:52.492: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  3 22:42:52.532: INFO: Waiting up to 5m0s for pod "pod-5b393b57-e3a3-4357-aab1-64d5af5c6376" in namespace "emptydir-6276" to be "Succeeded or Failed"
Sep  3 22:42:52.534: INFO: Pod "pod-5b393b57-e3a3-4357-aab1-64d5af5c6376": Phase="Pending", Reason="", readiness=false. Elapsed: 2.156835ms
Sep  3 22:42:54.537: INFO: Pod "pod-5b393b57-e3a3-4357-aab1-64d5af5c6376": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004964707s
STEP: Saw pod success
Sep  3 22:42:54.537: INFO: Pod "pod-5b393b57-e3a3-4357-aab1-64d5af5c6376" satisfied condition "Succeeded or Failed"
Sep  3 22:42:54.540: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-5b393b57-e3a3-4357-aab1-64d5af5c6376 container test-container: <nil>
STEP: delete the pod
Sep  3 22:42:54.560: INFO: Waiting for pod pod-5b393b57-e3a3-4357-aab1-64d5af5c6376 to disappear
Sep  3 22:42:54.562: INFO: Pod pod-5b393b57-e3a3-4357-aab1-64d5af5c6376 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:54.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6276" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":153,"skipped":2596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:54.571: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:42:54.599: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:55.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8842" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":277,"completed":154,"skipped":2619,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:55.815: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:42:55.847: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:42:56.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4087" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":277,"completed":155,"skipped":2630,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:42:56.880: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:42:56.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7cfa3ea0-fb96-4f9a-985d-574b9262693a" in namespace "downward-api-6436" to be "Succeeded or Failed"
Sep  3 22:42:56.922: INFO: Pod "downwardapi-volume-7cfa3ea0-fb96-4f9a-985d-574b9262693a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.278452ms
Sep  3 22:42:58.927: INFO: Pod "downwardapi-volume-7cfa3ea0-fb96-4f9a-985d-574b9262693a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006920347s
Sep  3 22:43:00.930: INFO: Pod "downwardapi-volume-7cfa3ea0-fb96-4f9a-985d-574b9262693a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010081801s
STEP: Saw pod success
Sep  3 22:43:00.930: INFO: Pod "downwardapi-volume-7cfa3ea0-fb96-4f9a-985d-574b9262693a" satisfied condition "Succeeded or Failed"
Sep  3 22:43:00.932: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-7cfa3ea0-fb96-4f9a-985d-574b9262693a container client-container: <nil>
STEP: delete the pod
Sep  3 22:43:00.951: INFO: Waiting for pod downwardapi-volume-7cfa3ea0-fb96-4f9a-985d-574b9262693a to disappear
Sep  3 22:43:00.953: INFO: Pod downwardapi-volume-7cfa3ea0-fb96-4f9a-985d-574b9262693a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:43:00.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6436" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":156,"skipped":2655,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:43:00.962: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:43:01.001: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c46afe8b-1ad9-4ceb-8c56-85dd9036a5b6" in namespace "downward-api-3604" to be "Succeeded or Failed"
Sep  3 22:43:01.004: INFO: Pod "downwardapi-volume-c46afe8b-1ad9-4ceb-8c56-85dd9036a5b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.703702ms
Sep  3 22:43:03.009: INFO: Pod "downwardapi-volume-c46afe8b-1ad9-4ceb-8c56-85dd9036a5b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007852774s
Sep  3 22:43:05.013: INFO: Pod "downwardapi-volume-c46afe8b-1ad9-4ceb-8c56-85dd9036a5b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012498023s
STEP: Saw pod success
Sep  3 22:43:05.013: INFO: Pod "downwardapi-volume-c46afe8b-1ad9-4ceb-8c56-85dd9036a5b6" satisfied condition "Succeeded or Failed"
Sep  3 22:43:05.016: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-c46afe8b-1ad9-4ceb-8c56-85dd9036a5b6 container client-container: <nil>
STEP: delete the pod
Sep  3 22:43:05.039: INFO: Waiting for pod downwardapi-volume-c46afe8b-1ad9-4ceb-8c56-85dd9036a5b6 to disappear
Sep  3 22:43:05.042: INFO: Pod downwardapi-volume-c46afe8b-1ad9-4ceb-8c56-85dd9036a5b6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:43:05.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3604" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":157,"skipped":2662,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:43:05.054: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Sep  3 22:43:05.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 api-versions'
Sep  3 22:43:05.181: INFO: stderr: ""
Sep  3 22:43:05.181: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:43:05.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9581" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":277,"completed":158,"skipped":2669,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:43:05.191: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep  3 22:43:05.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-1505'
Sep  3 22:43:05.321: INFO: stderr: ""
Sep  3 22:43:05.322: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep  3 22:43:10.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 get pod e2e-test-httpd-pod --namespace=kubectl-1505 -o json'
Sep  3 22:43:10.449: INFO: stderr: ""
Sep  3 22:43:10.449: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-09-03T22:43:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-03T22:43:05Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.20.2.188\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-03T22:43:07Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1505\",\n        \"resourceVersion\": \"21197\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1505/pods/e2e-test-httpd-pod\",\n        \"uid\": \"280ea2c6-7812-4d4c-a4d3-d2c9f7aae4cf\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-ctwxq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"karbon-test1186mm-f1ba59-k8s-worker-0\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-ctwxq\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-ctwxq\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-03T22:43:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-03T22:43:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-03T22:43:07Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-03T22:43:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://e1c498dd0c63d14e3afae6b8a4ab153580b8d8404cfeaa5c8244f98978109e6c\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-09-03T22:43:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.45.43.119\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.20.2.188\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.20.2.188\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-09-03T22:43:05Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep  3 22:43:10.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 replace -f - --namespace=kubectl-1505'
Sep  3 22:43:10.678: INFO: stderr: ""
Sep  3 22:43:10.678: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Sep  3 22:43:10.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 delete pods e2e-test-httpd-pod --namespace=kubectl-1505'
Sep  3 22:43:17.097: INFO: stderr: ""
Sep  3 22:43:17.097: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:43:17.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1505" for this suite.

• [SLOW TEST:11.918 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":277,"completed":159,"skipped":2690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:43:17.110: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  3 22:43:19.166: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:43:19.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2972" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":160,"skipped":2712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:43:19.193: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Sep  3 22:43:19.235: INFO: Waiting up to 5m0s for pod "pod-74bc9fe2-e703-48a1-ab33-ea6a5cd38fae" in namespace "emptydir-1937" to be "Succeeded or Failed"
Sep  3 22:43:19.237: INFO: Pod "pod-74bc9fe2-e703-48a1-ab33-ea6a5cd38fae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.490735ms
Sep  3 22:43:21.242: INFO: Pod "pod-74bc9fe2-e703-48a1-ab33-ea6a5cd38fae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007003854s
STEP: Saw pod success
Sep  3 22:43:21.242: INFO: Pod "pod-74bc9fe2-e703-48a1-ab33-ea6a5cd38fae" satisfied condition "Succeeded or Failed"
Sep  3 22:43:21.244: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-74bc9fe2-e703-48a1-ab33-ea6a5cd38fae container test-container: <nil>
STEP: delete the pod
Sep  3 22:43:21.264: INFO: Waiting for pod pod-74bc9fe2-e703-48a1-ab33-ea6a5cd38fae to disappear
Sep  3 22:43:21.267: INFO: Pod pod-74bc9fe2-e703-48a1-ab33-ea6a5cd38fae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:43:21.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1937" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":161,"skipped":2794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:43:21.283: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-1548
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Sep  3 22:43:21.331: INFO: Found 0 stateful pods, waiting for 3
Sep  3 22:43:31.335: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 22:43:31.335: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 22:43:31.335: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 22:43:31.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-1548 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 22:43:31.545: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 22:43:31.545: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 22:43:31.545: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep  3 22:43:41.578: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep  3 22:43:51.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-1548 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 22:43:51.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 22:43:51.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 22:43:51.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 22:44:01.813: INFO: Waiting for StatefulSet statefulset-1548/ss2 to complete update
Sep  3 22:44:01.813: INFO: Waiting for Pod statefulset-1548/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep  3 22:44:01.813: INFO: Waiting for Pod statefulset-1548/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep  3 22:44:01.813: INFO: Waiting for Pod statefulset-1548/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep  3 22:44:11.821: INFO: Waiting for StatefulSet statefulset-1548/ss2 to complete update
Sep  3 22:44:11.821: INFO: Waiting for Pod statefulset-1548/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep  3 22:44:11.821: INFO: Waiting for Pod statefulset-1548/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep  3 22:44:21.821: INFO: Waiting for StatefulSet statefulset-1548/ss2 to complete update
Sep  3 22:44:21.822: INFO: Waiting for Pod statefulset-1548/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep  3 22:44:21.822: INFO: Waiting for Pod statefulset-1548/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep  3 22:44:31.820: INFO: Waiting for StatefulSet statefulset-1548/ss2 to complete update
STEP: Rolling back to a previous revision
Sep  3 22:44:41.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-1548 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 22:44:42.033: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 22:44:42.034: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 22:44:42.034: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 22:44:52.069: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep  3 22:45:02.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-1548 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 22:45:02.291: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 22:45:02.291: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 22:45:02.291: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep  3 22:45:22.311: INFO: Deleting all statefulset in ns statefulset-1548
Sep  3 22:45:22.313: INFO: Scaling statefulset ss2 to 0
Sep  3 22:45:52.330: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 22:45:52.333: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:45:52.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1548" for this suite.

• [SLOW TEST:151.073 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":277,"completed":162,"skipped":2829,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:45:52.356: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  3 22:45:52.393: INFO: Waiting up to 5m0s for pod "pod-97e064c5-b1a6-46a8-9b22-89259ff42685" in namespace "emptydir-1415" to be "Succeeded or Failed"
Sep  3 22:45:52.396: INFO: Pod "pod-97e064c5-b1a6-46a8-9b22-89259ff42685": Phase="Pending", Reason="", readiness=false. Elapsed: 2.846359ms
Sep  3 22:45:54.401: INFO: Pod "pod-97e064c5-b1a6-46a8-9b22-89259ff42685": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007127854s
STEP: Saw pod success
Sep  3 22:45:54.401: INFO: Pod "pod-97e064c5-b1a6-46a8-9b22-89259ff42685" satisfied condition "Succeeded or Failed"
Sep  3 22:45:54.403: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-97e064c5-b1a6-46a8-9b22-89259ff42685 container test-container: <nil>
STEP: delete the pod
Sep  3 22:45:54.436: INFO: Waiting for pod pod-97e064c5-b1a6-46a8-9b22-89259ff42685 to disappear
Sep  3 22:45:54.439: INFO: Pod pod-97e064c5-b1a6-46a8-9b22-89259ff42685 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:45:54.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1415" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":163,"skipped":2838,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:45:54.453: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-9cf3a390-f56b-4e5e-beff-93cccb396b09
STEP: Creating a pod to test consume secrets
Sep  3 22:45:54.553: INFO: Waiting up to 5m0s for pod "pod-secrets-e686e866-418d-40aa-9ce7-b0aea5029d56" in namespace "secrets-1241" to be "Succeeded or Failed"
Sep  3 22:45:54.555: INFO: Pod "pod-secrets-e686e866-418d-40aa-9ce7-b0aea5029d56": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926886ms
Sep  3 22:45:56.561: INFO: Pod "pod-secrets-e686e866-418d-40aa-9ce7-b0aea5029d56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007994614s
STEP: Saw pod success
Sep  3 22:45:56.562: INFO: Pod "pod-secrets-e686e866-418d-40aa-9ce7-b0aea5029d56" satisfied condition "Succeeded or Failed"
Sep  3 22:45:56.565: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-secrets-e686e866-418d-40aa-9ce7-b0aea5029d56 container secret-volume-test: <nil>
STEP: delete the pod
Sep  3 22:45:56.584: INFO: Waiting for pod pod-secrets-e686e866-418d-40aa-9ce7-b0aea5029d56 to disappear
Sep  3 22:45:56.588: INFO: Pod pod-secrets-e686e866-418d-40aa-9ce7-b0aea5029d56 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:45:56.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1241" for this suite.
STEP: Destroying namespace "secret-namespace-2316" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":277,"completed":164,"skipped":2848,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:45:56.605: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-4778
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4778
STEP: Deleting pre-stop pod
Sep  3 22:46:05.676: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:46:05.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4778" for this suite.

• [SLOW TEST:9.089 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":277,"completed":165,"skipped":2856,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:46:05.695: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8118.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8118.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8118.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8118.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8118.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8118.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  3 22:46:07.756: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local from pod dns-8118/dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3: the server could not find the requested resource (get pods dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3)
Sep  3 22:46:07.759: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local from pod dns-8118/dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3: the server could not find the requested resource (get pods dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3)
Sep  3 22:46:07.762: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8118.svc.cluster.local from pod dns-8118/dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3: the server could not find the requested resource (get pods dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3)
Sep  3 22:46:07.764: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8118.svc.cluster.local from pod dns-8118/dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3: the server could not find the requested resource (get pods dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3)
Sep  3 22:46:07.773: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local from pod dns-8118/dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3: the server could not find the requested resource (get pods dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3)
Sep  3 22:46:07.776: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local from pod dns-8118/dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3: the server could not find the requested resource (get pods dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3)
Sep  3 22:46:07.779: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8118.svc.cluster.local from pod dns-8118/dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3: the server could not find the requested resource (get pods dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3)
Sep  3 22:46:07.781: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8118.svc.cluster.local from pod dns-8118/dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3: the server could not find the requested resource (get pods dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3)
Sep  3 22:46:07.787: INFO: Lookups using dns-8118/dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8118.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8118.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8118.svc.cluster.local jessie_udp@dns-test-service-2.dns-8118.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8118.svc.cluster.local]

Sep  3 22:46:12.822: INFO: DNS probes using dns-8118/dns-test-3e0fdc7f-7aff-4245-b32b-6a50a9d829d3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:46:12.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8118" for this suite.

• [SLOW TEST:7.185 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":277,"completed":166,"skipped":2866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:46:12.880: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:46:12.914: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:46:13.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3829" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":277,"completed":167,"skipped":2888,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:46:13.489: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-767ec9b9-7bf7-4299-b1fc-338d105caff4 in namespace container-probe-7394
Sep  3 22:46:15.558: INFO: Started pod busybox-767ec9b9-7bf7-4299-b1fc-338d105caff4 in namespace container-probe-7394
STEP: checking the pod's current state and verifying that restartCount is present
Sep  3 22:46:15.560: INFO: Initial restart count of pod busybox-767ec9b9-7bf7-4299-b1fc-338d105caff4 is 0
Sep  3 22:47:09.663: INFO: Restart count of pod container-probe-7394/busybox-767ec9b9-7bf7-4299-b1fc-338d105caff4 is now 1 (54.102400299s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:47:09.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7394" for this suite.

• [SLOW TEST:56.203 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":168,"skipped":2902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:47:09.692: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1680 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1680;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1680 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1680;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1680.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1680.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1680.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1680.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1680.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1680.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1680.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1680.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1680.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1680.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1680.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1680.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1680.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 222.102.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.102.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.102.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.102.222_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1680 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1680;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1680 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1680;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1680.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1680.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1680.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1680.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1680.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1680.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1680.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1680.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1680.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1680.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1680.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1680.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1680.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 222.102.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.102.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.102.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.102.222_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  3 22:47:13.879: INFO: DNS probes using dns-1680/dns-test-66c1bc92-c618-4ca4-af64-00e02263f9d7 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:47:13.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1680" for this suite.
•{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":277,"completed":169,"skipped":2970,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:47:13.966: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:47:13.995: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep  3 22:47:16.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-5220 create -f -'
Sep  3 22:47:16.615: INFO: stderr: ""
Sep  3 22:47:16.615: INFO: stdout: "e2e-test-crd-publish-openapi-3917-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  3 22:47:16.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-5220 delete e2e-test-crd-publish-openapi-3917-crds test-foo'
Sep  3 22:47:16.711: INFO: stderr: ""
Sep  3 22:47:16.711: INFO: stdout: "e2e-test-crd-publish-openapi-3917-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  3 22:47:16.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-5220 apply -f -'
Sep  3 22:47:16.986: INFO: stderr: ""
Sep  3 22:47:16.986: INFO: stdout: "e2e-test-crd-publish-openapi-3917-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  3 22:47:16.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-5220 delete e2e-test-crd-publish-openapi-3917-crds test-foo'
Sep  3 22:47:17.079: INFO: stderr: ""
Sep  3 22:47:17.080: INFO: stdout: "e2e-test-crd-publish-openapi-3917-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep  3 22:47:17.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-5220 create -f -'
Sep  3 22:47:17.230: INFO: rc: 1
Sep  3 22:47:17.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-5220 apply -f -'
Sep  3 22:47:17.440: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep  3 22:47:17.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-5220 create -f -'
Sep  3 22:47:17.594: INFO: rc: 1
Sep  3 22:47:17.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-5220 apply -f -'
Sep  3 22:47:17.739: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep  3 22:47:17.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 explain e2e-test-crd-publish-openapi-3917-crds'
Sep  3 22:47:17.976: INFO: stderr: ""
Sep  3 22:47:17.976: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3917-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep  3 22:47:17.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 explain e2e-test-crd-publish-openapi-3917-crds.metadata'
Sep  3 22:47:18.119: INFO: stderr: ""
Sep  3 22:47:18.119: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3917-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  3 22:47:18.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 explain e2e-test-crd-publish-openapi-3917-crds.spec'
Sep  3 22:47:18.270: INFO: stderr: ""
Sep  3 22:47:18.270: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3917-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  3 22:47:18.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 explain e2e-test-crd-publish-openapi-3917-crds.spec.bars'
Sep  3 22:47:18.414: INFO: stderr: ""
Sep  3 22:47:18.414: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3917-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep  3 22:47:18.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 explain e2e-test-crd-publish-openapi-3917-crds.spec.bars2'
Sep  3 22:47:18.560: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:47:22.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5220" for this suite.

• [SLOW TEST:8.245 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":277,"completed":170,"skipped":2972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:47:22.212: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-aa08a4d5-463f-4375-b219-1111c9cd769a
STEP: Creating a pod to test consume configMaps
Sep  3 22:47:22.259: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-65032c38-92cc-41cb-978e-3f12ab6e6e55" in namespace "projected-6326" to be "Succeeded or Failed"
Sep  3 22:47:22.262: INFO: Pod "pod-projected-configmaps-65032c38-92cc-41cb-978e-3f12ab6e6e55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.613736ms
Sep  3 22:47:24.266: INFO: Pod "pod-projected-configmaps-65032c38-92cc-41cb-978e-3f12ab6e6e55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006202036s
STEP: Saw pod success
Sep  3 22:47:24.266: INFO: Pod "pod-projected-configmaps-65032c38-92cc-41cb-978e-3f12ab6e6e55" satisfied condition "Succeeded or Failed"
Sep  3 22:47:24.268: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-configmaps-65032c38-92cc-41cb-978e-3f12ab6e6e55 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:47:24.290: INFO: Waiting for pod pod-projected-configmaps-65032c38-92cc-41cb-978e-3f12ab6e6e55 to disappear
Sep  3 22:47:24.292: INFO: Pod pod-projected-configmaps-65032c38-92cc-41cb-978e-3f12ab6e6e55 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:47:24.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6326" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":171,"skipped":3009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:47:24.303: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-39
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-39
Sep  3 22:47:24.352: INFO: Found 0 stateful pods, waiting for 1
Sep  3 22:47:34.357: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep  3 22:47:34.375: INFO: Deleting all statefulset in ns statefulset-39
Sep  3 22:47:34.380: INFO: Scaling statefulset ss to 0
Sep  3 22:47:54.399: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 22:47:54.402: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:47:54.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-39" for this suite.

• [SLOW TEST:30.124 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":277,"completed":172,"skipped":3042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:47:54.428: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-84f5a8cd-901b-4956-af92-f7ba8d0e2f42
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:47:54.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8098" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":277,"completed":173,"skipped":3130,"failed":0}

------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:47:54.474: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:00.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5998" for this suite.
STEP: Destroying namespace "nsdeletetest-9654" for this suite.
Sep  3 22:48:00.604: INFO: Namespace nsdeletetest-9654 was already deleted
STEP: Destroying namespace "nsdeletetest-4622" for this suite.

• [SLOW TEST:6.135 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":277,"completed":174,"skipped":3130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:00.610: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:02.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1937" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":277,"completed":175,"skipped":3183,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:02.716: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:48:03.651: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  3 22:48:05.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770083, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770083, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770083, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770083, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:48:08.682: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:08.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8153" for this suite.
STEP: Destroying namespace "webhook-8153-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.082 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":277,"completed":176,"skipped":3202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:08.798: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep  3 22:48:08.841: INFO: Waiting up to 5m0s for pod "pod-2e2b889d-b0b8-4015-b985-8d6f83a7396d" in namespace "emptydir-9958" to be "Succeeded or Failed"
Sep  3 22:48:08.843: INFO: Pod "pod-2e2b889d-b0b8-4015-b985-8d6f83a7396d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355225ms
Sep  3 22:48:10.847: INFO: Pod "pod-2e2b889d-b0b8-4015-b985-8d6f83a7396d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005804968s
Sep  3 22:48:12.851: INFO: Pod "pod-2e2b889d-b0b8-4015-b985-8d6f83a7396d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010066419s
STEP: Saw pod success
Sep  3 22:48:12.851: INFO: Pod "pod-2e2b889d-b0b8-4015-b985-8d6f83a7396d" satisfied condition "Succeeded or Failed"
Sep  3 22:48:12.853: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-2e2b889d-b0b8-4015-b985-8d6f83a7396d container test-container: <nil>
STEP: delete the pod
Sep  3 22:48:12.876: INFO: Waiting for pod pod-2e2b889d-b0b8-4015-b985-8d6f83a7396d to disappear
Sep  3 22:48:12.878: INFO: Pod pod-2e2b889d-b0b8-4015-b985-8d6f83a7396d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:12.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9958" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":177,"skipped":3241,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:12.889: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:14.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5942" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":178,"skipped":3252,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:14.953: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-9807
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9807 to expose endpoints map[]
Sep  3 22:48:15.006: INFO: Get endpoints failed (12.329992ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep  3 22:48:16.010: INFO: successfully validated that service multi-endpoint-test in namespace services-9807 exposes endpoints map[] (1.016635323s elapsed)
STEP: Creating pod pod1 in namespace services-9807
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9807 to expose endpoints map[pod1:[100]]
Sep  3 22:48:18.042: INFO: successfully validated that service multi-endpoint-test in namespace services-9807 exposes endpoints map[pod1:[100]] (2.022771038s elapsed)
STEP: Creating pod pod2 in namespace services-9807
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9807 to expose endpoints map[pod1:[100] pod2:[101]]
Sep  3 22:48:20.083: INFO: successfully validated that service multi-endpoint-test in namespace services-9807 exposes endpoints map[pod1:[100] pod2:[101]] (2.032396617s elapsed)
STEP: Deleting pod pod1 in namespace services-9807
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9807 to expose endpoints map[pod2:[101]]
Sep  3 22:48:21.118: INFO: successfully validated that service multi-endpoint-test in namespace services-9807 exposes endpoints map[pod2:[101]] (1.013633607s elapsed)
STEP: Deleting pod pod2 in namespace services-9807
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9807 to expose endpoints map[]
Sep  3 22:48:22.131: INFO: successfully validated that service multi-endpoint-test in namespace services-9807 exposes endpoints map[] (1.005725086s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:22.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9807" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:7.213 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":277,"completed":179,"skipped":3265,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:22.166: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Sep  3 22:48:22.217: INFO: Waiting up to 5m0s for pod "var-expansion-6170ab45-8f2b-4deb-b765-7398d795fa7b" in namespace "var-expansion-7399" to be "Succeeded or Failed"
Sep  3 22:48:22.221: INFO: Pod "var-expansion-6170ab45-8f2b-4deb-b765-7398d795fa7b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.408984ms
Sep  3 22:48:24.225: INFO: Pod "var-expansion-6170ab45-8f2b-4deb-b765-7398d795fa7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00794963s
STEP: Saw pod success
Sep  3 22:48:24.225: INFO: Pod "var-expansion-6170ab45-8f2b-4deb-b765-7398d795fa7b" satisfied condition "Succeeded or Failed"
Sep  3 22:48:24.227: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod var-expansion-6170ab45-8f2b-4deb-b765-7398d795fa7b container dapi-container: <nil>
STEP: delete the pod
Sep  3 22:48:24.253: INFO: Waiting for pod var-expansion-6170ab45-8f2b-4deb-b765-7398d795fa7b to disappear
Sep  3 22:48:24.256: INFO: Pod var-expansion-6170ab45-8f2b-4deb-b765-7398d795fa7b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:24.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7399" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":277,"completed":180,"skipped":3286,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:24.266: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-82f4ff96-1023-46f4-ab28-13ee6d4f7401
STEP: Creating secret with name secret-projected-all-test-volume-07c7b759-1cd3-4f58-8c38-cfc21064b080
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep  3 22:48:24.342: INFO: Waiting up to 5m0s for pod "projected-volume-596e138d-b9b6-479a-ac48-81a0c50890fe" in namespace "projected-8896" to be "Succeeded or Failed"
Sep  3 22:48:24.344: INFO: Pod "projected-volume-596e138d-b9b6-479a-ac48-81a0c50890fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.861743ms
Sep  3 22:48:26.348: INFO: Pod "projected-volume-596e138d-b9b6-479a-ac48-81a0c50890fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006652075s
STEP: Saw pod success
Sep  3 22:48:26.348: INFO: Pod "projected-volume-596e138d-b9b6-479a-ac48-81a0c50890fe" satisfied condition "Succeeded or Failed"
Sep  3 22:48:26.351: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod projected-volume-596e138d-b9b6-479a-ac48-81a0c50890fe container projected-all-volume-test: <nil>
STEP: delete the pod
Sep  3 22:48:26.371: INFO: Waiting for pod projected-volume-596e138d-b9b6-479a-ac48-81a0c50890fe to disappear
Sep  3 22:48:26.374: INFO: Pod projected-volume-596e138d-b9b6-479a-ac48-81a0c50890fe no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:26.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8896" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":277,"completed":181,"skipped":3295,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:26.384: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 22:48:26.987: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 22:48:30.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:30.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4193" for this suite.
STEP: Destroying namespace "webhook-4193-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":277,"completed":182,"skipped":3309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:30.271: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-35926642-e4a0-4a05-9c65-8d87b0983add
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:30.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-877" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":277,"completed":183,"skipped":3333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:30.376: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:48:30.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5ebd93a-86dd-4ef6-9351-a5edfb44b04f" in namespace "projected-7548" to be "Succeeded or Failed"
Sep  3 22:48:30.425: INFO: Pod "downwardapi-volume-f5ebd93a-86dd-4ef6-9351-a5edfb44b04f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673056ms
Sep  3 22:48:32.430: INFO: Pod "downwardapi-volume-f5ebd93a-86dd-4ef6-9351-a5edfb44b04f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007087884s
STEP: Saw pod success
Sep  3 22:48:32.430: INFO: Pod "downwardapi-volume-f5ebd93a-86dd-4ef6-9351-a5edfb44b04f" satisfied condition "Succeeded or Failed"
Sep  3 22:48:32.432: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-f5ebd93a-86dd-4ef6-9351-a5edfb44b04f container client-container: <nil>
STEP: delete the pod
Sep  3 22:48:32.458: INFO: Waiting for pod downwardapi-volume-f5ebd93a-86dd-4ef6-9351-a5edfb44b04f to disappear
Sep  3 22:48:32.462: INFO: Pod downwardapi-volume-f5ebd93a-86dd-4ef6-9351-a5edfb44b04f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:32.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7548" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":184,"skipped":3365,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:32.476: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-3066c486-5b09-40e7-aa04-3640b041b613
STEP: Creating a pod to test consume configMaps
Sep  3 22:48:32.522: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ab388093-d026-4a28-900e-be4068eb6d05" in namespace "projected-122" to be "Succeeded or Failed"
Sep  3 22:48:32.525: INFO: Pod "pod-projected-configmaps-ab388093-d026-4a28-900e-be4068eb6d05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991057ms
Sep  3 22:48:34.527: INFO: Pod "pod-projected-configmaps-ab388093-d026-4a28-900e-be4068eb6d05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00547526s
STEP: Saw pod success
Sep  3 22:48:34.527: INFO: Pod "pod-projected-configmaps-ab388093-d026-4a28-900e-be4068eb6d05" satisfied condition "Succeeded or Failed"
Sep  3 22:48:34.529: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-configmaps-ab388093-d026-4a28-900e-be4068eb6d05 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 22:48:34.550: INFO: Waiting for pod pod-projected-configmaps-ab388093-d026-4a28-900e-be4068eb6d05 to disappear
Sep  3 22:48:34.555: INFO: Pod pod-projected-configmaps-ab388093-d026-4a28-900e-be4068eb6d05 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:34.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-122" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":185,"skipped":3427,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:34.563: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:48:34.617: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep  3 22:48:34.624: INFO: Number of nodes with available pods: 0
Sep  3 22:48:34.624: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep  3 22:48:34.647: INFO: Number of nodes with available pods: 0
Sep  3 22:48:34.647: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-2 is running more than one daemon pod
Sep  3 22:48:35.650: INFO: Number of nodes with available pods: 1
Sep  3 22:48:35.650: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep  3 22:48:35.664: INFO: Number of nodes with available pods: 1
Sep  3 22:48:35.664: INFO: Number of running nodes: 0, number of available pods: 1
Sep  3 22:48:36.669: INFO: Number of nodes with available pods: 0
Sep  3 22:48:36.669: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep  3 22:48:36.684: INFO: Number of nodes with available pods: 0
Sep  3 22:48:36.684: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-2 is running more than one daemon pod
Sep  3 22:48:37.688: INFO: Number of nodes with available pods: 0
Sep  3 22:48:37.688: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-2 is running more than one daemon pod
Sep  3 22:48:38.688: INFO: Number of nodes with available pods: 0
Sep  3 22:48:38.688: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-2 is running more than one daemon pod
Sep  3 22:48:39.690: INFO: Number of nodes with available pods: 0
Sep  3 22:48:39.690: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-2 is running more than one daemon pod
Sep  3 22:48:40.688: INFO: Number of nodes with available pods: 0
Sep  3 22:48:40.688: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-2 is running more than one daemon pod
Sep  3 22:48:41.689: INFO: Number of nodes with available pods: 1
Sep  3 22:48:41.689: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-990, will wait for the garbage collector to delete the pods
Sep  3 22:48:41.757: INFO: Deleting DaemonSet.extensions daemon-set took: 8.667988ms
Sep  3 22:48:42.157: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.230527ms
Sep  3 22:48:52.860: INFO: Number of nodes with available pods: 0
Sep  3 22:48:52.860: INFO: Number of running nodes: 0, number of available pods: 0
Sep  3 22:48:52.863: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-990/daemonsets","resourceVersion":"23744"},"items":null}

Sep  3 22:48:52.867: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-990/pods","resourceVersion":"23744"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:48:52.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-990" for this suite.

• [SLOW TEST:18.334 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":277,"completed":186,"skipped":3430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:48:52.898: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:49:22.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8678" for this suite.
STEP: Destroying namespace "nsdeletetest-9624" for this suite.
Sep  3 22:49:22.032: INFO: Namespace nsdeletetest-9624 was already deleted
STEP: Destroying namespace "nsdeletetest-2202" for this suite.

• [SLOW TEST:29.139 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":277,"completed":187,"skipped":3455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:49:22.038: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9830
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9830
STEP: creating replication controller externalsvc in namespace services-9830
I0903 22:49:22.107937      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9830, replica count: 2
I0903 22:49:25.158435      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep  3 22:49:25.179: INFO: Creating new exec pod
Sep  3 22:49:27.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-9830 execpod8fkqm -- /bin/sh -x -c nslookup clusterip-service'
Sep  3 22:49:27.417: INFO: stderr: "+ nslookup clusterip-service\n"
Sep  3 22:49:27.417: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nclusterip-service.services-9830.svc.cluster.local\tcanonical name = externalsvc.services-9830.svc.cluster.local.\nName:\texternalsvc.services-9830.svc.cluster.local\nAddress: 172.19.77.156\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9830, will wait for the garbage collector to delete the pods
Sep  3 22:49:27.478: INFO: Deleting ReplicationController externalsvc took: 6.986779ms
Sep  3 22:49:27.878: INFO: Terminating ReplicationController externalsvc pods took: 400.23705ms
Sep  3 22:49:42.810: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:49:42.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9830" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:20.800 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":277,"completed":188,"skipped":3494,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:49:42.838: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-805, will wait for the garbage collector to delete the pods
Sep  3 22:49:44.942: INFO: Deleting Job.batch foo took: 6.516975ms
Sep  3 22:49:45.043: INFO: Terminating Job.batch foo pods took: 100.270407ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:50:18.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-805" for this suite.

• [SLOW TEST:35.218 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":277,"completed":189,"skipped":3506,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:50:18.057: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Sep  3 22:50:19.138: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
W0903 22:50:19.138568      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  3 22:50:19.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7880" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":277,"completed":190,"skipped":3511,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:50:19.148: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-476e44db-0205-42b0-ade9-4f6b64e6d1ae
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-476e44db-0205-42b0-ade9-4f6b64e6d1ae
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:50:23.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2481" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":191,"skipped":3521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:50:23.257: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-eb9cbc90-a95b-4c91-be8a-333e45c2ca7d in namespace container-probe-8404
Sep  3 22:50:25.305: INFO: Started pod busybox-eb9cbc90-a95b-4c91-be8a-333e45c2ca7d in namespace container-probe-8404
STEP: checking the pod's current state and verifying that restartCount is present
Sep  3 22:50:25.308: INFO: Initial restart count of pod busybox-eb9cbc90-a95b-4c91-be8a-333e45c2ca7d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:54:25.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8404" for this suite.

• [SLOW TEST:242.558 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":192,"skipped":3582,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:54:25.815: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Sep  3 22:54:25.854: INFO: Waiting up to 5m0s for pod "var-expansion-9bff1f68-736d-457b-b67b-6f6a21a3e6ac" in namespace "var-expansion-7230" to be "Succeeded or Failed"
Sep  3 22:54:25.856: INFO: Pod "var-expansion-9bff1f68-736d-457b-b67b-6f6a21a3e6ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191333ms
Sep  3 22:54:27.861: INFO: Pod "var-expansion-9bff1f68-736d-457b-b67b-6f6a21a3e6ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006705232s
STEP: Saw pod success
Sep  3 22:54:27.861: INFO: Pod "var-expansion-9bff1f68-736d-457b-b67b-6f6a21a3e6ac" satisfied condition "Succeeded or Failed"
Sep  3 22:54:27.864: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod var-expansion-9bff1f68-736d-457b-b67b-6f6a21a3e6ac container dapi-container: <nil>
STEP: delete the pod
Sep  3 22:54:27.897: INFO: Waiting for pod var-expansion-9bff1f68-736d-457b-b67b-6f6a21a3e6ac to disappear
Sep  3 22:54:27.899: INFO: Pod var-expansion-9bff1f68-736d-457b-b67b-6f6a21a3e6ac no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:54:27.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7230" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":277,"completed":193,"skipped":3594,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:54:27.907: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-svg5
STEP: Creating a pod to test atomic-volume-subpath
Sep  3 22:54:27.957: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-svg5" in namespace "subpath-748" to be "Succeeded or Failed"
Sep  3 22:54:27.960: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.80834ms
Sep  3 22:54:29.965: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007695067s
Sep  3 22:54:31.968: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Running", Reason="", readiness=true. Elapsed: 4.011478707s
Sep  3 22:54:33.972: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Running", Reason="", readiness=true. Elapsed: 6.015366885s
Sep  3 22:54:35.977: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Running", Reason="", readiness=true. Elapsed: 8.019958041s
Sep  3 22:54:37.980: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Running", Reason="", readiness=true. Elapsed: 10.023364279s
Sep  3 22:54:39.985: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Running", Reason="", readiness=true. Elapsed: 12.02772607s
Sep  3 22:54:41.989: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Running", Reason="", readiness=true. Elapsed: 14.031996281s
Sep  3 22:54:43.993: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Running", Reason="", readiness=true. Elapsed: 16.036505666s
Sep  3 22:54:45.997: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Running", Reason="", readiness=true. Elapsed: 18.040249589s
Sep  3 22:54:48.002: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Running", Reason="", readiness=true. Elapsed: 20.044703595s
Sep  3 22:54:50.005: INFO: Pod "pod-subpath-test-configmap-svg5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.048136477s
STEP: Saw pod success
Sep  3 22:54:50.005: INFO: Pod "pod-subpath-test-configmap-svg5" satisfied condition "Succeeded or Failed"
Sep  3 22:54:50.008: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-subpath-test-configmap-svg5 container test-container-subpath-configmap-svg5: <nil>
STEP: delete the pod
Sep  3 22:54:50.027: INFO: Waiting for pod pod-subpath-test-configmap-svg5 to disappear
Sep  3 22:54:50.029: INFO: Pod pod-subpath-test-configmap-svg5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-svg5
Sep  3 22:54:50.029: INFO: Deleting pod "pod-subpath-test-configmap-svg5" in namespace "subpath-748"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:54:50.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-748" for this suite.

• [SLOW TEST:22.132 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":277,"completed":194,"skipped":3594,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:54:50.040: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-84bf6a97-9489-45eb-9855-98d96c493b15
STEP: Creating a pod to test consume secrets
Sep  3 22:54:50.078: INFO: Waiting up to 5m0s for pod "pod-secrets-52ce46f2-078c-4343-8c22-4c6edf4647ae" in namespace "secrets-3367" to be "Succeeded or Failed"
Sep  3 22:54:50.081: INFO: Pod "pod-secrets-52ce46f2-078c-4343-8c22-4c6edf4647ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587042ms
Sep  3 22:54:52.085: INFO: Pod "pod-secrets-52ce46f2-078c-4343-8c22-4c6edf4647ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006862254s
STEP: Saw pod success
Sep  3 22:54:52.085: INFO: Pod "pod-secrets-52ce46f2-078c-4343-8c22-4c6edf4647ae" satisfied condition "Succeeded or Failed"
Sep  3 22:54:52.088: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-secrets-52ce46f2-078c-4343-8c22-4c6edf4647ae container secret-volume-test: <nil>
STEP: delete the pod
Sep  3 22:54:52.108: INFO: Waiting for pod pod-secrets-52ce46f2-078c-4343-8c22-4c6edf4647ae to disappear
Sep  3 22:54:52.111: INFO: Pod pod-secrets-52ce46f2-078c-4343-8c22-4c6edf4647ae no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:54:52.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3367" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":195,"skipped":3598,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:54:52.121: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4380.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4380.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4380.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4380.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4380.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4380.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4380.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 29.104.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.104.29_udp@PTR;check="$$(dig +tcp +noall +answer +search 29.104.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.104.29_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4380.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4380.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4380.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4380.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4380.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4380.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4380.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4380.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 29.104.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.104.29_udp@PTR;check="$$(dig +tcp +noall +answer +search 29.104.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.104.29_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  3 22:54:56.274: INFO: DNS probes using dns-4380/dns-test-e5a7e7cf-674b-4985-9da0-c3ee40de771f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:54:56.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4380" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":277,"completed":196,"skipped":3605,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:54:56.351: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-f1a75225-d9af-408a-8d39-a08d639e1db7
STEP: Creating configMap with name cm-test-opt-upd-1a83d0bd-77a7-4ad6-b3aa-2a26b60b4978
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f1a75225-d9af-408a-8d39-a08d639e1db7
STEP: Updating configmap cm-test-opt-upd-1a83d0bd-77a7-4ad6-b3aa-2a26b60b4978
STEP: Creating configMap with name cm-test-opt-create-730c449c-47fe-4724-9b0e-2670938e55d7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:56:18.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6008" for this suite.

• [SLOW TEST:82.582 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":197,"skipped":3608,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:56:18.933: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  3 22:56:21.493: INFO: Successfully updated pod "pod-update-c81b894f-e4d5-4e0d-a877-bf648a29aa4b"
STEP: verifying the updated pod is in kubernetes
Sep  3 22:56:21.497: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:56:21.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-433" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":277,"completed":198,"skipped":3624,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:56:21.506: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Sep  3 22:56:23.551: INFO: Pod pod-hostip-67d1c088-45ec-4b6f-9bac-af783b08dcf7 has hostIP: 10.45.43.112
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:56:23.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2720" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":277,"completed":199,"skipped":3636,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:56:23.561: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:56:23.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8399" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":277,"completed":200,"skipped":3639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:56:23.607: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  3 22:56:23.646: INFO: Waiting up to 5m0s for pod "pod-24c9832a-4aff-4905-854c-e20ae96db7c3" in namespace "emptydir-9544" to be "Succeeded or Failed"
Sep  3 22:56:23.648: INFO: Pod "pod-24c9832a-4aff-4905-854c-e20ae96db7c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.482621ms
Sep  3 22:56:25.653: INFO: Pod "pod-24c9832a-4aff-4905-854c-e20ae96db7c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006888455s
STEP: Saw pod success
Sep  3 22:56:25.653: INFO: Pod "pod-24c9832a-4aff-4905-854c-e20ae96db7c3" satisfied condition "Succeeded or Failed"
Sep  3 22:56:25.656: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-24c9832a-4aff-4905-854c-e20ae96db7c3 container test-container: <nil>
STEP: delete the pod
Sep  3 22:56:25.679: INFO: Waiting for pod pod-24c9832a-4aff-4905-854c-e20ae96db7c3 to disappear
Sep  3 22:56:25.682: INFO: Pod pod-24c9832a-4aff-4905-854c-e20ae96db7c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:56:25.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9544" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":201,"skipped":3670,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:56:25.694: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 22:56:25.737: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70d7002a-c502-427b-86a6-f6bd840a641a" in namespace "projected-3273" to be "Succeeded or Failed"
Sep  3 22:56:25.740: INFO: Pod "downwardapi-volume-70d7002a-c502-427b-86a6-f6bd840a641a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.701373ms
Sep  3 22:56:27.745: INFO: Pod "downwardapi-volume-70d7002a-c502-427b-86a6-f6bd840a641a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007383895s
STEP: Saw pod success
Sep  3 22:56:27.745: INFO: Pod "downwardapi-volume-70d7002a-c502-427b-86a6-f6bd840a641a" satisfied condition "Succeeded or Failed"
Sep  3 22:56:27.748: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-70d7002a-c502-427b-86a6-f6bd840a641a container client-container: <nil>
STEP: delete the pod
Sep  3 22:56:27.772: INFO: Waiting for pod downwardapi-volume-70d7002a-c502-427b-86a6-f6bd840a641a to disappear
Sep  3 22:56:27.775: INFO: Pod downwardapi-volume-70d7002a-c502-427b-86a6-f6bd840a641a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:56:27.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3273" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":202,"skipped":3678,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:56:27.786: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep  3 22:56:27.822: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  3 22:56:27.832: INFO: Waiting for terminating namespaces to be deleted...
Sep  3 22:56:27.835: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-master-0 before test
Sep  3 22:56:27.852: INFO: kube-proxy-ds-rtvz4 from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.852: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:56:27.852: INFO: fluent-bit-6wlnp from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.852: INFO: 	Container fluent-bit ready: false, restart count 9
Sep  3 22:56:27.852: INFO: csi-node-ntnx-plugin-xhqx9 from ntnx-system started at 2020-09-03 22:03:17 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.852: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:56:27.852: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:56:27.852: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:56:27.852: INFO: kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-0 from kube-system started at 2020-09-03 21:51:20 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.852: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  3 22:56:27.852: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  3 22:56:27.852: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  3 22:56:27.852: INFO: kube-flannel-ds-q7xlv from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.852: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:56:27.852: INFO: node-exporter-rvmxt from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.852: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:56:27.852: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:56:27.852: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-722pc from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.852: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:56:27.852: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:56:27.852: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-master-1 before test
Sep  3 22:56:27.870: INFO: fluent-bit-cpczk from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.871: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:56:27.871: INFO: node-exporter-4wwlp from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.871: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:56:27.871: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:56:27.871: INFO: kube-flannel-ds-8qrb7 from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.871: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:56:27.871: INFO: kube-proxy-ds-8vwrs from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.871: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:56:27.871: INFO: kube-dns-75bfbbdcb6-v6vf9 from kube-system started at 2020-09-03 21:55:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.871: INFO: 	Container dnsmasq ready: true, restart count 0
Sep  3 22:56:27.871: INFO: 	Container kubedns ready: true, restart count 0
Sep  3 22:56:27.871: INFO: 	Container sidecar ready: true, restart count 0
Sep  3 22:56:27.871: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-2qnxc from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.871: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:56:27.871: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:56:27.871: INFO: csi-node-ntnx-plugin-wgm9j from ntnx-system started at 2020-09-03 22:03:21 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.871: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:56:27.871: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:56:27.871: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:56:27.871: INFO: kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-1 from kube-system started at 2020-09-03 21:52:32 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.871: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  3 22:56:27.871: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  3 22:56:27.871: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  3 22:56:27.871: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-0 before test
Sep  3 22:56:27.880: INFO: fluent-bit-25t4p from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:56:27.880: INFO: prometheus-operator-8c5f5b4-9r8jx from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  3 22:56:27.880: INFO: kube-proxy-ds-cbqh9 from kube-system started at 2020-09-03 21:55:15 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:56:27.880: INFO: kube-state-metrics-7b754ff76b-vpckb from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (4 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container addon-resizer ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  3 22:56:27.880: INFO: node-exporter-fl4nm from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:56:27.880: INFO: alertmanager-main-0 from ntnx-system started at 2020-09-03 21:59:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container alertmanager ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 	Container config-reloader ready: true, restart count 0
Sep  3 22:56:27.880: INFO: prometheus-k8s-0 from ntnx-system started at 2020-09-03 21:59:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container prometheus ready: true, restart count 1
Sep  3 22:56:27.880: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  3 22:56:27.880: INFO: sonobuoy from sonobuoy started at 2020-09-03 22:02:43 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  3 22:56:27.880: INFO: pod-projected-configmaps-9047063f-c136-488f-9272-c958859abf37 from projected-6008 started at 2020-09-03 22:54:56 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container createcm-volume-test ready: false, restart count 0
Sep  3 22:56:27.880: INFO: 	Container delcm-volume-test ready: false, restart count 0
Sep  3 22:56:27.880: INFO: 	Container updcm-volume-test ready: false, restart count 0
Sep  3 22:56:27.880: INFO: kibana-logging-58dfc7bc95-bk4wt from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container kibana-logging ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 	Container nginxhttp ready: true, restart count 0
Sep  3 22:56:27.880: INFO: csi-node-ntnx-plugin-fxlcg from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:56:27.880: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-tcdxz from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:56:27.880: INFO: kube-flannel-ds-m9clk from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.880: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:56:27.880: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-1 before test
Sep  3 22:56:27.901: INFO: kube-proxy-ds-w7kvw from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.901: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:56:27.901: INFO: alertmanager-main-1 from ntnx-system started at 2020-09-03 21:59:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.901: INFO: 	Container alertmanager ready: true, restart count 0
Sep  3 22:56:27.901: INFO: 	Container config-reloader ready: true, restart count 0
Sep  3 22:56:27.901: INFO: pod-hostip-67d1c088-45ec-4b6f-9bac-af783b08dcf7 from pods-2720 started at 2020-09-03 22:56:21 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.901: INFO: 	Container test ready: true, restart count 0
Sep  3 22:56:27.901: INFO: kube-flannel-ds-4mqbc from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.901: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:56:27.901: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (4 container statuses recorded)
Sep  3 22:56:27.901: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep  3 22:56:27.901: INFO: 	Container csi-resizer ready: true, restart count 0
Sep  3 22:56:27.901: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:56:27.901: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Sep  3 22:56:27.901: INFO: pod-update-c81b894f-e4d5-4e0d-a877-bf648a29aa4b from pods-433 started at 2020-09-03 22:56:18 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.901: INFO: 	Container nginx ready: false, restart count 0
Sep  3 22:56:27.901: INFO: fluent-bit-ngkvm from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.901: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:56:27.901: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-gdgws from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.901: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:56:27.901: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:56:27.901: INFO: csi-node-ntnx-plugin-qqhtx from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.901: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:56:27.901: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:56:27.901: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:56:27.902: INFO: node-exporter-kzwh4 from ntnx-system started at 2020-09-03 21:59:12 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.902: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:56:27.902: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:56:27.902: INFO: prometheus-k8s-1 from ntnx-system started at 2020-09-03 21:59:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.902: INFO: 	Container prometheus ready: true, restart count 1
Sep  3 22:56:27.902: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  3 22:56:27.902: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  3 22:56:27.902: INFO: sonobuoy-e2e-job-38c4ed60912f46a6 from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.902: INFO: 	Container e2e ready: true, restart count 0
Sep  3 22:56:27.902: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:56:27.902: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-2 before test
Sep  3 22:56:27.918: INFO: kubernetes-events-printer-5767b7d649-8m9sl from ntnx-system started at 2020-09-03 21:56:21 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.919: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Sep  3 22:56:27.919: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-f4pr7 from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.919: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:56:27.919: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:56:27.919: INFO: kube-flannel-ds-4vfnt from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.919: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:56:27.919: INFO: node-exporter-wdljm from ntnx-system started at 2020-09-03 21:59:12 +0000 UTC (2 container statuses recorded)
Sep  3 22:56:27.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:56:27.919: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:56:27.919: INFO: kube-proxy-ds-rsdbq from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.919: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:56:27.919: INFO: csi-node-ntnx-plugin-l9tvt from ntnx-system started at 2020-09-03 21:55:53 +0000 UTC (3 container statuses recorded)
Sep  3 22:56:27.919: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:56:27.919: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:56:27.919: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:56:27.919: INFO: elasticsearch-logging-0 from ntnx-system started at 2020-09-03 21:56:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.919: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Sep  3 22:56:27.919: INFO: fluent-bit-wsm97 from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:56:27.919: INFO: 	Container fluent-bit ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c22e26b8-d9e0-4d07-b405-402960739a03 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-c22e26b8-d9e0-4d07-b405-402960739a03 off the node karbon-test1186mm-f1ba59-k8s-worker-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c22e26b8-d9e0-4d07-b405-402960739a03
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:56:36.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5525" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:8.286 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":277,"completed":203,"skipped":3686,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:56:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep  3 22:56:36.104: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:56:39.639: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:56:53.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5064" for this suite.

• [SLOW TEST:17.518 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":277,"completed":204,"skipped":3690,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:56:53.590: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep  3 22:56:53.624: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  3 22:57:53.662: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:57:53.665: INFO: Starting informer...
STEP: Starting pods...
Sep  3 22:57:53.882: INFO: Pod1 is running on karbon-test1186mm-f1ba59-k8s-worker-0. Tainting Node
Sep  3 22:57:56.100: INFO: Pod2 is running on karbon-test1186mm-f1ba59-k8s-worker-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep  3 22:58:07.152: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  3 22:58:27.093: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:58:27.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2448" for this suite.

• [SLOW TEST:93.529 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":277,"completed":205,"skipped":3695,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:58:27.120: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep  3 22:58:27.174: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  3 22:58:27.184: INFO: Waiting for terminating namespaces to be deleted...
Sep  3 22:58:27.186: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-master-0 before test
Sep  3 22:58:27.200: INFO: kube-proxy-ds-rtvz4 from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.200: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:58:27.200: INFO: fluent-bit-6wlnp from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.200: INFO: 	Container fluent-bit ready: false, restart count 10
Sep  3 22:58:27.200: INFO: csi-node-ntnx-plugin-xhqx9 from ntnx-system started at 2020-09-03 22:03:17 +0000 UTC (3 container statuses recorded)
Sep  3 22:58:27.200: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:58:27.200: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:58:27.200: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:58:27.200: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-722pc from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.200: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:58:27.200: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:58:27.200: INFO: kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-0 from kube-system started at 2020-09-03 21:51:20 +0000 UTC (3 container statuses recorded)
Sep  3 22:58:27.200: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  3 22:58:27.200: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  3 22:58:27.200: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  3 22:58:27.200: INFO: kube-flannel-ds-q7xlv from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.200: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:58:27.200: INFO: node-exporter-rvmxt from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.200: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:58:27.200: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:58:27.200: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-master-1 before test
Sep  3 22:58:27.215: INFO: kube-flannel-ds-8qrb7 from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.215: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:58:27.215: INFO: fluent-bit-cpczk from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.215: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:58:27.215: INFO: node-exporter-4wwlp from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.215: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:58:27.215: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:58:27.215: INFO: kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-1 from kube-system started at 2020-09-03 21:52:32 +0000 UTC (3 container statuses recorded)
Sep  3 22:58:27.215: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  3 22:58:27.215: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  3 22:58:27.215: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  3 22:58:27.215: INFO: kube-proxy-ds-8vwrs from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.215: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:58:27.215: INFO: kube-dns-75bfbbdcb6-v6vf9 from kube-system started at 2020-09-03 21:55:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:58:27.215: INFO: 	Container dnsmasq ready: true, restart count 0
Sep  3 22:58:27.215: INFO: 	Container kubedns ready: true, restart count 0
Sep  3 22:58:27.215: INFO: 	Container sidecar ready: true, restart count 0
Sep  3 22:58:27.215: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-2qnxc from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.215: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:58:27.215: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:58:27.215: INFO: csi-node-ntnx-plugin-wgm9j from ntnx-system started at 2020-09-03 22:03:21 +0000 UTC (3 container statuses recorded)
Sep  3 22:58:27.215: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:58:27.215: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:58:27.215: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:58:27.215: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-0 before test
Sep  3 22:58:27.230: INFO: kube-flannel-ds-m9clk from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.230: INFO: 	Container kube-flannel ready: false, restart count 0
Sep  3 22:58:27.230: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-tcdxz from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.230: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:58:27.230: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:58:27.230: INFO: csi-node-ntnx-plugin-ffz49 from ntnx-system started at <nil> (0 container statuses recorded)
Sep  3 22:58:27.230: INFO: kube-proxy-ds-cbqh9 from kube-system started at 2020-09-03 21:55:15 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.230: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:58:27.230: INFO: kibana-logging-58dfc7bc95-bk4wt from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.230: INFO: 	Container kibana-logging ready: false, restart count 0
Sep  3 22:58:27.230: INFO: 	Container nginxhttp ready: false, restart count 0
Sep  3 22:58:27.230: INFO: fluent-bit-k4dlt from ntnx-system started at <nil> (0 container statuses recorded)
Sep  3 22:58:27.230: INFO: node-exporter-fl4nm from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:58:27.230: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:58:27.230: INFO: prometheus-k8s-0 from ntnx-system started at 2020-09-03 21:59:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:58:27.230: INFO: 	Container prometheus ready: false, restart count 1
Sep  3 22:58:27.230: INFO: 	Container prometheus-config-reloader ready: false, restart count 0
Sep  3 22:58:27.230: INFO: 	Container rules-configmap-reloader ready: false, restart count 0
Sep  3 22:58:27.230: INFO: sonobuoy from sonobuoy started at 2020-09-03 22:02:43 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.230: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  3 22:58:27.230: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-1 before test
Sep  3 22:58:27.248: INFO: csi-node-ntnx-plugin-qqhtx from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (3 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:58:27.248: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:58:27.248: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:58:27.248: INFO: node-exporter-kzwh4 from ntnx-system started at 2020-09-03 21:59:12 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:58:27.248: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:58:27.248: INFO: prometheus-k8s-1 from ntnx-system started at 2020-09-03 21:59:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container prometheus ready: true, restart count 1
Sep  3 22:58:27.248: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  3 22:58:27.248: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  3 22:58:27.248: INFO: sonobuoy-e2e-job-38c4ed60912f46a6 from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container e2e ready: true, restart count 0
Sep  3 22:58:27.248: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:58:27.248: INFO: kube-proxy-ds-w7kvw from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:58:27.248: INFO: alertmanager-main-1 from ntnx-system started at 2020-09-03 21:59:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container alertmanager ready: true, restart count 0
Sep  3 22:58:27.248: INFO: 	Container config-reloader ready: true, restart count 0
Sep  3 22:58:27.248: INFO: kube-state-metrics-7b754ff76b-29m7v from ntnx-system started at 2020-09-03 22:57:56 +0000 UTC (4 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container addon-resizer ready: false, restart count 0
Sep  3 22:58:27.248: INFO: 	Container kube-rbac-proxy-main ready: false, restart count 0
Sep  3 22:58:27.248: INFO: 	Container kube-rbac-proxy-self ready: false, restart count 0
Sep  3 22:58:27.248: INFO: 	Container kube-state-metrics ready: false, restart count 0
Sep  3 22:58:27.248: INFO: kube-flannel-ds-4mqbc from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:58:27.248: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (4 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep  3 22:58:27.248: INFO: 	Container csi-resizer ready: true, restart count 0
Sep  3 22:58:27.248: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:58:27.248: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Sep  3 22:58:27.248: INFO: fluent-bit-ngkvm from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:58:27.248: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-gdgws from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:58:27.248: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:58:27.248: INFO: prometheus-operator-8c5f5b4-mz9rl from ntnx-system started at 2020-09-03 22:57:56 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  3 22:58:27.248: INFO: kibana-logging-58dfc7bc95-fb4rz from ntnx-system started at 2020-09-03 22:57:56 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.248: INFO: 	Container kibana-logging ready: false, restart count 0
Sep  3 22:58:27.248: INFO: 	Container nginxhttp ready: false, restart count 0
Sep  3 22:58:27.248: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-2 before test
Sep  3 22:58:27.265: INFO: kube-flannel-ds-4vfnt from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.265: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:58:27.265: INFO: node-exporter-wdljm from ntnx-system started at 2020-09-03 21:59:12 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.265: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:58:27.265: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:58:27.265: INFO: alertmanager-main-0 from ntnx-system started at 2020-09-03 22:57:59 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.265: INFO: 	Container alertmanager ready: true, restart count 0
Sep  3 22:58:27.265: INFO: 	Container config-reloader ready: true, restart count 0
Sep  3 22:58:27.265: INFO: kube-proxy-ds-rsdbq from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.265: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:58:27.265: INFO: csi-node-ntnx-plugin-l9tvt from ntnx-system started at 2020-09-03 21:55:53 +0000 UTC (3 container statuses recorded)
Sep  3 22:58:27.265: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:58:27.265: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:58:27.265: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:58:27.265: INFO: elasticsearch-logging-0 from ntnx-system started at 2020-09-03 21:56:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.265: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Sep  3 22:58:27.265: INFO: fluent-bit-wsm97 from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.265: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:58:27.265: INFO: kubernetes-events-printer-5767b7d649-8m9sl from ntnx-system started at 2020-09-03 21:56:21 +0000 UTC (1 container statuses recorded)
Sep  3 22:58:27.265: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Sep  3 22:58:27.265: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-f4pr7 from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:58:27.265: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:58:27.265: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-241465d8-b42f-4597-b40c-d6748be204bf 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-241465d8-b42f-4597-b40c-d6748be204bf off the node karbon-test1186mm-f1ba59-k8s-worker-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-241465d8-b42f-4597-b40c-d6748be204bf
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:58:35.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5696" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:8.244 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":277,"completed":206,"skipped":3697,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:58:35.364: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:58:35.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9264" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":277,"completed":207,"skipped":3714,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:58:35.404: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-df2866d4-afa6-4521-86a2-606b266b2763
STEP: Creating a pod to test consume secrets
Sep  3 22:58:35.444: INFO: Waiting up to 5m0s for pod "pod-secrets-86986418-fc23-441e-afb9-95675b1a5b4a" in namespace "secrets-2648" to be "Succeeded or Failed"
Sep  3 22:58:35.447: INFO: Pod "pod-secrets-86986418-fc23-441e-afb9-95675b1a5b4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.299082ms
Sep  3 22:58:37.450: INFO: Pod "pod-secrets-86986418-fc23-441e-afb9-95675b1a5b4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005588567s
Sep  3 22:58:39.454: INFO: Pod "pod-secrets-86986418-fc23-441e-afb9-95675b1a5b4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009701128s
Sep  3 22:58:41.457: INFO: Pod "pod-secrets-86986418-fc23-441e-afb9-95675b1a5b4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012353682s
STEP: Saw pod success
Sep  3 22:58:41.457: INFO: Pod "pod-secrets-86986418-fc23-441e-afb9-95675b1a5b4a" satisfied condition "Succeeded or Failed"
Sep  3 22:58:41.459: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-secrets-86986418-fc23-441e-afb9-95675b1a5b4a container secret-volume-test: <nil>
STEP: delete the pod
Sep  3 22:58:41.480: INFO: Waiting for pod pod-secrets-86986418-fc23-441e-afb9-95675b1a5b4a to disappear
Sep  3 22:58:41.482: INFO: Pod pod-secrets-86986418-fc23-441e-afb9-95675b1a5b4a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:58:41.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2648" for this suite.

• [SLOW TEST:6.090 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":208,"skipped":3718,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:58:41.495: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Sep  3 22:58:41.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 cluster-info'
Sep  3 22:58:41.855: INFO: stderr: ""
Sep  3 22:58:41.855: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:58:41.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6243" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":277,"completed":209,"skipped":3727,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:58:41.865: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:58:41.957: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  3 22:58:46.961: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  3 22:58:46.961: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep  3 22:58:46.979: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6395 /apis/apps/v1/namespaces/deployment-6395/deployments/test-cleanup-deployment d517756f-77da-419a-acde-46b2fab2e7ae 26737 1 2020-09-03 22:58:46 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-09-03 22:58:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00672eba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep  3 22:58:46.985: INFO: New ReplicaSet "test-cleanup-deployment-b4867b47f" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-b4867b47f  deployment-6395 /apis/apps/v1/namespaces/deployment-6395/replicasets/test-cleanup-deployment-b4867b47f 5b1b4d37-02a1-45d6-bf22-71aa46ab8355 26739 1 2020-09-03 22:58:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment d517756f-77da-419a-acde-46b2fab2e7ae 0xc00672f0b0 0xc00672f0b1}] []  [{kube-controller-manager Update apps/v1 2020-09-03 22:58:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 53 49 55 55 53 54 102 45 55 55 100 97 45 52 49 57 97 45 97 99 100 101 45 52 54 98 50 102 97 98 50 101 55 97 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: b4867b47f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00672f128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 22:58:46.985: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  3 22:58:46.985: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6395 /apis/apps/v1/namespaces/deployment-6395/replicasets/test-cleanup-controller 0ef38a85-cee6-4320-8109-e41e177612c9 26738 1 2020-09-03 22:58:41 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d517756f-77da-419a-acde-46b2fab2e7ae 0xc00672ef97 0xc00672ef98}] []  [{e2e.test Update apps/v1 2020-09-03 22:58:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-03 22:58:46 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 53 49 55 55 53 54 102 45 55 55 100 97 45 52 49 57 97 45 97 99 100 101 45 52 54 98 50 102 97 98 50 101 55 97 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00672f038 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  3 22:58:46.994: INFO: Pod "test-cleanup-controller-zcg9j" is available:
&Pod{ObjectMeta:{test-cleanup-controller-zcg9j test-cleanup-controller- deployment-6395 /api/v1/namespaces/deployment-6395/pods/test-cleanup-controller-zcg9j 770661fc-864b-4fcc-ad0e-4e4054597939 26692 0 2020-09-03 22:58:41 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 0ef38a85-cee6-4320-8109-e41e177612c9 0xc00672f717 0xc00672f718}] []  [{kube-controller-manager Update v1 2020-09-03 22:58:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 101 102 51 56 97 56 53 45 99 101 101 54 45 52 51 50 48 45 56 49 48 57 45 101 52 49 101 49 55 55 54 49 50 99 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 22:58:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 50 46 50 52 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c87bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c87bc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c87bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:58:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:58:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:58:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 22:58:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:172.20.2.242,StartTime:2020-09-03 22:58:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 22:58:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f335fadf270f61d0afa38580de94223d5b7131bdc5ca1e5d607a7f1a12b25004,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.2.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  3 22:58:46.994: INFO: Pod "test-cleanup-deployment-b4867b47f-zxjdq" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-b4867b47f-zxjdq test-cleanup-deployment-b4867b47f- deployment-6395 /api/v1/namespaces/deployment-6395/pods/test-cleanup-deployment-b4867b47f-zxjdq e6a63d78-b7e8-4620-a658-315d54a48db2 26740 0 2020-09-03 22:58:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-b4867b47f 5b1b4d37-02a1-45d6-bf22-71aa46ab8355 0xc00672f8c0 0xc00672f8c1}] []  [{kube-controller-manager Update v1 2020-09-03 22:58:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 53 98 49 98 52 100 51 55 45 48 50 97 49 45 52 53 100 54 45 98 102 50 50 45 55 49 97 97 52 54 97 98 56 51 53 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c87bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c87bc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c87bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:58:46.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6395" for this suite.

• [SLOW TEST:5.154 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":277,"completed":210,"skipped":3736,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:58:47.019: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:58:47.072: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-a48308e4-140b-41c4-8ea1-3ca89f6ba00a" in namespace "security-context-test-8420" to be "Succeeded or Failed"
Sep  3 22:58:47.076: INFO: Pod "alpine-nnp-false-a48308e4-140b-41c4-8ea1-3ca89f6ba00a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.619213ms
Sep  3 22:58:49.080: INFO: Pod "alpine-nnp-false-a48308e4-140b-41c4-8ea1-3ca89f6ba00a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00753396s
Sep  3 22:58:51.085: INFO: Pod "alpine-nnp-false-a48308e4-140b-41c4-8ea1-3ca89f6ba00a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012347774s
Sep  3 22:58:51.085: INFO: Pod "alpine-nnp-false-a48308e4-140b-41c4-8ea1-3ca89f6ba00a" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:58:51.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8420" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":211,"skipped":3744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:58:51.102: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-0b61012a-1c62-4cf6-a448-98509097f719
Sep  3 22:58:51.149: INFO: Pod name my-hostname-basic-0b61012a-1c62-4cf6-a448-98509097f719: Found 0 pods out of 1
Sep  3 22:58:56.152: INFO: Pod name my-hostname-basic-0b61012a-1c62-4cf6-a448-98509097f719: Found 1 pods out of 1
Sep  3 22:58:56.152: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0b61012a-1c62-4cf6-a448-98509097f719" are running
Sep  3 22:58:56.155: INFO: Pod "my-hostname-basic-0b61012a-1c62-4cf6-a448-98509097f719-whz76" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-03 22:58:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-03 22:58:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-03 22:58:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-03 22:58:51 +0000 UTC Reason: Message:}])
Sep  3 22:58:56.155: INFO: Trying to dial the pod
Sep  3 22:59:01.167: INFO: Controller my-hostname-basic-0b61012a-1c62-4cf6-a448-98509097f719: Got expected result from replica 1 [my-hostname-basic-0b61012a-1c62-4cf6-a448-98509097f719-whz76]: "my-hostname-basic-0b61012a-1c62-4cf6-a448-98509097f719-whz76", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:59:01.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9053" for this suite.

• [SLOW TEST:10.077 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":212,"skipped":3800,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:59:01.179: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:59:05.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8409" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":277,"completed":213,"skipped":3802,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:59:05.256: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 22:59:05.300: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5a318d0a-e291-489f-93fc-ee67892b628b" in namespace "security-context-test-2450" to be "Succeeded or Failed"
Sep  3 22:59:05.303: INFO: Pod "busybox-readonly-false-5a318d0a-e291-489f-93fc-ee67892b628b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.926848ms
Sep  3 22:59:07.307: INFO: Pod "busybox-readonly-false-5a318d0a-e291-489f-93fc-ee67892b628b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006921467s
Sep  3 22:59:09.310: INFO: Pod "busybox-readonly-false-5a318d0a-e291-489f-93fc-ee67892b628b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010687906s
Sep  3 22:59:09.311: INFO: Pod "busybox-readonly-false-5a318d0a-e291-489f-93fc-ee67892b628b" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:59:09.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2450" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":277,"completed":214,"skipped":3823,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:59:09.321: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep  3 22:59:09.355: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 22:59:12.865: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:59:25.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1213" for this suite.

• [SLOW TEST:16.013 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":277,"completed":215,"skipped":3832,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:59:25.333: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-fwd9
STEP: Creating a pod to test atomic-volume-subpath
Sep  3 22:59:25.382: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fwd9" in namespace "subpath-9534" to be "Succeeded or Failed"
Sep  3 22:59:25.388: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.278512ms
Sep  3 22:59:27.391: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009409469s
Sep  3 22:59:29.394: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Running", Reason="", readiness=true. Elapsed: 4.012644133s
Sep  3 22:59:31.398: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Running", Reason="", readiness=true. Elapsed: 6.015890507s
Sep  3 22:59:33.401: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Running", Reason="", readiness=true. Elapsed: 8.019660629s
Sep  3 22:59:35.406: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Running", Reason="", readiness=true. Elapsed: 10.024676678s
Sep  3 22:59:37.410: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Running", Reason="", readiness=true. Elapsed: 12.028545652s
Sep  3 22:59:39.414: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Running", Reason="", readiness=true. Elapsed: 14.032560096s
Sep  3 22:59:41.419: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Running", Reason="", readiness=true. Elapsed: 16.036899859s
Sep  3 22:59:43.422: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Running", Reason="", readiness=true. Elapsed: 18.040642365s
Sep  3 22:59:45.427: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Running", Reason="", readiness=true. Elapsed: 20.045579784s
Sep  3 22:59:47.432: INFO: Pod "pod-subpath-test-configmap-fwd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.050612361s
STEP: Saw pod success
Sep  3 22:59:47.432: INFO: Pod "pod-subpath-test-configmap-fwd9" satisfied condition "Succeeded or Failed"
Sep  3 22:59:47.435: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-subpath-test-configmap-fwd9 container test-container-subpath-configmap-fwd9: <nil>
STEP: delete the pod
Sep  3 22:59:47.459: INFO: Waiting for pod pod-subpath-test-configmap-fwd9 to disappear
Sep  3 22:59:47.461: INFO: Pod pod-subpath-test-configmap-fwd9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fwd9
Sep  3 22:59:47.462: INFO: Deleting pod "pod-subpath-test-configmap-fwd9" in namespace "subpath-9534"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:59:47.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9534" for this suite.

• [SLOW TEST:22.139 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":277,"completed":216,"skipped":3836,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:59:47.473: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep  3 22:59:47.505: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  3 22:59:47.519: INFO: Waiting for terminating namespaces to be deleted...
Sep  3 22:59:47.521: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-master-0 before test
Sep  3 22:59:47.533: INFO: kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-0 from kube-system started at 2020-09-03 21:51:20 +0000 UTC (3 container statuses recorded)
Sep  3 22:59:47.533: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  3 22:59:47.533: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  3 22:59:47.533: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  3 22:59:47.533: INFO: kube-flannel-ds-q7xlv from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.533: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:59:47.533: INFO: node-exporter-rvmxt from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:59:47.533: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:59:47.533: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-722pc from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:59:47.533: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:59:47.533: INFO: kube-proxy-ds-rtvz4 from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.533: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:59:47.533: INFO: fluent-bit-6wlnp from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.533: INFO: 	Container fluent-bit ready: false, restart count 10
Sep  3 22:59:47.533: INFO: csi-node-ntnx-plugin-xhqx9 from ntnx-system started at 2020-09-03 22:03:17 +0000 UTC (3 container statuses recorded)
Sep  3 22:59:47.533: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:59:47.533: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:59:47.533: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:59:47.533: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-master-1 before test
Sep  3 22:59:47.542: INFO: node-exporter-4wwlp from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.542: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:59:47.542: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:59:47.542: INFO: kube-flannel-ds-8qrb7 from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.542: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:59:47.542: INFO: fluent-bit-cpczk from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.542: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:59:47.542: INFO: kube-dns-75bfbbdcb6-v6vf9 from kube-system started at 2020-09-03 21:55:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:59:47.542: INFO: 	Container dnsmasq ready: true, restart count 0
Sep  3 22:59:47.542: INFO: 	Container kubedns ready: true, restart count 0
Sep  3 22:59:47.542: INFO: 	Container sidecar ready: true, restart count 0
Sep  3 22:59:47.542: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-2qnxc from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.542: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:59:47.542: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:59:47.542: INFO: csi-node-ntnx-plugin-wgm9j from ntnx-system started at 2020-09-03 22:03:21 +0000 UTC (3 container statuses recorded)
Sep  3 22:59:47.542: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:59:47.542: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:59:47.542: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:59:47.542: INFO: kube-apiserver-karbon-test1186mm-f1ba59-k8s-master-1 from kube-system started at 2020-09-03 21:52:32 +0000 UTC (3 container statuses recorded)
Sep  3 22:59:47.542: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  3 22:59:47.542: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  3 22:59:47.542: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  3 22:59:47.542: INFO: kube-proxy-ds-8vwrs from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.542: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:59:47.542: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-0 before test
Sep  3 22:59:47.550: INFO: csi-node-ntnx-plugin-ffz49 from ntnx-system started at 2020-09-03 22:58:27 +0000 UTC (3 container statuses recorded)
Sep  3 22:59:47.551: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:59:47.551: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:59:47.551: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:59:47.551: INFO: kube-proxy-ds-cbqh9 from kube-system started at 2020-09-03 21:55:15 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.551: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:59:47.551: INFO: prometheus-k8s-0 from ntnx-system started at 2020-09-03 22:58:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:59:47.551: INFO: 	Container prometheus ready: true, restart count 0
Sep  3 22:59:47.551: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  3 22:59:47.551: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  3 22:59:47.551: INFO: fluent-bit-k4dlt from ntnx-system started at 2020-09-03 22:58:27 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.551: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:59:47.551: INFO: node-exporter-fl4nm from ntnx-system started at 2020-09-03 21:59:11 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.551: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:59:47.551: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:59:47.551: INFO: sonobuoy from sonobuoy started at 2020-09-03 22:02:43 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.551: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  3 22:59:47.551: INFO: kube-flannel-ds-c62w7 from kube-system started at 2020-09-03 22:58:29 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.551: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:59:47.551: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-tcdxz from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.551: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:59:47.551: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:59:47.551: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-1 before test
Sep  3 22:59:47.562: INFO: kube-flannel-ds-4mqbc from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:59:47.562: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (4 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container csi-resizer ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Sep  3 22:59:47.562: INFO: fluent-bit-ngkvm from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:59:47.562: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-gdgws from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:59:47.562: INFO: prometheus-operator-8c5f5b4-mz9rl from ntnx-system started at 2020-09-03 22:57:56 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  3 22:59:47.562: INFO: kibana-logging-58dfc7bc95-fb4rz from ntnx-system started at 2020-09-03 22:57:56 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container kibana-logging ready: false, restart count 0
Sep  3 22:59:47.562: INFO: 	Container nginxhttp ready: true, restart count 0
Sep  3 22:59:47.562: INFO: csi-node-ntnx-plugin-qqhtx from ntnx-system started at 2020-09-03 21:55:52 +0000 UTC (3 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:59:47.562: INFO: node-exporter-kzwh4 from ntnx-system started at 2020-09-03 21:59:12 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:59:47.562: INFO: prometheus-k8s-1 from ntnx-system started at 2020-09-03 21:59:37 +0000 UTC (3 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container prometheus ready: true, restart count 1
Sep  3 22:59:47.562: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  3 22:59:47.562: INFO: sonobuoy-e2e-job-38c4ed60912f46a6 from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container e2e ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:59:47.562: INFO: kube-proxy-ds-w7kvw from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:59:47.562: INFO: alertmanager-main-1 from ntnx-system started at 2020-09-03 21:59:19 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container alertmanager ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container config-reloader ready: true, restart count 0
Sep  3 22:59:47.562: INFO: kube-state-metrics-7b754ff76b-29m7v from ntnx-system started at 2020-09-03 22:57:56 +0000 UTC (4 container statuses recorded)
Sep  3 22:59:47.562: INFO: 	Container addon-resizer ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  3 22:59:47.562: INFO: 
Logging pods the kubelet thinks is on node karbon-test1186mm-f1ba59-k8s-worker-2 before test
Sep  3 22:59:47.569: INFO: kube-proxy-ds-rsdbq from kube-system started at 2020-09-03 21:55:16 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.569: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  3 22:59:47.569: INFO: csi-node-ntnx-plugin-l9tvt from ntnx-system started at 2020-09-03 21:55:53 +0000 UTC (3 container statuses recorded)
Sep  3 22:59:47.569: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Sep  3 22:59:47.569: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  3 22:59:47.569: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  3 22:59:47.569: INFO: elasticsearch-logging-0 from ntnx-system started at 2020-09-03 21:56:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.569: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Sep  3 22:59:47.569: INFO: fluent-bit-wsm97 from ntnx-system started at 2020-09-03 21:56:19 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.569: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  3 22:59:47.569: INFO: kubernetes-events-printer-5767b7d649-8m9sl from ntnx-system started at 2020-09-03 21:56:21 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.569: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Sep  3 22:59:47.569: INFO: sonobuoy-systemd-logs-daemon-set-e29827cbd1744d01-f4pr7 from sonobuoy started at 2020-09-03 22:02:48 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.569: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  3 22:59:47.569: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  3 22:59:47.569: INFO: kube-flannel-ds-4vfnt from kube-system started at 2020-09-03 21:55:28 +0000 UTC (1 container statuses recorded)
Sep  3 22:59:47.569: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  3 22:59:47.569: INFO: node-exporter-wdljm from ntnx-system started at 2020-09-03 21:59:12 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  3 22:59:47.569: INFO: 	Container node-exporter ready: true, restart count 0
Sep  3 22:59:47.569: INFO: alertmanager-main-0 from ntnx-system started at 2020-09-03 22:57:59 +0000 UTC (2 container statuses recorded)
Sep  3 22:59:47.569: INFO: 	Container alertmanager ready: true, restart count 0
Sep  3 22:59:47.569: INFO: 	Container config-reloader ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16316844d8bec519], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16316844d93e61ec], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 22:59:48.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3430" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":277,"completed":217,"skipped":3855,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 22:59:48.607: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0903 23:00:28.664948      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  3 23:00:28.665: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:00:28.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6609" for this suite.

• [SLOW TEST:40.068 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":277,"completed":218,"skipped":3858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:00:28.676: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  3 23:00:28.714: INFO: Waiting up to 5m0s for pod "pod-70c05065-bc12-4363-acdb-212e6e7d3471" in namespace "emptydir-822" to be "Succeeded or Failed"
Sep  3 23:00:28.719: INFO: Pod "pod-70c05065-bc12-4363-acdb-212e6e7d3471": Phase="Pending", Reason="", readiness=false. Elapsed: 5.142105ms
Sep  3 23:00:30.724: INFO: Pod "pod-70c05065-bc12-4363-acdb-212e6e7d3471": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009922821s
STEP: Saw pod success
Sep  3 23:00:30.724: INFO: Pod "pod-70c05065-bc12-4363-acdb-212e6e7d3471" satisfied condition "Succeeded or Failed"
Sep  3 23:00:30.727: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-70c05065-bc12-4363-acdb-212e6e7d3471 container test-container: <nil>
STEP: delete the pod
Sep  3 23:00:30.746: INFO: Waiting for pod pod-70c05065-bc12-4363-acdb-212e6e7d3471 to disappear
Sep  3 23:00:30.748: INFO: Pod pod-70c05065-bc12-4363-acdb-212e6e7d3471 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:00:30.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-822" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":219,"skipped":3901,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:00:30.758: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:00:30.824: INFO: Create a RollingUpdate DaemonSet
Sep  3 23:00:30.829: INFO: Check that daemon pods launch on every node of the cluster
Sep  3 23:00:30.838: INFO: Number of nodes with available pods: 0
Sep  3 23:00:30.838: INFO: Node karbon-test1186mm-f1ba59-k8s-master-0 is running more than one daemon pod
Sep  3 23:00:31.850: INFO: Number of nodes with available pods: 0
Sep  3 23:00:31.850: INFO: Node karbon-test1186mm-f1ba59-k8s-master-0 is running more than one daemon pod
Sep  3 23:00:32.848: INFO: Number of nodes with available pods: 4
Sep  3 23:00:32.848: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-1 is running more than one daemon pod
Sep  3 23:00:33.846: INFO: Number of nodes with available pods: 5
Sep  3 23:00:33.846: INFO: Number of running nodes: 5, number of available pods: 5
Sep  3 23:00:33.846: INFO: Update the DaemonSet to trigger a rollout
Sep  3 23:00:33.855: INFO: Updating DaemonSet daemon-set
Sep  3 23:00:47.869: INFO: Roll back the DaemonSet before rollout is complete
Sep  3 23:00:47.878: INFO: Updating DaemonSet daemon-set
Sep  3 23:00:47.878: INFO: Make sure DaemonSet rollback is complete
Sep  3 23:00:47.881: INFO: Wrong image for pod: daemon-set-2s6bv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep  3 23:00:47.881: INFO: Pod daemon-set-2s6bv is not available
Sep  3 23:00:48.889: INFO: Wrong image for pod: daemon-set-2s6bv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep  3 23:00:48.889: INFO: Pod daemon-set-2s6bv is not available
Sep  3 23:00:49.889: INFO: Wrong image for pod: daemon-set-2s6bv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep  3 23:00:49.889: INFO: Pod daemon-set-2s6bv is not available
Sep  3 23:00:50.889: INFO: Pod daemon-set-7hv22 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7474, will wait for the garbage collector to delete the pods
Sep  3 23:00:50.964: INFO: Deleting DaemonSet.extensions daemon-set took: 11.265081ms
Sep  3 23:00:51.364: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.244676ms
Sep  3 23:02:05.667: INFO: Number of nodes with available pods: 0
Sep  3 23:02:05.667: INFO: Number of running nodes: 0, number of available pods: 0
Sep  3 23:02:05.669: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7474/daemonsets","resourceVersion":"28001"},"items":null}

Sep  3 23:02:05.671: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7474/pods","resourceVersion":"28001"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:02:05.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7474" for this suite.

• [SLOW TEST:94.935 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":277,"completed":220,"skipped":3908,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:02:05.693: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  3 23:02:11.776: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  3 23:02:11.779: INFO: Pod pod-with-poststart-http-hook still exists
Sep  3 23:02:13.779: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  3 23:02:13.783: INFO: Pod pod-with-poststart-http-hook still exists
Sep  3 23:02:15.779: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  3 23:02:15.784: INFO: Pod pod-with-poststart-http-hook still exists
Sep  3 23:02:17.779: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  3 23:02:17.784: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:02:17.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3230" for this suite.

• [SLOW TEST:12.101 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":277,"completed":221,"skipped":3914,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:02:17.794: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-9475
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  3 23:02:17.832: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  3 23:02:17.893: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 23:02:19.898: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 23:02:21.897: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 23:02:23.898: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 23:02:25.898: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:02:27.898: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:02:29.898: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:02:31.897: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:02:33.898: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:02:35.897: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:02:37.898: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:02:39.897: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:02:41.898: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  3 23:02:41.904: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  3 23:02:43.907: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  3 23:02:43.912: INFO: The status of Pod netserver-2 is Running (Ready = true)
Sep  3 23:02:43.916: INFO: The status of Pod netserver-3 is Running (Ready = true)
Sep  3 23:02:43.920: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Sep  3 23:02:45.941: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.13:8080/dial?request=hostname&protocol=http&host=172.20.0.10&port=8080&tries=1'] Namespace:pod-network-test-9475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:02:45.941: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:02:46.064: INFO: Waiting for responses: map[]
Sep  3 23:02:46.068: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.13:8080/dial?request=hostname&protocol=http&host=172.20.1.14&port=8080&tries=1'] Namespace:pod-network-test-9475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:02:46.068: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:02:46.191: INFO: Waiting for responses: map[]
Sep  3 23:02:46.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.13:8080/dial?request=hostname&protocol=http&host=172.20.2.12&port=8080&tries=1'] Namespace:pod-network-test-9475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:02:46.194: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:02:46.324: INFO: Waiting for responses: map[]
Sep  3 23:02:46.328: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.13:8080/dial?request=hostname&protocol=http&host=172.20.4.62&port=8080&tries=1'] Namespace:pod-network-test-9475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:02:46.328: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:02:46.459: INFO: Waiting for responses: map[]
Sep  3 23:02:46.467: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.13:8080/dial?request=hostname&protocol=http&host=172.20.3.30&port=8080&tries=1'] Namespace:pod-network-test-9475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:02:46.467: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:02:46.594: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:02:46.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9475" for this suite.

• [SLOW TEST:28.812 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":277,"completed":222,"skipped":3922,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:02:46.606: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 23:02:47.720: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 23:02:50.745: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:02:50.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7106" for this suite.
STEP: Destroying namespace "webhook-7106-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":277,"completed":223,"skipped":3940,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:02:50.871: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 23:02:51.456: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  3 23:02:53.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770971, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770971, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770971, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770971, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 23:02:56.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:02:56.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7859" for this suite.
STEP: Destroying namespace "webhook-7859-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.714 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":277,"completed":224,"skipped":3949,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:02:56.585: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep  3 23:03:00.640: INFO: &Pod{ObjectMeta:{send-events-0ae641da-a2c2-498b-9515-811c4b6520f6  events-1551 /api/v1/namespaces/events-1551/pods/send-events-0ae641da-a2c2-498b-9515-811c4b6520f6 b01ac927-7855-4e90-aa37-34a4667d272f 28550 0 2020-09-03 23:02:56 +0000 UTC <nil> <nil> map[name:foo time:618936415] map[] [] []  [{e2e.test Update v1 2020-09-03 23:02:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 116 105 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 112 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 114 116 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 99 111 110 116 97 105 110 101 114 80 111 114 116 92 34 58 56 48 44 92 34 112 114 111 116 111 99 111 108 92 34 58 92 34 84 67 80 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 99 111 110 116 97 105 110 101 114 80 111 114 116 34 58 123 125 44 34 102 58 112 114 111 116 111 99 111 108 34 58 123 125 125 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 23:02:58 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 50 46 49 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kpr97,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kpr97,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kpr97,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 23:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 23:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 23:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 23:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:172.20.2.16,StartTime:2020-09-03 23:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 23:02:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://ff73d82902040357d4aa259fabb24a8553ffa4595953476fec7caf1ad3039dad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.2.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep  3 23:03:02.645: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep  3 23:03:04.649: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:03:04.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1551" for this suite.

• [SLOW TEST:8.086 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":277,"completed":225,"skipped":3958,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:03:04.672: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep  3 23:03:05.215: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep  3 23:03:07.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770985, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770985, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770985, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734770985, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 23:03:10.260: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:03:10.264: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:03:11.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3834" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.781 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":277,"completed":226,"skipped":3992,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:03:11.453: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:03:15.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7757" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":277,"completed":227,"skipped":4001,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:03:15.913: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:03:23.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5842" for this suite.

• [SLOW TEST:8.056 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":277,"completed":228,"skipped":4005,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:03:23.970: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep  3 23:03:34.094: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0903 23:03:34.094847      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:03:34.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1065" for this suite.

• [SLOW TEST:10.134 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":277,"completed":229,"skipped":4022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:03:34.105: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:03:34.161: INFO: (0) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 22.619137ms)
Sep  3 23:03:34.165: INFO: (1) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.751045ms)
Sep  3 23:03:34.169: INFO: (2) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.710758ms)
Sep  3 23:03:34.171: INFO: (3) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.520377ms)
Sep  3 23:03:34.174: INFO: (4) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.883039ms)
Sep  3 23:03:34.177: INFO: (5) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.368341ms)
Sep  3 23:03:34.181: INFO: (6) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.989533ms)
Sep  3 23:03:34.184: INFO: (7) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.524779ms)
Sep  3 23:03:34.188: INFO: (8) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.541434ms)
Sep  3 23:03:34.191: INFO: (9) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.987876ms)
Sep  3 23:03:34.194: INFO: (10) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.311021ms)
Sep  3 23:03:34.197: INFO: (11) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.19153ms)
Sep  3 23:03:34.200: INFO: (12) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.788752ms)
Sep  3 23:03:34.203: INFO: (13) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.923671ms)
Sep  3 23:03:34.206: INFO: (14) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.805231ms)
Sep  3 23:03:34.208: INFO: (15) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.646297ms)
Sep  3 23:03:34.211: INFO: (16) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.727506ms)
Sep  3 23:03:34.214: INFO: (17) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.159297ms)
Sep  3 23:03:34.217: INFO: (18) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.798621ms)
Sep  3 23:03:34.221: INFO: (19) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.356434ms)
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:03:34.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6133" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":277,"completed":230,"skipped":4045,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:03:34.230: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep  3 23:03:34.262: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:03:38.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7206" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":277,"completed":231,"skipped":4062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:03:38.186: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  3 23:03:38.222: INFO: Waiting up to 5m0s for pod "pod-728a78d4-80c7-4bae-a9ff-817dbbe9e010" in namespace "emptydir-7311" to be "Succeeded or Failed"
Sep  3 23:03:38.225: INFO: Pod "pod-728a78d4-80c7-4bae-a9ff-817dbbe9e010": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258511ms
Sep  3 23:03:40.229: INFO: Pod "pod-728a78d4-80c7-4bae-a9ff-817dbbe9e010": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007203085s
Sep  3 23:03:42.233: INFO: Pod "pod-728a78d4-80c7-4bae-a9ff-817dbbe9e010": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011210024s
STEP: Saw pod success
Sep  3 23:03:42.233: INFO: Pod "pod-728a78d4-80c7-4bae-a9ff-817dbbe9e010" satisfied condition "Succeeded or Failed"
Sep  3 23:03:42.236: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-728a78d4-80c7-4bae-a9ff-817dbbe9e010 container test-container: <nil>
STEP: delete the pod
Sep  3 23:03:42.266: INFO: Waiting for pod pod-728a78d4-80c7-4bae-a9ff-817dbbe9e010 to disappear
Sep  3 23:03:42.269: INFO: Pod pod-728a78d4-80c7-4bae-a9ff-817dbbe9e010 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:03:42.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7311" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":232,"skipped":4106,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:03:42.278: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-6z278 in namespace proxy-4364
I0903 23:03:42.334790      22 runners.go:190] Created replication controller with name: proxy-service-6z278, namespace: proxy-4364, replica count: 1
I0903 23:03:43.385348      22 runners.go:190] proxy-service-6z278 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0903 23:03:44.385575      22 runners.go:190] proxy-service-6z278 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0903 23:03:45.385765      22 runners.go:190] proxy-service-6z278 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  3 23:03:45.388: INFO: setup took 3.077208257s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep  3 23:03:45.395: INFO: (0) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 6.192349ms)
Sep  3 23:03:45.396: INFO: (0) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 7.322824ms)
Sep  3 23:03:45.396: INFO: (0) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 6.681892ms)
Sep  3 23:03:45.396: INFO: (0) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 6.75872ms)
Sep  3 23:03:45.396: INFO: (0) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 6.637807ms)
Sep  3 23:03:45.396: INFO: (0) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 6.830217ms)
Sep  3 23:03:45.396: INFO: (0) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 7.087295ms)
Sep  3 23:03:45.396: INFO: (0) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 7.614784ms)
Sep  3 23:03:45.406: INFO: (0) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 16.194021ms)
Sep  3 23:03:45.406: INFO: (0) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 16.342434ms)
Sep  3 23:03:45.406: INFO: (0) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 16.231867ms)
Sep  3 23:03:45.408: INFO: (0) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 19.574496ms)
Sep  3 23:03:45.408: INFO: (0) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 19.384608ms)
Sep  3 23:03:45.408: INFO: (0) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 18.835906ms)
Sep  3 23:03:45.410: INFO: (0) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 20.133419ms)
Sep  3 23:03:45.410: INFO: (0) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 21.433423ms)
Sep  3 23:03:45.413: INFO: (1) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 3.100803ms)
Sep  3 23:03:45.414: INFO: (1) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 3.318332ms)
Sep  3 23:03:45.414: INFO: (1) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 3.838235ms)
Sep  3 23:03:45.414: INFO: (1) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 3.961758ms)
Sep  3 23:03:45.415: INFO: (1) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 4.768631ms)
Sep  3 23:03:45.416: INFO: (1) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 5.186683ms)
Sep  3 23:03:45.416: INFO: (1) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 5.841945ms)
Sep  3 23:03:45.416: INFO: (1) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.376599ms)
Sep  3 23:03:45.417: INFO: (1) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 6.137053ms)
Sep  3 23:03:45.417: INFO: (1) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 6.45931ms)
Sep  3 23:03:45.417: INFO: (1) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 6.536814ms)
Sep  3 23:03:45.417: INFO: (1) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 6.266743ms)
Sep  3 23:03:45.417: INFO: (1) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 6.331251ms)
Sep  3 23:03:45.417: INFO: (1) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 6.894018ms)
Sep  3 23:03:45.418: INFO: (1) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 7.199694ms)
Sep  3 23:03:45.418: INFO: (1) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 7.605201ms)
Sep  3 23:03:45.421: INFO: (2) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 3.454202ms)
Sep  3 23:03:45.422: INFO: (2) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 3.819147ms)
Sep  3 23:03:45.423: INFO: (2) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 4.697082ms)
Sep  3 23:03:45.423: INFO: (2) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 4.794861ms)
Sep  3 23:03:45.424: INFO: (2) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.243399ms)
Sep  3 23:03:45.424: INFO: (2) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 5.351598ms)
Sep  3 23:03:45.424: INFO: (2) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 5.553531ms)
Sep  3 23:03:45.424: INFO: (2) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 6.180556ms)
Sep  3 23:03:45.424: INFO: (2) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 6.007788ms)
Sep  3 23:03:45.424: INFO: (2) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 6.085861ms)
Sep  3 23:03:45.425: INFO: (2) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 6.343051ms)
Sep  3 23:03:45.425: INFO: (2) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 6.584398ms)
Sep  3 23:03:45.426: INFO: (2) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 7.735529ms)
Sep  3 23:03:45.426: INFO: (2) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 7.609596ms)
Sep  3 23:03:45.426: INFO: (2) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 8.072717ms)
Sep  3 23:03:45.426: INFO: (2) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 8.250249ms)
Sep  3 23:03:45.430: INFO: (3) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 3.029261ms)
Sep  3 23:03:45.430: INFO: (3) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 3.565201ms)
Sep  3 23:03:45.431: INFO: (3) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 4.173972ms)
Sep  3 23:03:45.431: INFO: (3) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.029954ms)
Sep  3 23:03:45.432: INFO: (3) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.945194ms)
Sep  3 23:03:45.432: INFO: (3) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 4.628821ms)
Sep  3 23:03:45.433: INFO: (3) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 5.891581ms)
Sep  3 23:03:45.433: INFO: (3) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 6.254105ms)
Sep  3 23:03:45.433: INFO: (3) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 5.199859ms)
Sep  3 23:03:45.433: INFO: (3) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 5.059883ms)
Sep  3 23:03:45.433: INFO: (3) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 5.885633ms)
Sep  3 23:03:45.433: INFO: (3) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 5.563019ms)
Sep  3 23:03:45.433: INFO: (3) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 6.08491ms)
Sep  3 23:03:45.434: INFO: (3) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 5.910887ms)
Sep  3 23:03:45.435: INFO: (3) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 6.190772ms)
Sep  3 23:03:45.435: INFO: (3) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 6.654029ms)
Sep  3 23:03:45.438: INFO: (4) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 3.053558ms)
Sep  3 23:03:45.438: INFO: (4) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 2.610468ms)
Sep  3 23:03:45.439: INFO: (4) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 3.635477ms)
Sep  3 23:03:45.439: INFO: (4) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.124728ms)
Sep  3 23:03:45.440: INFO: (4) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 4.104418ms)
Sep  3 23:03:45.440: INFO: (4) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.685053ms)
Sep  3 23:03:45.440: INFO: (4) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 4.98617ms)
Sep  3 23:03:45.440: INFO: (4) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 4.984047ms)
Sep  3 23:03:45.440: INFO: (4) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 5.067016ms)
Sep  3 23:03:45.440: INFO: (4) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.013729ms)
Sep  3 23:03:45.441: INFO: (4) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.152258ms)
Sep  3 23:03:45.445: INFO: (4) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 9.863263ms)
Sep  3 23:03:45.445: INFO: (4) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 9.926894ms)
Sep  3 23:03:45.445: INFO: (4) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 10.057769ms)
Sep  3 23:03:45.446: INFO: (4) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 10.123452ms)
Sep  3 23:03:45.446: INFO: (4) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 10.158311ms)
Sep  3 23:03:45.449: INFO: (5) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 2.565633ms)
Sep  3 23:03:45.449: INFO: (5) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 2.927279ms)
Sep  3 23:03:45.449: INFO: (5) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 2.892793ms)
Sep  3 23:03:45.450: INFO: (5) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 3.612471ms)
Sep  3 23:03:45.450: INFO: (5) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 4.576054ms)
Sep  3 23:03:45.451: INFO: (5) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.678653ms)
Sep  3 23:03:45.451: INFO: (5) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 4.675241ms)
Sep  3 23:03:45.451: INFO: (5) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 4.664501ms)
Sep  3 23:03:45.452: INFO: (5) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 5.583426ms)
Sep  3 23:03:45.452: INFO: (5) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 5.205006ms)
Sep  3 23:03:45.452: INFO: (5) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 5.345708ms)
Sep  3 23:03:45.452: INFO: (5) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.622574ms)
Sep  3 23:03:45.452: INFO: (5) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 6.494531ms)
Sep  3 23:03:45.452: INFO: (5) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 5.806787ms)
Sep  3 23:03:45.453: INFO: (5) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 5.665972ms)
Sep  3 23:03:45.453: INFO: (5) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 6.306478ms)
Sep  3 23:03:45.456: INFO: (6) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 3.038234ms)
Sep  3 23:03:45.457: INFO: (6) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 3.562784ms)
Sep  3 23:03:45.457: INFO: (6) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 3.667419ms)
Sep  3 23:03:45.457: INFO: (6) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 3.429415ms)
Sep  3 23:03:45.457: INFO: (6) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 4.334926ms)
Sep  3 23:03:45.457: INFO: (6) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 4.266621ms)
Sep  3 23:03:45.458: INFO: (6) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.310218ms)
Sep  3 23:03:45.458: INFO: (6) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 4.299038ms)
Sep  3 23:03:45.458: INFO: (6) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.247155ms)
Sep  3 23:03:45.458: INFO: (6) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.375369ms)
Sep  3 23:03:45.459: INFO: (6) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 5.564391ms)
Sep  3 23:03:45.459: INFO: (6) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 5.182143ms)
Sep  3 23:03:45.459: INFO: (6) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 5.647042ms)
Sep  3 23:03:45.459: INFO: (6) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 5.334588ms)
Sep  3 23:03:45.459: INFO: (6) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 5.978946ms)
Sep  3 23:03:45.460: INFO: (6) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 6.249366ms)
Sep  3 23:03:45.462: INFO: (7) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 2.657084ms)
Sep  3 23:03:45.463: INFO: (7) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 3.521708ms)
Sep  3 23:03:45.463: INFO: (7) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 3.457729ms)
Sep  3 23:03:45.463: INFO: (7) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 3.555128ms)
Sep  3 23:03:45.464: INFO: (7) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 3.828811ms)
Sep  3 23:03:45.464: INFO: (7) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.232068ms)
Sep  3 23:03:45.464: INFO: (7) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.160076ms)
Sep  3 23:03:45.465: INFO: (7) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 5.171938ms)
Sep  3 23:03:45.466: INFO: (7) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 5.124636ms)
Sep  3 23:03:45.466: INFO: (7) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 5.579546ms)
Sep  3 23:03:45.467: INFO: (7) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 6.77709ms)
Sep  3 23:03:45.467: INFO: (7) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 6.838047ms)
Sep  3 23:03:45.468: INFO: (7) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 6.836423ms)
Sep  3 23:03:45.468: INFO: (7) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 7.338049ms)
Sep  3 23:03:45.468: INFO: (7) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 7.572088ms)
Sep  3 23:03:45.470: INFO: (7) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 9.299039ms)
Sep  3 23:03:45.473: INFO: (8) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 2.696054ms)
Sep  3 23:03:45.473: INFO: (8) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 2.511419ms)
Sep  3 23:03:45.474: INFO: (8) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 3.330899ms)
Sep  3 23:03:45.474: INFO: (8) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 3.085991ms)
Sep  3 23:03:45.474: INFO: (8) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 3.342216ms)
Sep  3 23:03:45.475: INFO: (8) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.070634ms)
Sep  3 23:03:45.475: INFO: (8) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.176399ms)
Sep  3 23:03:45.475: INFO: (8) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 5.04795ms)
Sep  3 23:03:45.476: INFO: (8) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 4.734553ms)
Sep  3 23:03:45.476: INFO: (8) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 5.139721ms)
Sep  3 23:03:45.476: INFO: (8) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 5.050937ms)
Sep  3 23:03:45.476: INFO: (8) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 5.57643ms)
Sep  3 23:03:45.476: INFO: (8) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 6.426956ms)
Sep  3 23:03:45.477: INFO: (8) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 5.770973ms)
Sep  3 23:03:45.477: INFO: (8) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 6.014424ms)
Sep  3 23:03:45.477: INFO: (8) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 6.640308ms)
Sep  3 23:03:45.480: INFO: (9) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 3.29577ms)
Sep  3 23:03:45.481: INFO: (9) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 3.869904ms)
Sep  3 23:03:45.481: INFO: (9) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 4.133769ms)
Sep  3 23:03:45.482: INFO: (9) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 4.529106ms)
Sep  3 23:03:45.482: INFO: (9) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 4.385408ms)
Sep  3 23:03:45.482: INFO: (9) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 4.963811ms)
Sep  3 23:03:45.482: INFO: (9) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 5.032617ms)
Sep  3 23:03:45.483: INFO: (9) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.316482ms)
Sep  3 23:03:45.483: INFO: (9) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.700821ms)
Sep  3 23:03:45.483: INFO: (9) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 5.494895ms)
Sep  3 23:03:45.483: INFO: (9) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 5.686338ms)
Sep  3 23:03:45.483: INFO: (9) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 5.932243ms)
Sep  3 23:03:45.483: INFO: (9) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 6.233171ms)
Sep  3 23:03:45.483: INFO: (9) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 5.840443ms)
Sep  3 23:03:45.484: INFO: (9) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 6.174543ms)
Sep  3 23:03:45.485: INFO: (9) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 7.136245ms)
Sep  3 23:03:45.490: INFO: (10) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 4.20262ms)
Sep  3 23:03:45.491: INFO: (10) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 4.624868ms)
Sep  3 23:03:45.491: INFO: (10) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 4.768393ms)
Sep  3 23:03:45.491: INFO: (10) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.721194ms)
Sep  3 23:03:45.491: INFO: (10) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 5.630033ms)
Sep  3 23:03:45.491: INFO: (10) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 5.524242ms)
Sep  3 23:03:45.491: INFO: (10) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 6.706672ms)
Sep  3 23:03:45.491: INFO: (10) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 6.556031ms)
Sep  3 23:03:45.491: INFO: (10) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 6.299638ms)
Sep  3 23:03:45.492: INFO: (10) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 6.977042ms)
Sep  3 23:03:45.492: INFO: (10) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 6.921383ms)
Sep  3 23:03:45.492: INFO: (10) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 6.345231ms)
Sep  3 23:03:45.493: INFO: (10) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 6.211494ms)
Sep  3 23:03:45.493: INFO: (10) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 6.859692ms)
Sep  3 23:03:45.493: INFO: (10) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 6.534481ms)
Sep  3 23:03:45.493: INFO: (10) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 7.283144ms)
Sep  3 23:03:45.497: INFO: (11) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 3.000844ms)
Sep  3 23:03:45.497: INFO: (11) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 3.505217ms)
Sep  3 23:03:45.497: INFO: (11) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 3.052213ms)
Sep  3 23:03:45.498: INFO: (11) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 2.782491ms)
Sep  3 23:03:45.498: INFO: (11) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 3.571633ms)
Sep  3 23:03:45.499: INFO: (11) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 4.168345ms)
Sep  3 23:03:45.499: INFO: (11) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 4.935675ms)
Sep  3 23:03:45.499: INFO: (11) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 3.84395ms)
Sep  3 23:03:45.499: INFO: (11) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.334457ms)
Sep  3 23:03:45.500: INFO: (11) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.601983ms)
Sep  3 23:03:45.500: INFO: (11) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 5.734609ms)
Sep  3 23:03:45.500: INFO: (11) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 6.369842ms)
Sep  3 23:03:45.501: INFO: (11) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 5.718241ms)
Sep  3 23:03:45.501: INFO: (11) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 6.502628ms)
Sep  3 23:03:45.501: INFO: (11) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 5.438143ms)
Sep  3 23:03:45.501: INFO: (11) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 6.06128ms)
Sep  3 23:03:45.505: INFO: (12) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 3.590765ms)
Sep  3 23:03:45.505: INFO: (12) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 3.79118ms)
Sep  3 23:03:45.506: INFO: (12) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.040678ms)
Sep  3 23:03:45.506: INFO: (12) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 4.475048ms)
Sep  3 23:03:45.506: INFO: (12) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.292426ms)
Sep  3 23:03:45.506: INFO: (12) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.14878ms)
Sep  3 23:03:45.506: INFO: (12) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 4.239355ms)
Sep  3 23:03:45.506: INFO: (12) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 4.615124ms)
Sep  3 23:03:45.506: INFO: (12) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 4.650296ms)
Sep  3 23:03:45.507: INFO: (12) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 4.641516ms)
Sep  3 23:03:45.508: INFO: (12) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 6.182778ms)
Sep  3 23:03:45.508: INFO: (12) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 6.553306ms)
Sep  3 23:03:45.508: INFO: (12) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 6.349911ms)
Sep  3 23:03:45.509: INFO: (12) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 6.907877ms)
Sep  3 23:03:45.509: INFO: (12) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 7.189797ms)
Sep  3 23:03:45.511: INFO: (12) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 9.463573ms)
Sep  3 23:03:45.515: INFO: (13) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 3.72237ms)
Sep  3 23:03:45.517: INFO: (13) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.55211ms)
Sep  3 23:03:45.517: INFO: (13) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 5.209159ms)
Sep  3 23:03:45.517: INFO: (13) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.717619ms)
Sep  3 23:03:45.517: INFO: (13) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.827664ms)
Sep  3 23:03:45.517: INFO: (13) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 5.180788ms)
Sep  3 23:03:45.517: INFO: (13) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 5.055017ms)
Sep  3 23:03:45.518: INFO: (13) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 5.911008ms)
Sep  3 23:03:45.518: INFO: (13) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 5.927484ms)
Sep  3 23:03:45.518: INFO: (13) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 5.883652ms)
Sep  3 23:03:45.518: INFO: (13) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 5.793958ms)
Sep  3 23:03:45.518: INFO: (13) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 6.298318ms)
Sep  3 23:03:45.519: INFO: (13) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 7.028068ms)
Sep  3 23:03:45.519: INFO: (13) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 6.59408ms)
Sep  3 23:03:45.519: INFO: (13) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 6.514886ms)
Sep  3 23:03:45.520: INFO: (13) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 7.415837ms)
Sep  3 23:03:45.523: INFO: (14) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 2.986333ms)
Sep  3 23:03:45.524: INFO: (14) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 3.468694ms)
Sep  3 23:03:45.524: INFO: (14) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 3.861981ms)
Sep  3 23:03:45.524: INFO: (14) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 4.411477ms)
Sep  3 23:03:45.524: INFO: (14) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 4.362906ms)
Sep  3 23:03:45.525: INFO: (14) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.725232ms)
Sep  3 23:03:45.525: INFO: (14) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 4.243843ms)
Sep  3 23:03:45.526: INFO: (14) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 5.703342ms)
Sep  3 23:03:45.527: INFO: (14) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 6.688962ms)
Sep  3 23:03:45.527: INFO: (14) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 5.596889ms)
Sep  3 23:03:45.527: INFO: (14) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 6.394077ms)
Sep  3 23:03:45.527: INFO: (14) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.496044ms)
Sep  3 23:03:45.527: INFO: (14) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 6.459874ms)
Sep  3 23:03:45.528: INFO: (14) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 6.284594ms)
Sep  3 23:03:45.528: INFO: (14) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 6.717504ms)
Sep  3 23:03:45.528: INFO: (14) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 7.248654ms)
Sep  3 23:03:45.532: INFO: (15) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 3.480355ms)
Sep  3 23:03:45.532: INFO: (15) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 3.234058ms)
Sep  3 23:03:45.534: INFO: (15) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 3.992187ms)
Sep  3 23:03:45.534: INFO: (15) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 4.69804ms)
Sep  3 23:03:45.534: INFO: (15) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 3.902612ms)
Sep  3 23:03:45.534: INFO: (15) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 5.491899ms)
Sep  3 23:03:45.536: INFO: (15) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 5.782865ms)
Sep  3 23:03:45.536: INFO: (15) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 6.433785ms)
Sep  3 23:03:45.536: INFO: (15) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 5.617377ms)
Sep  3 23:03:45.536: INFO: (15) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 6.663503ms)
Sep  3 23:03:45.536: INFO: (15) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 6.048541ms)
Sep  3 23:03:45.536: INFO: (15) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 6.433559ms)
Sep  3 23:03:45.536: INFO: (15) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 6.371255ms)
Sep  3 23:03:45.536: INFO: (15) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.984637ms)
Sep  3 23:03:45.539: INFO: (15) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 9.362812ms)
Sep  3 23:03:45.540: INFO: (15) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 10.860086ms)
Sep  3 23:03:45.544: INFO: (16) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.087984ms)
Sep  3 23:03:45.545: INFO: (16) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.411106ms)
Sep  3 23:03:45.545: INFO: (16) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 4.457003ms)
Sep  3 23:03:45.545: INFO: (16) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 4.420676ms)
Sep  3 23:03:45.545: INFO: (16) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 4.57383ms)
Sep  3 23:03:45.546: INFO: (16) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 5.255057ms)
Sep  3 23:03:45.546: INFO: (16) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 5.677936ms)
Sep  3 23:03:45.546: INFO: (16) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 5.647641ms)
Sep  3 23:03:45.547: INFO: (16) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 6.189605ms)
Sep  3 23:03:45.547: INFO: (16) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 6.759185ms)
Sep  3 23:03:45.547: INFO: (16) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 6.892365ms)
Sep  3 23:03:45.547: INFO: (16) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 7.013817ms)
Sep  3 23:03:45.547: INFO: (16) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 6.772097ms)
Sep  3 23:03:45.548: INFO: (16) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 7.311589ms)
Sep  3 23:03:45.548: INFO: (16) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 7.239782ms)
Sep  3 23:03:45.548: INFO: (16) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 7.251637ms)
Sep  3 23:03:45.552: INFO: (17) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 3.884552ms)
Sep  3 23:03:45.552: INFO: (17) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 4.191002ms)
Sep  3 23:03:45.552: INFO: (17) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 4.381632ms)
Sep  3 23:03:45.552: INFO: (17) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 4.367394ms)
Sep  3 23:03:45.553: INFO: (17) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 4.590807ms)
Sep  3 23:03:45.553: INFO: (17) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 5.191361ms)
Sep  3 23:03:45.553: INFO: (17) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 5.430681ms)
Sep  3 23:03:45.553: INFO: (17) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 5.442158ms)
Sep  3 23:03:45.553: INFO: (17) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 5.507787ms)
Sep  3 23:03:45.554: INFO: (17) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 6.367442ms)
Sep  3 23:03:45.554: INFO: (17) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 6.583352ms)
Sep  3 23:03:45.555: INFO: (17) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 7.083464ms)
Sep  3 23:03:45.555: INFO: (17) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 7.279109ms)
Sep  3 23:03:45.555: INFO: (17) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 7.609396ms)
Sep  3 23:03:45.556: INFO: (17) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 7.716227ms)
Sep  3 23:03:45.556: INFO: (17) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 7.735756ms)
Sep  3 23:03:45.561: INFO: (18) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 4.751779ms)
Sep  3 23:03:45.561: INFO: (18) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 5.210069ms)
Sep  3 23:03:45.562: INFO: (18) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 5.97974ms)
Sep  3 23:03:45.563: INFO: (18) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 6.348665ms)
Sep  3 23:03:45.563: INFO: (18) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 6.953455ms)
Sep  3 23:03:45.563: INFO: (18) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 7.167352ms)
Sep  3 23:03:45.564: INFO: (18) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 7.028726ms)
Sep  3 23:03:45.565: INFO: (18) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 8.401866ms)
Sep  3 23:03:45.565: INFO: (18) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 9.203898ms)
Sep  3 23:03:45.565: INFO: (18) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 8.507818ms)
Sep  3 23:03:45.565: INFO: (18) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 9.363354ms)
Sep  3 23:03:45.565: INFO: (18) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 8.668974ms)
Sep  3 23:03:45.565: INFO: (18) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 8.722259ms)
Sep  3 23:03:45.565: INFO: (18) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 8.965992ms)
Sep  3 23:03:45.565: INFO: (18) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 9.146069ms)
Sep  3 23:03:45.565: INFO: (18) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 8.840465ms)
Sep  3 23:03:45.570: INFO: (19) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:443/proxy/tlsrewritem... (200; 4.557547ms)
Sep  3 23:03:45.571: INFO: (19) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:460/proxy/: tls baz (200; 5.558528ms)
Sep  3 23:03:45.572: INFO: (19) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname1/proxy/: foo (200; 7.028236ms)
Sep  3 23:03:45.573: INFO: (19) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:1080/proxy/rewriteme">... (200; 6.962189ms)
Sep  3 23:03:45.574: INFO: (19) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:162/proxy/: bar (200; 7.977403ms)
Sep  3 23:03:45.574: INFO: (19) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp/proxy/rewriteme">test</a> (200; 7.874571ms)
Sep  3 23:03:45.574: INFO: (19) /api/v1/namespaces/proxy-4364/pods/http:proxy-service-6z278-t6wmp:160/proxy/: foo (200; 8.198251ms)
Sep  3 23:03:45.574: INFO: (19) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:1080/proxy/rewriteme">test<... (200; 7.84325ms)
Sep  3 23:03:45.574: INFO: (19) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:160/proxy/: foo (200; 7.577343ms)
Sep  3 23:03:45.574: INFO: (19) /api/v1/namespaces/proxy-4364/pods/proxy-service-6z278-t6wmp:162/proxy/: bar (200; 8.426473ms)
Sep  3 23:03:45.575: INFO: (19) /api/v1/namespaces/proxy-4364/pods/https:proxy-service-6z278-t6wmp:462/proxy/: tls qux (200; 8.527592ms)
Sep  3 23:03:45.575: INFO: (19) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname2/proxy/: tls qux (200; 8.77624ms)
Sep  3 23:03:45.575: INFO: (19) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname2/proxy/: bar (200; 8.546841ms)
Sep  3 23:03:45.575: INFO: (19) /api/v1/namespaces/proxy-4364/services/http:proxy-service-6z278:portname1/proxy/: foo (200; 8.875279ms)
Sep  3 23:03:45.576: INFO: (19) /api/v1/namespaces/proxy-4364/services/proxy-service-6z278:portname2/proxy/: bar (200; 8.993967ms)
Sep  3 23:03:45.576: INFO: (19) /api/v1/namespaces/proxy-4364/services/https:proxy-service-6z278:tlsportname1/proxy/: tls baz (200; 9.072439ms)
STEP: deleting ReplicationController proxy-service-6z278 in namespace proxy-4364, will wait for the garbage collector to delete the pods
Sep  3 23:03:45.636: INFO: Deleting ReplicationController proxy-service-6z278 took: 8.192602ms
Sep  3 23:03:45.736: INFO: Terminating ReplicationController proxy-service-6z278 pods took: 100.208317ms
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:03:57.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4364" for this suite.

• [SLOW TEST:14.869 seconds]
[sig-network] Proxy
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":277,"completed":233,"skipped":4107,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:03:57.148: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:03:57.203: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep  3 23:03:57.212: INFO: Number of nodes with available pods: 0
Sep  3 23:03:57.212: INFO: Node karbon-test1186mm-f1ba59-k8s-master-0 is running more than one daemon pod
Sep  3 23:03:58.223: INFO: Number of nodes with available pods: 0
Sep  3 23:03:58.223: INFO: Node karbon-test1186mm-f1ba59-k8s-master-0 is running more than one daemon pod
Sep  3 23:03:59.221: INFO: Number of nodes with available pods: 4
Sep  3 23:03:59.221: INFO: Node karbon-test1186mm-f1ba59-k8s-master-1 is running more than one daemon pod
Sep  3 23:04:00.220: INFO: Number of nodes with available pods: 5
Sep  3 23:04:00.220: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep  3 23:04:00.246: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:00.246: INFO: Wrong image for pod: daemon-set-br9g7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:00.246: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:00.246: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:00.246: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:01.255: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:01.255: INFO: Wrong image for pod: daemon-set-br9g7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:01.255: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:01.255: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:01.255: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:02.255: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:02.255: INFO: Wrong image for pod: daemon-set-br9g7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:02.255: INFO: Pod daemon-set-br9g7 is not available
Sep  3 23:04:02.255: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:02.255: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:02.255: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:03.258: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:03.258: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:03.258: INFO: Pod daemon-set-dr59b is not available
Sep  3 23:04:03.258: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:03.258: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:04.254: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:04.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:04.254: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:04.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:05.255: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:05.255: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:05.255: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:05.255: INFO: Pod daemon-set-fw8hl is not available
Sep  3 23:04:05.255: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:06.254: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:06.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:06.254: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:06.254: INFO: Pod daemon-set-fw8hl is not available
Sep  3 23:04:06.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:07.253: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:07.253: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:07.253: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:07.253: INFO: Pod daemon-set-fw8hl is not available
Sep  3 23:04:07.253: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:08.254: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:08.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:08.254: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:08.254: INFO: Pod daemon-set-fw8hl is not available
Sep  3 23:04:08.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:09.255: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:09.256: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:09.256: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:09.256: INFO: Pod daemon-set-fw8hl is not available
Sep  3 23:04:09.256: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:10.253: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:10.253: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:10.253: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:10.253: INFO: Pod daemon-set-fw8hl is not available
Sep  3 23:04:10.253: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:11.254: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:11.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:11.254: INFO: Wrong image for pod: daemon-set-fw8hl. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:11.254: INFO: Pod daemon-set-fw8hl is not available
Sep  3 23:04:11.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:12.255: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:12.255: INFO: Pod daemon-set-bpkqf is not available
Sep  3 23:04:12.255: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:12.255: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:13.254: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:13.254: INFO: Pod daemon-set-bpkqf is not available
Sep  3 23:04:13.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:13.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:14.254: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:14.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:14.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:15.254: INFO: Wrong image for pod: daemon-set-9rs7j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:15.254: INFO: Pod daemon-set-9rs7j is not available
Sep  3 23:04:15.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:15.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:16.255: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:16.255: INFO: Pod daemon-set-jbbw9 is not available
Sep  3 23:04:16.255: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:17.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:17.254: INFO: Pod daemon-set-jbbw9 is not available
Sep  3 23:04:17.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:18.255: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:18.255: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:19.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:19.255: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:20.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:20.254: INFO: Pod daemon-set-cckg8 is not available
Sep  3 23:04:20.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:21.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:21.254: INFO: Pod daemon-set-cckg8 is not available
Sep  3 23:04:21.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:22.254: INFO: Wrong image for pod: daemon-set-cckg8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:22.254: INFO: Pod daemon-set-cckg8 is not available
Sep  3 23:04:22.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:23.255: INFO: Pod daemon-set-bp9kg is not available
Sep  3 23:04:23.255: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:24.254: INFO: Pod daemon-set-bp9kg is not available
Sep  3 23:04:24.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:25.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:26.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:26.254: INFO: Pod daemon-set-mdzsr is not available
Sep  3 23:04:27.255: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:27.255: INFO: Pod daemon-set-mdzsr is not available
Sep  3 23:04:28.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:28.254: INFO: Pod daemon-set-mdzsr is not available
Sep  3 23:04:29.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:29.254: INFO: Pod daemon-set-mdzsr is not available
Sep  3 23:04:30.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:30.254: INFO: Pod daemon-set-mdzsr is not available
Sep  3 23:04:31.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:31.255: INFO: Pod daemon-set-mdzsr is not available
Sep  3 23:04:32.254: INFO: Wrong image for pod: daemon-set-mdzsr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep  3 23:04:32.254: INFO: Pod daemon-set-mdzsr is not available
Sep  3 23:04:33.254: INFO: Pod daemon-set-qtmr7 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep  3 23:04:33.266: INFO: Number of nodes with available pods: 4
Sep  3 23:04:33.266: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-2 is running more than one daemon pod
Sep  3 23:04:34.274: INFO: Number of nodes with available pods: 4
Sep  3 23:04:34.274: INFO: Node karbon-test1186mm-f1ba59-k8s-worker-2 is running more than one daemon pod
Sep  3 23:04:35.275: INFO: Number of nodes with available pods: 5
Sep  3 23:04:35.275: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-216, will wait for the garbage collector to delete the pods
Sep  3 23:04:35.349: INFO: Deleting DaemonSet.extensions daemon-set took: 8.114894ms
Sep  3 23:04:35.750: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.216132ms
Sep  3 23:04:47.153: INFO: Number of nodes with available pods: 0
Sep  3 23:04:47.153: INFO: Number of running nodes: 0, number of available pods: 0
Sep  3 23:04:47.155: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-216/daemonsets","resourceVersion":"29833"},"items":null}

Sep  3 23:04:47.158: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-216/pods","resourceVersion":"29833"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:04:47.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-216" for this suite.

• [SLOW TEST:50.033 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":277,"completed":234,"skipped":4112,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:04:47.181: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep  3 23:04:47.226: INFO: Waiting up to 5m0s for pod "downward-api-00289f1d-bb0b-430a-b380-5ae3b63f6d1e" in namespace "downward-api-6277" to be "Succeeded or Failed"
Sep  3 23:04:47.230: INFO: Pod "downward-api-00289f1d-bb0b-430a-b380-5ae3b63f6d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.121842ms
Sep  3 23:04:49.234: INFO: Pod "downward-api-00289f1d-bb0b-430a-b380-5ae3b63f6d1e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008477311s
Sep  3 23:04:51.238: INFO: Pod "downward-api-00289f1d-bb0b-430a-b380-5ae3b63f6d1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012411085s
STEP: Saw pod success
Sep  3 23:04:51.238: INFO: Pod "downward-api-00289f1d-bb0b-430a-b380-5ae3b63f6d1e" satisfied condition "Succeeded or Failed"
Sep  3 23:04:51.241: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downward-api-00289f1d-bb0b-430a-b380-5ae3b63f6d1e container dapi-container: <nil>
STEP: delete the pod
Sep  3 23:04:51.265: INFO: Waiting for pod downward-api-00289f1d-bb0b-430a-b380-5ae3b63f6d1e to disappear
Sep  3 23:04:51.268: INFO: Pod downward-api-00289f1d-bb0b-430a-b380-5ae3b63f6d1e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:04:51.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6277" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":277,"completed":235,"skipped":4119,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:04:51.277: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep  3 23:04:55.334: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1171 PodName:pod-sharedvolume-b06f2d02-f3bc-40d8-bb52-97690532f79d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:04:55.334: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:04:55.448: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:04:55.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1171" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":277,"completed":236,"skipped":4184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:04:55.459: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:04:55.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 version'
Sep  3 23:04:55.564: INFO: stderr: ""
Sep  3 23:04:55.565: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:58:53Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:51:04Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:04:55.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7109" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":277,"completed":237,"skipped":4206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:04:55.577: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  3 23:04:59.655: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  3 23:04:59.658: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  3 23:05:01.658: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  3 23:05:01.662: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  3 23:05:03.658: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  3 23:05:03.662: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:05:03.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3823" for this suite.

• [SLOW TEST:8.097 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":277,"completed":238,"skipped":4228,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:05:03.674: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-8b00f2b4-0602-4d41-b1b0-b04b62906133
STEP: Creating a pod to test consume configMaps
Sep  3 23:05:03.724: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-01d3a24d-b260-4ae4-9219-f304360b1b35" in namespace "projected-5454" to be "Succeeded or Failed"
Sep  3 23:05:03.727: INFO: Pod "pod-projected-configmaps-01d3a24d-b260-4ae4-9219-f304360b1b35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.005527ms
Sep  3 23:05:05.731: INFO: Pod "pod-projected-configmaps-01d3a24d-b260-4ae4-9219-f304360b1b35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00735412s
STEP: Saw pod success
Sep  3 23:05:05.731: INFO: Pod "pod-projected-configmaps-01d3a24d-b260-4ae4-9219-f304360b1b35" satisfied condition "Succeeded or Failed"
Sep  3 23:05:05.734: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-configmaps-01d3a24d-b260-4ae4-9219-f304360b1b35 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 23:05:05.755: INFO: Waiting for pod pod-projected-configmaps-01d3a24d-b260-4ae4-9219-f304360b1b35 to disappear
Sep  3 23:05:05.758: INFO: Pod pod-projected-configmaps-01d3a24d-b260-4ae4-9219-f304360b1b35 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:05:05.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5454" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":239,"skipped":4232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:05:05.771: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:05:07.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8812" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":240,"skipped":4269,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:05:07.843: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Sep  3 23:05:07.881: INFO: Waiting up to 5m0s for pod "client-containers-6cb69c69-ab74-442d-90a6-f7d36ab56ec6" in namespace "containers-1147" to be "Succeeded or Failed"
Sep  3 23:05:07.883: INFO: Pod "client-containers-6cb69c69-ab74-442d-90a6-f7d36ab56ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.683697ms
Sep  3 23:05:09.887: INFO: Pod "client-containers-6cb69c69-ab74-442d-90a6-f7d36ab56ec6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006106163s
STEP: Saw pod success
Sep  3 23:05:09.887: INFO: Pod "client-containers-6cb69c69-ab74-442d-90a6-f7d36ab56ec6" satisfied condition "Succeeded or Failed"
Sep  3 23:05:09.889: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod client-containers-6cb69c69-ab74-442d-90a6-f7d36ab56ec6 container test-container: <nil>
STEP: delete the pod
Sep  3 23:05:09.909: INFO: Waiting for pod client-containers-6cb69c69-ab74-442d-90a6-f7d36ab56ec6 to disappear
Sep  3 23:05:09.911: INFO: Pod client-containers-6cb69c69-ab74-442d-90a6-f7d36ab56ec6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:05:09.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1147" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":277,"completed":241,"skipped":4275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:05:09.923: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:05:09.953: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  3 23:05:12.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2174 create -f -'
Sep  3 23:05:12.565: INFO: stderr: ""
Sep  3 23:05:12.565: INFO: stdout: "e2e-test-crd-publish-openapi-7397-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  3 23:05:12.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2174 delete e2e-test-crd-publish-openapi-7397-crds test-cr'
Sep  3 23:05:12.662: INFO: stderr: ""
Sep  3 23:05:12.662: INFO: stdout: "e2e-test-crd-publish-openapi-7397-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  3 23:05:12.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2174 apply -f -'
Sep  3 23:05:12.860: INFO: stderr: ""
Sep  3 23:05:12.860: INFO: stdout: "e2e-test-crd-publish-openapi-7397-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  3 23:05:12.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 --namespace=crd-publish-openapi-2174 delete e2e-test-crd-publish-openapi-7397-crds test-cr'
Sep  3 23:05:12.958: INFO: stderr: ""
Sep  3 23:05:12.958: INFO: stdout: "e2e-test-crd-publish-openapi-7397-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep  3 23:05:12.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 explain e2e-test-crd-publish-openapi-7397-crds'
Sep  3 23:05:13.238: INFO: stderr: ""
Sep  3 23:05:13.238: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7397-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:05:16.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2174" for this suite.

• [SLOW TEST:6.889 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":277,"completed":242,"skipped":4329,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:05:16.812: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:05:44.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4716" for this suite.

• [SLOW TEST:27.262 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    when starting a container that exits
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":277,"completed":243,"skipped":4332,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:05:44.075: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:05:44.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3612" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":277,"completed":244,"skipped":4335,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:05:44.154: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-206
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-206
STEP: creating replication controller externalsvc in namespace services-206
I0903 23:05:44.230082      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-206, replica count: 2
I0903 23:05:47.280630      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep  3 23:05:47.305: INFO: Creating new exec pod
Sep  3 23:05:51.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=services-206 execpod4qr76 -- /bin/sh -x -c nslookup nodeport-service'
Sep  3 23:05:51.545: INFO: stderr: "+ nslookup nodeport-service\n"
Sep  3 23:05:51.545: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nnodeport-service.services-206.svc.cluster.local\tcanonical name = externalsvc.services-206.svc.cluster.local.\nName:\texternalsvc.services-206.svc.cluster.local\nAddress: 172.19.139.130\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-206, will wait for the garbage collector to delete the pods
Sep  3 23:05:51.603: INFO: Deleting ReplicationController externalsvc took: 6.010855ms
Sep  3 23:05:52.004: INFO: Terminating ReplicationController externalsvc pods took: 400.271499ms
Sep  3 23:06:02.829: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:06:02.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-206" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:18.701 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":277,"completed":245,"skipped":4339,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:06:02.855: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-ae98bbfc-c80d-41e1-b3bf-013179ef7829
STEP: Creating a pod to test consume configMaps
Sep  3 23:06:02.899: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d2ecce83-7ef5-4cfc-abee-5ad027c695ab" in namespace "projected-7647" to be "Succeeded or Failed"
Sep  3 23:06:02.901: INFO: Pod "pod-projected-configmaps-d2ecce83-7ef5-4cfc-abee-5ad027c695ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.550581ms
Sep  3 23:06:04.906: INFO: Pod "pod-projected-configmaps-d2ecce83-7ef5-4cfc-abee-5ad027c695ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007405602s
STEP: Saw pod success
Sep  3 23:06:04.906: INFO: Pod "pod-projected-configmaps-d2ecce83-7ef5-4cfc-abee-5ad027c695ab" satisfied condition "Succeeded or Failed"
Sep  3 23:06:04.910: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-configmaps-d2ecce83-7ef5-4cfc-abee-5ad027c695ab container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 23:06:04.934: INFO: Waiting for pod pod-projected-configmaps-d2ecce83-7ef5-4cfc-abee-5ad027c695ab to disappear
Sep  3 23:06:04.937: INFO: Pod pod-projected-configmaps-d2ecce83-7ef5-4cfc-abee-5ad027c695ab no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:06:04.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7647" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":246,"skipped":4347,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:06:04.952: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:06:04.989: INFO: Waiting up to 5m0s for pod "busybox-user-65534-43433c21-e507-419a-8168-bd3d9ccc5249" in namespace "security-context-test-3757" to be "Succeeded or Failed"
Sep  3 23:06:04.992: INFO: Pod "busybox-user-65534-43433c21-e507-419a-8168-bd3d9ccc5249": Phase="Pending", Reason="", readiness=false. Elapsed: 3.098694ms
Sep  3 23:06:06.996: INFO: Pod "busybox-user-65534-43433c21-e507-419a-8168-bd3d9ccc5249": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007258581s
Sep  3 23:06:06.996: INFO: Pod "busybox-user-65534-43433c21-e507-419a-8168-bd3d9ccc5249" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:06:06.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3757" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":247,"skipped":4375,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:06:07.008: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-5969
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  3 23:06:07.050: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  3 23:06:07.115: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 23:06:09.119: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:06:11.119: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:06:13.120: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:06:15.119: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:06:17.119: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:06:19.119: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:06:21.118: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  3 23:06:21.123: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  3 23:06:23.128: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  3 23:06:25.127: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  3 23:06:27.127: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  3 23:06:29.127: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  3 23:06:29.132: INFO: The status of Pod netserver-2 is Running (Ready = true)
Sep  3 23:06:29.138: INFO: The status of Pod netserver-3 is Running (Ready = true)
Sep  3 23:06:29.142: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Sep  3 23:06:31.169: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.0.13:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5969 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:06:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:06:31.281: INFO: Found all expected endpoints: [netserver-0]
Sep  3 23:06:31.285: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.1.17:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5969 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:06:31.285: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:06:31.394: INFO: Found all expected endpoints: [netserver-1]
Sep  3 23:06:31.397: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.2.54:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5969 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:06:31.397: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:06:31.515: INFO: Found all expected endpoints: [netserver-2]
Sep  3 23:06:31.518: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.4.67:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5969 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:06:31.518: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:06:31.640: INFO: Found all expected endpoints: [netserver-3]
Sep  3 23:06:31.643: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.3.33:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5969 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:06:31.643: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:06:31.769: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:06:31.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5969" for this suite.

• [SLOW TEST:24.771 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":248,"skipped":4382,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:06:31.780: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-71e30012-d2bf-4ac0-8110-0adb28e409e0
STEP: Creating a pod to test consume configMaps
Sep  3 23:06:31.827: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98067609-58e2-418d-9855-7dee366bcd45" in namespace "projected-5669" to be "Succeeded or Failed"
Sep  3 23:06:31.831: INFO: Pod "pod-projected-configmaps-98067609-58e2-418d-9855-7dee366bcd45": Phase="Pending", Reason="", readiness=false. Elapsed: 3.324294ms
Sep  3 23:06:33.835: INFO: Pod "pod-projected-configmaps-98067609-58e2-418d-9855-7dee366bcd45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007743237s
STEP: Saw pod success
Sep  3 23:06:33.835: INFO: Pod "pod-projected-configmaps-98067609-58e2-418d-9855-7dee366bcd45" satisfied condition "Succeeded or Failed"
Sep  3 23:06:33.838: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-configmaps-98067609-58e2-418d-9855-7dee366bcd45 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  3 23:06:33.859: INFO: Waiting for pod pod-projected-configmaps-98067609-58e2-418d-9855-7dee366bcd45 to disappear
Sep  3 23:06:33.861: INFO: Pod pod-projected-configmaps-98067609-58e2-418d-9855-7dee366bcd45 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:06:33.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5669" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":249,"skipped":4422,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:06:33.870: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-2776
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2776
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2776
Sep  3 23:06:33.921: INFO: Found 0 stateful pods, waiting for 1
Sep  3 23:06:43.926: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep  3 23:06:43.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-2776 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 23:06:44.145: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 23:06:44.145: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 23:06:44.145: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 23:06:44.149: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  3 23:06:54.153: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 23:06:54.153: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 23:06:54.167: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999654s
Sep  3 23:06:55.171: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997114153s
Sep  3 23:06:56.176: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992790433s
Sep  3 23:06:57.180: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988242868s
Sep  3 23:06:58.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984004916s
Sep  3 23:06:59.189: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979796923s
Sep  3 23:07:00.194: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.974611713s
Sep  3 23:07:01.199: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.970114546s
Sep  3 23:07:02.204: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.965386045s
Sep  3 23:07:03.208: INFO: Verifying statefulset ss doesn't scale past 1 for another 960.441486ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2776
Sep  3 23:07:04.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-2776 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 23:07:04.424: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 23:07:04.425: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 23:07:04.425: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 23:07:04.430: INFO: Found 1 stateful pods, waiting for 3
Sep  3 23:07:14.435: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 23:07:14.435: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 23:07:14.435: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep  3 23:07:14.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-2776 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 23:07:14.666: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 23:07:14.666: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 23:07:14.666: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 23:07:14.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-2776 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 23:07:14.904: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 23:07:14.904: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 23:07:14.904: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 23:07:14.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-2776 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  3 23:07:15.122: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  3 23:07:15.122: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  3 23:07:15.122: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  3 23:07:15.122: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 23:07:15.125: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  3 23:07:25.132: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 23:07:25.133: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 23:07:25.133: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  3 23:07:25.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999594s
Sep  3 23:07:26.148: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997017461s
Sep  3 23:07:27.154: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992171673s
Sep  3 23:07:28.159: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986434887s
Sep  3 23:07:29.164: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981433597s
Sep  3 23:07:30.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976592993s
Sep  3 23:07:31.173: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972154224s
Sep  3 23:07:32.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967275389s
Sep  3 23:07:33.183: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962932104s
Sep  3 23:07:34.187: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.930271ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2776
Sep  3 23:07:35.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-2776 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 23:07:35.397: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 23:07:35.397: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 23:07:35.397: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 23:07:35.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-2776 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 23:07:35.593: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 23:07:35.593: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 23:07:35.593: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 23:07:35.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-476854905 exec --namespace=statefulset-2776 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  3 23:07:35.787: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  3 23:07:35.788: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  3 23:07:35.788: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  3 23:07:35.788: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep  3 23:08:05.821: INFO: Deleting all statefulset in ns statefulset-2776
Sep  3 23:08:05.824: INFO: Scaling statefulset ss to 0
Sep  3 23:08:05.831: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 23:08:05.834: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:08:05.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2776" for this suite.

• [SLOW TEST:91.991 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":277,"completed":250,"skipped":4428,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:08:05.861: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep  3 23:08:05.890: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  3 23:09:05.927: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:09:05.929: INFO: Starting informer...
STEP: Starting pod...
Sep  3 23:09:06.141: INFO: Pod is running on karbon-test1186mm-f1ba59-k8s-worker-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep  3 23:09:06.158: INFO: Pod wasn't evicted. Proceeding
Sep  3 23:09:06.158: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep  3 23:10:21.189: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:10:21.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3969" for this suite.

• [SLOW TEST:135.339 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":277,"completed":251,"skipped":4434,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:10:21.200: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 23:10:21.926: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 23:10:24.950: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:10:25.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6690" for this suite.
STEP: Destroying namespace "webhook-6690-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":277,"completed":252,"skipped":4455,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:10:25.106: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-5279
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  3 23:10:25.143: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  3 23:10:25.225: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 23:10:27.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:10:29.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:10:31.228: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:10:33.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:10:35.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:10:37.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:10:39.230: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:10:41.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:10:43.228: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:10:45.228: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:10:47.230: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  3 23:10:47.235: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  3 23:10:47.240: INFO: The status of Pod netserver-2 is Running (Ready = true)
Sep  3 23:10:47.245: INFO: The status of Pod netserver-3 is Running (Ready = true)
Sep  3 23:10:47.249: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Sep  3 23:10:49.268: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.63:8080/dial?request=hostname&protocol=udp&host=172.20.0.14&port=8081&tries=1'] Namespace:pod-network-test-5279 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:10:49.268: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:10:49.396: INFO: Waiting for responses: map[]
Sep  3 23:10:49.399: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.63:8080/dial?request=hostname&protocol=udp&host=172.20.1.18&port=8081&tries=1'] Namespace:pod-network-test-5279 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:10:49.399: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:10:49.519: INFO: Waiting for responses: map[]
Sep  3 23:10:49.523: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.63:8080/dial?request=hostname&protocol=udp&host=172.20.2.62&port=8081&tries=1'] Namespace:pod-network-test-5279 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:10:49.523: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:10:49.649: INFO: Waiting for responses: map[]
Sep  3 23:10:49.653: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.63:8080/dial?request=hostname&protocol=udp&host=172.20.4.69&port=8081&tries=1'] Namespace:pod-network-test-5279 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:10:49.653: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:10:49.781: INFO: Waiting for responses: map[]
Sep  3 23:10:49.785: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.63:8080/dial?request=hostname&protocol=udp&host=172.20.3.35&port=8081&tries=1'] Namespace:pod-network-test-5279 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:10:49.785: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:10:49.906: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:10:49.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5279" for this suite.

• [SLOW TEST:24.812 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":277,"completed":253,"skipped":4473,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:10:49.918: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:10:52.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3271" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":277,"completed":254,"skipped":4495,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:10:52.994: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 23:10:53.043: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7aa26861-1d0f-484f-a85c-a12c77f13d04" in namespace "projected-79" to be "Succeeded or Failed"
Sep  3 23:10:53.049: INFO: Pod "downwardapi-volume-7aa26861-1d0f-484f-a85c-a12c77f13d04": Phase="Pending", Reason="", readiness=false. Elapsed: 5.504061ms
Sep  3 23:10:55.052: INFO: Pod "downwardapi-volume-7aa26861-1d0f-484f-a85c-a12c77f13d04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008861602s
STEP: Saw pod success
Sep  3 23:10:55.052: INFO: Pod "downwardapi-volume-7aa26861-1d0f-484f-a85c-a12c77f13d04" satisfied condition "Succeeded or Failed"
Sep  3 23:10:55.055: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-7aa26861-1d0f-484f-a85c-a12c77f13d04 container client-container: <nil>
STEP: delete the pod
Sep  3 23:10:55.089: INFO: Waiting for pod downwardapi-volume-7aa26861-1d0f-484f-a85c-a12c77f13d04 to disappear
Sep  3 23:10:55.091: INFO: Pod downwardapi-volume-7aa26861-1d0f-484f-a85c-a12c77f13d04 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:10:55.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-79" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":255,"skipped":4500,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:10:55.104: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:10:55.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5118" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":277,"completed":256,"skipped":4515,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:10:55.171: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  3 23:10:57.286: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:10:57.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9520" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":257,"skipped":4521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:10:57.310: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep  3 23:11:07.373: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0903 23:11:07.373701      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:11:07.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8731" for this suite.

• [SLOW TEST:10.073 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":277,"completed":258,"skipped":4555,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:11:07.384: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Sep  3 23:11:07.411: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:11:25.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6948" for this suite.

• [SLOW TEST:18.364 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":277,"completed":259,"skipped":4555,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:11:25.748: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:11:25.794: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-ca48bb05-7503-45a8-84ee-ac5892b201af" in namespace "security-context-test-3086" to be "Succeeded or Failed"
Sep  3 23:11:25.799: INFO: Pod "busybox-privileged-false-ca48bb05-7503-45a8-84ee-ac5892b201af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.198292ms
Sep  3 23:11:27.804: INFO: Pod "busybox-privileged-false-ca48bb05-7503-45a8-84ee-ac5892b201af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009669354s
Sep  3 23:11:27.804: INFO: Pod "busybox-privileged-false-ca48bb05-7503-45a8-84ee-ac5892b201af" satisfied condition "Succeeded or Failed"
Sep  3 23:11:27.812: INFO: Got logs for pod "busybox-privileged-false-ca48bb05-7503-45a8-84ee-ac5892b201af": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:11:27.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3086" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":260,"skipped":4572,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:11:27.823: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:11:27.855: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  3 23:11:27.868: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  3 23:11:32.871: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  3 23:11:32.871: INFO: Creating deployment "test-rolling-update-deployment"
Sep  3 23:11:32.875: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  3 23:11:32.882: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  3 23:11:34.890: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  3 23:11:34.892: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep  3 23:11:34.900: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6212 /apis/apps/v1/namespaces/deployment-6212/deployments/test-rolling-update-deployment 380aeda0-77a4-4e2d-91cd-3420795090d5 32654 1 2020-09-03 23:11:32 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-09-03 23:11:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-03 23:11:34 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002f01408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-09-03 23:11:32 +0000 UTC,LastTransitionTime:2020-09-03 23:11:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2020-09-03 23:11:34 +0000 UTC,LastTransitionTime:2020-09-03 23:11:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  3 23:11:34.903: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-6212 /apis/apps/v1/namespaces/deployment-6212/replicasets/test-rolling-update-deployment-59d5cb45c7 0a12925c-d048-4b16-9bba-cb23d02f7685 32643 1 2020-09-03 23:11:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 380aeda0-77a4-4e2d-91cd-3420795090d5 0xc002d8ea37 0xc002d8ea38}] []  [{kube-controller-manager Update apps/v1 2020-09-03 23:11:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 56 48 97 101 100 97 48 45 55 55 97 52 45 52 101 50 100 45 57 49 99 100 45 51 52 50 48 55 57 53 48 57 48 100 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d8ead8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  3 23:11:34.903: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  3 23:11:34.903: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6212 /apis/apps/v1/namespaces/deployment-6212/replicasets/test-rolling-update-controller 597cb328-c0da-4eb0-b702-94d4a70874c2 32653 2 2020-09-03 23:11:27 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 380aeda0-77a4-4e2d-91cd-3420795090d5 0xc002d8e8d7 0xc002d8e8d8}] []  [{e2e.test Update apps/v1 2020-09-03 23:11:27 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-03 23:11:34 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 56 48 97 101 100 97 48 45 55 55 97 52 45 52 101 50 100 45 57 49 99 100 45 51 52 50 48 55 57 53 48 57 48 100 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d8e9a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  3 23:11:34.906: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-c5lml" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-c5lml test-rolling-update-deployment-59d5cb45c7- deployment-6212 /api/v1/namespaces/deployment-6212/pods/test-rolling-update-deployment-59d5cb45c7-c5lml 19c50677-ae45-4f8d-a8cc-82f7a934429b 32642 0 2020-09-03 23:11:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 0a12925c-d048-4b16-9bba-cb23d02f7685 0xc002f01987 0xc002f01988}] []  [{kube-controller-manager Update v1 2020-09-03 23:11:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 97 49 50 57 50 53 99 45 100 48 52 56 45 52 98 49 54 45 57 98 98 97 45 99 98 50 51 100 48 50 102 55 54 56 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-03 23:11:34 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 48 46 50 46 55 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-djvrb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-djvrb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-djvrb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-test1186mm-f1ba59-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 23:11:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 23:11:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 23:11:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-03 23:11:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.43.119,PodIP:172.20.2.71,StartTime:2020-09-03 23:11:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-03 23:11:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://43763b2f56c7e4c7adaebdaa29ab4b964b17c4588455e79a5fa4d74412857b77,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.2.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:11:34.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6212" for this suite.

• [SLOW TEST:7.093 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":261,"skipped":4575,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:11:34.916: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-1db2a054-d3a7-4e32-9f4c-54a212fa37c6
STEP: Creating a pod to test consume secrets
Sep  3 23:11:34.957: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ea381962-5f9b-4e06-8eb4-6d3a51e34042" in namespace "projected-4473" to be "Succeeded or Failed"
Sep  3 23:11:34.962: INFO: Pod "pod-projected-secrets-ea381962-5f9b-4e06-8eb4-6d3a51e34042": Phase="Pending", Reason="", readiness=false. Elapsed: 5.293232ms
Sep  3 23:11:36.965: INFO: Pod "pod-projected-secrets-ea381962-5f9b-4e06-8eb4-6d3a51e34042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008299953s
STEP: Saw pod success
Sep  3 23:11:36.965: INFO: Pod "pod-projected-secrets-ea381962-5f9b-4e06-8eb4-6d3a51e34042" satisfied condition "Succeeded or Failed"
Sep  3 23:11:36.968: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-secrets-ea381962-5f9b-4e06-8eb4-6d3a51e34042 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  3 23:11:36.989: INFO: Waiting for pod pod-projected-secrets-ea381962-5f9b-4e06-8eb4-6d3a51e34042 to disappear
Sep  3 23:11:36.992: INFO: Pod pod-projected-secrets-ea381962-5f9b-4e06-8eb4-6d3a51e34042 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:11:36.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4473" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":262,"skipped":4575,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:11:37.001: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep  3 23:11:37.058: INFO: (0) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 18.268788ms)
Sep  3 23:11:37.062: INFO: (1) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.297467ms)
Sep  3 23:11:37.065: INFO: (2) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.643396ms)
Sep  3 23:11:37.068: INFO: (3) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.559842ms)
Sep  3 23:11:37.072: INFO: (4) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.707483ms)
Sep  3 23:11:37.075: INFO: (5) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.724681ms)
Sep  3 23:11:37.077: INFO: (6) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.404995ms)
Sep  3 23:11:37.080: INFO: (7) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 3.286002ms)
Sep  3 23:11:37.083: INFO: (8) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.757038ms)
Sep  3 23:11:37.086: INFO: (9) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.447057ms)
Sep  3 23:11:37.088: INFO: (10) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.812141ms)
Sep  3 23:11:37.091: INFO: (11) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.817781ms)
Sep  3 23:11:37.096: INFO: (12) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 5.19398ms)
Sep  3 23:11:37.099: INFO: (13) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.678581ms)
Sep  3 23:11:37.102: INFO: (14) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.701474ms)
Sep  3 23:11:37.104: INFO: (15) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.435341ms)
Sep  3 23:11:37.107: INFO: (16) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.576163ms)
Sep  3 23:11:37.110: INFO: (17) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.731983ms)
Sep  3 23:11:37.112: INFO: (18) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.548113ms)
Sep  3 23:11:37.115: INFO: (19) /api/v1/nodes/karbon-test1186mm-f1ba59-k8s-master-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="boot.log">boot.log</a>
<a href="bt... (200; 2.646886ms)
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:11:37.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3269" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":277,"completed":263,"skipped":4576,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:11:37.124: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-47
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Sep  3 23:11:37.179: INFO: Found 0 stateful pods, waiting for 3
Sep  3 23:11:47.183: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 23:11:47.183: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 23:11:47.183: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep  3 23:11:47.211: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep  3 23:11:57.242: INFO: Updating stateful set ss2
Sep  3 23:11:57.248: INFO: Waiting for Pod statefulset-47/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep  3 23:12:07.298: INFO: Found 2 stateful pods, waiting for 3
Sep  3 23:12:17.302: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 23:12:17.302: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  3 23:12:17.302: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep  3 23:12:17.327: INFO: Updating stateful set ss2
Sep  3 23:12:17.332: INFO: Waiting for Pod statefulset-47/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep  3 23:12:27.358: INFO: Updating stateful set ss2
Sep  3 23:12:27.363: INFO: Waiting for StatefulSet statefulset-47/ss2 to complete update
Sep  3 23:12:27.363: INFO: Waiting for Pod statefulset-47/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep  3 23:12:37.371: INFO: Deleting all statefulset in ns statefulset-47
Sep  3 23:12:37.374: INFO: Scaling statefulset ss2 to 0
Sep  3 23:12:57.389: INFO: Waiting for statefulset status.replicas updated to 0
Sep  3 23:12:57.392: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:12:57.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-47" for this suite.

• [SLOW TEST:80.299 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":277,"completed":264,"skipped":4583,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:12:57.423: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-31fe4027-8e35-4c87-b47a-9477b7e8a323
STEP: Creating a pod to test consume secrets
Sep  3 23:12:57.462: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2693799d-61da-43a5-b141-09024888872a" in namespace "projected-4840" to be "Succeeded or Failed"
Sep  3 23:12:57.467: INFO: Pod "pod-projected-secrets-2693799d-61da-43a5-b141-09024888872a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.630905ms
Sep  3 23:12:59.471: INFO: Pod "pod-projected-secrets-2693799d-61da-43a5-b141-09024888872a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00891796s
STEP: Saw pod success
Sep  3 23:12:59.471: INFO: Pod "pod-projected-secrets-2693799d-61da-43a5-b141-09024888872a" satisfied condition "Succeeded or Failed"
Sep  3 23:12:59.474: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-secrets-2693799d-61da-43a5-b141-09024888872a container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  3 23:12:59.495: INFO: Waiting for pod pod-projected-secrets-2693799d-61da-43a5-b141-09024888872a to disappear
Sep  3 23:12:59.498: INFO: Pod pod-projected-secrets-2693799d-61da-43a5-b141-09024888872a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:12:59.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4840" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":265,"skipped":4591,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:12:59.507: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-3426
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  3 23:12:59.541: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  3 23:12:59.610: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  3 23:13:01.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:13:03.615: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:13:05.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:13:07.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:13:09.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:13:11.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  3 23:13:13.614: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  3 23:13:13.619: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  3 23:13:15.624: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  3 23:13:17.623: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  3 23:13:19.625: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  3 23:13:19.635: INFO: The status of Pod netserver-2 is Running (Ready = true)
Sep  3 23:13:19.640: INFO: The status of Pod netserver-3 is Running (Ready = true)
Sep  3 23:13:19.644: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Sep  3 23:13:21.678: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.0.15 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3426 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:13:21.678: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:13:22.793: INFO: Found all expected endpoints: [netserver-0]
Sep  3 23:13:22.797: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.1.19 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3426 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:13:22.797: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:13:23.911: INFO: Found all expected endpoints: [netserver-1]
Sep  3 23:13:23.914: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.2.77 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3426 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:13:23.914: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:13:25.027: INFO: Found all expected endpoints: [netserver-2]
Sep  3 23:13:25.031: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.4.72 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3426 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:13:25.031: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:13:26.147: INFO: Found all expected endpoints: [netserver-3]
Sep  3 23:13:26.150: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.3.39 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3426 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  3 23:13:26.150: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
Sep  3 23:13:27.266: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:13:27.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3426" for this suite.

• [SLOW TEST:27.770 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":266,"skipped":4604,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:13:27.277: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep  3 23:13:27.313: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b44ce9b4-9977-43e9-a943-f8bf6dd95add" in namespace "projected-5287" to be "Succeeded or Failed"
Sep  3 23:13:27.316: INFO: Pod "downwardapi-volume-b44ce9b4-9977-43e9-a943-f8bf6dd95add": Phase="Pending", Reason="", readiness=false. Elapsed: 2.469842ms
Sep  3 23:13:29.320: INFO: Pod "downwardapi-volume-b44ce9b4-9977-43e9-a943-f8bf6dd95add": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007142105s
STEP: Saw pod success
Sep  3 23:13:29.320: INFO: Pod "downwardapi-volume-b44ce9b4-9977-43e9-a943-f8bf6dd95add" satisfied condition "Succeeded or Failed"
Sep  3 23:13:29.324: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod downwardapi-volume-b44ce9b4-9977-43e9-a943-f8bf6dd95add container client-container: <nil>
STEP: delete the pod
Sep  3 23:13:29.343: INFO: Waiting for pod downwardapi-volume-b44ce9b4-9977-43e9-a943-f8bf6dd95add to disappear
Sep  3 23:13:29.345: INFO: Pod downwardapi-volume-b44ce9b4-9977-43e9-a943-f8bf6dd95add no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:13:29.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5287" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":277,"completed":267,"skipped":4609,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:13:29.358: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep  3 23:13:29.389: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:13:33.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3693" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":277,"completed":268,"skipped":4614,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:13:33.845: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  3 23:13:34.723: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  3 23:13:37.742: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:13:37.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3816" for this suite.
STEP: Destroying namespace "webhook-3816-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":277,"completed":269,"skipped":4625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:13:37.826: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  3 23:13:37.864: INFO: Waiting up to 5m0s for pod "pod-da32adc3-cb0f-471c-a1da-430aeec2d5d1" in namespace "emptydir-1067" to be "Succeeded or Failed"
Sep  3 23:13:37.866: INFO: Pod "pod-da32adc3-cb0f-471c-a1da-430aeec2d5d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.123504ms
Sep  3 23:13:39.870: INFO: Pod "pod-da32adc3-cb0f-471c-a1da-430aeec2d5d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006239037s
Sep  3 23:13:41.873: INFO: Pod "pod-da32adc3-cb0f-471c-a1da-430aeec2d5d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009912836s
STEP: Saw pod success
Sep  3 23:13:41.874: INFO: Pod "pod-da32adc3-cb0f-471c-a1da-430aeec2d5d1" satisfied condition "Succeeded or Failed"
Sep  3 23:13:41.876: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-da32adc3-cb0f-471c-a1da-430aeec2d5d1 container test-container: <nil>
STEP: delete the pod
Sep  3 23:13:41.898: INFO: Waiting for pod pod-da32adc3-cb0f-471c-a1da-430aeec2d5d1 to disappear
Sep  3 23:13:41.900: INFO: Pod pod-da32adc3-cb0f-471c-a1da-430aeec2d5d1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:13:41.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1067" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":270,"skipped":4653,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:13:41.908: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Sep  3 23:13:41.939: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-476854905 proxy --unix-socket=/tmp/kubectl-proxy-unix278963220/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:13:42.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7172" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":277,"completed":271,"skipped":4654,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:13:42.018: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:13:58.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1873" for this suite.

• [SLOW TEST:16.090 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":277,"completed":272,"skipped":4654,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:13:58.108: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-cde5f89e-7067-4d73-bcda-8307aa203e2e
STEP: Creating a pod to test consume secrets
Sep  3 23:13:58.157: INFO: Waiting up to 5m0s for pod "pod-secrets-d05c84b1-d848-4fb9-902e-fb39e8366133" in namespace "secrets-7314" to be "Succeeded or Failed"
Sep  3 23:13:58.193: INFO: Pod "pod-secrets-d05c84b1-d848-4fb9-902e-fb39e8366133": Phase="Pending", Reason="", readiness=false. Elapsed: 36.601923ms
Sep  3 23:14:00.197: INFO: Pod "pod-secrets-d05c84b1-d848-4fb9-902e-fb39e8366133": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040805217s
Sep  3 23:14:02.201: INFO: Pod "pod-secrets-d05c84b1-d848-4fb9-902e-fb39e8366133": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044172735s
STEP: Saw pod success
Sep  3 23:14:02.201: INFO: Pod "pod-secrets-d05c84b1-d848-4fb9-902e-fb39e8366133" satisfied condition "Succeeded or Failed"
Sep  3 23:14:02.203: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-secrets-d05c84b1-d848-4fb9-902e-fb39e8366133 container secret-env-test: <nil>
STEP: delete the pod
Sep  3 23:14:02.224: INFO: Waiting for pod pod-secrets-d05c84b1-d848-4fb9-902e-fb39e8366133 to disappear
Sep  3 23:14:02.228: INFO: Pod pod-secrets-d05c84b1-d848-4fb9-902e-fb39e8366133 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:14:02.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7314" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":277,"completed":273,"skipped":4671,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:14:02.238: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep  3 23:14:06.821: INFO: Successfully updated pod "annotationupdateafbda6c4-b840-4f95-ae10-693e5dc83f69"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:14:08.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1750" for this suite.

• [SLOW TEST:6.607 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":274,"skipped":4685,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:14:08.846: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-68ba88f7-b933-4347-b5e2-bb5a9d41a915
STEP: Creating a pod to test consume secrets
Sep  3 23:14:08.888: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b74bec9-838b-475c-84ef-e67afa65d469" in namespace "projected-1166" to be "Succeeded or Failed"
Sep  3 23:14:08.890: INFO: Pod "pod-projected-secrets-6b74bec9-838b-475c-84ef-e67afa65d469": Phase="Pending", Reason="", readiness=false. Elapsed: 2.316012ms
Sep  3 23:14:10.893: INFO: Pod "pod-projected-secrets-6b74bec9-838b-475c-84ef-e67afa65d469": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005539611s
STEP: Saw pod success
Sep  3 23:14:10.893: INFO: Pod "pod-projected-secrets-6b74bec9-838b-475c-84ef-e67afa65d469" satisfied condition "Succeeded or Failed"
Sep  3 23:14:10.895: INFO: Trying to get logs from node karbon-test1186mm-f1ba59-k8s-worker-0 pod pod-projected-secrets-6b74bec9-838b-475c-84ef-e67afa65d469 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  3 23:14:10.916: INFO: Waiting for pod pod-projected-secrets-6b74bec9-838b-475c-84ef-e67afa65d469 to disappear
Sep  3 23:14:10.919: INFO: Pod pod-projected-secrets-6b74bec9-838b-475c-84ef-e67afa65d469 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:14:10.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1166" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":275,"skipped":4703,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:14:10.930: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:14:12.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2312" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":277,"completed":276,"skipped":4704,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep  3 23:14:13.000: INFO: >>> kubeConfig: /tmp/kubeconfig-476854905
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep  3 23:14:13.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9282" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":277,"completed":277,"skipped":4713,"failed":0}
SSSep  3 23:14:13.048: INFO: Running AfterSuite actions on all nodes
Sep  3 23:14:13.048: INFO: Running AfterSuite actions on node 1
Sep  3 23:14:13.048: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":277,"completed":277,"skipped":4715,"failed":0}

Ran 277 of 4992 Specs in 4255.911 seconds
SUCCESS! -- 277 Passed | 0 Failed | 0 Pending | 4715 Skipped
PASS

Ginkgo ran 1 suite in 1h10m57.490761285s
Test Suite Passed
