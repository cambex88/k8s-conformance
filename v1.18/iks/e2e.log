I0329 21:08:11.568672      26 test_context.go:410] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-820232969
I0329 21:08:11.568703      26 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0329 21:08:11.568882      26 e2e.go:124] Starting e2e run "82791058-0d6f-4387-bc6a-2d2f137fddeb" on Ginkgo node 1
{"msg":"Test Suite starting","total":277,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1617052090 - Will randomize all specs
Will run 277 of 4994 specs

Mar 29 21:08:11.600: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:08:11.610: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 29 21:08:11.659: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 29 21:08:11.744: INFO: 25 / 25 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 29 21:08:11.744: INFO: expected 13 pod replicas in namespace 'kube-system', 13 are Running and Ready.
Mar 29 21:08:11.744: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 29 21:08:11.775: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar 29 21:08:11.775: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Mar 29 21:08:11.775: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-kubelet-monitor' (0 seconds elapsed)
Mar 29 21:08:11.775: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Mar 29 21:08:11.775: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Mar 29 21:08:11.775: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Mar 29 21:08:11.775: INFO: e2e test version: v1.18.17
Mar 29 21:08:11.779: INFO: kube-apiserver version: v1.18.17+IKS
Mar 29 21:08:11.779: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:08:11.795: INFO: Cluster IP family: ipv4
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:08:11.795: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
Mar 29 21:08:11.900: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar 29 21:08:11.949: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9789
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 29 21:08:16.799: INFO: Successfully updated pod "labelsupdate38e9f3d9-7ab5-44e4-a42f-3c0e9025af68"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:08:18.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9789" for this suite.

• [SLOW TEST:7.100 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":1,"skipped":5,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:08:18.896: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-8e75ed07-cf7d-44dd-b77f-969fb3c54868 in namespace container-probe-8307
Mar 29 21:08:23.182: INFO: Started pod busybox-8e75ed07-cf7d-44dd-b77f-969fb3c54868 in namespace container-probe-8307
STEP: checking the pod's current state and verifying that restartCount is present
Mar 29 21:08:23.198: INFO: Initial restart count of pod busybox-8e75ed07-cf7d-44dd-b77f-969fb3c54868 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:12:23.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8307" for this suite.

• [SLOW TEST:244.826 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":2,"skipped":35,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:12:23.722: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5370
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-86f5d44d-a279-4db9-88c4-2995772fb608
STEP: Creating a pod to test consume secrets
Mar 29 21:12:23.988: INFO: Waiting up to 5m0s for pod "pod-secrets-94dd9654-3063-4b7a-95a1-8b6dc892e542" in namespace "secrets-5370" to be "Succeeded or Failed"
Mar 29 21:12:24.005: INFO: Pod "pod-secrets-94dd9654-3063-4b7a-95a1-8b6dc892e542": Phase="Pending", Reason="", readiness=false. Elapsed: 17.40649ms
Mar 29 21:12:26.017: INFO: Pod "pod-secrets-94dd9654-3063-4b7a-95a1-8b6dc892e542": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029487179s
Mar 29 21:12:28.031: INFO: Pod "pod-secrets-94dd9654-3063-4b7a-95a1-8b6dc892e542": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043036657s
STEP: Saw pod success
Mar 29 21:12:28.031: INFO: Pod "pod-secrets-94dd9654-3063-4b7a-95a1-8b6dc892e542" satisfied condition "Succeeded or Failed"
Mar 29 21:12:28.045: INFO: Trying to get logs from node 10.189.118.196 pod pod-secrets-94dd9654-3063-4b7a-95a1-8b6dc892e542 container secret-volume-test: <nil>
STEP: delete the pod
Mar 29 21:12:28.146: INFO: Waiting for pod pod-secrets-94dd9654-3063-4b7a-95a1-8b6dc892e542 to disappear
Mar 29 21:12:28.156: INFO: Pod pod-secrets-94dd9654-3063-4b7a-95a1-8b6dc892e542 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:12:28.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5370" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":3,"skipped":57,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:12:28.190: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1131
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Mar 29 21:12:28.427: INFO: Waiting up to 5m0s for pod "var-expansion-4437e1f0-c00b-456c-bb58-87dec8a7ccb8" in namespace "var-expansion-1131" to be "Succeeded or Failed"
Mar 29 21:12:28.440: INFO: Pod "var-expansion-4437e1f0-c00b-456c-bb58-87dec8a7ccb8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.189618ms
Mar 29 21:12:30.451: INFO: Pod "var-expansion-4437e1f0-c00b-456c-bb58-87dec8a7ccb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02394231s
STEP: Saw pod success
Mar 29 21:12:30.451: INFO: Pod "var-expansion-4437e1f0-c00b-456c-bb58-87dec8a7ccb8" satisfied condition "Succeeded or Failed"
Mar 29 21:12:30.462: INFO: Trying to get logs from node 10.189.118.196 pod var-expansion-4437e1f0-c00b-456c-bb58-87dec8a7ccb8 container dapi-container: <nil>
STEP: delete the pod
Mar 29 21:12:30.519: INFO: Waiting for pod var-expansion-4437e1f0-c00b-456c-bb58-87dec8a7ccb8 to disappear
Mar 29 21:12:30.528: INFO: Pod var-expansion-4437e1f0-c00b-456c-bb58-87dec8a7ccb8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:12:30.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1131" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":277,"completed":4,"skipped":83,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:12:30.562: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-e7a3a5a1-afb3-4f5f-9533-f6ddc76d697f
Mar 29 21:12:30.810: INFO: Pod name my-hostname-basic-e7a3a5a1-afb3-4f5f-9533-f6ddc76d697f: Found 0 pods out of 1
Mar 29 21:12:35.826: INFO: Pod name my-hostname-basic-e7a3a5a1-afb3-4f5f-9533-f6ddc76d697f: Found 1 pods out of 1
Mar 29 21:12:35.826: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e7a3a5a1-afb3-4f5f-9533-f6ddc76d697f" are running
Mar 29 21:12:37.853: INFO: Pod "my-hostname-basic-e7a3a5a1-afb3-4f5f-9533-f6ddc76d697f-qlw9g" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-29 21:12:30 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-29 21:12:30 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-e7a3a5a1-afb3-4f5f-9533-f6ddc76d697f]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-29 21:12:30 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-e7a3a5a1-afb3-4f5f-9533-f6ddc76d697f]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-29 21:12:30 +0000 UTC Reason: Message:}])
Mar 29 21:12:37.853: INFO: Trying to dial the pod
Mar 29 21:12:42.903: INFO: Controller my-hostname-basic-e7a3a5a1-afb3-4f5f-9533-f6ddc76d697f: Got expected result from replica 1 [my-hostname-basic-e7a3a5a1-afb3-4f5f-9533-f6ddc76d697f-qlw9g]: "my-hostname-basic-e7a3a5a1-afb3-4f5f-9533-f6ddc76d697f-qlw9g", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:12:42.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6882" for this suite.

• [SLOW TEST:12.373 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":5,"skipped":103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:12:42.938: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 29 21:12:47.754: INFO: Successfully updated pod "pod-update-7db35ffb-80fc-41fc-ad01-a932075fb273"
STEP: verifying the updated pod is in kubernetes
Mar 29 21:12:47.780: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:12:47.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5122" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":277,"completed":6,"skipped":133,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:12:47.816: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:12:48.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649168, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649168, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649168, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649168, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 21:12:50.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649168, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649168, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649168, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649168, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:12:53.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:12:54.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4945" for this suite.
STEP: Destroying namespace "webhook-4945-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.050 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":277,"completed":7,"skipped":144,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:12:54.866: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-472c5d67-0579-4571-923a-ba939798a7ea in namespace container-probe-6306
Mar 29 21:12:59.132: INFO: Started pod busybox-472c5d67-0579-4571-923a-ba939798a7ea in namespace container-probe-6306
STEP: checking the pod's current state and verifying that restartCount is present
Mar 29 21:12:59.142: INFO: Initial restart count of pod busybox-472c5d67-0579-4571-923a-ba939798a7ea is 0
Mar 29 21:13:45.455: INFO: Restart count of pod container-probe-6306/busybox-472c5d67-0579-4571-923a-ba939798a7ea is now 1 (46.312915818s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:13:45.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6306" for this suite.

• [SLOW TEST:50.657 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":8,"skipped":154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:13:45.524: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:13:45.733: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:13:48.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4330" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":277,"completed":9,"skipped":209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:13:48.042: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 29 21:13:58.541: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0329 21:13:58.541590      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:13:58.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7324" for this suite.

• [SLOW TEST:10.531 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":277,"completed":10,"skipped":273,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:13:58.573: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:13:58.811: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e2423c69-b543-4d6f-a4d4-3ca5c28a15d5" in namespace "security-context-test-2461" to be "Succeeded or Failed"
Mar 29 21:13:58.822: INFO: Pod "busybox-user-65534-e2423c69-b543-4d6f-a4d4-3ca5c28a15d5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.782307ms
Mar 29 21:14:00.834: INFO: Pod "busybox-user-65534-e2423c69-b543-4d6f-a4d4-3ca5c28a15d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022007108s
Mar 29 21:14:02.847: INFO: Pod "busybox-user-65534-e2423c69-b543-4d6f-a4d4-3ca5c28a15d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035733203s
Mar 29 21:14:02.847: INFO: Pod "busybox-user-65534-e2423c69-b543-4d6f-a4d4-3ca5c28a15d5" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:14:02.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2461" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":11,"skipped":289,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:14:02.881: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:14:03.133: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10155d26-f479-4616-96f2-f2c8b729a07c" in namespace "projected-6081" to be "Succeeded or Failed"
Mar 29 21:14:03.144: INFO: Pod "downwardapi-volume-10155d26-f479-4616-96f2-f2c8b729a07c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.864777ms
Mar 29 21:14:05.155: INFO: Pod "downwardapi-volume-10155d26-f479-4616-96f2-f2c8b729a07c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021390253s
STEP: Saw pod success
Mar 29 21:14:05.155: INFO: Pod "downwardapi-volume-10155d26-f479-4616-96f2-f2c8b729a07c" satisfied condition "Succeeded or Failed"
Mar 29 21:14:05.166: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-10155d26-f479-4616-96f2-f2c8b729a07c container client-container: <nil>
STEP: delete the pod
Mar 29 21:14:05.280: INFO: Waiting for pod downwardapi-volume-10155d26-f479-4616-96f2-f2c8b729a07c to disappear
Mar 29 21:14:05.292: INFO: Pod downwardapi-volume-10155d26-f479-4616-96f2-f2c8b729a07c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:14:05.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6081" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":12,"skipped":309,"failed":0}
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:14:05.362: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8794
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-37a850b6-5cf1-4868-8c59-9b3bcc098c25
STEP: Creating secret with name secret-projected-all-test-volume-1807fc7f-9cbb-4525-9e3c-7b3c3f352a7b
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 29 21:14:05.650: INFO: Waiting up to 5m0s for pod "projected-volume-ccdeb63d-7b08-4347-ba17-735f38eb81ff" in namespace "projected-8794" to be "Succeeded or Failed"
Mar 29 21:14:05.665: INFO: Pod "projected-volume-ccdeb63d-7b08-4347-ba17-735f38eb81ff": Phase="Pending", Reason="", readiness=false. Elapsed: 14.102392ms
Mar 29 21:14:07.676: INFO: Pod "projected-volume-ccdeb63d-7b08-4347-ba17-735f38eb81ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025331565s
Mar 29 21:14:09.688: INFO: Pod "projected-volume-ccdeb63d-7b08-4347-ba17-735f38eb81ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037896193s
STEP: Saw pod success
Mar 29 21:14:09.689: INFO: Pod "projected-volume-ccdeb63d-7b08-4347-ba17-735f38eb81ff" satisfied condition "Succeeded or Failed"
Mar 29 21:14:09.700: INFO: Trying to get logs from node 10.189.118.196 pod projected-volume-ccdeb63d-7b08-4347-ba17-735f38eb81ff container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 29 21:14:09.778: INFO: Waiting for pod projected-volume-ccdeb63d-7b08-4347-ba17-735f38eb81ff to disappear
Mar 29 21:14:09.804: INFO: Pod projected-volume-ccdeb63d-7b08-4347-ba17-735f38eb81ff no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:14:09.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8794" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":277,"completed":13,"skipped":310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:14:09.846: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5535
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 29 21:14:10.075: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:14:14.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5535" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":277,"completed":14,"skipped":345,"failed":0}
S
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:14:14.041: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-255 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-255;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-255 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-255;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-255.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-255.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-255.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-255.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-255.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-255.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-255.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-255.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-255.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-255.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-255.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-255.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 247.237.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.237.247_udp@PTR;check="$$(dig +tcp +noall +answer +search 247.237.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.237.247_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-255 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-255;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-255 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-255;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-255.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-255.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-255.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-255.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-255.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-255.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-255.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-255.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-255.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-255.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-255.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-255.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-255.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 247.237.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.237.247_udp@PTR;check="$$(dig +tcp +noall +answer +search 247.237.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.237.247_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 29 21:14:24.457: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.474: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.489: INFO: Unable to read wheezy_udp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.507: INFO: Unable to read wheezy_tcp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.524: INFO: Unable to read wheezy_udp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.557: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.578: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.721: INFO: Unable to read jessie_udp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.751: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.811: INFO: Unable to read jessie_udp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.836: INFO: Unable to read jessie_tcp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.865: INFO: Unable to read jessie_udp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.918: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:24.942: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:25.087: INFO: Lookups using dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-255 wheezy_tcp@dns-test-service.dns-255 wheezy_udp@dns-test-service.dns-255.svc wheezy_tcp@dns-test-service.dns-255.svc wheezy_udp@_http._tcp.dns-test-service.dns-255.svc wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-255 jessie_tcp@dns-test-service.dns-255 jessie_udp@dns-test-service.dns-255.svc jessie_tcp@dns-test-service.dns-255.svc jessie_udp@_http._tcp.dns-test-service.dns-255.svc jessie_tcp@_http._tcp.dns-test-service.dns-255.svc]

Mar 29 21:14:30.241: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.282: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.318: INFO: Unable to read wheezy_udp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.359: INFO: Unable to read wheezy_tcp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.400: INFO: Unable to read wheezy_udp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.441: INFO: Unable to read wheezy_tcp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.474: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.512: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.766: INFO: Unable to read jessie_udp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.800: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.836: INFO: Unable to read jessie_udp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.863: INFO: Unable to read jessie_tcp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.887: INFO: Unable to read jessie_udp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.913: INFO: Unable to read jessie_tcp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.939: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:30.964: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:31.130: INFO: Lookups using dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-255 wheezy_tcp@dns-test-service.dns-255 wheezy_udp@dns-test-service.dns-255.svc wheezy_tcp@dns-test-service.dns-255.svc wheezy_udp@_http._tcp.dns-test-service.dns-255.svc wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-255 jessie_tcp@dns-test-service.dns-255 jessie_udp@dns-test-service.dns-255.svc jessie_tcp@dns-test-service.dns-255.svc jessie_udp@_http._tcp.dns-test-service.dns-255.svc jessie_tcp@_http._tcp.dns-test-service.dns-255.svc]

Mar 29 21:14:35.107: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.125: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.142: INFO: Unable to read wheezy_udp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.162: INFO: Unable to read wheezy_tcp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.182: INFO: Unable to read wheezy_udp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.205: INFO: Unable to read wheezy_tcp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.222: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.267: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.804: INFO: Unable to read jessie_udp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.821: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.841: INFO: Unable to read jessie_udp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.862: INFO: Unable to read jessie_tcp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.881: INFO: Unable to read jessie_udp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.901: INFO: Unable to read jessie_tcp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.918: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:35.937: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:36.083: INFO: Lookups using dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-255 wheezy_tcp@dns-test-service.dns-255 wheezy_udp@dns-test-service.dns-255.svc wheezy_tcp@dns-test-service.dns-255.svc wheezy_udp@_http._tcp.dns-test-service.dns-255.svc wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-255 jessie_tcp@dns-test-service.dns-255 jessie_udp@dns-test-service.dns-255.svc jessie_tcp@dns-test-service.dns-255.svc jessie_udp@_http._tcp.dns-test-service.dns-255.svc jessie_tcp@_http._tcp.dns-test-service.dns-255.svc]

Mar 29 21:14:40.110: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.126: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.143: INFO: Unable to read wheezy_udp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.159: INFO: Unable to read wheezy_tcp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.178: INFO: Unable to read wheezy_udp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.200: INFO: Unable to read wheezy_tcp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.216: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.379: INFO: Unable to read jessie_udp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.402: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.422: INFO: Unable to read jessie_udp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.442: INFO: Unable to read jessie_tcp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.463: INFO: Unable to read jessie_udp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.488: INFO: Unable to read jessie_tcp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.509: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.528: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:40.642: INFO: Lookups using dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-255 wheezy_tcp@dns-test-service.dns-255 wheezy_udp@dns-test-service.dns-255.svc wheezy_tcp@dns-test-service.dns-255.svc wheezy_udp@_http._tcp.dns-test-service.dns-255.svc wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-255 jessie_tcp@dns-test-service.dns-255 jessie_udp@dns-test-service.dns-255.svc jessie_tcp@dns-test-service.dns-255.svc jessie_udp@_http._tcp.dns-test-service.dns-255.svc jessie_tcp@_http._tcp.dns-test-service.dns-255.svc]

Mar 29 21:14:45.107: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.128: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.146: INFO: Unable to read wheezy_udp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.165: INFO: Unable to read wheezy_tcp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.190: INFO: Unable to read wheezy_udp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.210: INFO: Unable to read wheezy_tcp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.233: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.259: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.404: INFO: Unable to read jessie_udp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.427: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.445: INFO: Unable to read jessie_udp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.461: INFO: Unable to read jessie_tcp@dns-test-service.dns-255 from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.477: INFO: Unable to read jessie_udp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.496: INFO: Unable to read jessie_tcp@dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.516: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.531: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-255.svc from pod dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14: the server could not find the requested resource (get pods dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14)
Mar 29 21:14:45.662: INFO: Lookups using dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-255 wheezy_tcp@dns-test-service.dns-255 wheezy_udp@dns-test-service.dns-255.svc wheezy_tcp@dns-test-service.dns-255.svc wheezy_udp@_http._tcp.dns-test-service.dns-255.svc wheezy_tcp@_http._tcp.dns-test-service.dns-255.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-255 jessie_tcp@dns-test-service.dns-255 jessie_udp@dns-test-service.dns-255.svc jessie_tcp@dns-test-service.dns-255.svc jessie_udp@_http._tcp.dns-test-service.dns-255.svc jessie_tcp@_http._tcp.dns-test-service.dns-255.svc]

Mar 29 21:14:50.725: INFO: DNS probes using dns-255/dns-test-206aefe3-4f36-4f61-ab2f-1dc3abd7af14 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:14:50.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-255" for this suite.

• [SLOW TEST:36.880 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":277,"completed":15,"skipped":346,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:14:50.923: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:14:51.173: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf505b43-984a-41bc-b0ad-84c284f37c9e" in namespace "projected-6588" to be "Succeeded or Failed"
Mar 29 21:14:51.183: INFO: Pod "downwardapi-volume-bf505b43-984a-41bc-b0ad-84c284f37c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.85186ms
Mar 29 21:14:53.194: INFO: Pod "downwardapi-volume-bf505b43-984a-41bc-b0ad-84c284f37c9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02030137s
Mar 29 21:14:55.209: INFO: Pod "downwardapi-volume-bf505b43-984a-41bc-b0ad-84c284f37c9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035736772s
STEP: Saw pod success
Mar 29 21:14:55.209: INFO: Pod "downwardapi-volume-bf505b43-984a-41bc-b0ad-84c284f37c9e" satisfied condition "Succeeded or Failed"
Mar 29 21:14:55.219: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-bf505b43-984a-41bc-b0ad-84c284f37c9e container client-container: <nil>
STEP: delete the pod
Mar 29 21:14:55.308: INFO: Waiting for pod downwardapi-volume-bf505b43-984a-41bc-b0ad-84c284f37c9e to disappear
Mar 29 21:14:55.329: INFO: Pod downwardapi-volume-bf505b43-984a-41bc-b0ad-84c284f37c9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:14:55.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6588" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":16,"skipped":367,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:14:55.365: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 29 21:14:55.619: INFO: Waiting up to 5m0s for pod "pod-69c4d8a2-de78-40de-96b0-80a67075e220" in namespace "emptydir-2257" to be "Succeeded or Failed"
Mar 29 21:14:55.639: INFO: Pod "pod-69c4d8a2-de78-40de-96b0-80a67075e220": Phase="Pending", Reason="", readiness=false. Elapsed: 19.157863ms
Mar 29 21:14:57.651: INFO: Pod "pod-69c4d8a2-de78-40de-96b0-80a67075e220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03144183s
Mar 29 21:14:59.666: INFO: Pod "pod-69c4d8a2-de78-40de-96b0-80a67075e220": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046377239s
STEP: Saw pod success
Mar 29 21:14:59.666: INFO: Pod "pod-69c4d8a2-de78-40de-96b0-80a67075e220" satisfied condition "Succeeded or Failed"
Mar 29 21:14:59.676: INFO: Trying to get logs from node 10.189.118.196 pod pod-69c4d8a2-de78-40de-96b0-80a67075e220 container test-container: <nil>
STEP: delete the pod
Mar 29 21:14:59.739: INFO: Waiting for pod pod-69c4d8a2-de78-40de-96b0-80a67075e220 to disappear
Mar 29 21:14:59.752: INFO: Pod pod-69c4d8a2-de78-40de-96b0-80a67075e220 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:14:59.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2257" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":17,"skipped":372,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:14:59.784: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 29 21:15:00.030: INFO: Waiting up to 5m0s for pod "downward-api-78763908-0894-4957-8666-7cf10e57260b" in namespace "downward-api-2392" to be "Succeeded or Failed"
Mar 29 21:15:00.040: INFO: Pod "downward-api-78763908-0894-4957-8666-7cf10e57260b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.625783ms
Mar 29 21:15:02.051: INFO: Pod "downward-api-78763908-0894-4957-8666-7cf10e57260b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021326242s
Mar 29 21:15:04.067: INFO: Pod "downward-api-78763908-0894-4957-8666-7cf10e57260b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036913066s
STEP: Saw pod success
Mar 29 21:15:04.067: INFO: Pod "downward-api-78763908-0894-4957-8666-7cf10e57260b" satisfied condition "Succeeded or Failed"
Mar 29 21:15:04.086: INFO: Trying to get logs from node 10.189.118.196 pod downward-api-78763908-0894-4957-8666-7cf10e57260b container dapi-container: <nil>
STEP: delete the pod
Mar 29 21:15:04.208: INFO: Waiting for pod downward-api-78763908-0894-4957-8666-7cf10e57260b to disappear
Mar 29 21:15:04.222: INFO: Pod downward-api-78763908-0894-4957-8666-7cf10e57260b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:04.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2392" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":277,"completed":18,"skipped":388,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:04.270: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2805
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2805
I0329 21:15:04.577816      26 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2805, replica count: 2
I0329 21:15:07.628375      26 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 21:15:10.628: INFO: Creating new exec pod
I0329 21:15:10.628581      26 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 21:15:13.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-2805 execpod9jlws -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 29 21:15:14.134: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 29 21:15:14.134: INFO: stdout: ""
Mar 29 21:15:14.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-2805 execpod9jlws -- /bin/sh -x -c nc -zv -t -w 2 172.21.196.11 80'
Mar 29 21:15:14.446: INFO: stderr: "+ nc -zv -t -w 2 172.21.196.11 80\nConnection to 172.21.196.11 80 port [tcp/http] succeeded!\n"
Mar 29 21:15:14.447: INFO: stdout: ""
Mar 29 21:15:14.447: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:14.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2805" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:10.281 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":277,"completed":19,"skipped":409,"failed":0}
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:14.551: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-2675
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Mar 29 21:15:14.812: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2675" to be "Succeeded or Failed"
Mar 29 21:15:14.829: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 17.292032ms
Mar 29 21:15:16.841: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029058544s
Mar 29 21:15:18.855: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043796773s
STEP: Saw pod success
Mar 29 21:15:18.856: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Mar 29 21:15:18.867: INFO: Trying to get logs from node 10.189.118.208 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 29 21:15:18.994: INFO: Waiting for pod pod-host-path-test to disappear
Mar 29 21:15:19.006: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:19.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2675" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":20,"skipped":409,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:19.042: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Mar 29 21:15:19.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-3433'
Mar 29 21:15:19.624: INFO: stderr: ""
Mar 29 21:15:19.624: INFO: stdout: "pod/pause created\n"
Mar 29 21:15:19.624: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 29 21:15:19.624: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3433" to be "running and ready"
Mar 29 21:15:19.637: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.768484ms
Mar 29 21:15:22.526: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.901551282s
Mar 29 21:15:22.526: INFO: Pod "pause" satisfied condition "running and ready"
Mar 29 21:15:22.526: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 29 21:15:22.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 label pods pause testing-label=testing-label-value --namespace=kubectl-3433'
Mar 29 21:15:22.631: INFO: stderr: ""
Mar 29 21:15:22.631: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 29 21:15:22.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pod pause -L testing-label --namespace=kubectl-3433'
Mar 29 21:15:22.760: INFO: stderr: ""
Mar 29 21:15:22.760: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 29 21:15:22.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 label pods pause testing-label- --namespace=kubectl-3433'
Mar 29 21:15:22.887: INFO: stderr: ""
Mar 29 21:15:22.887: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 29 21:15:22.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pod pause -L testing-label --namespace=kubectl-3433'
Mar 29 21:15:22.981: INFO: stderr: ""
Mar 29 21:15:22.981: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Mar 29 21:15:22.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete --grace-period=0 --force -f - --namespace=kubectl-3433'
Mar 29 21:15:23.108: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 21:15:23.108: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 29 21:15:23.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get rc,svc -l name=pause --no-headers --namespace=kubectl-3433'
Mar 29 21:15:23.213: INFO: stderr: "No resources found in kubectl-3433 namespace.\n"
Mar 29 21:15:23.213: INFO: stdout: ""
Mar 29 21:15:23.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -l name=pause --namespace=kubectl-3433 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 29 21:15:23.301: INFO: stderr: ""
Mar 29 21:15:23.301: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:23.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3433" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":277,"completed":21,"skipped":418,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:23.333: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 29 21:15:23.594: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 29 21:15:28.607: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:29.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7551" for this suite.

• [SLOW TEST:6.360 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":277,"completed":22,"skipped":430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:29.694: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3939
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-3586
STEP: Creating secret with name secret-test-1e539b8b-8277-4d37-8aa1-5681936e519a
STEP: Creating a pod to test consume secrets
Mar 29 21:15:30.206: INFO: Waiting up to 5m0s for pod "pod-secrets-886e261c-0641-47e4-af80-0d2e84232368" in namespace "secrets-3939" to be "Succeeded or Failed"
Mar 29 21:15:30.216: INFO: Pod "pod-secrets-886e261c-0641-47e4-af80-0d2e84232368": Phase="Pending", Reason="", readiness=false. Elapsed: 10.530946ms
Mar 29 21:15:32.231: INFO: Pod "pod-secrets-886e261c-0641-47e4-af80-0d2e84232368": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025084024s
STEP: Saw pod success
Mar 29 21:15:32.231: INFO: Pod "pod-secrets-886e261c-0641-47e4-af80-0d2e84232368" satisfied condition "Succeeded or Failed"
Mar 29 21:15:32.245: INFO: Trying to get logs from node 10.189.118.208 pod pod-secrets-886e261c-0641-47e4-af80-0d2e84232368 container secret-volume-test: <nil>
STEP: delete the pod
Mar 29 21:15:32.319: INFO: Waiting for pod pod-secrets-886e261c-0641-47e4-af80-0d2e84232368 to disappear
Mar 29 21:15:32.330: INFO: Pod pod-secrets-886e261c-0641-47e4-af80-0d2e84232368 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:32.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3939" for this suite.
STEP: Destroying namespace "secret-namespace-3586" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":277,"completed":23,"skipped":481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:32.389: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 29 21:15:35.699: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:35.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9667" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":24,"skipped":515,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:35.782: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:15:36.040: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8f30d7c-5b86-44f3-809c-3cae979ef189" in namespace "projected-2602" to be "Succeeded or Failed"
Mar 29 21:15:36.052: INFO: Pod "downwardapi-volume-b8f30d7c-5b86-44f3-809c-3cae979ef189": Phase="Pending", Reason="", readiness=false. Elapsed: 12.211771ms
Mar 29 21:15:38.121: INFO: Pod "downwardapi-volume-b8f30d7c-5b86-44f3-809c-3cae979ef189": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081119404s
Mar 29 21:15:40.135: INFO: Pod "downwardapi-volume-b8f30d7c-5b86-44f3-809c-3cae979ef189": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.095593741s
STEP: Saw pod success
Mar 29 21:15:40.136: INFO: Pod "downwardapi-volume-b8f30d7c-5b86-44f3-809c-3cae979ef189" satisfied condition "Succeeded or Failed"
Mar 29 21:15:40.151: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-b8f30d7c-5b86-44f3-809c-3cae979ef189 container client-container: <nil>
STEP: delete the pod
Mar 29 21:15:40.222: INFO: Waiting for pod downwardapi-volume-b8f30d7c-5b86-44f3-809c-3cae979ef189 to disappear
Mar 29 21:15:40.231: INFO: Pod downwardapi-volume-b8f30d7c-5b86-44f3-809c-3cae979ef189 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:40.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2602" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":277,"completed":25,"skipped":517,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:40.269: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:40.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8275" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":277,"completed":26,"skipped":520,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:40.877: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:45.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4771" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":27,"skipped":528,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:45.246: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4033
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:45.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4033" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":277,"completed":28,"skipped":543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:45.657: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-227
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-227
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-227 to expose endpoints map[]
Mar 29 21:15:45.921: INFO: Get endpoints failed (9.05522ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar 29 21:15:46.930: INFO: successfully validated that service endpoint-test2 in namespace services-227 exposes endpoints map[] (1.018143118s elapsed)
STEP: Creating pod pod1 in namespace services-227
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-227 to expose endpoints map[pod1:[80]]
Mar 29 21:15:49.097: INFO: successfully validated that service endpoint-test2 in namespace services-227 exposes endpoints map[pod1:[80]] (2.111448522s elapsed)
STEP: Creating pod pod2 in namespace services-227
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-227 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 29 21:15:52.381: INFO: successfully validated that service endpoint-test2 in namespace services-227 exposes endpoints map[pod1:[80] pod2:[80]] (3.268456187s elapsed)
STEP: Deleting pod pod1 in namespace services-227
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-227 to expose endpoints map[pod2:[80]]
Mar 29 21:15:52.430: INFO: successfully validated that service endpoint-test2 in namespace services-227 exposes endpoints map[pod2:[80]] (24.995465ms elapsed)
STEP: Deleting pod pod2 in namespace services-227
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-227 to expose endpoints map[]
Mar 29 21:15:53.468: INFO: successfully validated that service endpoint-test2 in namespace services-227 exposes endpoints map[] (1.018445385s elapsed)
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:53.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-227" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:7.901 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":277,"completed":29,"skipped":592,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:53.558: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1022
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 29 21:15:53.813: INFO: Waiting up to 5m0s for pod "pod-9e1b8361-89ff-4503-b0bf-8f2cd90b243d" in namespace "emptydir-1022" to be "Succeeded or Failed"
Mar 29 21:15:53.825: INFO: Pod "pod-9e1b8361-89ff-4503-b0bf-8f2cd90b243d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.280638ms
Mar 29 21:15:55.837: INFO: Pod "pod-9e1b8361-89ff-4503-b0bf-8f2cd90b243d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023974573s
Mar 29 21:15:57.851: INFO: Pod "pod-9e1b8361-89ff-4503-b0bf-8f2cd90b243d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038940601s
STEP: Saw pod success
Mar 29 21:15:57.852: INFO: Pod "pod-9e1b8361-89ff-4503-b0bf-8f2cd90b243d" satisfied condition "Succeeded or Failed"
Mar 29 21:15:57.867: INFO: Trying to get logs from node 10.189.118.194 pod pod-9e1b8361-89ff-4503-b0bf-8f2cd90b243d container test-container: <nil>
STEP: delete the pod
Mar 29 21:15:57.975: INFO: Waiting for pod pod-9e1b8361-89ff-4503-b0bf-8f2cd90b243d to disappear
Mar 29 21:15:57.989: INFO: Pod pod-9e1b8361-89ff-4503-b0bf-8f2cd90b243d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:15:57.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1022" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":30,"skipped":609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:15:58.024: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9058
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:15:58.317: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 29 21:15:58.345: INFO: Number of nodes with available pods: 0
Mar 29 21:15:58.345: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 29 21:15:58.407: INFO: Number of nodes with available pods: 0
Mar 29 21:15:58.407: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:15:59.418: INFO: Number of nodes with available pods: 0
Mar 29 21:15:59.418: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:00.420: INFO: Number of nodes with available pods: 0
Mar 29 21:16:00.420: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:01.418: INFO: Number of nodes with available pods: 0
Mar 29 21:16:01.418: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:02.424: INFO: Number of nodes with available pods: 0
Mar 29 21:16:02.424: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:03.424: INFO: Number of nodes with available pods: 0
Mar 29 21:16:03.424: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:04.422: INFO: Number of nodes with available pods: 1
Mar 29 21:16:04.422: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 29 21:16:04.488: INFO: Number of nodes with available pods: 1
Mar 29 21:16:04.488: INFO: Number of running nodes: 0, number of available pods: 1
Mar 29 21:16:05.499: INFO: Number of nodes with available pods: 0
Mar 29 21:16:05.499: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 29 21:16:05.528: INFO: Number of nodes with available pods: 0
Mar 29 21:16:05.528: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:06.538: INFO: Number of nodes with available pods: 0
Mar 29 21:16:06.538: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:07.539: INFO: Number of nodes with available pods: 0
Mar 29 21:16:07.539: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:08.540: INFO: Number of nodes with available pods: 0
Mar 29 21:16:08.540: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:09.541: INFO: Number of nodes with available pods: 0
Mar 29 21:16:09.541: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:10.540: INFO: Number of nodes with available pods: 0
Mar 29 21:16:10.540: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:11.539: INFO: Number of nodes with available pods: 0
Mar 29 21:16:11.539: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:12.543: INFO: Number of nodes with available pods: 0
Mar 29 21:16:12.543: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:13.543: INFO: Number of nodes with available pods: 0
Mar 29 21:16:13.543: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:14.541: INFO: Number of nodes with available pods: 0
Mar 29 21:16:14.541: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 21:16:15.539: INFO: Number of nodes with available pods: 1
Mar 29 21:16:15.539: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9058, will wait for the garbage collector to delete the pods
Mar 29 21:16:15.645: INFO: Deleting DaemonSet.extensions daemon-set took: 23.618509ms
Mar 29 21:16:15.746: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.286537ms
Mar 29 21:16:22.663: INFO: Number of nodes with available pods: 0
Mar 29 21:16:22.663: INFO: Number of running nodes: 0, number of available pods: 0
Mar 29 21:16:22.675: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9058/daemonsets","resourceVersion":"27863"},"items":null}

Mar 29 21:16:22.687: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9058/pods","resourceVersion":"27863"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:16:22.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9058" for this suite.

• [SLOW TEST:24.796 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":277,"completed":31,"skipped":670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:16:22.827: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-7ba668f4-842a-4487-984e-0db3e74afb49 in namespace container-probe-7806
Mar 29 21:16:25.116: INFO: Started pod liveness-7ba668f4-842a-4487-984e-0db3e74afb49 in namespace container-probe-7806
STEP: checking the pod's current state and verifying that restartCount is present
Mar 29 21:16:25.127: INFO: Initial restart count of pod liveness-7ba668f4-842a-4487-984e-0db3e74afb49 is 0
Mar 29 21:16:41.289: INFO: Restart count of pod container-probe-7806/liveness-7ba668f4-842a-4487-984e-0db3e74afb49 is now 1 (16.162273533s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:16:41.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7806" for this suite.

• [SLOW TEST:18.536 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":32,"skipped":714,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:16:41.363: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7904
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-50c25f96-7d77-4a85-b795-45493a58262f
STEP: Creating a pod to test consume configMaps
Mar 29 21:16:41.634: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4137ccd3-9867-41bd-92f7-0e329a5e1a3b" in namespace "projected-7904" to be "Succeeded or Failed"
Mar 29 21:16:41.646: INFO: Pod "pod-projected-configmaps-4137ccd3-9867-41bd-92f7-0e329a5e1a3b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.305153ms
Mar 29 21:16:43.658: INFO: Pod "pod-projected-configmaps-4137ccd3-9867-41bd-92f7-0e329a5e1a3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024049555s
Mar 29 21:16:45.671: INFO: Pod "pod-projected-configmaps-4137ccd3-9867-41bd-92f7-0e329a5e1a3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037442562s
STEP: Saw pod success
Mar 29 21:16:45.671: INFO: Pod "pod-projected-configmaps-4137ccd3-9867-41bd-92f7-0e329a5e1a3b" satisfied condition "Succeeded or Failed"
Mar 29 21:16:45.686: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-configmaps-4137ccd3-9867-41bd-92f7-0e329a5e1a3b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 21:16:45.761: INFO: Waiting for pod pod-projected-configmaps-4137ccd3-9867-41bd-92f7-0e329a5e1a3b to disappear
Mar 29 21:16:45.771: INFO: Pod pod-projected-configmaps-4137ccd3-9867-41bd-92f7-0e329a5e1a3b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:16:45.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7904" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":33,"skipped":734,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:16:45.815: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5850
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7054
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6032
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:17:17.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5850" for this suite.
STEP: Destroying namespace "nsdeletetest-7054" for this suite.
Mar 29 21:17:17.713: INFO: Namespace nsdeletetest-7054 was already deleted
STEP: Destroying namespace "nsdeletetest-6032" for this suite.

• [SLOW TEST:31.915 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":277,"completed":34,"skipped":736,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:17:17.732: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 29 21:17:26.147: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 29 21:17:26.163: INFO: Pod pod-with-prestop-http-hook still exists
Mar 29 21:17:28.164: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 29 21:17:28.175: INFO: Pod pod-with-prestop-http-hook still exists
Mar 29 21:17:30.164: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 29 21:17:30.174: INFO: Pod pod-with-prestop-http-hook still exists
Mar 29 21:17:32.164: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 29 21:17:32.177: INFO: Pod pod-with-prestop-http-hook still exists
Mar 29 21:17:34.164: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 29 21:17:34.174: INFO: Pod pod-with-prestop-http-hook still exists
Mar 29 21:17:36.164: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 29 21:17:36.177: INFO: Pod pod-with-prestop-http-hook still exists
Mar 29 21:17:38.164: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 29 21:17:38.176: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:17:38.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9115" for this suite.

• [SLOW TEST:20.526 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":277,"completed":35,"skipped":754,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:17:38.258: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8822
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 29 21:17:38.530: INFO: Waiting up to 5m0s for pod "pod-a85971ac-e27f-49cb-9a19-64d065b0ecb1" in namespace "emptydir-8822" to be "Succeeded or Failed"
Mar 29 21:17:38.544: INFO: Pod "pod-a85971ac-e27f-49cb-9a19-64d065b0ecb1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.786537ms
Mar 29 21:17:40.555: INFO: Pod "pod-a85971ac-e27f-49cb-9a19-64d065b0ecb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02490493s
Mar 29 21:17:42.606: INFO: Pod "pod-a85971ac-e27f-49cb-9a19-64d065b0ecb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076608579s
STEP: Saw pod success
Mar 29 21:17:42.606: INFO: Pod "pod-a85971ac-e27f-49cb-9a19-64d065b0ecb1" satisfied condition "Succeeded or Failed"
Mar 29 21:17:42.625: INFO: Trying to get logs from node 10.189.118.196 pod pod-a85971ac-e27f-49cb-9a19-64d065b0ecb1 container test-container: <nil>
STEP: delete the pod
Mar 29 21:17:42.775: INFO: Waiting for pod pod-a85971ac-e27f-49cb-9a19-64d065b0ecb1 to disappear
Mar 29 21:17:42.785: INFO: Pod pod-a85971ac-e27f-49cb-9a19-64d065b0ecb1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:17:42.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8822" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":36,"skipped":763,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:17:42.821: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 29 21:17:43.217: INFO: Waiting up to 5m0s for pod "pod-c9aaa9b1-346a-48ad-99ea-70ee109f0ec1" in namespace "emptydir-7155" to be "Succeeded or Failed"
Mar 29 21:17:43.236: INFO: Pod "pod-c9aaa9b1-346a-48ad-99ea-70ee109f0ec1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.025689ms
Mar 29 21:17:45.539: INFO: Pod "pod-c9aaa9b1-346a-48ad-99ea-70ee109f0ec1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.32291304s
STEP: Saw pod success
Mar 29 21:17:45.540: INFO: Pod "pod-c9aaa9b1-346a-48ad-99ea-70ee109f0ec1" satisfied condition "Succeeded or Failed"
Mar 29 21:17:45.552: INFO: Trying to get logs from node 10.189.118.196 pod pod-c9aaa9b1-346a-48ad-99ea-70ee109f0ec1 container test-container: <nil>
STEP: delete the pod
Mar 29 21:17:45.625: INFO: Waiting for pod pod-c9aaa9b1-346a-48ad-99ea-70ee109f0ec1 to disappear
Mar 29 21:17:45.639: INFO: Pod pod-c9aaa9b1-346a-48ad-99ea-70ee109f0ec1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:17:45.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7155" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":37,"skipped":778,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:17:45.680: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7083
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 29 21:17:52.036: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
W0329 21:17:52.036371      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 29 21:17:52.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7083" for this suite.

• [SLOW TEST:6.396 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":277,"completed":38,"skipped":785,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:17:52.076: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:17:52.376: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4a845f49-8f3e-44b9-82e7-a4d70c308ac5" in namespace "security-context-test-4973" to be "Succeeded or Failed"
Mar 29 21:17:52.396: INFO: Pod "busybox-readonly-false-4a845f49-8f3e-44b9-82e7-a4d70c308ac5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.213061ms
Mar 29 21:17:54.407: INFO: Pod "busybox-readonly-false-4a845f49-8f3e-44b9-82e7-a4d70c308ac5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031180654s
Mar 29 21:17:54.407: INFO: Pod "busybox-readonly-false-4a845f49-8f3e-44b9-82e7-a4d70c308ac5" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:17:54.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4973" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":277,"completed":39,"skipped":808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:17:54.440: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:17:56.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5388" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":40,"skipped":832,"failed":0}

------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:17:56.774: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-9249
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9249 to expose endpoints map[]
Mar 29 21:17:57.047: INFO: Get endpoints failed (9.451668ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar 29 21:17:58.056: INFO: successfully validated that service multi-endpoint-test in namespace services-9249 exposes endpoints map[] (1.018529961s elapsed)
STEP: Creating pod pod1 in namespace services-9249
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9249 to expose endpoints map[pod1:[100]]
Mar 29 21:18:01.187: INFO: successfully validated that service multi-endpoint-test in namespace services-9249 exposes endpoints map[pod1:[100]] (3.109328486s elapsed)
STEP: Creating pod pod2 in namespace services-9249
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9249 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 29 21:18:04.357: INFO: successfully validated that service multi-endpoint-test in namespace services-9249 exposes endpoints map[pod1:[100] pod2:[101]] (3.153402952s elapsed)
STEP: Deleting pod pod1 in namespace services-9249
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9249 to expose endpoints map[pod2:[101]]
Mar 29 21:18:04.405: INFO: successfully validated that service multi-endpoint-test in namespace services-9249 exposes endpoints map[pod2:[101]] (22.591714ms elapsed)
STEP: Deleting pod pod2 in namespace services-9249
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9249 to expose endpoints map[]
Mar 29 21:18:04.446: INFO: successfully validated that service multi-endpoint-test in namespace services-9249 exposes endpoints map[] (12.813341ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:18:04.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9249" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:7.770 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":277,"completed":41,"skipped":832,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:18:04.545: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 29 21:18:04.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7838'
Mar 29 21:18:04.872: INFO: stderr: ""
Mar 29 21:18:04.872: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Mar 29 21:18:04.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete pods e2e-test-httpd-pod --namespace=kubectl-7838'
Mar 29 21:18:17.036: INFO: stderr: ""
Mar 29 21:18:17.036: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:18:17.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7838" for this suite.

• [SLOW TEST:12.531 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":277,"completed":42,"skipped":842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:18:17.076: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:18:17.330: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 29 21:18:22.347: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 29 21:18:22.347: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 29 21:18:22.416: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3235 /apis/apps/v1/namespaces/deployment-3235/deployments/test-cleanup-deployment f45343c2-5852-4cb8-846f-fd2604de6ae7 28990 1 2021-03-29 21:18:22 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-03-29 21:18:22 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002ccd208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 29 21:18:22.429: INFO: New ReplicaSet "test-cleanup-deployment-b4867b47f" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-b4867b47f  deployment-3235 /apis/apps/v1/namespaces/deployment-3235/replicasets/test-cleanup-deployment-b4867b47f d2045155-6a8c-48fe-90ce-4011c7adf66c 28994 1 2021-03-29 21:18:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment f45343c2-5852-4cb8-846f-fd2604de6ae7 0xc002ccd750 0xc002ccd751}] []  [{kube-controller-manager Update apps/v1 2021-03-29 21:18:22 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 52 53 51 52 51 99 50 45 53 56 53 50 45 52 99 98 56 45 56 52 54 102 45 102 100 50 54 48 52 100 101 54 97 101 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: b4867b47f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002ccd7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:18:22.430: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 29 21:18:22.430: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3235 /apis/apps/v1/namespaces/deployment-3235/replicasets/test-cleanup-controller 316b7ad6-d911-4f13-b739-3e0dbe05891a 28992 1 2021-03-29 21:18:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment f45343c2-5852-4cb8-846f-fd2604de6ae7 0xc002ccd617 0xc002ccd618}] []  [{e2e.test Update apps/v1 2021-03-29 21:18:17 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-29 21:18:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 52 53 51 52 51 99 50 45 53 56 53 50 45 52 99 98 56 45 56 52 54 102 45 102 100 50 54 48 52 100 101 54 97 101 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002ccd6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:18:22.444: INFO: Pod "test-cleanup-controller-7nbf8" is available:
&Pod{ObjectMeta:{test-cleanup-controller-7nbf8 test-cleanup-controller- deployment-3235 /api/v1/namespaces/deployment-3235/pods/test-cleanup-controller-7nbf8 71042605-0f8e-4514-9ec7-aa1c60da05fd 28978 0 2021-03-29 21:18:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 316b7ad6-d911-4f13-b739-3e0dbe05891a 0xc002ccddb7 0xc002ccddb8}] []  [{kube-controller-manager Update v1 2021-03-29 21:18:17 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 49 54 98 55 97 100 54 45 100 57 49 49 45 52 102 49 51 45 98 55 51 57 45 51 101 48 100 98 101 48 53 56 57 49 97 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:18:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 53 56 46 53 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t6vzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t6vzb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t6vzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:18:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:172.30.58.53,StartTime:2021-03-29 21:18:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:18:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2754e1667e8c3f735f0e38fa3eb07b26ed5ad5af1e3ba382890679434cacbd1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:18:22.444: INFO: Pod "test-cleanup-deployment-b4867b47f-nzt7l" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-b4867b47f-nzt7l test-cleanup-deployment-b4867b47f- deployment-3235 /api/v1/namespaces/deployment-3235/pods/test-cleanup-deployment-b4867b47f-nzt7l 5f6c1fc0-1d15-4158-b92b-ecb2977a29d6 28997 0 2021-03-29 21:18:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-b4867b47f d2045155-6a8c-48fe-90ce-4011c7adf66c 0xc002ccdf70 0xc002ccdf71}] []  [{kube-controller-manager Update v1 2021-03-29 21:18:22 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 50 48 52 53 49 53 53 45 54 97 56 99 45 52 56 102 101 45 57 48 99 101 45 52 48 49 49 99 55 97 100 102 54 54 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t6vzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t6vzb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t6vzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:18:22.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3235" for this suite.

• [SLOW TEST:5.414 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":277,"completed":43,"skipped":864,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:18:22.491: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:18:23.173: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 21:18:25.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649503, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649503, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649503, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649503, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:18:28.343: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 29 21:18:28.409: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:18:28.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9607" for this suite.
STEP: Destroying namespace "webhook-9607-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.170 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":277,"completed":44,"skipped":873,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:18:28.661: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6918
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 29 21:18:37.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 29 21:18:37.074: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 29 21:18:39.074: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 29 21:18:39.086: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 29 21:18:41.074: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 29 21:18:41.087: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 29 21:18:43.074: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 29 21:18:43.085: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 29 21:18:45.074: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 29 21:18:45.255: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 29 21:18:47.074: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 29 21:18:47.084: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:18:47.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6918" for this suite.

• [SLOW TEST:18.454 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":277,"completed":45,"skipped":882,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:18:47.115: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6882
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:18:47.370: INFO: The status of Pod test-webserver-de9daa68-ab16-467a-bf31-87520062f496 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 21:18:49.382: INFO: The status of Pod test-webserver-de9daa68-ab16-467a-bf31-87520062f496 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 21:18:51.379: INFO: The status of Pod test-webserver-de9daa68-ab16-467a-bf31-87520062f496 is Running (Ready = false)
Mar 29 21:18:53.378: INFO: The status of Pod test-webserver-de9daa68-ab16-467a-bf31-87520062f496 is Running (Ready = false)
Mar 29 21:18:55.379: INFO: The status of Pod test-webserver-de9daa68-ab16-467a-bf31-87520062f496 is Running (Ready = false)
Mar 29 21:18:57.382: INFO: The status of Pod test-webserver-de9daa68-ab16-467a-bf31-87520062f496 is Running (Ready = false)
Mar 29 21:18:59.383: INFO: The status of Pod test-webserver-de9daa68-ab16-467a-bf31-87520062f496 is Running (Ready = false)
Mar 29 21:19:01.383: INFO: The status of Pod test-webserver-de9daa68-ab16-467a-bf31-87520062f496 is Running (Ready = false)
Mar 29 21:19:03.378: INFO: The status of Pod test-webserver-de9daa68-ab16-467a-bf31-87520062f496 is Running (Ready = false)
Mar 29 21:19:05.380: INFO: The status of Pod test-webserver-de9daa68-ab16-467a-bf31-87520062f496 is Running (Ready = true)
Mar 29 21:19:05.390: INFO: Container started at 2021-03-29 21:18:48 +0000 UTC, pod became ready at 2021-03-29 21:19:04 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:19:05.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6882" for this suite.

• [SLOW TEST:18.307 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":277,"completed":46,"skipped":908,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:19:05.423: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1892
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:19:05.650: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:19:06.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1892" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":277,"completed":47,"skipped":912,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:19:06.747: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:19:07.004: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf9ac4be-82d3-4a0e-9fc9-2b7ffe173ea0" in namespace "projected-4228" to be "Succeeded or Failed"
Mar 29 21:19:07.016: INFO: Pod "downwardapi-volume-cf9ac4be-82d3-4a0e-9fc9-2b7ffe173ea0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.754244ms
Mar 29 21:19:09.024: INFO: Pod "downwardapi-volume-cf9ac4be-82d3-4a0e-9fc9-2b7ffe173ea0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020023273s
STEP: Saw pod success
Mar 29 21:19:09.024: INFO: Pod "downwardapi-volume-cf9ac4be-82d3-4a0e-9fc9-2b7ffe173ea0" satisfied condition "Succeeded or Failed"
Mar 29 21:19:09.032: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-cf9ac4be-82d3-4a0e-9fc9-2b7ffe173ea0 container client-container: <nil>
STEP: delete the pod
Mar 29 21:19:09.128: INFO: Waiting for pod downwardapi-volume-cf9ac4be-82d3-4a0e-9fc9-2b7ffe173ea0 to disappear
Mar 29 21:19:09.135: INFO: Pod downwardapi-volume-cf9ac4be-82d3-4a0e-9fc9-2b7ffe173ea0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:19:09.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4228" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":48,"skipped":921,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:19:09.167: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2516
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:19:10.004: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 21:19:12.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649550, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649550, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649550, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649549, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:19:15.165: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:19:15.175: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9773-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:19:16.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2516" for this suite.
STEP: Destroying namespace "webhook-2516-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.517 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":277,"completed":49,"skipped":924,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:19:16.684: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8718
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:19:16.939: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c581becd-9875-4b76-9395-42bf943fdf6d" in namespace "projected-8718" to be "Succeeded or Failed"
Mar 29 21:19:16.950: INFO: Pod "downwardapi-volume-c581becd-9875-4b76-9395-42bf943fdf6d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.063972ms
Mar 29 21:19:18.960: INFO: Pod "downwardapi-volume-c581becd-9875-4b76-9395-42bf943fdf6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020450644s
STEP: Saw pod success
Mar 29 21:19:18.960: INFO: Pod "downwardapi-volume-c581becd-9875-4b76-9395-42bf943fdf6d" satisfied condition "Succeeded or Failed"
Mar 29 21:19:18.968: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-c581becd-9875-4b76-9395-42bf943fdf6d container client-container: <nil>
STEP: delete the pod
Mar 29 21:19:19.040: INFO: Waiting for pod downwardapi-volume-c581becd-9875-4b76-9395-42bf943fdf6d to disappear
Mar 29 21:19:19.048: INFO: Pod downwardapi-volume-c581becd-9875-4b76-9395-42bf943fdf6d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:19:19.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8718" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":50,"skipped":935,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:19:19.087: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-677
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-6f30185f-c545-4d25-aad6-8618e3e6d056-1563
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:19:19.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-677" for this suite.
STEP: Destroying namespace "nspatchtest-6f30185f-c545-4d25-aad6-8618e3e6d056-1563" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":277,"completed":51,"skipped":945,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:19:19.619: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 29 21:19:19.932: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3666 /api/v1/namespaces/watch-3666/configmaps/e2e-watch-test-resource-version d6e3c4a4-26a5-4f3e-93a6-6a0805238959 29586 0 2021-03-29 21:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-29 21:19:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 21:19:19.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3666 /api/v1/namespaces/watch-3666/configmaps/e2e-watch-test-resource-version d6e3c4a4-26a5-4f3e-93a6-6a0805238959 29587 0 2021-03-29 21:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-29 21:19:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:19:19.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3666" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":277,"completed":52,"skipped":952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:19:19.968: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:19:31.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7967" for this suite.

• [SLOW TEST:11.418 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":277,"completed":53,"skipped":987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:19:31.390: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-4810
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-4810
Mar 29 21:19:31.660: INFO: Found 0 stateful pods, waiting for 1
Mar 29 21:19:41.671: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 29 21:19:41.734: INFO: Deleting all statefulset in ns statefulset-4810
Mar 29 21:19:41.745: INFO: Scaling statefulset ss to 0
Mar 29 21:20:01.831: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 21:20:01.843: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:20:01.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4810" for this suite.

• [SLOW TEST:30.531 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":277,"completed":54,"skipped":1034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:20:01.922: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:20:02.669: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 29 21:20:04.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649602, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649602, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649602, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649602, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:20:07.767: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:20:07.778: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5544-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:20:09.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1046" for this suite.
STEP: Destroying namespace "webhook-1046-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.496 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":277,"completed":55,"skipped":1062,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:20:09.420: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-884
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-a20f3c07-8915-4082-abb0-c4af95f0281a
STEP: Creating a pod to test consume secrets
Mar 29 21:20:09.666: INFO: Waiting up to 5m0s for pod "pod-secrets-4886775e-db94-4e5f-8ab0-90e01beeace0" in namespace "secrets-884" to be "Succeeded or Failed"
Mar 29 21:20:09.674: INFO: Pod "pod-secrets-4886775e-db94-4e5f-8ab0-90e01beeace0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.187727ms
Mar 29 21:20:11.683: INFO: Pod "pod-secrets-4886775e-db94-4e5f-8ab0-90e01beeace0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017558615s
Mar 29 21:20:13.694: INFO: Pod "pod-secrets-4886775e-db94-4e5f-8ab0-90e01beeace0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028152942s
STEP: Saw pod success
Mar 29 21:20:13.694: INFO: Pod "pod-secrets-4886775e-db94-4e5f-8ab0-90e01beeace0" satisfied condition "Succeeded or Failed"
Mar 29 21:20:13.706: INFO: Trying to get logs from node 10.189.118.196 pod pod-secrets-4886775e-db94-4e5f-8ab0-90e01beeace0 container secret-volume-test: <nil>
STEP: delete the pod
Mar 29 21:20:13.762: INFO: Waiting for pod pod-secrets-4886775e-db94-4e5f-8ab0-90e01beeace0 to disappear
Mar 29 21:20:13.771: INFO: Pod pod-secrets-4886775e-db94-4e5f-8ab0-90e01beeace0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:20:13.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-884" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":56,"skipped":1067,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:20:13.811: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-18df16a0-98d0-47b0-ba63-da0b3c78bfab
STEP: Creating a pod to test consume secrets
Mar 29 21:20:14.257: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-87e60c48-8a21-4907-9646-17f61bb75577" in namespace "projected-7154" to be "Succeeded or Failed"
Mar 29 21:20:14.268: INFO: Pod "pod-projected-secrets-87e60c48-8a21-4907-9646-17f61bb75577": Phase="Pending", Reason="", readiness=false. Elapsed: 10.240831ms
Mar 29 21:20:16.279: INFO: Pod "pod-projected-secrets-87e60c48-8a21-4907-9646-17f61bb75577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021609589s
STEP: Saw pod success
Mar 29 21:20:16.279: INFO: Pod "pod-projected-secrets-87e60c48-8a21-4907-9646-17f61bb75577" satisfied condition "Succeeded or Failed"
Mar 29 21:20:16.287: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-secrets-87e60c48-8a21-4907-9646-17f61bb75577 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 29 21:20:16.348: INFO: Waiting for pod pod-projected-secrets-87e60c48-8a21-4907-9646-17f61bb75577 to disappear
Mar 29 21:20:16.357: INFO: Pod pod-projected-secrets-87e60c48-8a21-4907-9646-17f61bb75577 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:20:16.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7154" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":57,"skipped":1087,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:20:16.395: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7811.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7811.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7811.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7811.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7811.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7811.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 29 21:20:20.816: INFO: DNS probes using dns-7811/dns-test-2d322753-3c06-4101-8d82-40db51ae2194 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:20:20.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7811" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":277,"completed":58,"skipped":1096,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:20:20.941: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Mar 29 21:20:21.157: INFO: namespace kubectl-926
Mar 29 21:20:21.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-926'
Mar 29 21:20:21.530: INFO: stderr: ""
Mar 29 21:20:21.530: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 29 21:20:22.539: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 21:20:22.539: INFO: Found 0 / 1
Mar 29 21:20:23.539: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 21:20:23.539: INFO: Found 1 / 1
Mar 29 21:20:23.539: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 29 21:20:23.547: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 21:20:23.547: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 29 21:20:23.547: INFO: wait on agnhost-master startup in kubectl-926 
Mar 29 21:20:23.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 logs agnhost-master-k2tn7 agnhost-master --namespace=kubectl-926'
Mar 29 21:20:23.703: INFO: stderr: ""
Mar 29 21:20:23.703: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 29 21:20:23.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-926'
Mar 29 21:20:23.850: INFO: stderr: ""
Mar 29 21:20:23.850: INFO: stdout: "service/rm2 exposed\n"
Mar 29 21:20:23.867: INFO: Service rm2 in namespace kubectl-926 found.
STEP: exposing service
Mar 29 21:20:25.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-926'
Mar 29 21:20:26.044: INFO: stderr: ""
Mar 29 21:20:26.044: INFO: stdout: "service/rm3 exposed\n"
Mar 29 21:20:26.056: INFO: Service rm3 in namespace kubectl-926 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:20:28.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-926" for this suite.

• [SLOW TEST:7.165 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1119
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":277,"completed":59,"skipped":1114,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:20:28.106: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8658
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8658, will wait for the garbage collector to delete the pods
Mar 29 21:20:32.492: INFO: Deleting Job.batch foo took: 18.438519ms
Mar 29 21:20:32.592: INFO: Terminating Job.batch foo pods took: 100.255555ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:21:07.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8658" for this suite.

• [SLOW TEST:38.928 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":277,"completed":60,"skipped":1122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:21:07.036: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-9353
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:21:07.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9353" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":277,"completed":61,"skipped":1146,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:21:07.464: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:21:07.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b5b2656e-e70f-4548-8511-42078e30e05c" in namespace "downward-api-8495" to be "Succeeded or Failed"
Mar 29 21:21:07.716: INFO: Pod "downwardapi-volume-b5b2656e-e70f-4548-8511-42078e30e05c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006453ms
Mar 29 21:21:09.726: INFO: Pod "downwardapi-volume-b5b2656e-e70f-4548-8511-42078e30e05c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018023816s
STEP: Saw pod success
Mar 29 21:21:09.726: INFO: Pod "downwardapi-volume-b5b2656e-e70f-4548-8511-42078e30e05c" satisfied condition "Succeeded or Failed"
Mar 29 21:21:09.735: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-b5b2656e-e70f-4548-8511-42078e30e05c container client-container: <nil>
STEP: delete the pod
Mar 29 21:21:09.803: INFO: Waiting for pod downwardapi-volume-b5b2656e-e70f-4548-8511-42078e30e05c to disappear
Mar 29 21:21:09.813: INFO: Pod downwardapi-volume-b5b2656e-e70f-4548-8511-42078e30e05c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:21:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8495" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":277,"completed":62,"skipped":1148,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:21:09.844: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5470
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:21:10.091: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4860273d-2e50-4b48-bfe0-64a5a6540741" in namespace "projected-5470" to be "Succeeded or Failed"
Mar 29 21:21:10.106: INFO: Pod "downwardapi-volume-4860273d-2e50-4b48-bfe0-64a5a6540741": Phase="Pending", Reason="", readiness=false. Elapsed: 15.129556ms
Mar 29 21:21:12.116: INFO: Pod "downwardapi-volume-4860273d-2e50-4b48-bfe0-64a5a6540741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025386979s
Mar 29 21:21:14.126: INFO: Pod "downwardapi-volume-4860273d-2e50-4b48-bfe0-64a5a6540741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035292136s
STEP: Saw pod success
Mar 29 21:21:14.126: INFO: Pod "downwardapi-volume-4860273d-2e50-4b48-bfe0-64a5a6540741" satisfied condition "Succeeded or Failed"
Mar 29 21:21:14.135: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-4860273d-2e50-4b48-bfe0-64a5a6540741 container client-container: <nil>
STEP: delete the pod
Mar 29 21:21:14.229: INFO: Waiting for pod downwardapi-volume-4860273d-2e50-4b48-bfe0-64a5a6540741 to disappear
Mar 29 21:21:14.237: INFO: Pod downwardapi-volume-4860273d-2e50-4b48-bfe0-64a5a6540741 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:21:14.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5470" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":63,"skipped":1157,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:21:14.294: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7621
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7621
STEP: creating replication controller externalsvc in namespace services-7621
I0329 21:21:14.622941      26 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7621, replica count: 2
I0329 21:21:17.673616      26 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 29 21:21:17.759: INFO: Creating new exec pod
Mar 29 21:21:21.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-7621 execpodvjznm -- /bin/sh -x -c nslookup nodeport-service'
Mar 29 21:21:22.174: INFO: stderr: "+ nslookup nodeport-service\n"
Mar 29 21:21:22.174: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-7621.svc.cluster.local\tcanonical name = externalsvc.services-7621.svc.cluster.local.\nName:\texternalsvc.services-7621.svc.cluster.local\nAddress: 172.21.74.54\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7621, will wait for the garbage collector to delete the pods
Mar 29 21:21:22.252: INFO: Deleting ReplicationController externalsvc took: 18.560516ms
Mar 29 21:21:22.352: INFO: Terminating ReplicationController externalsvc pods took: 100.28208ms
Mar 29 21:21:37.007: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:21:37.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7621" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:22.796 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":277,"completed":64,"skipped":1165,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:21:37.090: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-376e1593-238e-4e40-bd12-6ba939906b31
STEP: Creating a pod to test consume secrets
Mar 29 21:21:37.456: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-56e87b27-7f8c-4cd4-b567-e0b1abef7000" in namespace "projected-6730" to be "Succeeded or Failed"
Mar 29 21:21:37.465: INFO: Pod "pod-projected-secrets-56e87b27-7f8c-4cd4-b567-e0b1abef7000": Phase="Pending", Reason="", readiness=false. Elapsed: 8.956118ms
Mar 29 21:21:39.475: INFO: Pod "pod-projected-secrets-56e87b27-7f8c-4cd4-b567-e0b1abef7000": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01917807s
Mar 29 21:21:41.488: INFO: Pod "pod-projected-secrets-56e87b27-7f8c-4cd4-b567-e0b1abef7000": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031744648s
STEP: Saw pod success
Mar 29 21:21:41.488: INFO: Pod "pod-projected-secrets-56e87b27-7f8c-4cd4-b567-e0b1abef7000" satisfied condition "Succeeded or Failed"
Mar 29 21:21:41.496: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-secrets-56e87b27-7f8c-4cd4-b567-e0b1abef7000 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 29 21:21:41.550: INFO: Waiting for pod pod-projected-secrets-56e87b27-7f8c-4cd4-b567-e0b1abef7000 to disappear
Mar 29 21:21:41.560: INFO: Pod pod-projected-secrets-56e87b27-7f8c-4cd4-b567-e0b1abef7000 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:21:41.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6730" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":65,"skipped":1177,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:21:41.591: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-a85e84d9-a4ef-4297-9d6d-e832d9faeb06
STEP: Creating a pod to test consume configMaps
Mar 29 21:21:41.856: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a612070-b9f6-47d7-b666-e85ffe92b504" in namespace "configmap-477" to be "Succeeded or Failed"
Mar 29 21:21:41.865: INFO: Pod "pod-configmaps-5a612070-b9f6-47d7-b666-e85ffe92b504": Phase="Pending", Reason="", readiness=false. Elapsed: 8.537867ms
Mar 29 21:21:43.874: INFO: Pod "pod-configmaps-5a612070-b9f6-47d7-b666-e85ffe92b504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018457393s
STEP: Saw pod success
Mar 29 21:21:43.875: INFO: Pod "pod-configmaps-5a612070-b9f6-47d7-b666-e85ffe92b504" satisfied condition "Succeeded or Failed"
Mar 29 21:21:43.882: INFO: Trying to get logs from node 10.189.118.208 pod pod-configmaps-5a612070-b9f6-47d7-b666-e85ffe92b504 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 21:21:43.959: INFO: Waiting for pod pod-configmaps-5a612070-b9f6-47d7-b666-e85ffe92b504 to disappear
Mar 29 21:21:43.967: INFO: Pod pod-configmaps-5a612070-b9f6-47d7-b666-e85ffe92b504 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:21:43.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-477" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":66,"skipped":1184,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:21:44.006: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:21:44.886: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 21:21:46.918: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649704, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649704, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649704, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649704, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:21:50.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Mar 29 21:22:00.200: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:22:00.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8071" for this suite.
STEP: Destroying namespace "webhook-8071-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.759 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":277,"completed":67,"skipped":1192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:22:00.768: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9900
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 29 21:22:05.579: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9900 pod-service-account-5b18a568-80c2-47a9-9e94-1d9bd1800ecf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 29 21:22:05.894: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9900 pod-service-account-5b18a568-80c2-47a9-9e94-1d9bd1800ecf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 29 21:22:06.175: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9900 pod-service-account-5b18a568-80c2-47a9-9e94-1d9bd1800ecf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:22:06.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9900" for this suite.

• [SLOW TEST:6.032 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":277,"completed":68,"skipped":1217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:22:06.801: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:22:18.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8753" for this suite.

• [SLOW TEST:11.358 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":277,"completed":69,"skipped":1280,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:22:18.158: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-9106
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:22:19.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649739, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649739, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649739, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649738, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 21:22:21.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649739, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649739, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649739, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649738, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:22:24.226: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:22:24.236: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:22:25.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9106" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.566 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":277,"completed":70,"skipped":1295,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:22:25.728: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:22:39.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9394" for this suite.

• [SLOW TEST:13.429 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":277,"completed":71,"skipped":1313,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:22:39.159: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-d467772c-50b9-451c-9ffd-c1efdde9e898
STEP: Creating a pod to test consume configMaps
Mar 29 21:22:39.410: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-381f41f9-110f-4be5-91ef-e1d79f144b7e" in namespace "projected-1371" to be "Succeeded or Failed"
Mar 29 21:22:39.418: INFO: Pod "pod-projected-configmaps-381f41f9-110f-4be5-91ef-e1d79f144b7e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08153ms
Mar 29 21:22:41.427: INFO: Pod "pod-projected-configmaps-381f41f9-110f-4be5-91ef-e1d79f144b7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016823025s
Mar 29 21:22:43.445: INFO: Pod "pod-projected-configmaps-381f41f9-110f-4be5-91ef-e1d79f144b7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034598204s
STEP: Saw pod success
Mar 29 21:22:43.445: INFO: Pod "pod-projected-configmaps-381f41f9-110f-4be5-91ef-e1d79f144b7e" satisfied condition "Succeeded or Failed"
Mar 29 21:22:43.453: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-configmaps-381f41f9-110f-4be5-91ef-e1d79f144b7e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 21:22:43.516: INFO: Waiting for pod pod-projected-configmaps-381f41f9-110f-4be5-91ef-e1d79f144b7e to disappear
Mar 29 21:22:43.523: INFO: Pod pod-projected-configmaps-381f41f9-110f-4be5-91ef-e1d79f144b7e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:22:43.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1371" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":72,"skipped":1315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:22:43.556: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:22:43.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4293" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":277,"completed":73,"skipped":1342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:22:43.850: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4683
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar 29 21:22:45.207: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0329 21:22:45.207910      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:22:45.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4683" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":277,"completed":74,"skipped":1372,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:22:45.237: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-963
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:22:45.451: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 29 21:22:49.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-963 create -f -'
Mar 29 21:22:49.716: INFO: stderr: ""
Mar 29 21:22:49.716: INFO: stdout: "e2e-test-crd-publish-openapi-7858-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 29 21:22:49.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-963 delete e2e-test-crd-publish-openapi-7858-crds test-cr'
Mar 29 21:22:49.827: INFO: stderr: ""
Mar 29 21:22:49.827: INFO: stdout: "e2e-test-crd-publish-openapi-7858-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 29 21:22:49.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-963 apply -f -'
Mar 29 21:22:50.237: INFO: stderr: ""
Mar 29 21:22:50.237: INFO: stdout: "e2e-test-crd-publish-openapi-7858-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 29 21:22:50.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-963 delete e2e-test-crd-publish-openapi-7858-crds test-cr'
Mar 29 21:22:50.412: INFO: stderr: ""
Mar 29 21:22:50.412: INFO: stdout: "e2e-test-crd-publish-openapi-7858-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 29 21:22:50.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 explain e2e-test-crd-publish-openapi-7858-crds'
Mar 29 21:22:50.733: INFO: stderr: ""
Mar 29 21:22:50.733: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7858-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:22:54.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-963" for this suite.

• [SLOW TEST:9.311 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":277,"completed":75,"skipped":1387,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:22:54.548: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:22:54.782: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 29 21:22:54.805: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 29 21:22:59.817: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 29 21:22:59.817: INFO: Creating deployment "test-rolling-update-deployment"
Mar 29 21:22:59.832: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 29 21:22:59.861: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Mar 29 21:23:01.888: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 29 21:23:01.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649779, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649779, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649779, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752649779, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 21:23:03.913: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 29 21:23:03.942: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-555 /apis/apps/v1/namespaces/deployment-555/deployments/test-rolling-update-deployment 99e02e49-cec5-4b8a-883d-54ca7cf8bf66 31529 1 2021-03-29 21:22:59 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-03-29 21:22:59 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-29 21:23:01 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003995098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-29 21:22:59 +0000 UTC,LastTransitionTime:2021-03-29 21:22:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2021-03-29 21:23:02 +0000 UTC,LastTransitionTime:2021-03-29 21:22:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 29 21:23:03.954: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-555 /apis/apps/v1/namespaces/deployment-555/replicasets/test-rolling-update-deployment-59d5cb45c7 a52df933-41c3-4b5b-9b00-41f8109c4294 31517 1 2021-03-29 21:22:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 99e02e49-cec5-4b8a-883d-54ca7cf8bf66 0xc00364a807 0xc00364a808}] []  [{kube-controller-manager Update apps/v1 2021-03-29 21:23:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 48 50 101 52 57 45 99 101 99 53 45 52 98 56 97 45 56 56 51 100 45 53 52 99 97 55 99 102 56 98 102 54 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00364a8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:23:03.954: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 29 21:23:03.954: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-555 /apis/apps/v1/namespaces/deployment-555/replicasets/test-rolling-update-controller 0343c286-ca6d-4a72-b5f3-2e9d9aa81dcd 31528 2 2021-03-29 21:22:54 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 99e02e49-cec5-4b8a-883d-54ca7cf8bf66 0xc00364a647 0xc00364a648}] []  [{e2e.test Update apps/v1 2021-03-29 21:22:54 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-29 21:23:02 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 48 50 101 52 57 45 99 101 99 53 45 52 98 56 97 45 56 56 51 100 45 53 52 99 97 55 99 102 56 98 102 54 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00364a748 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:23:03.964: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-sg9d4" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-sg9d4 test-rolling-update-deployment-59d5cb45c7- deployment-555 /api/v1/namespaces/deployment-555/pods/test-rolling-update-deployment-59d5cb45c7-sg9d4 20d9ef77-e3e9-4a17-979c-4d2f70ba2c04 31516 0 2021-03-29 21:22:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 a52df933-41c3-4b5b-9b00-41f8109c4294 0xc003711387 0xc003711388}] []  [{kube-controller-manager Update v1 2021-03-29 21:22:59 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 53 50 100 102 57 51 51 45 52 49 99 51 45 52 98 53 98 45 57 98 48 48 45 52 49 102 56 49 48 57 99 52 50 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:23:01 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 53 56 46 49 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-54tlz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-54tlz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-54tlz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:22:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:23:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:23:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:22:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:172.30.58.16,StartTime:2021-03-29 21:22:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:23:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:containerd://57e0b578265e483885a150cf0422902ea7c20ad57c0eb59a5664f376badf9c93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:23:03.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-555" for this suite.

• [SLOW TEST:9.448 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":76,"skipped":1395,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:23:03.998: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2229
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:23:04.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2229" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":277,"completed":77,"skipped":1410,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:23:04.262: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:23:04.587: INFO: Create a RollingUpdate DaemonSet
Mar 29 21:23:04.603: INFO: Check that daemon pods launch on every node of the cluster
Mar 29 21:23:04.632: INFO: Number of nodes with available pods: 0
Mar 29 21:23:04.632: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 21:23:05.661: INFO: Number of nodes with available pods: 0
Mar 29 21:23:05.661: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 21:23:06.667: INFO: Number of nodes with available pods: 0
Mar 29 21:23:06.667: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 21:23:07.658: INFO: Number of nodes with available pods: 2
Mar 29 21:23:07.658: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 21:23:08.659: INFO: Number of nodes with available pods: 2
Mar 29 21:23:08.659: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 21:23:09.658: INFO: Number of nodes with available pods: 2
Mar 29 21:23:09.658: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 21:23:10.660: INFO: Number of nodes with available pods: 3
Mar 29 21:23:10.660: INFO: Number of running nodes: 3, number of available pods: 3
Mar 29 21:23:10.660: INFO: Update the DaemonSet to trigger a rollout
Mar 29 21:23:10.685: INFO: Updating DaemonSet daemon-set
Mar 29 21:23:13.735: INFO: Roll back the DaemonSet before rollout is complete
Mar 29 21:23:13.763: INFO: Updating DaemonSet daemon-set
Mar 29 21:23:13.763: INFO: Make sure DaemonSet rollback is complete
Mar 29 21:23:13.773: INFO: Wrong image for pod: daemon-set-gczbj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 29 21:23:13.773: INFO: Pod daemon-set-gczbj is not available
Mar 29 21:23:14.808: INFO: Wrong image for pod: daemon-set-gczbj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 29 21:23:14.808: INFO: Pod daemon-set-gczbj is not available
Mar 29 21:23:15.804: INFO: Pod daemon-set-7bgqr is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2040, will wait for the garbage collector to delete the pods
Mar 29 21:23:15.942: INFO: Deleting DaemonSet.extensions daemon-set took: 27.687075ms
Mar 29 21:23:16.043: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.286339ms
Mar 29 21:23:27.057: INFO: Number of nodes with available pods: 0
Mar 29 21:23:27.057: INFO: Number of running nodes: 0, number of available pods: 0
Mar 29 21:23:27.068: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2040/daemonsets","resourceVersion":"31773"},"items":null}

Mar 29 21:23:27.076: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2040/pods","resourceVersion":"31773"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:23:27.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2040" for this suite.

• [SLOW TEST:22.900 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":277,"completed":78,"skipped":1421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:23:27.163: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:23:27.374: INFO: Creating deployment "test-recreate-deployment"
Mar 29 21:23:27.388: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 29 21:23:27.474: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 29 21:23:29.495: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 29 21:23:29.506: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 29 21:23:29.535: INFO: Updating deployment test-recreate-deployment
Mar 29 21:23:29.535: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 29 21:23:29.671: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-906 /apis/apps/v1/namespaces/deployment-906/deployments/test-recreate-deployment 1608c1c1-a34f-4456-8442-ba9f6042dc74 31832 2 2021-03-29 21:23:27 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-29 21:23:29 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-29 21:23:29 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0023636f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-29 21:23:29 +0000 UTC,LastTransitionTime:2021-03-29 21:23:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2021-03-29 21:23:29 +0000 UTC,LastTransitionTime:2021-03-29 21:23:27 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 29 21:23:29.679: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-906 /apis/apps/v1/namespaces/deployment-906/replicasets/test-recreate-deployment-d5667d9c7 bf8e80dc-de26-4404-b9b3-caae12d88d97 31830 1 2021-03-29 21:23:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 1608c1c1-a34f-4456-8442-ba9f6042dc74 0xc0006fbc10 0xc0006fbc11}] []  [{kube-controller-manager Update apps/v1 2021-03-29 21:23:29 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 49 54 48 56 99 49 99 49 45 97 51 52 102 45 52 52 53 54 45 56 52 52 50 45 98 97 57 102 54 48 52 50 100 99 55 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0006fbce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:23:29.680: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 29 21:23:29.680: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-906 /apis/apps/v1/namespaces/deployment-906/replicasets/test-recreate-deployment-74d98b5f7c 18ca3f2c-6835-49a6-9034-dc06a08008a2 31820 2 2021-03-29 21:23:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 1608c1c1-a34f-4456-8442-ba9f6042dc74 0xc0006fb9a7 0xc0006fb9a8}] []  [{kube-controller-manager Update apps/v1 2021-03-29 21:23:29 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 49 54 48 56 99 49 99 49 45 97 51 52 102 45 52 52 53 54 45 56 52 52 50 45 98 97 57 102 54 48 52 50 100 99 55 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0006fbac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:23:29.690: INFO: Pod "test-recreate-deployment-d5667d9c7-czlmx" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-czlmx test-recreate-deployment-d5667d9c7- deployment-906 /api/v1/namespaces/deployment-906/pods/test-recreate-deployment-d5667d9c7-czlmx d6157ee1-4988-47a8-b2ed-90b9a4edd973 31833 0 2021-03-29 21:23:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 bf8e80dc-de26-4404-b9b3-caae12d88d97 0xc002ece620 0xc002ece621}] []  [{kube-controller-manager Update v1 2021-03-29 21:23:29 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 102 56 101 56 48 100 99 45 100 101 50 54 45 52 52 48 52 45 98 57 98 51 45 99 97 97 101 49 50 100 56 56 100 57 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:23:29 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-842lz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-842lz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-842lz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:23:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:23:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:23:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:23:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:,StartTime:2021-03-29 21:23:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:23:29.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-906" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":79,"skipped":1450,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:23:29.719: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 29 21:23:34.498: INFO: Successfully updated pod "adopt-release-frdnr"
STEP: Checking that the Job readopts the Pod
Mar 29 21:23:34.498: INFO: Waiting up to 15m0s for pod "adopt-release-frdnr" in namespace "job-4703" to be "adopted"
Mar 29 21:23:34.508: INFO: Pod "adopt-release-frdnr": Phase="Running", Reason="", readiness=true. Elapsed: 9.500418ms
Mar 29 21:23:36.518: INFO: Pod "adopt-release-frdnr": Phase="Running", Reason="", readiness=true. Elapsed: 2.020009991s
Mar 29 21:23:36.518: INFO: Pod "adopt-release-frdnr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 29 21:23:37.042: INFO: Successfully updated pod "adopt-release-frdnr"
STEP: Checking that the Job releases the Pod
Mar 29 21:23:37.042: INFO: Waiting up to 15m0s for pod "adopt-release-frdnr" in namespace "job-4703" to be "released"
Mar 29 21:23:37.052: INFO: Pod "adopt-release-frdnr": Phase="Running", Reason="", readiness=true. Elapsed: 9.785838ms
Mar 29 21:23:39.061: INFO: Pod "adopt-release-frdnr": Phase="Running", Reason="", readiness=true. Elapsed: 2.018805368s
Mar 29 21:23:39.062: INFO: Pod "adopt-release-frdnr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:23:39.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4703" for this suite.

• [SLOW TEST:9.375 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":277,"completed":80,"skipped":1455,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:23:39.095: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-126
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-fe9d82dc-5782-4244-8cb6-8181cd51168b
STEP: Creating a pod to test consume configMaps
Mar 29 21:23:39.336: INFO: Waiting up to 5m0s for pod "pod-configmaps-fc668975-87bd-4342-b577-7d0a71afc5c5" in namespace "configmap-126" to be "Succeeded or Failed"
Mar 29 21:23:39.345: INFO: Pod "pod-configmaps-fc668975-87bd-4342-b577-7d0a71afc5c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.904185ms
Mar 29 21:23:41.354: INFO: Pod "pod-configmaps-fc668975-87bd-4342-b577-7d0a71afc5c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017524372s
STEP: Saw pod success
Mar 29 21:23:41.354: INFO: Pod "pod-configmaps-fc668975-87bd-4342-b577-7d0a71afc5c5" satisfied condition "Succeeded or Failed"
Mar 29 21:23:41.362: INFO: Trying to get logs from node 10.189.118.196 pod pod-configmaps-fc668975-87bd-4342-b577-7d0a71afc5c5 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 21:23:41.420: INFO: Waiting for pod pod-configmaps-fc668975-87bd-4342-b577-7d0a71afc5c5 to disappear
Mar 29 21:23:41.429: INFO: Pod pod-configmaps-fc668975-87bd-4342-b577-7d0a71afc5c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:23:41.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-126" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":81,"skipped":1457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:23:41.458: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3548
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 29 21:23:41.694: INFO: Waiting up to 5m0s for pod "pod-1c0463e4-3745-4521-8daf-0e1377a5cec0" in namespace "emptydir-3548" to be "Succeeded or Failed"
Mar 29 21:23:41.701: INFO: Pod "pod-1c0463e4-3745-4521-8daf-0e1377a5cec0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.428432ms
Mar 29 21:23:43.716: INFO: Pod "pod-1c0463e4-3745-4521-8daf-0e1377a5cec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021967475s
Mar 29 21:23:45.725: INFO: Pod "pod-1c0463e4-3745-4521-8daf-0e1377a5cec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031463044s
STEP: Saw pod success
Mar 29 21:23:45.725: INFO: Pod "pod-1c0463e4-3745-4521-8daf-0e1377a5cec0" satisfied condition "Succeeded or Failed"
Mar 29 21:23:45.735: INFO: Trying to get logs from node 10.189.118.196 pod pod-1c0463e4-3745-4521-8daf-0e1377a5cec0 container test-container: <nil>
STEP: delete the pod
Mar 29 21:23:45.797: INFO: Waiting for pod pod-1c0463e4-3745-4521-8daf-0e1377a5cec0 to disappear
Mar 29 21:23:45.805: INFO: Pod pod-1c0463e4-3745-4521-8daf-0e1377a5cec0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:23:45.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3548" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":82,"skipped":1481,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:23:45.836: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:23:46.073: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:23:50.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5186" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":277,"completed":83,"skipped":1496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:23:50.283: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0329 21:24:00.581550      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 29 21:24:00.581: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:24:00.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7689" for this suite.

• [SLOW TEST:10.332 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":277,"completed":84,"skipped":1520,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:24:00.615: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 29 21:24:03.899: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:24:03.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4563" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":85,"skipped":1526,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:24:03.967: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:24:04.213: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef0dc0ff-93b3-4632-a342-a0cebd1f012f" in namespace "projected-9757" to be "Succeeded or Failed"
Mar 29 21:24:04.224: INFO: Pod "downwardapi-volume-ef0dc0ff-93b3-4632-a342-a0cebd1f012f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.590864ms
Mar 29 21:24:06.233: INFO: Pod "downwardapi-volume-ef0dc0ff-93b3-4632-a342-a0cebd1f012f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020122901s
Mar 29 21:24:08.242: INFO: Pod "downwardapi-volume-ef0dc0ff-93b3-4632-a342-a0cebd1f012f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028850932s
STEP: Saw pod success
Mar 29 21:24:08.242: INFO: Pod "downwardapi-volume-ef0dc0ff-93b3-4632-a342-a0cebd1f012f" satisfied condition "Succeeded or Failed"
Mar 29 21:24:08.254: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-ef0dc0ff-93b3-4632-a342-a0cebd1f012f container client-container: <nil>
STEP: delete the pod
Mar 29 21:24:08.311: INFO: Waiting for pod downwardapi-volume-ef0dc0ff-93b3-4632-a342-a0cebd1f012f to disappear
Mar 29 21:24:08.319: INFO: Pod downwardapi-volume-ef0dc0ff-93b3-4632-a342-a0cebd1f012f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:24:08.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9757" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":86,"skipped":1538,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:24:08.353: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-995/configmap-test-fb09587d-c38e-4a52-a17e-df6dd49d5ec6
STEP: Creating a pod to test consume configMaps
Mar 29 21:24:08.597: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb292c85-859c-4b26-8ec6-b7e94665c662" in namespace "configmap-995" to be "Succeeded or Failed"
Mar 29 21:24:08.605: INFO: Pod "pod-configmaps-bb292c85-859c-4b26-8ec6-b7e94665c662": Phase="Pending", Reason="", readiness=false. Elapsed: 7.474938ms
Mar 29 21:24:10.615: INFO: Pod "pod-configmaps-bb292c85-859c-4b26-8ec6-b7e94665c662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01781263s
STEP: Saw pod success
Mar 29 21:24:10.615: INFO: Pod "pod-configmaps-bb292c85-859c-4b26-8ec6-b7e94665c662" satisfied condition "Succeeded or Failed"
Mar 29 21:24:10.622: INFO: Trying to get logs from node 10.189.118.196 pod pod-configmaps-bb292c85-859c-4b26-8ec6-b7e94665c662 container env-test: <nil>
STEP: delete the pod
Mar 29 21:24:10.696: INFO: Waiting for pod pod-configmaps-bb292c85-859c-4b26-8ec6-b7e94665c662 to disappear
Mar 29 21:24:10.704: INFO: Pod pod-configmaps-bb292c85-859c-4b26-8ec6-b7e94665c662 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:24:10.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-995" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":277,"completed":87,"skipped":1546,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:24:10.737: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:24:15.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9920" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":277,"completed":88,"skipped":1559,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:24:15.535: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 29 21:24:15.766: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:24:27.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5845" for this suite.

• [SLOW TEST:11.672 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":277,"completed":89,"skipped":1560,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:24:27.207: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2253
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-2253
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Mar 29 21:24:27.644: INFO: Found 0 stateful pods, waiting for 3
Mar 29 21:24:37.656: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 21:24:37.656: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 21:24:37.656: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 29 21:24:37.726: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 29 21:24:47.806: INFO: Updating stateful set ss2
Mar 29 21:24:47.833: INFO: Waiting for Pod statefulset-2253/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 29 21:24:57.854: INFO: Waiting for Pod statefulset-2253/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 29 21:25:07.926: INFO: Found 2 stateful pods, waiting for 3
Mar 29 21:25:17.937: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 21:25:17.937: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 21:25:17.937: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 29 21:25:18.005: INFO: Updating stateful set ss2
Mar 29 21:25:18.027: INFO: Waiting for Pod statefulset-2253/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 29 21:25:28.097: INFO: Updating stateful set ss2
Mar 29 21:25:28.119: INFO: Waiting for StatefulSet statefulset-2253/ss2 to complete update
Mar 29 21:25:28.119: INFO: Waiting for Pod statefulset-2253/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 29 21:25:38.139: INFO: Waiting for StatefulSet statefulset-2253/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 29 21:25:48.140: INFO: Deleting all statefulset in ns statefulset-2253
Mar 29 21:25:48.153: INFO: Scaling statefulset ss2 to 0
Mar 29 21:26:18.196: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 21:26:18.208: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:26:18.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2253" for this suite.

• [SLOW TEST:111.103 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":277,"completed":90,"skipped":1560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:26:18.311: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-2076
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar 29 21:26:18.574: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 29 21:27:18.650: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:27:18.661: INFO: Starting informer...
STEP: Starting pods...
Mar 29 21:27:18.913: INFO: Pod1 is running on 10.189.118.196. Tainting Node
Mar 29 21:27:23.165: INFO: Pod2 is running on 10.189.118.196. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 29 21:27:36.951: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 29 21:27:56.996: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:27:57.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2076" for this suite.

• [SLOW TEST:98.764 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":277,"completed":91,"skipped":1594,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:27:57.076: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2048
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar 29 21:28:37.375: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
W0329 21:28:37.375578      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 29 21:28:37.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2048" for this suite.

• [SLOW TEST:40.334 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":277,"completed":92,"skipped":1612,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:28:37.410: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:28:37.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 version'
Mar 29 21:28:37.703: INFO: stderr: ""
Mar 29 21:28:37.703: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.17\", GitCommit:\"68b4e26caf6ede7af577db4af62fb405b4dd47e6\", GitTreeState:\"clean\", BuildDate:\"2021-03-18T01:02:41Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.17+IKS\", GitCommit:\"c03e8afa333a77f4520a179b803bfa9b94770216\", GitTreeState:\"clean\", BuildDate:\"2021-03-19T14:00:22Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:28:37.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1107" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":277,"completed":93,"skipped":1629,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:28:37.734: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-7492k in namespace proxy-9633
I0329 21:28:37.984903      26 runners.go:190] Created replication controller with name: proxy-service-7492k, namespace: proxy-9633, replica count: 1
I0329 21:28:39.035397      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0329 21:28:40.035592      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0329 21:28:41.035858      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0329 21:28:42.036095      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0329 21:28:43.036355      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0329 21:28:44.036604      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0329 21:28:45.036823      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0329 21:28:46.037057      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0329 21:28:47.037323      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0329 21:28:48.040799      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0329 21:28:49.041036      26 runners.go:190] proxy-service-7492k Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 21:28:49.050: INFO: setup took 11.109830914s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 29 21:28:49.076: INFO: (0) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 24.981879ms)
Mar 29 21:28:49.076: INFO: (0) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 25.085944ms)
Mar 29 21:28:49.078: INFO: (0) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 27.741216ms)
Mar 29 21:28:49.083: INFO: (0) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 32.369265ms)
Mar 29 21:28:49.083: INFO: (0) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 32.523421ms)
Mar 29 21:28:49.083: INFO: (0) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 32.588286ms)
Mar 29 21:28:49.085: INFO: (0) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 35.18005ms)
Mar 29 21:28:49.085: INFO: (0) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 35.006056ms)
Mar 29 21:28:49.087: INFO: (0) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 37.005285ms)
Mar 29 21:28:49.087: INFO: (0) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 36.691732ms)
Mar 29 21:28:49.090: INFO: (0) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 39.807047ms)
Mar 29 21:28:49.096: INFO: (0) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 45.695156ms)
Mar 29 21:28:49.097: INFO: (0) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 46.008042ms)
Mar 29 21:28:49.097: INFO: (0) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 46.05703ms)
Mar 29 21:28:49.097: INFO: (0) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 46.251773ms)
Mar 29 21:28:49.100: INFO: (0) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 49.209089ms)
Mar 29 21:28:49.121: INFO: (1) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 20.633143ms)
Mar 29 21:28:49.123: INFO: (1) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 22.491031ms)
Mar 29 21:28:49.123: INFO: (1) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 22.68838ms)
Mar 29 21:28:49.123: INFO: (1) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 22.711366ms)
Mar 29 21:28:49.123: INFO: (1) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 22.640312ms)
Mar 29 21:28:49.123: INFO: (1) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 22.794662ms)
Mar 29 21:28:49.124: INFO: (1) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 23.656045ms)
Mar 29 21:28:49.125: INFO: (1) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 24.660473ms)
Mar 29 21:28:49.125: INFO: (1) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 24.99056ms)
Mar 29 21:28:49.125: INFO: (1) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 25.107989ms)
Mar 29 21:28:49.134: INFO: (1) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 33.540915ms)
Mar 29 21:28:49.134: INFO: (1) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 33.531096ms)
Mar 29 21:28:49.134: INFO: (1) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 33.238467ms)
Mar 29 21:28:49.134: INFO: (1) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 33.887537ms)
Mar 29 21:28:49.137: INFO: (1) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 37.072096ms)
Mar 29 21:28:49.138: INFO: (1) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 37.479611ms)
Mar 29 21:28:49.154: INFO: (2) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 16.375165ms)
Mar 29 21:28:49.156: INFO: (2) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 17.797903ms)
Mar 29 21:28:49.156: INFO: (2) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 17.85058ms)
Mar 29 21:28:49.157: INFO: (2) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 18.401002ms)
Mar 29 21:28:49.157: INFO: (2) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 17.99117ms)
Mar 29 21:28:49.157: INFO: (2) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 18.106416ms)
Mar 29 21:28:49.157: INFO: (2) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 18.605473ms)
Mar 29 21:28:49.157: INFO: (2) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 18.895606ms)
Mar 29 21:28:49.157: INFO: (2) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 19.033992ms)
Mar 29 21:28:49.157: INFO: (2) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 19.26555ms)
Mar 29 21:28:49.166: INFO: (2) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 28.560179ms)
Mar 29 21:28:49.167: INFO: (2) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 28.289158ms)
Mar 29 21:28:49.167: INFO: (2) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 28.611528ms)
Mar 29 21:28:49.167: INFO: (2) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 28.538702ms)
Mar 29 21:28:49.167: INFO: (2) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 28.977907ms)
Mar 29 21:28:49.167: INFO: (2) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 28.625782ms)
Mar 29 21:28:49.182: INFO: (3) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 14.555231ms)
Mar 29 21:28:49.183: INFO: (3) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 15.83966ms)
Mar 29 21:28:49.184: INFO: (3) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 15.904796ms)
Mar 29 21:28:49.184: INFO: (3) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 16.487079ms)
Mar 29 21:28:49.184: INFO: (3) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 17.040737ms)
Mar 29 21:28:49.186: INFO: (3) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 18.368473ms)
Mar 29 21:28:49.186: INFO: (3) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 18.418359ms)
Mar 29 21:28:49.186: INFO: (3) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 18.686362ms)
Mar 29 21:28:49.186: INFO: (3) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 19.209814ms)
Mar 29 21:28:49.186: INFO: (3) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 18.648522ms)
Mar 29 21:28:49.190: INFO: (3) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 22.454549ms)
Mar 29 21:28:49.190: INFO: (3) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 22.308897ms)
Mar 29 21:28:49.193: INFO: (3) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 25.279209ms)
Mar 29 21:28:49.193: INFO: (3) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 25.596752ms)
Mar 29 21:28:49.193: INFO: (3) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 25.79145ms)
Mar 29 21:28:49.193: INFO: (3) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 25.981518ms)
Mar 29 21:28:49.207: INFO: (4) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 13.859876ms)
Mar 29 21:28:49.211: INFO: (4) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 17.178103ms)
Mar 29 21:28:49.211: INFO: (4) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 16.80939ms)
Mar 29 21:28:49.214: INFO: (4) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 19.959611ms)
Mar 29 21:28:49.214: INFO: (4) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 19.79564ms)
Mar 29 21:28:49.214: INFO: (4) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 20.417113ms)
Mar 29 21:28:49.214: INFO: (4) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 20.472103ms)
Mar 29 21:28:49.214: INFO: (4) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 20.334688ms)
Mar 29 21:28:49.214: INFO: (4) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 20.289999ms)
Mar 29 21:28:49.216: INFO: (4) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 21.65189ms)
Mar 29 21:28:49.218: INFO: (4) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 23.436146ms)
Mar 29 21:28:49.219: INFO: (4) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 24.805141ms)
Mar 29 21:28:49.221: INFO: (4) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 27.085716ms)
Mar 29 21:28:49.222: INFO: (4) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 27.425473ms)
Mar 29 21:28:49.222: INFO: (4) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 28.856044ms)
Mar 29 21:28:49.223: INFO: (4) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 28.764525ms)
Mar 29 21:28:49.236: INFO: (5) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 13.562881ms)
Mar 29 21:28:49.237: INFO: (5) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 13.793099ms)
Mar 29 21:28:49.237: INFO: (5) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 14.139852ms)
Mar 29 21:28:49.238: INFO: (5) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 14.349331ms)
Mar 29 21:28:49.238: INFO: (5) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 14.090909ms)
Mar 29 21:28:49.238: INFO: (5) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 14.775748ms)
Mar 29 21:28:49.239: INFO: (5) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 14.997172ms)
Mar 29 21:28:49.239: INFO: (5) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 15.186875ms)
Mar 29 21:28:49.241: INFO: (5) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 17.739707ms)
Mar 29 21:28:49.244: INFO: (5) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 21.282273ms)
Mar 29 21:28:49.245: INFO: (5) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 21.737735ms)
Mar 29 21:28:49.245: INFO: (5) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 21.621651ms)
Mar 29 21:28:49.245: INFO: (5) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 21.766348ms)
Mar 29 21:28:49.245: INFO: (5) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 21.57706ms)
Mar 29 21:28:49.245: INFO: (5) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 21.482543ms)
Mar 29 21:28:49.245: INFO: (5) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 21.25476ms)
Mar 29 21:28:49.270: INFO: (6) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 24.216707ms)
Mar 29 21:28:49.270: INFO: (6) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 24.192864ms)
Mar 29 21:28:49.270: INFO: (6) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 24.25641ms)
Mar 29 21:28:49.270: INFO: (6) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 24.076004ms)
Mar 29 21:28:49.270: INFO: (6) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 24.040706ms)
Mar 29 21:28:49.270: INFO: (6) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 24.323975ms)
Mar 29 21:28:49.270: INFO: (6) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 24.409907ms)
Mar 29 21:28:49.271: INFO: (6) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 25.153082ms)
Mar 29 21:28:49.271: INFO: (6) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 24.897157ms)
Mar 29 21:28:49.271: INFO: (6) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 25.061887ms)
Mar 29 21:28:49.271: INFO: (6) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 25.250017ms)
Mar 29 21:28:49.271: INFO: (6) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 25.754436ms)
Mar 29 21:28:49.276: INFO: (6) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 30.12273ms)
Mar 29 21:28:49.276: INFO: (6) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 30.258329ms)
Mar 29 21:28:49.276: INFO: (6) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 30.111081ms)
Mar 29 21:28:49.276: INFO: (6) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 30.409401ms)
Mar 29 21:28:49.305: INFO: (7) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 28.59823ms)
Mar 29 21:28:49.305: INFO: (7) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 28.593803ms)
Mar 29 21:28:49.305: INFO: (7) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 28.814625ms)
Mar 29 21:28:49.305: INFO: (7) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 28.645939ms)
Mar 29 21:28:49.305: INFO: (7) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 28.499409ms)
Mar 29 21:28:49.305: INFO: (7) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 28.574531ms)
Mar 29 21:28:49.306: INFO: (7) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 29.554313ms)
Mar 29 21:28:49.306: INFO: (7) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 29.825575ms)
Mar 29 21:28:49.306: INFO: (7) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 29.892586ms)
Mar 29 21:28:49.307: INFO: (7) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 30.125899ms)
Mar 29 21:28:49.307: INFO: (7) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 30.252268ms)
Mar 29 21:28:49.307: INFO: (7) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 30.56564ms)
Mar 29 21:28:49.309: INFO: (7) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 32.035307ms)
Mar 29 21:28:49.310: INFO: (7) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 33.204462ms)
Mar 29 21:28:49.310: INFO: (7) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 33.278561ms)
Mar 29 21:28:49.310: INFO: (7) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 33.545435ms)
Mar 29 21:28:49.345: INFO: (8) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 35.085619ms)
Mar 29 21:28:49.346: INFO: (8) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 35.376562ms)
Mar 29 21:28:49.346: INFO: (8) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 33.586328ms)
Mar 29 21:28:49.346: INFO: (8) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 35.331987ms)
Mar 29 21:28:49.346: INFO: (8) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 36.019139ms)
Mar 29 21:28:49.346: INFO: (8) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 35.784341ms)
Mar 29 21:28:49.346: INFO: (8) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 36.187126ms)
Mar 29 21:28:49.346: INFO: (8) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 35.759234ms)
Mar 29 21:28:49.347: INFO: (8) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 36.161002ms)
Mar 29 21:28:49.347: INFO: (8) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 36.24274ms)
Mar 29 21:28:49.347: INFO: (8) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 34.679503ms)
Mar 29 21:28:49.352: INFO: (8) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 40.092444ms)
Mar 29 21:28:49.352: INFO: (8) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 40.428763ms)
Mar 29 21:28:49.352: INFO: (8) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 41.556509ms)
Mar 29 21:28:49.352: INFO: (8) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 40.684631ms)
Mar 29 21:28:49.353: INFO: (8) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 41.482944ms)
Mar 29 21:28:49.367: INFO: (9) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 13.739022ms)
Mar 29 21:28:49.372: INFO: (9) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 19.295895ms)
Mar 29 21:28:49.372: INFO: (9) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 19.63022ms)
Mar 29 21:28:49.372: INFO: (9) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 19.439527ms)
Mar 29 21:28:49.372: INFO: (9) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 19.276585ms)
Mar 29 21:28:49.372: INFO: (9) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 19.53897ms)
Mar 29 21:28:49.374: INFO: (9) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 21.542056ms)
Mar 29 21:28:49.376: INFO: (9) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 22.672445ms)
Mar 29 21:28:49.376: INFO: (9) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 22.462104ms)
Mar 29 21:28:49.377: INFO: (9) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 23.756122ms)
Mar 29 21:28:49.377: INFO: (9) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 23.720163ms)
Mar 29 21:28:49.377: INFO: (9) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 24.172699ms)
Mar 29 21:28:49.377: INFO: (9) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 24.479822ms)
Mar 29 21:28:49.377: INFO: (9) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 24.055161ms)
Mar 29 21:28:49.378: INFO: (9) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 25.031919ms)
Mar 29 21:28:49.380: INFO: (9) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 26.631872ms)
Mar 29 21:28:49.396: INFO: (10) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 16.091485ms)
Mar 29 21:28:49.403: INFO: (10) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 23.037257ms)
Mar 29 21:28:49.404: INFO: (10) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 23.878967ms)
Mar 29 21:28:49.404: INFO: (10) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 24.02309ms)
Mar 29 21:28:49.406: INFO: (10) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 26.020942ms)
Mar 29 21:28:49.407: INFO: (10) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 27.43974ms)
Mar 29 21:28:49.408: INFO: (10) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 27.818758ms)
Mar 29 21:28:49.408: INFO: (10) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 27.681806ms)
Mar 29 21:28:49.408: INFO: (10) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 27.75929ms)
Mar 29 21:28:49.408: INFO: (10) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 27.88891ms)
Mar 29 21:28:49.408: INFO: (10) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 27.706979ms)
Mar 29 21:28:49.408: INFO: (10) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 27.937337ms)
Mar 29 21:28:49.408: INFO: (10) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 27.939351ms)
Mar 29 21:28:49.408: INFO: (10) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 27.815166ms)
Mar 29 21:28:49.411: INFO: (10) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 31.085602ms)
Mar 29 21:28:49.411: INFO: (10) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 31.425056ms)
Mar 29 21:28:49.436: INFO: (11) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 24.199842ms)
Mar 29 21:28:49.436: INFO: (11) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 24.413218ms)
Mar 29 21:28:49.437: INFO: (11) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 24.808822ms)
Mar 29 21:28:49.437: INFO: (11) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 24.946387ms)
Mar 29 21:28:49.437: INFO: (11) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 25.608002ms)
Mar 29 21:28:49.437: INFO: (11) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 25.922686ms)
Mar 29 21:28:49.437: INFO: (11) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 26.042756ms)
Mar 29 21:28:49.437: INFO: (11) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 26.063133ms)
Mar 29 21:28:49.437: INFO: (11) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 25.791009ms)
Mar 29 21:28:49.437: INFO: (11) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 25.652219ms)
Mar 29 21:28:49.438: INFO: (11) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 25.830745ms)
Mar 29 21:28:49.439: INFO: (11) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 27.743278ms)
Mar 29 21:28:49.449: INFO: (11) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 37.755143ms)
Mar 29 21:28:49.450: INFO: (11) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 38.602598ms)
Mar 29 21:28:49.452: INFO: (11) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 40.825773ms)
Mar 29 21:28:49.456: INFO: (11) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 44.679378ms)
Mar 29 21:28:49.491: INFO: (12) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 34.375115ms)
Mar 29 21:28:49.492: INFO: (12) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 34.933012ms)
Mar 29 21:28:49.492: INFO: (12) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 35.232193ms)
Mar 29 21:28:49.492: INFO: (12) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 35.194951ms)
Mar 29 21:28:49.492: INFO: (12) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 35.30742ms)
Mar 29 21:28:49.492: INFO: (12) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 35.68755ms)
Mar 29 21:28:49.493: INFO: (12) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 35.783112ms)
Mar 29 21:28:49.496: INFO: (12) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 39.734924ms)
Mar 29 21:28:49.497: INFO: (12) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 40.170724ms)
Mar 29 21:28:49.497: INFO: (12) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 40.435449ms)
Mar 29 21:28:49.499: INFO: (12) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 42.254679ms)
Mar 29 21:28:49.499: INFO: (12) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 42.514658ms)
Mar 29 21:28:49.499: INFO: (12) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 42.471378ms)
Mar 29 21:28:49.501: INFO: (12) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 44.203718ms)
Mar 29 21:28:49.505: INFO: (12) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 47.945563ms)
Mar 29 21:28:49.508: INFO: (12) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 50.788919ms)
Mar 29 21:28:49.529: INFO: (13) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 20.92085ms)
Mar 29 21:28:49.529: INFO: (13) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 20.576691ms)
Mar 29 21:28:49.529: INFO: (13) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 21.221964ms)
Mar 29 21:28:49.530: INFO: (13) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 21.453365ms)
Mar 29 21:28:49.530: INFO: (13) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 21.447987ms)
Mar 29 21:28:49.530: INFO: (13) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 21.883797ms)
Mar 29 21:28:49.530: INFO: (13) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 22.013609ms)
Mar 29 21:28:49.530: INFO: (13) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 22.014537ms)
Mar 29 21:28:49.530: INFO: (13) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 21.618193ms)
Mar 29 21:28:49.530: INFO: (13) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 21.968573ms)
Mar 29 21:28:49.531: INFO: (13) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 22.968917ms)
Mar 29 21:28:49.531: INFO: (13) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 22.932589ms)
Mar 29 21:28:49.533: INFO: (13) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 25.038097ms)
Mar 29 21:28:49.534: INFO: (13) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 25.909911ms)
Mar 29 21:28:49.534: INFO: (13) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 25.998808ms)
Mar 29 21:28:49.534: INFO: (13) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 25.941978ms)
Mar 29 21:28:49.549: INFO: (14) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 14.650293ms)
Mar 29 21:28:49.552: INFO: (14) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 17.736094ms)
Mar 29 21:28:49.552: INFO: (14) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 17.787633ms)
Mar 29 21:28:49.553: INFO: (14) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 17.918042ms)
Mar 29 21:28:49.553: INFO: (14) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 18.029474ms)
Mar 29 21:28:49.553: INFO: (14) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 18.807501ms)
Mar 29 21:28:49.553: INFO: (14) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 18.460996ms)
Mar 29 21:28:49.553: INFO: (14) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 18.88861ms)
Mar 29 21:28:49.554: INFO: (14) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 19.199951ms)
Mar 29 21:28:49.557: INFO: (14) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 21.964564ms)
Mar 29 21:28:49.559: INFO: (14) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 23.80796ms)
Mar 29 21:28:49.560: INFO: (14) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 25.482219ms)
Mar 29 21:28:49.560: INFO: (14) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 25.053551ms)
Mar 29 21:28:49.560: INFO: (14) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 25.563486ms)
Mar 29 21:28:49.560: INFO: (14) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 25.059359ms)
Mar 29 21:28:49.560: INFO: (14) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 25.487418ms)
Mar 29 21:28:49.574: INFO: (15) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 12.981024ms)
Mar 29 21:28:49.577: INFO: (15) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 15.662605ms)
Mar 29 21:28:49.577: INFO: (15) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 15.759331ms)
Mar 29 21:28:49.577: INFO: (15) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 16.500109ms)
Mar 29 21:28:49.577: INFO: (15) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 16.244346ms)
Mar 29 21:28:49.577: INFO: (15) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 16.952589ms)
Mar 29 21:28:49.578: INFO: (15) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 16.59129ms)
Mar 29 21:28:49.578: INFO: (15) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 16.99829ms)
Mar 29 21:28:49.584: INFO: (15) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 23.123322ms)
Mar 29 21:28:49.584: INFO: (15) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 22.949442ms)
Mar 29 21:28:49.584: INFO: (15) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 23.253349ms)
Mar 29 21:28:49.584: INFO: (15) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 23.653234ms)
Mar 29 21:28:49.588: INFO: (15) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 27.794688ms)
Mar 29 21:28:49.588: INFO: (15) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 27.525774ms)
Mar 29 21:28:49.589: INFO: (15) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 27.952311ms)
Mar 29 21:28:49.596: INFO: (15) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 35.207603ms)
Mar 29 21:28:49.612: INFO: (16) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 15.806106ms)
Mar 29 21:28:49.616: INFO: (16) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 19.60662ms)
Mar 29 21:28:49.617: INFO: (16) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 19.897246ms)
Mar 29 21:28:49.617: INFO: (16) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 20.432839ms)
Mar 29 21:28:49.617: INFO: (16) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 20.384623ms)
Mar 29 21:28:49.617: INFO: (16) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 21.114901ms)
Mar 29 21:28:49.618: INFO: (16) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 21.077814ms)
Mar 29 21:28:49.619: INFO: (16) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 22.682146ms)
Mar 29 21:28:49.620: INFO: (16) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 22.873125ms)
Mar 29 21:28:49.622: INFO: (16) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 25.685685ms)
Mar 29 21:28:49.623: INFO: (16) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 26.676741ms)
Mar 29 21:28:49.626: INFO: (16) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 29.683846ms)
Mar 29 21:28:49.626: INFO: (16) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 29.955963ms)
Mar 29 21:28:49.628: INFO: (16) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 31.048708ms)
Mar 29 21:28:49.628: INFO: (16) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 30.909717ms)
Mar 29 21:28:49.628: INFO: (16) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 31.06412ms)
Mar 29 21:28:49.644: INFO: (17) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 15.778644ms)
Mar 29 21:28:49.647: INFO: (17) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 18.826149ms)
Mar 29 21:28:49.647: INFO: (17) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 18.953606ms)
Mar 29 21:28:49.648: INFO: (17) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 19.6989ms)
Mar 29 21:28:49.648: INFO: (17) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 20.192565ms)
Mar 29 21:28:49.649: INFO: (17) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 20.667577ms)
Mar 29 21:28:49.650: INFO: (17) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 21.521778ms)
Mar 29 21:28:49.652: INFO: (17) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 24.221681ms)
Mar 29 21:28:49.653: INFO: (17) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 24.702021ms)
Mar 29 21:28:49.653: INFO: (17) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 25.005661ms)
Mar 29 21:28:49.654: INFO: (17) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 25.637019ms)
Mar 29 21:28:49.657: INFO: (17) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 28.983696ms)
Mar 29 21:28:49.658: INFO: (17) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 30.300884ms)
Mar 29 21:28:49.659: INFO: (17) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 30.79576ms)
Mar 29 21:28:49.659: INFO: (17) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 30.835158ms)
Mar 29 21:28:49.659: INFO: (17) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 30.873346ms)
Mar 29 21:28:49.675: INFO: (18) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 15.915303ms)
Mar 29 21:28:49.675: INFO: (18) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 15.911771ms)
Mar 29 21:28:49.676: INFO: (18) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 16.951805ms)
Mar 29 21:28:49.677: INFO: (18) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 17.166236ms)
Mar 29 21:28:49.678: INFO: (18) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 17.975921ms)
Mar 29 21:28:49.678: INFO: (18) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 18.332277ms)
Mar 29 21:28:49.678: INFO: (18) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 18.532398ms)
Mar 29 21:28:49.678: INFO: (18) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 18.724761ms)
Mar 29 21:28:49.678: INFO: (18) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 18.932916ms)
Mar 29 21:28:49.679: INFO: (18) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 18.950831ms)
Mar 29 21:28:49.680: INFO: (18) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 20.730193ms)
Mar 29 21:28:49.680: INFO: (18) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 21.113849ms)
Mar 29 21:28:49.680: INFO: (18) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 21.017043ms)
Mar 29 21:28:49.681: INFO: (18) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 21.106768ms)
Mar 29 21:28:49.681: INFO: (18) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 21.203691ms)
Mar 29 21:28:49.683: INFO: (18) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 23.548295ms)
Mar 29 21:28:49.699: INFO: (19) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:460/proxy/: tls baz (200; 15.788567ms)
Mar 29 21:28:49.706: INFO: (19) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7/proxy/rewriteme">test</a> (200; 21.603809ms)
Mar 29 21:28:49.706: INFO: (19) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:160/proxy/: foo (200; 21.718728ms)
Mar 29 21:28:49.706: INFO: (19) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:443/proxy/tlsrewritem... (200; 21.93757ms)
Mar 29 21:28:49.706: INFO: (19) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:1080/proxy/rewriteme">test<... (200; 22.752386ms)
Mar 29 21:28:49.706: INFO: (19) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:162/proxy/: bar (200; 22.337056ms)
Mar 29 21:28:49.706: INFO: (19) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:1080/proxy/rewriteme">... (200; 22.206057ms)
Mar 29 21:28:49.706: INFO: (19) /api/v1/namespaces/proxy-9633/pods/https:proxy-service-7492k-ckbh7:462/proxy/: tls qux (200; 22.583428ms)
Mar 29 21:28:49.707: INFO: (19) /api/v1/namespaces/proxy-9633/pods/http:proxy-service-7492k-ckbh7:162/proxy/: bar (200; 23.495617ms)
Mar 29 21:28:49.708: INFO: (19) /api/v1/namespaces/proxy-9633/pods/proxy-service-7492k-ckbh7:160/proxy/: foo (200; 23.806526ms)
Mar 29 21:28:49.708: INFO: (19) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname2/proxy/: tls qux (200; 23.762237ms)
Mar 29 21:28:49.711: INFO: (19) /api/v1/namespaces/proxy-9633/services/https:proxy-service-7492k:tlsportname1/proxy/: tls baz (200; 26.717899ms)
Mar 29 21:28:49.711: INFO: (19) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname1/proxy/: foo (200; 27.413777ms)
Mar 29 21:28:49.711: INFO: (19) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname1/proxy/: foo (200; 27.741375ms)
Mar 29 21:28:49.712: INFO: (19) /api/v1/namespaces/proxy-9633/services/proxy-service-7492k:portname2/proxy/: bar (200; 27.844176ms)
Mar 29 21:28:49.712: INFO: (19) /api/v1/namespaces/proxy-9633/services/http:proxy-service-7492k:portname2/proxy/: bar (200; 27.713949ms)
STEP: deleting ReplicationController proxy-service-7492k in namespace proxy-9633, will wait for the garbage collector to delete the pods
Mar 29 21:28:49.788: INFO: Deleting ReplicationController proxy-service-7492k took: 17.458578ms
Mar 29 21:28:49.889: INFO: Terminating ReplicationController proxy-service-7492k pods took: 100.273455ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:28:51.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9633" for this suite.

• [SLOW TEST:14.191 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":277,"completed":94,"skipped":1640,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:28:51.925: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7625
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 29 21:28:52.137: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 29 21:28:52.183: INFO: Waiting for terminating namespaces to be deleted...
Mar 29 21:28:52.192: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.194 before test
Mar 29 21:28:52.279: INFO: ibm-kubelet-monitor-jrnhz from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:28:52.280: INFO: ibm-cloud-provider-ip-169-63-130-250-65c6959456-tm4wj from ibm-system started at 2021-03-29 20:12:35 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container ibm-cloud-provider-ip-169-63-130-250 ready: true, restart count 0
Mar 29 21:28:52.280: INFO: sonobuoy from sonobuoy started at 2021-03-29 21:07:51 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 29 21:28:52.280: INFO: ibm-keepalived-watcher-5j4dz from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 21:28:52.280: INFO: calico-node-9tz6n from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 21:28:52.280: INFO: public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-vbjdf from kube-system started at 2021-03-29 20:11:55 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 29 21:28:52.280: INFO: addon-catalog-source-lgq2p from ibm-system started at 2021-03-29 19:53:09 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container configmap-registry-server ready: true, restart count 0
Mar 29 21:28:52.280: INFO: sonobuoy-e2e-job-95ac96a8209247c9 from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container e2e ready: true, restart count 0
Mar 29 21:28:52.280: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:28:52.280: INFO: ibm-master-proxy-static-10.189.118.194 from kube-system started at 2021-03-29 19:48:41 +0000 UTC (2 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 21:28:52.280: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:28:52.280: INFO: coredns-7cc79848cf-8xnhq from kube-system started at 2021-03-29 19:57:06 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container coredns ready: true, restart count 0
Mar 29 21:28:52.280: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-ngv5d from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:28:52.280: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:28:52.280: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 21:28:52.280: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.196 before test
Mar 29 21:28:52.341: INFO: ibm-keepalived-watcher-4x7cv from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.341: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 21:28:52.342: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-l4nd5 from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:28:52.342: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:28:52.342: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 21:28:52.342: INFO: calico-node-mtdnd from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.342: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 21:28:52.342: INFO: ibm-master-proxy-static-10.189.118.196 from kube-system started at 2021-03-29 19:48:58 +0000 UTC (2 container statuses recorded)
Mar 29 21:28:52.342: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 21:28:52.342: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:28:52.342: INFO: ibm-kubelet-monitor-cmcxj from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.342: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:28:52.342: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.208 before test
Mar 29 21:28:52.443: INFO: olm-operator-8496678794-gg5v7 from ibm-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container olm-operator ready: true, restart count 0
Mar 29 21:28:52.443: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-n7lmz from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:28:52.443: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 21:28:52.443: INFO: ibm-keepalived-watcher-ps2vt from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 21:28:52.443: INFO: ibm-file-plugin-c76c68fd9-bpt7r from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 29 21:28:52.443: INFO: calico-kube-controllers-6599f97f59-7g48l from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 29 21:28:52.443: INFO: catalog-operator-7bc4c797b5-kbbnz from ibm-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 29 21:28:52.443: INFO: dashboard-metrics-scraper-b585c6867-2k8ww from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 29 21:28:52.443: INFO: kubernetes-dashboard-6cf6cfdf4-kzffs from kube-system started at 2021-03-29 19:49:50 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 29 21:28:52.443: INFO: metrics-server-f9f7549df-6xdmf from kube-system started at 2021-03-29 19:50:53 +0000 UTC (2 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container metrics-server ready: true, restart count 0
Mar 29 21:28:52.443: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 29 21:28:52.443: INFO: ibm-cloud-provider-ip-169-63-130-250-65c6959456-gps6w from ibm-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container ibm-cloud-provider-ip-169-63-130-250 ready: true, restart count 0
Mar 29 21:28:52.443: INFO: calico-node-zjnmd from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 21:28:52.443: INFO: coredns-autoscaler-bff977695-9wkhn from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container autoscaler ready: true, restart count 0
Mar 29 21:28:52.443: INFO: public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-2lcqz from kube-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 29 21:28:52.443: INFO: ibm-master-proxy-static-10.189.118.208 from kube-system started at 2021-03-29 19:48:43 +0000 UTC (2 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 21:28:52.443: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:28:52.443: INFO: coredns-7cc79848cf-kjg7p from kube-system started at 2021-03-29 19:57:07 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container coredns ready: true, restart count 0
Mar 29 21:28:52.443: INFO: coredns-7cc79848cf-m5klb from kube-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container coredns ready: true, restart count 0
Mar 29 21:28:52.443: INFO: ibm-storage-watcher-659bdcc695-qqkhw from kube-system started at 2021-03-29 19:49:50 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 29 21:28:52.443: INFO: ibm-kubelet-monitor-gbcvf from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:28:52.443: INFO: vpn-657cb5cdb6-v5h8m from kube-system started at 2021-03-29 19:56:31 +0000 UTC (1 container statuses recorded)
Mar 29 21:28:52.443: INFO: 	Container vpn ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-aae5b7fb-6e96-4762-8a71-285c55fe0cbf 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-aae5b7fb-6e96-4762-8a71-285c55fe0cbf off the node 10.189.118.196
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aae5b7fb-6e96-4762-8a71-285c55fe0cbf
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:00.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7625" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:8.723 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":277,"completed":95,"skipped":1640,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:00.650: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 29 21:29:00.897: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4484 /api/v1/namespaces/watch-4484/configmaps/e2e-watch-test-watch-closed 2ac05770-9ec5-45df-9412-4b3dff34973f 34337 0 2021-03-29 21:29:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-29 21:29:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 21:29:00.897: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4484 /api/v1/namespaces/watch-4484/configmaps/e2e-watch-test-watch-closed 2ac05770-9ec5-45df-9412-4b3dff34973f 34338 0 2021-03-29 21:29:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-29 21:29:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 29 21:29:00.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4484 /api/v1/namespaces/watch-4484/configmaps/e2e-watch-test-watch-closed 2ac05770-9ec5-45df-9412-4b3dff34973f 34339 0 2021-03-29 21:29:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-29 21:29:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 21:29:00.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4484 /api/v1/namespaces/watch-4484/configmaps/e2e-watch-test-watch-closed 2ac05770-9ec5-45df-9412-4b3dff34973f 34340 0 2021-03-29 21:29:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-29 21:29:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:00.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4484" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":277,"completed":96,"skipped":1644,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:00.974: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:29:01.204: INFO: Waiting up to 5m0s for pod "downwardapi-volume-218d9f08-6363-4006-ad05-858fa626797c" in namespace "downward-api-7972" to be "Succeeded or Failed"
Mar 29 21:29:01.213: INFO: Pod "downwardapi-volume-218d9f08-6363-4006-ad05-858fa626797c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.128817ms
Mar 29 21:29:03.221: INFO: Pod "downwardapi-volume-218d9f08-6363-4006-ad05-858fa626797c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016610391s
Mar 29 21:29:05.230: INFO: Pod "downwardapi-volume-218d9f08-6363-4006-ad05-858fa626797c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025088892s
STEP: Saw pod success
Mar 29 21:29:05.230: INFO: Pod "downwardapi-volume-218d9f08-6363-4006-ad05-858fa626797c" satisfied condition "Succeeded or Failed"
Mar 29 21:29:05.237: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-218d9f08-6363-4006-ad05-858fa626797c container client-container: <nil>
STEP: delete the pod
Mar 29 21:29:05.295: INFO: Waiting for pod downwardapi-volume-218d9f08-6363-4006-ad05-858fa626797c to disappear
Mar 29 21:29:05.302: INFO: Pod downwardapi-volume-218d9f08-6363-4006-ad05-858fa626797c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:05.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7972" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":97,"skipped":1644,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:05.336: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 29 21:29:05.567: INFO: Waiting up to 5m0s for pod "pod-c6373cdf-af7b-4103-a31f-c215123ab005" in namespace "emptydir-6835" to be "Succeeded or Failed"
Mar 29 21:29:05.575: INFO: Pod "pod-c6373cdf-af7b-4103-a31f-c215123ab005": Phase="Pending", Reason="", readiness=false. Elapsed: 7.641659ms
Mar 29 21:29:07.584: INFO: Pod "pod-c6373cdf-af7b-4103-a31f-c215123ab005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016874119s
Mar 29 21:29:09.593: INFO: Pod "pod-c6373cdf-af7b-4103-a31f-c215123ab005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025381911s
STEP: Saw pod success
Mar 29 21:29:09.593: INFO: Pod "pod-c6373cdf-af7b-4103-a31f-c215123ab005" satisfied condition "Succeeded or Failed"
Mar 29 21:29:09.600: INFO: Trying to get logs from node 10.189.118.196 pod pod-c6373cdf-af7b-4103-a31f-c215123ab005 container test-container: <nil>
STEP: delete the pod
Mar 29 21:29:09.652: INFO: Waiting for pod pod-c6373cdf-af7b-4103-a31f-c215123ab005 to disappear
Mar 29 21:29:09.659: INFO: Pod pod-c6373cdf-af7b-4103-a31f-c215123ab005 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:09.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6835" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":98,"skipped":1655,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:09.695: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 29 21:29:12.544: INFO: Successfully updated pod "annotationupdatef2358157-783b-4dee-9dc4-2f24e4fff5fb"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:14.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6498" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":99,"skipped":1659,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:14.634: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-0864a12e-56e8-4809-bf87-90322536c3a8
STEP: Creating a pod to test consume secrets
Mar 29 21:29:14.873: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f61d748-3f22-4d62-ad06-f5fa431fef73" in namespace "projected-6381" to be "Succeeded or Failed"
Mar 29 21:29:14.879: INFO: Pod "pod-projected-secrets-1f61d748-3f22-4d62-ad06-f5fa431fef73": Phase="Pending", Reason="", readiness=false. Elapsed: 6.40728ms
Mar 29 21:29:16.889: INFO: Pod "pod-projected-secrets-1f61d748-3f22-4d62-ad06-f5fa431fef73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016180421s
STEP: Saw pod success
Mar 29 21:29:16.889: INFO: Pod "pod-projected-secrets-1f61d748-3f22-4d62-ad06-f5fa431fef73" satisfied condition "Succeeded or Failed"
Mar 29 21:29:16.897: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-secrets-1f61d748-3f22-4d62-ad06-f5fa431fef73 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 29 21:29:16.946: INFO: Waiting for pod pod-projected-secrets-1f61d748-3f22-4d62-ad06-f5fa431fef73 to disappear
Mar 29 21:29:16.953: INFO: Pod pod-projected-secrets-1f61d748-3f22-4d62-ad06-f5fa431fef73 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:16.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6381" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":100,"skipped":1661,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:17.131: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5223
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:29:17.384: INFO: (0) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 24.05133ms)
Mar 29 21:29:17.401: INFO: (1) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 16.831481ms)
Mar 29 21:29:17.423: INFO: (2) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 22.209733ms)
Mar 29 21:29:17.441: INFO: (3) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 17.131975ms)
Mar 29 21:29:17.461: INFO: (4) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 20.135625ms)
Mar 29 21:29:17.479: INFO: (5) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 17.745333ms)
Mar 29 21:29:17.500: INFO: (6) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 21.025913ms)
Mar 29 21:29:17.521: INFO: (7) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 20.204742ms)
Mar 29 21:29:17.538: INFO: (8) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 17.541744ms)
Mar 29 21:29:17.558: INFO: (9) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 19.344635ms)
Mar 29 21:29:17.576: INFO: (10) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 18.621357ms)
Mar 29 21:29:17.610: INFO: (11) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 33.497733ms)
Mar 29 21:29:17.631: INFO: (12) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 20.51737ms)
Mar 29 21:29:17.649: INFO: (13) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 18.55027ms)
Mar 29 21:29:17.670: INFO: (14) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 21.127012ms)
Mar 29 21:29:17.689: INFO: (15) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 18.870626ms)
Mar 29 21:29:17.709: INFO: (16) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 19.613001ms)
Mar 29 21:29:17.727: INFO: (17) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 18.387495ms)
Mar 29 21:29:17.752: INFO: (18) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 24.634503ms)
Mar 29 21:29:17.798: INFO: (19) /api/v1/nodes/10.189.118.194/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 46.238972ms)
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:17.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5223" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":277,"completed":101,"skipped":1669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:17.836: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-899
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 29 21:29:18.073: INFO: Waiting up to 5m0s for pod "pod-60ce4ada-9891-4650-a857-f86cd3c0bea7" in namespace "emptydir-899" to be "Succeeded or Failed"
Mar 29 21:29:18.080: INFO: Pod "pod-60ce4ada-9891-4650-a857-f86cd3c0bea7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.16787ms
Mar 29 21:29:20.089: INFO: Pod "pod-60ce4ada-9891-4650-a857-f86cd3c0bea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016010877s
STEP: Saw pod success
Mar 29 21:29:20.089: INFO: Pod "pod-60ce4ada-9891-4650-a857-f86cd3c0bea7" satisfied condition "Succeeded or Failed"
Mar 29 21:29:20.097: INFO: Trying to get logs from node 10.189.118.196 pod pod-60ce4ada-9891-4650-a857-f86cd3c0bea7 container test-container: <nil>
STEP: delete the pod
Mar 29 21:29:20.151: INFO: Waiting for pod pod-60ce4ada-9891-4650-a857-f86cd3c0bea7 to disappear
Mar 29 21:29:20.162: INFO: Pod pod-60ce4ada-9891-4650-a857-f86cd3c0bea7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:20.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-899" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":102,"skipped":1727,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:20.191: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5772
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:29:20.959: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 21:29:23.106: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650160, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650160, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650161, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650160, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:29:26.145: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:29:26.153: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1632-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:27.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5772" for this suite.
STEP: Destroying namespace "webhook-5772-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.451 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":277,"completed":103,"skipped":1737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:27.642: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3093
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Mar 29 21:29:27.859: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:48.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3093" for this suite.

• [SLOW TEST:21.002 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":277,"completed":104,"skipped":1774,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:48.645: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 29 21:29:55.048: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 29 21:29:55.058: INFO: Pod pod-with-poststart-http-hook still exists
Mar 29 21:29:57.058: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 29 21:29:57.070: INFO: Pod pod-with-poststart-http-hook still exists
Mar 29 21:29:59.058: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 29 21:29:59.071: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:29:59.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-859" for this suite.

• [SLOW TEST:10.469 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":277,"completed":105,"skipped":1774,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:29:59.115: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6821
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-05cef987-2aa3-431d-af00-9980c64a505a
STEP: Creating a pod to test consume configMaps
Mar 29 21:29:59.367: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b3a394f3-0f3c-4377-9202-300bf9f88c0a" in namespace "projected-6821" to be "Succeeded or Failed"
Mar 29 21:29:59.379: INFO: Pod "pod-projected-configmaps-b3a394f3-0f3c-4377-9202-300bf9f88c0a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.526568ms
Mar 29 21:30:01.391: INFO: Pod "pod-projected-configmaps-b3a394f3-0f3c-4377-9202-300bf9f88c0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.023900379s
Mar 29 21:30:03.404: INFO: Pod "pod-projected-configmaps-b3a394f3-0f3c-4377-9202-300bf9f88c0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036555082s
STEP: Saw pod success
Mar 29 21:30:03.404: INFO: Pod "pod-projected-configmaps-b3a394f3-0f3c-4377-9202-300bf9f88c0a" satisfied condition "Succeeded or Failed"
Mar 29 21:30:03.421: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-configmaps-b3a394f3-0f3c-4377-9202-300bf9f88c0a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 21:30:03.491: INFO: Waiting for pod pod-projected-configmaps-b3a394f3-0f3c-4377-9202-300bf9f88c0a to disappear
Mar 29 21:30:03.502: INFO: Pod pod-projected-configmaps-b3a394f3-0f3c-4377-9202-300bf9f88c0a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:30:03.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6821" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":106,"skipped":1783,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:30:03.532: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:30:04.460: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 21:30:06.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650204, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650204, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650204, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650204, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:30:09.550: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:30:19.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5517" for this suite.
STEP: Destroying namespace "webhook-5517-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.638 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":277,"completed":107,"skipped":1783,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:30:20.170: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2386
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Mar 29 21:30:20.399: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-820232969 proxy --unix-socket=/tmp/kubectl-proxy-unix300115828/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:30:20.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2386" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":277,"completed":108,"skipped":1788,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:30:20.512: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-7581
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7581
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7581
Mar 29 21:30:20.796: INFO: Found 0 stateful pods, waiting for 1
Mar 29 21:30:30.809: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 29 21:30:30.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-7581 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 21:30:31.124: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 21:30:31.124: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 21:30:31.124: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 21:30:31.136: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 29 21:30:41.157: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 21:30:41.157: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 21:30:41.225: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998365s
Mar 29 21:30:42.241: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.985445105s
Mar 29 21:30:43.252: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.969587487s
Mar 29 21:30:44.264: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.958731549s
Mar 29 21:30:45.275: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.946643153s
Mar 29 21:30:46.288: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.935421229s
Mar 29 21:30:47.300: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.922726728s
Mar 29 21:30:48.311: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.910159475s
Mar 29 21:30:49.323: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.899373422s
Mar 29 21:30:50.336: INFO: Verifying statefulset ss doesn't scale past 1 for another 887.573132ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7581
Mar 29 21:30:51.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-7581 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 21:30:51.628: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 21:30:51.628: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 21:30:51.628: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 21:30:51.641: INFO: Found 1 stateful pods, waiting for 3
Mar 29 21:31:01.653: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 21:31:01.653: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 21:31:01.653: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 29 21:31:01.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-7581 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 21:31:01.963: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 21:31:01.963: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 21:31:01.963: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 21:31:01.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-7581 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 21:31:02.249: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 21:31:02.249: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 21:31:02.249: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 21:31:02.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-7581 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 21:31:02.575: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 21:31:02.575: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 21:31:02.575: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 21:31:02.575: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 21:31:02.587: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 29 21:31:12.631: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 21:31:12.631: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 21:31:12.631: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 21:31:12.680: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998249s
Mar 29 21:31:13.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989240859s
Mar 29 21:31:14.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977106738s
Mar 29 21:31:15.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.964762043s
Mar 29 21:31:16.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.953163172s
Mar 29 21:31:17.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.941091019s
Mar 29 21:31:18.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.927251984s
Mar 29 21:31:19.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.912465752s
Mar 29 21:31:20.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.898430847s
Mar 29 21:31:21.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 882.314683ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7581
Mar 29 21:31:22.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-7581 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 21:31:23.116: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 21:31:23.116: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 21:31:23.116: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 21:31:23.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-7581 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 21:31:23.377: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 21:31:23.377: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 21:31:23.377: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 21:31:23.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-7581 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 21:31:23.623: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 21:31:23.623: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 21:31:23.623: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 21:31:23.623: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 29 21:31:53.685: INFO: Deleting all statefulset in ns statefulset-7581
Mar 29 21:31:53.695: INFO: Scaling statefulset ss to 0
Mar 29 21:31:53.730: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 21:31:53.742: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:31:53.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7581" for this suite.

• [SLOW TEST:93.347 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":277,"completed":109,"skipped":1809,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:31:53.859: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4501
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:32:02.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4501" for this suite.

• [SLOW TEST:8.272 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":277,"completed":110,"skipped":1820,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:32:02.131: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:32:02.335: INFO: Creating ReplicaSet my-hostname-basic-8bc83262-66f2-4714-a78a-83a78dc83116
Mar 29 21:32:02.357: INFO: Pod name my-hostname-basic-8bc83262-66f2-4714-a78a-83a78dc83116: Found 0 pods out of 1
Mar 29 21:32:07.369: INFO: Pod name my-hostname-basic-8bc83262-66f2-4714-a78a-83a78dc83116: Found 1 pods out of 1
Mar 29 21:32:07.370: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8bc83262-66f2-4714-a78a-83a78dc83116" is running
Mar 29 21:32:07.381: INFO: Pod "my-hostname-basic-8bc83262-66f2-4714-a78a-83a78dc83116-2hmjg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-29 21:32:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-29 21:32:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-29 21:32:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-29 21:32:02 +0000 UTC Reason: Message:}])
Mar 29 21:32:07.381: INFO: Trying to dial the pod
Mar 29 21:32:12.427: INFO: Controller my-hostname-basic-8bc83262-66f2-4714-a78a-83a78dc83116: Got expected result from replica 1 [my-hostname-basic-8bc83262-66f2-4714-a78a-83a78dc83116-2hmjg]: "my-hostname-basic-8bc83262-66f2-4714-a78a-83a78dc83116-2hmjg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:32:12.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7702" for this suite.

• [SLOW TEST:10.322 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":111,"skipped":1824,"failed":0}
SSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:32:12.455: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:32:12.680: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b1cdea61-b57f-42ec-a356-5783df188dc5" in namespace "security-context-test-9365" to be "Succeeded or Failed"
Mar 29 21:32:12.690: INFO: Pod "busybox-privileged-false-b1cdea61-b57f-42ec-a356-5783df188dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.948309ms
Mar 29 21:32:14.701: INFO: Pod "busybox-privileged-false-b1cdea61-b57f-42ec-a356-5783df188dc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.020903926s
Mar 29 21:32:16.711: INFO: Pod "busybox-privileged-false-b1cdea61-b57f-42ec-a356-5783df188dc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031283957s
Mar 29 21:32:16.711: INFO: Pod "busybox-privileged-false-b1cdea61-b57f-42ec-a356-5783df188dc5" satisfied condition "Succeeded or Failed"
Mar 29 21:32:16.759: INFO: Got logs for pod "busybox-privileged-false-b1cdea61-b57f-42ec-a356-5783df188dc5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:32:16.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9365" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":112,"skipped":1827,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:32:16.788: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3691
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:32:16.991: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 29 21:32:21.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3691 create -f -'
Mar 29 21:32:21.815: INFO: stderr: ""
Mar 29 21:32:21.815: INFO: stdout: "e2e-test-crd-publish-openapi-1985-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 29 21:32:21.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3691 delete e2e-test-crd-publish-openapi-1985-crds test-foo'
Mar 29 21:32:21.922: INFO: stderr: ""
Mar 29 21:32:21.922: INFO: stdout: "e2e-test-crd-publish-openapi-1985-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 29 21:32:21.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3691 apply -f -'
Mar 29 21:32:22.377: INFO: stderr: ""
Mar 29 21:32:22.377: INFO: stdout: "e2e-test-crd-publish-openapi-1985-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 29 21:32:22.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3691 delete e2e-test-crd-publish-openapi-1985-crds test-foo'
Mar 29 21:32:22.485: INFO: stderr: ""
Mar 29 21:32:22.486: INFO: stdout: "e2e-test-crd-publish-openapi-1985-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 29 21:32:22.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3691 create -f -'
Mar 29 21:32:22.795: INFO: rc: 1
Mar 29 21:32:22.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3691 apply -f -'
Mar 29 21:32:23.095: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 29 21:32:23.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3691 create -f -'
Mar 29 21:32:23.420: INFO: rc: 1
Mar 29 21:32:23.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3691 apply -f -'
Mar 29 21:32:23.594: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 29 21:32:23.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 explain e2e-test-crd-publish-openapi-1985-crds'
Mar 29 21:32:23.761: INFO: stderr: ""
Mar 29 21:32:23.761: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1985-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 29 21:32:23.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 explain e2e-test-crd-publish-openapi-1985-crds.metadata'
Mar 29 21:32:24.060: INFO: stderr: ""
Mar 29 21:32:24.060: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1985-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 29 21:32:24.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 explain e2e-test-crd-publish-openapi-1985-crds.spec'
Mar 29 21:32:24.389: INFO: stderr: ""
Mar 29 21:32:24.389: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1985-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 29 21:32:24.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 explain e2e-test-crd-publish-openapi-1985-crds.spec.bars'
Mar 29 21:32:24.716: INFO: stderr: ""
Mar 29 21:32:24.716: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1985-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 29 21:32:24.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 explain e2e-test-crd-publish-openapi-1985-crds.spec.bars2'
Mar 29 21:32:24.889: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:32:28.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3691" for this suite.

• [SLOW TEST:11.908 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":277,"completed":113,"skipped":1837,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:32:28.699: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6012
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-c56b4ca2-f22b-4384-a628-12520d1f525b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:32:33.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6012" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":114,"skipped":1858,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:32:33.133: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4245
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-1c1174f6-8065-45df-9853-cbead3af6cad
STEP: Creating a pod to test consume secrets
Mar 29 21:32:33.370: INFO: Waiting up to 5m0s for pod "pod-secrets-6967ac3b-1c4e-44f9-879f-fda6862a02ed" in namespace "secrets-4245" to be "Succeeded or Failed"
Mar 29 21:32:33.378: INFO: Pod "pod-secrets-6967ac3b-1c4e-44f9-879f-fda6862a02ed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119709ms
Mar 29 21:32:35.386: INFO: Pod "pod-secrets-6967ac3b-1c4e-44f9-879f-fda6862a02ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01653424s
Mar 29 21:32:37.395: INFO: Pod "pod-secrets-6967ac3b-1c4e-44f9-879f-fda6862a02ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024778789s
STEP: Saw pod success
Mar 29 21:32:37.395: INFO: Pod "pod-secrets-6967ac3b-1c4e-44f9-879f-fda6862a02ed" satisfied condition "Succeeded or Failed"
Mar 29 21:32:37.403: INFO: Trying to get logs from node 10.189.118.196 pod pod-secrets-6967ac3b-1c4e-44f9-879f-fda6862a02ed container secret-env-test: <nil>
STEP: delete the pod
Mar 29 21:32:37.454: INFO: Waiting for pod pod-secrets-6967ac3b-1c4e-44f9-879f-fda6862a02ed to disappear
Mar 29 21:32:37.462: INFO: Pod pod-secrets-6967ac3b-1c4e-44f9-879f-fda6862a02ed no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:32:37.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4245" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":277,"completed":115,"skipped":1868,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:32:37.492: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 29 21:32:37.709: INFO: PodSpec: initContainers in spec.initContainers
Mar 29 21:33:23.910: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-74f2d4a4-c875-45c3-82d6-40ab4cf89018", GenerateName:"", Namespace:"init-container-4419", SelfLink:"/api/v1/namespaces/init-container-4419/pods/pod-init-74f2d4a4-c875-45c3-82d6-40ab4cf89018", UID:"8afbf574-ceb9-4716-a4d4-dd042088b3c8", ResourceVersion:"36247", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63752650357, loc:(*time.Location)(0x7b4c620)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"709925551"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00396f640), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00396f680)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00396f6a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00396f6e0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-lrgq4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0060a9ec0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lrgq4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lrgq4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lrgq4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004205bf8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.189.118.196", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001d809a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004205cf0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004205d30)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004205d38), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004205d3c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650357, loc:(*time.Location)(0x7b4c620)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650357, loc:(*time.Location)(0x7b4c620)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650357, loc:(*time.Location)(0x7b4c620)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650357, loc:(*time.Location)(0x7b4c620)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.189.118.196", PodIP:"172.30.58.62", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.58.62"}}, StartTime:(*v1.Time)(0xc00396f700), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001d80af0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001d80b60)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://aa6f37f591e9095c48cdf3caf161a597835772e01e5660c7fecea2823fb84aed", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00396f740), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00396f720), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc004205eaf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:33:23.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4419" for this suite.

• [SLOW TEST:46.453 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":277,"completed":116,"skipped":1889,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:33:23.946: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:33:24.151: INFO: Creating deployment "webserver-deployment"
Mar 29 21:33:24.167: INFO: Waiting for observed generation 1
Mar 29 21:33:26.218: INFO: Waiting for all required pods to come up
Mar 29 21:33:26.231: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 29 21:33:28.256: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 29 21:33:28.275: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 29 21:33:28.301: INFO: Updating deployment webserver-deployment
Mar 29 21:33:28.301: INFO: Waiting for observed generation 2
Mar 29 21:33:30.326: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 29 21:33:30.335: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 29 21:33:30.345: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 29 21:33:30.374: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 29 21:33:30.374: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 29 21:33:30.382: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 29 21:33:30.402: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 29 21:33:30.402: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 29 21:33:30.431: INFO: Updating deployment webserver-deployment
Mar 29 21:33:30.431: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 29 21:33:30.448: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 29 21:33:32.468: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 29 21:33:32.497: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9466 /apis/apps/v1/namespaces/deployment-9466/deployments/webserver-deployment e3791323-c01e-4ddd-9295-03068b78d444 36603 3 2021-03-29 21:33:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-29 21:33:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-29 21:33:28 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003325c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-29 21:33:30 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2021-03-29 21:33:30 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 29 21:33:32.509: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-9466 /apis/apps/v1/namespaces/deployment-9466/replicasets/webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 36595 3 2021-03-29 21:33:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e3791323-c01e-4ddd-9295-03068b78d444 0xc0027f6317 0xc0027f6318}] []  [{kube-controller-manager Update apps/v1 2021-03-29 21:33:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 55 57 49 51 50 51 45 99 48 49 101 45 52 100 100 100 45 57 50 57 53 45 48 51 48 54 56 98 55 56 100 52 52 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0027f63c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:33:32.510: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 29 21:33:32.510: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-9466 /apis/apps/v1/namespaces/deployment-9466/replicasets/webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 36593 3 2021-03-29 21:33:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e3791323-c01e-4ddd-9295-03068b78d444 0xc0027f6437 0xc0027f6438}] []  [{kube-controller-manager Update apps/v1 2021-03-29 21:33:25 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 55 57 49 51 50 51 45 99 48 49 101 45 52 100 100 100 45 57 50 57 53 45 48 51 48 54 56 98 55 56 100 52 52 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0027f64b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:33:32.528: INFO: Pod "webserver-deployment-6676bcd6d4-4lbkr" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-4lbkr webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-4lbkr 472c6544-ebe1-47e9-b52d-401b9955f189 36618 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc003454d97 0xc003454d98}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.529: INFO: Pod "webserver-deployment-6676bcd6d4-75dqg" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-75dqg webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-75dqg 51ca21df-0a93-4931-8ac6-fcd27857700b 36591 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc003454f67 0xc003454f68}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.194,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.529: INFO: Pod "webserver-deployment-6676bcd6d4-9k94w" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-9k94w webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-9k94w a8846f11-dd4e-4905-9dc7-1ac96d7ea981 36528 0 2021-03-29 21:33:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc003455117 0xc003455118}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 53 56 46 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:172.30.58.6,StartTime:2021-03-29 21:33:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.529: INFO: Pod "webserver-deployment-6676bcd6d4-b9c8d" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-b9c8d webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-b9c8d d7c98e65-d8c6-4fb1-9434-ee74c6e8e265 36616 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc003455380 0xc003455381}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.194,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.530: INFO: Pod "webserver-deployment-6676bcd6d4-bsbh6" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-bsbh6 webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-bsbh6 d74ad30c-b805-4e63-a1bd-5ffe82771cf1 36634 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc003455577 0xc003455578}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.530: INFO: Pod "webserver-deployment-6676bcd6d4-cmmvj" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-cmmvj webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-cmmvj 8af745e3-d0ab-4a2b-b4f0-2a2a06a9b2c1 36524 0 2021-03-29 21:33:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc003455777 0xc003455778}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:29 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 53 56 46 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:172.30.58.1,StartTime:2021-03-29 21:33:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.531: INFO: Pod "webserver-deployment-6676bcd6d4-fr98n" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-fr98n webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-fr98n 9f2a4cea-c92a-49ad-801a-3ccbfa1fd651 36741 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc003455a10 0xc003455a11}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:32 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 50 48 57 46 50 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.208,PodIP:172.30.209.228,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.209.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.531: INFO: Pod "webserver-deployment-6676bcd6d4-gdr59" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-gdr59 webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-gdr59 e39e1db6-86a5-426d-aeaa-834e70cc7f51 36515 0 2021-03-29 21:33:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc003455c87 0xc003455c88}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:29 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 49 50 52 46 49 52 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.194,PodIP:172.30.124.146,StartTime:2021-03-29 21:33:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.124.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.531: INFO: Pod "webserver-deployment-6676bcd6d4-kzmjc" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-kzmjc webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-kzmjc ee2d0d5a-9191-4a17-9df7-1bf2f2bfb3f2 36632 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc003455ef7 0xc003455ef8}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.208,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.532: INFO: Pod "webserver-deployment-6676bcd6d4-mdndx" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-mdndx webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-mdndx 6902839d-94c4-4071-9f1d-6e7aef5fdeca 36641 0 2021-03-29 21:33:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc00313e0d7 0xc00313e0d8}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 49 50 52 46 49 52 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.194,PodIP:172.30.124.143,StartTime:2021-03-29 21:33:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.124.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.532: INFO: Pod "webserver-deployment-6676bcd6d4-pn92n" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-pn92n webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-pn92n 5c3e29e4-35a1-45b1-8d7f-5076cb7336b9 36738 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc00313e4f7 0xc00313e4f8}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:32 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 50 48 57 46 50 50 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.208,PodIP:172.30.209.229,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.209.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.533: INFO: Pod "webserver-deployment-6676bcd6d4-tdgmh" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-tdgmh webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-tdgmh 3457a95c-07d8-45c8-a05d-48bf21fd38e9 36645 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc00313e7f7 0xc00313e7f8}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.533: INFO: Pod "webserver-deployment-6676bcd6d4-ztjx5" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-ztjx5 webserver-deployment-6676bcd6d4- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-6676bcd6d4-ztjx5 3e4aa03e-aa8e-4af3-8339-42ef181ae11c 36532 0 2021-03-29 21:33:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 99e4c0ef-c0b2-4937-860d-7e65449ef2d8 0xc00313eb27 0xc00313eb28}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 57 101 52 99 48 101 102 45 99 48 98 50 45 52 57 51 55 45 56 54 48 100 45 55 101 54 53 52 52 57 101 102 50 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 50 48 57 46 50 50 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.208,PodIP:172.30.209.227,StartTime:2021-03-29 21:33:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.209.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.533: INFO: Pod "webserver-deployment-84855cf797-2s9tt" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-2s9tt webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-2s9tt 85a81c92-302d-47f4-8515-971b101810d2 36600 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc00313ef67 0xc00313ef68}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.194,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.534: INFO: Pod "webserver-deployment-84855cf797-4dfxd" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-4dfxd webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-4dfxd f91d17dd-f1a2-4b35-a520-e0fc202e55ce 36373 0 2021-03-29 21:33:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc00313f2f7 0xc00313f2f8}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:25 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 53 56 46 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:172.30.58.4,StartTime:2021-03-29 21:33:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:33:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://abd41727045e347fc7a3c5f73f12df277569859f6721e1bf01dc680df7797499,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.534: INFO: Pod "webserver-deployment-84855cf797-5v56w" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-5v56w webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-5v56w 1bddd4b3-30c5-4938-b74a-b9cae909bc2c 36389 0 2021-03-29 21:33:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc00313f7f0 0xc00313f7f1}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 50 48 57 46 50 50 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.208,PodIP:172.30.209.224,StartTime:2021-03-29 21:33:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:33:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://9d1f109c88224f708d62ecac4712ca6e126486b3943097b8963475ffd56b7480,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.209.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.535: INFO: Pod "webserver-deployment-84855cf797-7j2v4" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-7j2v4 webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-7j2v4 c102475c-74d4-43bf-800e-7cb3c1a46a73 36597 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc00313faf7 0xc00313faf8}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.535: INFO: Pod "webserver-deployment-84855cf797-7l5j9" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-7l5j9 webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-7l5j9 3502ada5-858b-4e17-aac8-986a9bebd39e 36596 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc00313fe57 0xc00313fe58}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.208,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.536: INFO: Pod "webserver-deployment-84855cf797-7l7pg" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-7l7pg webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-7l7pg 9290292e-1be2-444a-a629-47041312cc92 36648 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc006a9e1a7 0xc006a9e1a8}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.536: INFO: Pod "webserver-deployment-84855cf797-8l5rh" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-8l5rh webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-8l5rh 87b9df2f-de1c-4acf-82ba-41b508e408a7 36392 0 2021-03-29 21:33:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc006a9e517 0xc006a9e518}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 50 48 57 46 50 50 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.208,PodIP:172.30.209.225,StartTime:2021-03-29 21:33:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:33:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7967d1ba197b2e22840924f4a6586a907587047aa834fb3f5774171a69e9e378,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.209.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.537: INFO: Pod "webserver-deployment-84855cf797-bb9gs" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-bb9gs webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-bb9gs 2da53743-bd25-4367-91f5-6c726991902d 36411 0 2021-03-29 21:33:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc006a9e947 0xc006a9e948}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 53 56 46 54 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:172.30.58.60,StartTime:2021-03-29 21:33:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:33:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://5dbf2e4f427d85d4567ea3cf5868e822a93917acbf3d3cc258212426b99fb07e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.537: INFO: Pod "webserver-deployment-84855cf797-bhfbv" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-bhfbv webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-bhfbv 8eae2401-349d-4138-b9da-ea7313890d1a 36624 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc006a9ec97 0xc006a9ec98}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.194,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.538: INFO: Pod "webserver-deployment-84855cf797-btpkd" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-btpkd webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-btpkd df15927c-1d81-43ce-94ac-1b786ba79e14 36402 0 2021-03-29 21:33:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc006a9ee77 0xc006a9ee78}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 49 50 52 46 49 52 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.194,PodIP:172.30.124.140,StartTime:2021-03-29 21:33:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:33:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://cbec33457cd18fc46da385951ea627e06c9d6a6c4d27b34699c4250326fca5a0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.124.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.538: INFO: Pod "webserver-deployment-84855cf797-cpgz9" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-cpgz9 webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-cpgz9 3faa592f-b0f1-4eb0-8eba-e54d400059d2 36626 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc006a9f0c7 0xc006a9f0c8}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.539: INFO: Pod "webserver-deployment-84855cf797-f6hlb" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-f6hlb webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-f6hlb 2c7bfecb-83bb-497a-bebd-ca71638491e0 36609 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc006a9f327 0xc006a9f328}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.194,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.539: INFO: Pod "webserver-deployment-84855cf797-gnqpv" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-gnqpv webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-gnqpv 5270b292-bfe5-4988-8b6a-267054b8f4a5 36622 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc006a9f657 0xc006a9f658}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.208,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.540: INFO: Pod "webserver-deployment-84855cf797-krrqh" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-krrqh webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-krrqh f677cdb0-0a7c-42b3-b7ba-2c5ca6295b8d 36357 0 2021-03-29 21:33:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc006a9f907 0xc006a9f908}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:25 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 49 50 52 46 49 51 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.194,PodIP:172.30.124.139,StartTime:2021-03-29 21:33:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:33:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e5a35f8d2706d398a09109a4e7bf55e5fa76408211b9c4548c3409b19e8d5826,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.124.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.540: INFO: Pod "webserver-deployment-84855cf797-pgr64" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-pgr64 webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-pgr64 45b6fd9e-edb5-415e-90c9-06a05f47c453 36608 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc006a9fcc7 0xc006a9fcc8}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.541: INFO: Pod "webserver-deployment-84855cf797-s6xcl" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-s6xcl webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-s6xcl 4ef8be6e-4d51-4511-9342-e7c8489c1815 36614 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc001e20077 0xc001e20078}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.208,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.541: INFO: Pod "webserver-deployment-84855cf797-tjpfn" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-tjpfn webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-tjpfn c6b5b471-ab46-48d5-ae64-94c50a91a2dc 36576 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc001e20517 0xc001e20518}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.541: INFO: Pod "webserver-deployment-84855cf797-tkhl9" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-tkhl9 webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-tkhl9 fc3eab7e-d6d2-46e6-ab5f-ba6218e640e3 36399 0 2021-03-29 21:33:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc001e20a57 0xc001e20a58}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 49 50 52 46 49 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.194,PodIP:172.30.124.141,StartTime:2021-03-29 21:33:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:33:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a953edb7bf6d22269582515520c0147cfb406cedf411d26d4fdcaaf2c00ce1f8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.124.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.542: INFO: Pod "webserver-deployment-84855cf797-tm2sk" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-tm2sk webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-tm2sk 566b4291-9049-48ed-be2e-e2d3d7ed5fbd 36396 0 2021-03-29 21:33:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc001e21047 0xc001e21048}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 50 48 57 46 50 50 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.208,PodIP:172.30.209.226,StartTime:2021-03-29 21:33:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:33:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2287459fa2df19b1d7596609ae39c71965e53cfc1c3eda4330cddcdb1d56aae2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.209.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 21:33:32.542: INFO: Pod "webserver-deployment-84855cf797-z9l9c" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-z9l9c webserver-deployment-84855cf797- deployment-9466 /api/v1/namespaces/deployment-9466/pods/webserver-deployment-84855cf797-z9l9c 9b94808e-852d-4cf9-bc66-7addb4ef86d8 36649 0 2021-03-29 21:33:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 7b0cd767-19ff-4cb6-9d9f-c9f3bd9c9794 0xc001e21817 0xc001e21818}] []  [{kube-controller-manager Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 98 48 99 100 55 54 55 45 49 57 102 102 45 52 99 98 54 45 57 100 57 102 45 99 57 102 51 98 100 57 99 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:33:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-426gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-426gm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-426gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:33:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:,StartTime:2021-03-29 21:33:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:33:32.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9466" for this suite.

• [SLOW TEST:8.626 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":277,"completed":117,"skipped":1905,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:33:32.572: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1130
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:33:32.813: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 29 21:33:37.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-1130 create -f -'
Mar 29 21:33:37.610: INFO: stderr: ""
Mar 29 21:33:37.610: INFO: stdout: "e2e-test-crd-publish-openapi-2563-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 29 21:33:37.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-1130 delete e2e-test-crd-publish-openapi-2563-crds test-cr'
Mar 29 21:33:37.813: INFO: stderr: ""
Mar 29 21:33:37.813: INFO: stdout: "e2e-test-crd-publish-openapi-2563-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 29 21:33:37.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-1130 apply -f -'
Mar 29 21:33:38.178: INFO: stderr: ""
Mar 29 21:33:38.178: INFO: stdout: "e2e-test-crd-publish-openapi-2563-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 29 21:33:38.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-1130 delete e2e-test-crd-publish-openapi-2563-crds test-cr'
Mar 29 21:33:38.296: INFO: stderr: ""
Mar 29 21:33:38.296: INFO: stdout: "e2e-test-crd-publish-openapi-2563-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 29 21:33:38.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 explain e2e-test-crd-publish-openapi-2563-crds'
Mar 29 21:33:38.472: INFO: stderr: ""
Mar 29 21:33:38.472: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2563-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:33:43.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1130" for this suite.

• [SLOW TEST:10.687 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":277,"completed":118,"skipped":1920,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:33:43.262: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3546
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:33:43.489: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 29 21:33:47.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3546 create -f -'
Mar 29 21:33:48.251: INFO: stderr: ""
Mar 29 21:33:48.251: INFO: stdout: "e2e-test-crd-publish-openapi-7602-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 29 21:33:48.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3546 delete e2e-test-crd-publish-openapi-7602-crds test-cr'
Mar 29 21:33:48.447: INFO: stderr: ""
Mar 29 21:33:48.447: INFO: stdout: "e2e-test-crd-publish-openapi-7602-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 29 21:33:48.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3546 apply -f -'
Mar 29 21:33:48.857: INFO: stderr: ""
Mar 29 21:33:48.857: INFO: stdout: "e2e-test-crd-publish-openapi-7602-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 29 21:33:48.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 --namespace=crd-publish-openapi-3546 delete e2e-test-crd-publish-openapi-7602-crds test-cr'
Mar 29 21:33:48.976: INFO: stderr: ""
Mar 29 21:33:48.976: INFO: stdout: "e2e-test-crd-publish-openapi-7602-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 29 21:33:48.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 explain e2e-test-crd-publish-openapi-7602-crds'
Mar 29 21:33:49.289: INFO: stderr: ""
Mar 29 21:33:49.289: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7602-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:33:53.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3546" for this suite.

• [SLOW TEST:9.859 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":277,"completed":119,"skipped":1959,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:33:53.121: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 29 21:33:53.351: INFO: Waiting up to 5m0s for pod "downward-api-6037e28f-1525-4f2c-b5d2-d2df82e58676" in namespace "downward-api-9730" to be "Succeeded or Failed"
Mar 29 21:33:53.364: INFO: Pod "downward-api-6037e28f-1525-4f2c-b5d2-d2df82e58676": Phase="Pending", Reason="", readiness=false. Elapsed: 13.635574ms
Mar 29 21:33:55.375: INFO: Pod "downward-api-6037e28f-1525-4f2c-b5d2-d2df82e58676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024007834s
STEP: Saw pod success
Mar 29 21:33:55.375: INFO: Pod "downward-api-6037e28f-1525-4f2c-b5d2-d2df82e58676" satisfied condition "Succeeded or Failed"
Mar 29 21:33:55.385: INFO: Trying to get logs from node 10.189.118.196 pod downward-api-6037e28f-1525-4f2c-b5d2-d2df82e58676 container dapi-container: <nil>
STEP: delete the pod
Mar 29 21:33:55.471: INFO: Waiting for pod downward-api-6037e28f-1525-4f2c-b5d2-d2df82e58676 to disappear
Mar 29 21:33:55.481: INFO: Pod downward-api-6037e28f-1525-4f2c-b5d2-d2df82e58676 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:33:55.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9730" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":277,"completed":120,"skipped":1968,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:33:55.512: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-d5436036-6a44-4697-b9cb-4be432499916
STEP: Creating a pod to test consume configMaps
Mar 29 21:33:55.782: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b49da76-9e4a-48aa-aa60-c613c46251ce" in namespace "projected-9588" to be "Succeeded or Failed"
Mar 29 21:33:55.791: INFO: Pod "pod-projected-configmaps-7b49da76-9e4a-48aa-aa60-c613c46251ce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.590961ms
Mar 29 21:33:57.805: INFO: Pod "pod-projected-configmaps-7b49da76-9e4a-48aa-aa60-c613c46251ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.022921655s
Mar 29 21:33:59.815: INFO: Pod "pod-projected-configmaps-7b49da76-9e4a-48aa-aa60-c613c46251ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03349514s
STEP: Saw pod success
Mar 29 21:33:59.815: INFO: Pod "pod-projected-configmaps-7b49da76-9e4a-48aa-aa60-c613c46251ce" satisfied condition "Succeeded or Failed"
Mar 29 21:33:59.825: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-configmaps-7b49da76-9e4a-48aa-aa60-c613c46251ce container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 21:33:59.880: INFO: Waiting for pod pod-projected-configmaps-7b49da76-9e4a-48aa-aa60-c613c46251ce to disappear
Mar 29 21:33:59.890: INFO: Pod pod-projected-configmaps-7b49da76-9e4a-48aa-aa60-c613c46251ce no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:33:59.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9588" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":121,"skipped":1974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:33:59.924: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-328
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-0f48bdbc-46cc-4eaf-9a4d-6de3aa9d6161
STEP: Creating a pod to test consume secrets
Mar 29 21:34:00.167: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4ed749c1-041b-41f5-b94d-29aedc846450" in namespace "projected-328" to be "Succeeded or Failed"
Mar 29 21:34:00.182: INFO: Pod "pod-projected-secrets-4ed749c1-041b-41f5-b94d-29aedc846450": Phase="Pending", Reason="", readiness=false. Elapsed: 14.453951ms
Mar 29 21:34:02.192: INFO: Pod "pod-projected-secrets-4ed749c1-041b-41f5-b94d-29aedc846450": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024830254s
Mar 29 21:34:04.203: INFO: Pod "pod-projected-secrets-4ed749c1-041b-41f5-b94d-29aedc846450": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036191084s
STEP: Saw pod success
Mar 29 21:34:04.204: INFO: Pod "pod-projected-secrets-4ed749c1-041b-41f5-b94d-29aedc846450" satisfied condition "Succeeded or Failed"
Mar 29 21:34:04.214: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-secrets-4ed749c1-041b-41f5-b94d-29aedc846450 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 29 21:34:04.284: INFO: Waiting for pod pod-projected-secrets-4ed749c1-041b-41f5-b94d-29aedc846450 to disappear
Mar 29 21:34:04.295: INFO: Pod pod-projected-secrets-4ed749c1-041b-41f5-b94d-29aedc846450 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:34:04.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-328" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":122,"skipped":2034,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:34:04.334: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:34:20.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2827" for this suite.

• [SLOW TEST:16.374 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":277,"completed":123,"skipped":2041,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:34:20.708: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar 29 21:34:22.055: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
W0329 21:34:22.055688      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 29 21:34:22.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3359" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":277,"completed":124,"skipped":2048,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:34:22.087: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:34:22.296: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 29 21:34:24.393: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:34:24.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8892" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":277,"completed":125,"skipped":2048,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:34:24.445: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-7647d6ba-21ff-40b3-91de-891df04bc19e
STEP: Creating a pod to test consume configMaps
Mar 29 21:34:24.727: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-688a7b0b-88c6-4f4d-87b5-67204e653825" in namespace "projected-366" to be "Succeeded or Failed"
Mar 29 21:34:24.738: INFO: Pod "pod-projected-configmaps-688a7b0b-88c6-4f4d-87b5-67204e653825": Phase="Pending", Reason="", readiness=false. Elapsed: 11.282403ms
Mar 29 21:34:26.772: INFO: Pod "pod-projected-configmaps-688a7b0b-88c6-4f4d-87b5-67204e653825": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044840621s
STEP: Saw pod success
Mar 29 21:34:26.772: INFO: Pod "pod-projected-configmaps-688a7b0b-88c6-4f4d-87b5-67204e653825" satisfied condition "Succeeded or Failed"
Mar 29 21:34:26.801: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-configmaps-688a7b0b-88c6-4f4d-87b5-67204e653825 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 21:34:26.979: INFO: Waiting for pod pod-projected-configmaps-688a7b0b-88c6-4f4d-87b5-67204e653825 to disappear
Mar 29 21:34:27.016: INFO: Pod pod-projected-configmaps-688a7b0b-88c6-4f4d-87b5-67204e653825 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:34:27.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-366" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":126,"skipped":2053,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:34:27.141: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Mar 29 21:34:27.508: INFO: Waiting up to 5m0s for pod "client-containers-322d1920-6e26-4136-95c0-cefc3e6ebc95" in namespace "containers-425" to be "Succeeded or Failed"
Mar 29 21:34:27.554: INFO: Pod "client-containers-322d1920-6e26-4136-95c0-cefc3e6ebc95": Phase="Pending", Reason="", readiness=false. Elapsed: 45.738973ms
Mar 29 21:34:29.605: INFO: Pod "client-containers-322d1920-6e26-4136-95c0-cefc3e6ebc95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097090225s
Mar 29 21:34:31.647: INFO: Pod "client-containers-322d1920-6e26-4136-95c0-cefc3e6ebc95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.139341521s
STEP: Saw pod success
Mar 29 21:34:31.647: INFO: Pod "client-containers-322d1920-6e26-4136-95c0-cefc3e6ebc95" satisfied condition "Succeeded or Failed"
Mar 29 21:34:31.700: INFO: Trying to get logs from node 10.189.118.196 pod client-containers-322d1920-6e26-4136-95c0-cefc3e6ebc95 container test-container: <nil>
STEP: delete the pod
Mar 29 21:34:31.876: INFO: Waiting for pod client-containers-322d1920-6e26-4136-95c0-cefc3e6ebc95 to disappear
Mar 29 21:34:31.907: INFO: Pod client-containers-322d1920-6e26-4136-95c0-cefc3e6ebc95 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:34:31.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-425" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":277,"completed":127,"skipped":2063,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:34:32.017: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-70
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 29 21:34:36.423: INFO: &Pod{ObjectMeta:{send-events-fcca6d59-72a4-4218-9d47-865b22c4a29d  events-70 /api/v1/namespaces/events-70/pods/send-events-fcca6d59-72a4-4218-9d47-865b22c4a29d 2fdd84d9-ce7e-4bce-8546-1d19d42257f5 37710 0 2021-03-29 21:34:32 +0000 UTC <nil> <nil> map[name:foo time:341073405] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-03-29 21:34:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 116 105 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 112 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 114 116 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 99 111 110 116 97 105 110 101 114 80 111 114 116 92 34 58 56 48 44 92 34 112 114 111 116 111 99 111 108 92 34 58 92 34 84 67 80 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 99 111 110 116 97 105 110 101 114 80 111 114 116 34 58 123 125 44 34 102 58 112 114 111 116 111 99 111 108 34 58 123 125 125 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:34:34 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 53 56 46 50 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2wk59,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2wk59,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2wk59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:34:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:34:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:34:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:172.30.58.26,StartTime:2021-03-29 21:34:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:34:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:containerd://67132ea78afab6c38f1459f1378f6ec54828a3e667bbcff3172d1ee7adf574e8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 29 21:34:38.434: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 29 21:34:40.446: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:34:40.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-70" for this suite.

• [SLOW TEST:8.481 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":277,"completed":128,"skipped":2066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:34:40.503: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-054d7a7c-17b9-4a2a-9572-fa9ca2549ee0
STEP: Creating a pod to test consume secrets
Mar 29 21:34:40.756: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-76fe2573-0053-4a4d-b188-0e37c3db870b" in namespace "projected-4199" to be "Succeeded or Failed"
Mar 29 21:34:40.766: INFO: Pod "pod-projected-secrets-76fe2573-0053-4a4d-b188-0e37c3db870b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.270962ms
Mar 29 21:34:42.809: INFO: Pod "pod-projected-secrets-76fe2573-0053-4a4d-b188-0e37c3db870b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052998317s
STEP: Saw pod success
Mar 29 21:34:42.810: INFO: Pod "pod-projected-secrets-76fe2573-0053-4a4d-b188-0e37c3db870b" satisfied condition "Succeeded or Failed"
Mar 29 21:34:42.829: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-secrets-76fe2573-0053-4a4d-b188-0e37c3db870b container secret-volume-test: <nil>
STEP: delete the pod
Mar 29 21:34:42.905: INFO: Waiting for pod pod-projected-secrets-76fe2573-0053-4a4d-b188-0e37c3db870b to disappear
Mar 29 21:34:42.915: INFO: Pod pod-projected-secrets-76fe2573-0053-4a4d-b188-0e37c3db870b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:34:42.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4199" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":129,"skipped":2110,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:34:42.943: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:34:43.789: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 21:34:45.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650483, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650483, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650483, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650483, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:34:48.869: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:34:49.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5491" for this suite.
STEP: Destroying namespace "webhook-5491-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.462 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":277,"completed":130,"skipped":2113,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:34:49.406: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 29 21:34:49.613: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:34:54.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4736" for this suite.

• [SLOW TEST:5.201 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":277,"completed":131,"skipped":2124,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:34:54.607: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9699
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:34:55.454: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 21:34:57.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650495, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650495, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650495, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752650495, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:35:00.529: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:35:11.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9699" for this suite.
STEP: Destroying namespace "webhook-9699-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.748 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":277,"completed":132,"skipped":2125,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:35:11.356: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:35:15.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3704" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":277,"completed":133,"skipped":2132,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:35:15.815: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8215
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:35:19.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8215" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":277,"completed":134,"skipped":2146,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:35:19.169: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-3974/secret-test-2ed1c4bc-9b86-48d5-9512-e133601c66e5
STEP: Creating a pod to test consume secrets
Mar 29 21:35:19.435: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee58f116-ecdc-4f05-8819-bca7e95bb470" in namespace "secrets-3974" to be "Succeeded or Failed"
Mar 29 21:35:19.453: INFO: Pod "pod-configmaps-ee58f116-ecdc-4f05-8819-bca7e95bb470": Phase="Pending", Reason="", readiness=false. Elapsed: 17.672766ms
Mar 29 21:35:21.465: INFO: Pod "pod-configmaps-ee58f116-ecdc-4f05-8819-bca7e95bb470": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030639934s
Mar 29 21:35:23.574: INFO: Pod "pod-configmaps-ee58f116-ecdc-4f05-8819-bca7e95bb470": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.138932748s
STEP: Saw pod success
Mar 29 21:35:23.574: INFO: Pod "pod-configmaps-ee58f116-ecdc-4f05-8819-bca7e95bb470" satisfied condition "Succeeded or Failed"
Mar 29 21:35:23.587: INFO: Trying to get logs from node 10.189.118.196 pod pod-configmaps-ee58f116-ecdc-4f05-8819-bca7e95bb470 container env-test: <nil>
STEP: delete the pod
Mar 29 21:35:23.652: INFO: Waiting for pod pod-configmaps-ee58f116-ecdc-4f05-8819-bca7e95bb470 to disappear
Mar 29 21:35:23.661: INFO: Pod pod-configmaps-ee58f116-ecdc-4f05-8819-bca7e95bb470 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:35:23.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3974" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":135,"skipped":2165,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:35:23.693: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5297
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Mar 29 21:35:23.917: INFO: Waiting up to 5m0s for pod "client-containers-b62829d8-071a-41d8-afb7-d9e03ccd943d" in namespace "containers-5297" to be "Succeeded or Failed"
Mar 29 21:35:23.928: INFO: Pod "client-containers-b62829d8-071a-41d8-afb7-d9e03ccd943d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.183783ms
Mar 29 21:35:25.939: INFO: Pod "client-containers-b62829d8-071a-41d8-afb7-d9e03ccd943d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021450328s
STEP: Saw pod success
Mar 29 21:35:25.939: INFO: Pod "client-containers-b62829d8-071a-41d8-afb7-d9e03ccd943d" satisfied condition "Succeeded or Failed"
Mar 29 21:35:25.949: INFO: Trying to get logs from node 10.189.118.196 pod client-containers-b62829d8-071a-41d8-afb7-d9e03ccd943d container test-container: <nil>
STEP: delete the pod
Mar 29 21:35:26.008: INFO: Waiting for pod client-containers-b62829d8-071a-41d8-afb7-d9e03ccd943d to disappear
Mar 29 21:35:26.022: INFO: Pod client-containers-b62829d8-071a-41d8-afb7-d9e03ccd943d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:35:26.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5297" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":277,"completed":136,"skipped":2179,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:35:26.067: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-143
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-9a902441-735d-43d5-9d34-f03683c92f06
STEP: Creating configMap with name cm-test-opt-upd-950c77a4-0bbf-499a-b131-8b69ecd84275
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9a902441-735d-43d5-9d34-f03683c92f06
STEP: Updating configmap cm-test-opt-upd-950c77a4-0bbf-499a-b131-8b69ecd84275
STEP: Creating configMap with name cm-test-opt-create-b9e85249-9056-4573-89b2-b2154f59f496
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:35:32.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-143" for this suite.

• [SLOW TEST:6.764 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":137,"skipped":2192,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:35:32.833: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1669
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Mar 29 21:35:33.041: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:35:55.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1669" for this suite.

• [SLOW TEST:22.533 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":277,"completed":138,"skipped":2207,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:35:55.368: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-c6603e16-9b66-48e0-97a2-d83c9266b786
STEP: Creating a pod to test consume configMaps
Mar 29 21:35:55.606: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-25d3fa65-b2ac-4d32-90ed-216e5bc57352" in namespace "projected-2707" to be "Succeeded or Failed"
Mar 29 21:35:55.616: INFO: Pod "pod-projected-configmaps-25d3fa65-b2ac-4d32-90ed-216e5bc57352": Phase="Pending", Reason="", readiness=false. Elapsed: 10.356517ms
Mar 29 21:35:57.627: INFO: Pod "pod-projected-configmaps-25d3fa65-b2ac-4d32-90ed-216e5bc57352": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021403343s
Mar 29 21:35:59.639: INFO: Pod "pod-projected-configmaps-25d3fa65-b2ac-4d32-90ed-216e5bc57352": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032836821s
STEP: Saw pod success
Mar 29 21:35:59.639: INFO: Pod "pod-projected-configmaps-25d3fa65-b2ac-4d32-90ed-216e5bc57352" satisfied condition "Succeeded or Failed"
Mar 29 21:35:59.649: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-configmaps-25d3fa65-b2ac-4d32-90ed-216e5bc57352 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 21:35:59.710: INFO: Waiting for pod pod-projected-configmaps-25d3fa65-b2ac-4d32-90ed-216e5bc57352 to disappear
Mar 29 21:35:59.720: INFO: Pod pod-projected-configmaps-25d3fa65-b2ac-4d32-90ed-216e5bc57352 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:35:59.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2707" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":139,"skipped":2220,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:35:59.751: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-405
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-8f64d7c3-0a38-4562-9731-eb6cd4086cd9
STEP: Creating secret with name s-test-opt-upd-40ac9d66-4817-4255-8101-639d6599b58c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8f64d7c3-0a38-4562-9731-eb6cd4086cd9
STEP: Updating secret s-test-opt-upd-40ac9d66-4817-4255-8101-639d6599b58c
STEP: Creating secret with name s-test-opt-create-527ef629-a9f5-4001-9db2-48364d770ea3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:37:25.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-405" for this suite.

• [SLOW TEST:85.748 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":140,"skipped":2225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:37:25.503: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7063
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 29 21:37:25.709: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:37:29.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7063" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":277,"completed":141,"skipped":2270,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:37:29.315: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 29 21:37:29.571: INFO: Waiting up to 5m0s for pod "downward-api-0faf5c01-fb53-4eda-be6d-f654829c893d" in namespace "downward-api-4731" to be "Succeeded or Failed"
Mar 29 21:37:29.589: INFO: Pod "downward-api-0faf5c01-fb53-4eda-be6d-f654829c893d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.728806ms
Mar 29 21:37:31.601: INFO: Pod "downward-api-0faf5c01-fb53-4eda-be6d-f654829c893d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030175947s
STEP: Saw pod success
Mar 29 21:37:31.601: INFO: Pod "downward-api-0faf5c01-fb53-4eda-be6d-f654829c893d" satisfied condition "Succeeded or Failed"
Mar 29 21:37:31.611: INFO: Trying to get logs from node 10.189.118.196 pod downward-api-0faf5c01-fb53-4eda-be6d-f654829c893d container dapi-container: <nil>
STEP: delete the pod
Mar 29 21:37:31.673: INFO: Waiting for pod downward-api-0faf5c01-fb53-4eda-be6d-f654829c893d to disappear
Mar 29 21:37:31.682: INFO: Pod downward-api-0faf5c01-fb53-4eda-be6d-f654829c893d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:37:31.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4731" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":277,"completed":142,"skipped":2278,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:37:31.717: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5884
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:37:43.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5884" for this suite.

• [SLOW TEST:11.340 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":277,"completed":143,"skipped":2278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:37:43.059: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2904
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:37:43.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75217319-871e-4cf7-9d96-d8d48c9c94bc" in namespace "downward-api-2904" to be "Succeeded or Failed"
Mar 29 21:37:43.304: INFO: Pod "downwardapi-volume-75217319-871e-4cf7-9d96-d8d48c9c94bc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.661494ms
Mar 29 21:37:45.314: INFO: Pod "downwardapi-volume-75217319-871e-4cf7-9d96-d8d48c9c94bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022456285s
Mar 29 21:37:47.332: INFO: Pod "downwardapi-volume-75217319-871e-4cf7-9d96-d8d48c9c94bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040364906s
STEP: Saw pod success
Mar 29 21:37:47.332: INFO: Pod "downwardapi-volume-75217319-871e-4cf7-9d96-d8d48c9c94bc" satisfied condition "Succeeded or Failed"
Mar 29 21:37:47.354: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-75217319-871e-4cf7-9d96-d8d48c9c94bc container client-container: <nil>
STEP: delete the pod
Mar 29 21:37:47.449: INFO: Waiting for pod downwardapi-volume-75217319-871e-4cf7-9d96-d8d48c9c94bc to disappear
Mar 29 21:37:47.466: INFO: Pod downwardapi-volume-75217319-871e-4cf7-9d96-d8d48c9c94bc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:37:47.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2904" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":144,"skipped":2353,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:37:47.510: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5976
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-f6ed1b5e-b0a3-477f-a430-23ed51b12562
STEP: Creating a pod to test consume secrets
Mar 29 21:37:47.764: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2c011bc6-c287-4568-8879-a1d977353853" in namespace "projected-5976" to be "Succeeded or Failed"
Mar 29 21:37:47.777: INFO: Pod "pod-projected-secrets-2c011bc6-c287-4568-8879-a1d977353853": Phase="Pending", Reason="", readiness=false. Elapsed: 12.260133ms
Mar 29 21:37:49.787: INFO: Pod "pod-projected-secrets-2c011bc6-c287-4568-8879-a1d977353853": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022620857s
Mar 29 21:37:51.806: INFO: Pod "pod-projected-secrets-2c011bc6-c287-4568-8879-a1d977353853": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041905354s
STEP: Saw pod success
Mar 29 21:37:51.806: INFO: Pod "pod-projected-secrets-2c011bc6-c287-4568-8879-a1d977353853" satisfied condition "Succeeded or Failed"
Mar 29 21:37:51.818: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-secrets-2c011bc6-c287-4568-8879-a1d977353853 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 29 21:37:51.878: INFO: Waiting for pod pod-projected-secrets-2c011bc6-c287-4568-8879-a1d977353853 to disappear
Mar 29 21:37:51.889: INFO: Pod pod-projected-secrets-2c011bc6-c287-4568-8879-a1d977353853 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:37:51.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5976" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":145,"skipped":2365,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:37:51.916: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Mar 29 21:37:52.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-6594'
Mar 29 21:37:52.466: INFO: stderr: ""
Mar 29 21:37:52.466: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 29 21:37:53.478: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 21:37:53.478: INFO: Found 0 / 1
Mar 29 21:37:54.478: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 21:37:54.478: INFO: Found 1 / 1
Mar 29 21:37:54.478: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 29 21:37:54.488: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 21:37:54.488: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 29 21:37:54.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 patch pod agnhost-master-9nzs8 --namespace=kubectl-6594 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 29 21:37:54.605: INFO: stderr: ""
Mar 29 21:37:54.605: INFO: stdout: "pod/agnhost-master-9nzs8 patched\n"
STEP: checking annotations
Mar 29 21:37:54.617: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 21:37:54.617: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:37:54.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6594" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":277,"completed":146,"skipped":2365,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:37:54.649: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5808
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:37:54.888: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99432dd2-2abb-4284-8ba0-4aada026b094" in namespace "downward-api-5808" to be "Succeeded or Failed"
Mar 29 21:37:54.903: INFO: Pod "downwardapi-volume-99432dd2-2abb-4284-8ba0-4aada026b094": Phase="Pending", Reason="", readiness=false. Elapsed: 14.836381ms
Mar 29 21:37:56.919: INFO: Pod "downwardapi-volume-99432dd2-2abb-4284-8ba0-4aada026b094": Phase="Running", Reason="", readiness=true. Elapsed: 2.030775727s
Mar 29 21:37:58.932: INFO: Pod "downwardapi-volume-99432dd2-2abb-4284-8ba0-4aada026b094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043380174s
STEP: Saw pod success
Mar 29 21:37:58.932: INFO: Pod "downwardapi-volume-99432dd2-2abb-4284-8ba0-4aada026b094" satisfied condition "Succeeded or Failed"
Mar 29 21:37:58.944: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-99432dd2-2abb-4284-8ba0-4aada026b094 container client-container: <nil>
STEP: delete the pod
Mar 29 21:37:59.023: INFO: Waiting for pod downwardapi-volume-99432dd2-2abb-4284-8ba0-4aada026b094 to disappear
Mar 29 21:37:59.038: INFO: Pod downwardapi-volume-99432dd2-2abb-4284-8ba0-4aada026b094 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:37:59.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5808" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":147,"skipped":2373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:37:59.100: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-5306
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5306
STEP: Creating statefulset with conflicting port in namespace statefulset-5306
STEP: Waiting until pod test-pod will start running in namespace statefulset-5306
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5306
Mar 29 21:38:03.408: INFO: Observed stateful pod in namespace: statefulset-5306, name: ss-0, uid: ff21f8bb-e784-472e-bccb-ffe2ae953673, status phase: Pending. Waiting for statefulset controller to delete.
Mar 29 21:38:03.446: INFO: Observed stateful pod in namespace: statefulset-5306, name: ss-0, uid: ff21f8bb-e784-472e-bccb-ffe2ae953673, status phase: Failed. Waiting for statefulset controller to delete.
Mar 29 21:38:03.467: INFO: Observed stateful pod in namespace: statefulset-5306, name: ss-0, uid: ff21f8bb-e784-472e-bccb-ffe2ae953673, status phase: Failed. Waiting for statefulset controller to delete.
Mar 29 21:38:03.479: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5306
STEP: Removing pod with conflicting port in namespace statefulset-5306
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5306 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 29 21:38:07.547: INFO: Deleting all statefulset in ns statefulset-5306
Mar 29 21:38:07.561: INFO: Scaling statefulset ss to 0
Mar 29 21:38:17.610: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 21:38:17.625: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:38:17.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5306" for this suite.

• [SLOW TEST:18.614 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":277,"completed":148,"skipped":2422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:38:17.714: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-95bb27e0-b9a9-4060-9d6f-6dc45395f884 in namespace container-probe-5926
Mar 29 21:38:21.982: INFO: Started pod test-webserver-95bb27e0-b9a9-4060-9d6f-6dc45395f884 in namespace container-probe-5926
STEP: checking the pod's current state and verifying that restartCount is present
Mar 29 21:38:21.992: INFO: Initial restart count of pod test-webserver-95bb27e0-b9a9-4060-9d6f-6dc45395f884 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:42:22.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5926" for this suite.

• [SLOW TEST:244.490 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":149,"skipped":2447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:42:22.205: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6916
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:42:24.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6916" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":277,"completed":150,"skipped":2504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:42:24.581: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:42:26.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1576" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":277,"completed":151,"skipped":2543,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:42:26.926: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:42:43.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1139" for this suite.

• [SLOW TEST:16.573 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":277,"completed":152,"skipped":2558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:42:43.499: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-lvth
STEP: Creating a pod to test atomic-volume-subpath
Mar 29 21:42:43.774: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lvth" in namespace "subpath-770" to be "Succeeded or Failed"
Mar 29 21:42:43.792: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Pending", Reason="", readiness=false. Elapsed: 17.978311ms
Mar 29 21:42:45.805: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Running", Reason="", readiness=true. Elapsed: 2.030887234s
Mar 29 21:42:47.818: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Running", Reason="", readiness=true. Elapsed: 4.043957373s
Mar 29 21:42:49.828: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Running", Reason="", readiness=true. Elapsed: 6.053448754s
Mar 29 21:42:51.843: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Running", Reason="", readiness=true. Elapsed: 8.068477381s
Mar 29 21:42:53.854: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Running", Reason="", readiness=true. Elapsed: 10.080030301s
Mar 29 21:42:55.865: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Running", Reason="", readiness=true. Elapsed: 12.090975626s
Mar 29 21:42:57.879: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Running", Reason="", readiness=true. Elapsed: 14.104790458s
Mar 29 21:42:59.891: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Running", Reason="", readiness=true. Elapsed: 16.116675872s
Mar 29 21:43:01.904: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Running", Reason="", readiness=true. Elapsed: 18.130156943s
Mar 29 21:43:03.915: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Running", Reason="", readiness=true. Elapsed: 20.140861632s
Mar 29 21:43:05.935: INFO: Pod "pod-subpath-test-configmap-lvth": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.161082647s
STEP: Saw pod success
Mar 29 21:43:05.935: INFO: Pod "pod-subpath-test-configmap-lvth" satisfied condition "Succeeded or Failed"
Mar 29 21:43:05.952: INFO: Trying to get logs from node 10.189.118.196 pod pod-subpath-test-configmap-lvth container test-container-subpath-configmap-lvth: <nil>
STEP: delete the pod
Mar 29 21:43:06.021: INFO: Waiting for pod pod-subpath-test-configmap-lvth to disappear
Mar 29 21:43:06.033: INFO: Pod pod-subpath-test-configmap-lvth no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lvth
Mar 29 21:43:06.033: INFO: Deleting pod "pod-subpath-test-configmap-lvth" in namespace "subpath-770"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:43:06.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-770" for this suite.

• [SLOW TEST:22.586 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":277,"completed":153,"skipped":2582,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:43:06.085: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-97a93452-74f7-44b4-affd-ad3a30ef35ce in namespace container-probe-3875
Mar 29 21:43:10.375: INFO: Started pod liveness-97a93452-74f7-44b4-affd-ad3a30ef35ce in namespace container-probe-3875
STEP: checking the pod's current state and verifying that restartCount is present
Mar 29 21:43:10.396: INFO: Initial restart count of pod liveness-97a93452-74f7-44b4-affd-ad3a30ef35ce is 0
Mar 29 21:43:26.510: INFO: Restart count of pod container-probe-3875/liveness-97a93452-74f7-44b4-affd-ad3a30ef35ce is now 1 (16.113950859s elapsed)
Mar 29 21:43:46.919: INFO: Restart count of pod container-probe-3875/liveness-97a93452-74f7-44b4-affd-ad3a30ef35ce is now 2 (36.523654059s elapsed)
Mar 29 21:44:07.045: INFO: Restart count of pod container-probe-3875/liveness-97a93452-74f7-44b4-affd-ad3a30ef35ce is now 3 (56.64893271s elapsed)
Mar 29 21:44:27.190: INFO: Restart count of pod container-probe-3875/liveness-97a93452-74f7-44b4-affd-ad3a30ef35ce is now 4 (1m16.79422598s elapsed)
Mar 29 21:45:39.702: INFO: Restart count of pod container-probe-3875/liveness-97a93452-74f7-44b4-affd-ad3a30ef35ce is now 5 (2m29.306032437s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:45:39.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3875" for this suite.

• [SLOW TEST:153.687 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":277,"completed":154,"skipped":2592,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:45:39.774: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2695
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:45:40.064: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3d0638ae-b6b0-49fa-a331-fffc1bcef1f9", Controller:(*bool)(0xc0031b486a), BlockOwnerDeletion:(*bool)(0xc0031b486b)}}
Mar 29 21:45:40.083: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"aeec9c80-1b06-40a8-9096-2a0b49221136", Controller:(*bool)(0xc0034e4ff6), BlockOwnerDeletion:(*bool)(0xc0034e4ff7)}}
Mar 29 21:45:40.105: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2b43c766-5b39-417c-8db3-02cabbaf9e13", Controller:(*bool)(0xc0032568c6), BlockOwnerDeletion:(*bool)(0xc0032568c7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:45:45.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2695" for this suite.

• [SLOW TEST:5.403 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":277,"completed":155,"skipped":2610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:45:45.181: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 29 21:45:45.764: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 29 21:45:45.818: INFO: Waiting for terminating namespaces to be deleted...
Mar 29 21:45:45.832: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.194 before test
Mar 29 21:45:45.934: INFO: public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-vbjdf from kube-system started at 2021-03-29 20:11:55 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:45.935: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 29 21:45:45.935: INFO: addon-catalog-source-lgq2p from ibm-system started at 2021-03-29 19:53:09 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:45.935: INFO: 	Container configmap-registry-server ready: true, restart count 0
Mar 29 21:45:45.935: INFO: sonobuoy-e2e-job-95ac96a8209247c9 from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:45.935: INFO: 	Container e2e ready: true, restart count 0
Mar 29 21:45:45.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:45:45.935: INFO: ibm-keepalived-watcher-5j4dz from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:45.935: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 21:45:45.935: INFO: calico-node-9tz6n from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:45.935: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 21:45:45.935: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-ngv5d from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:45.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:45:45.935: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 21:45:45.936: INFO: ibm-master-proxy-static-10.189.118.194 from kube-system started at 2021-03-29 19:48:41 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:45.936: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 21:45:45.936: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:45.936: INFO: coredns-7cc79848cf-8xnhq from kube-system started at 2021-03-29 19:57:06 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:45.936: INFO: 	Container coredns ready: true, restart count 0
Mar 29 21:45:45.936: INFO: ibm-cloud-provider-ip-169-63-130-250-65c6959456-tm4wj from ibm-system started at 2021-03-29 20:12:35 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:45.936: INFO: 	Container ibm-cloud-provider-ip-169-63-130-250 ready: true, restart count 0
Mar 29 21:45:45.936: INFO: ibm-kubelet-monitor-jrnhz from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:45.936: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:45.936: INFO: sonobuoy from sonobuoy started at 2021-03-29 21:07:51 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:45.936: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 29 21:45:45.936: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.196 before test
Mar 29 21:45:46.005: INFO: ibm-master-proxy-static-10.189.118.196 from kube-system started at 2021-03-29 19:48:58 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:46.005: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 21:45:46.005: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:46.005: INFO: ibm-kubelet-monitor-cmcxj from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.005: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:46.005: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-l4nd5 from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:46.005: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:45:46.005: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 21:45:46.005: INFO: ibm-keepalived-watcher-4x7cv from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.006: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 21:45:46.006: INFO: calico-node-mtdnd from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.006: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 21:45:46.006: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.208 before test
Mar 29 21:45:46.120: INFO: kubernetes-dashboard-6cf6cfdf4-kzffs from kube-system started at 2021-03-29 19:49:50 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 29 21:45:46.120: INFO: ibm-cloud-provider-ip-169-63-130-250-65c6959456-gps6w from ibm-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container ibm-cloud-provider-ip-169-63-130-250 ready: true, restart count 0
Mar 29 21:45:46.120: INFO: calico-node-zjnmd from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 21:45:46.120: INFO: public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-2lcqz from kube-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 29 21:45:46.120: INFO: vpn-657cb5cdb6-v5h8m from kube-system started at 2021-03-29 19:56:31 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container vpn ready: true, restart count 0
Mar 29 21:45:46.120: INFO: ibm-storage-watcher-659bdcc695-qqkhw from kube-system started at 2021-03-29 19:49:50 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 29 21:45:46.120: INFO: ibm-kubelet-monitor-gbcvf from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:46.120: INFO: olm-operator-8496678794-gg5v7 from ibm-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container olm-operator ready: true, restart count 0
Mar 29 21:45:46.120: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-n7lmz from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:45:46.120: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 21:45:46.120: INFO: ibm-keepalived-watcher-ps2vt from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 21:45:46.120: INFO: calico-kube-controllers-6599f97f59-7g48l from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 29 21:45:46.120: INFO: catalog-operator-7bc4c797b5-kbbnz from ibm-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 29 21:45:46.120: INFO: dashboard-metrics-scraper-b585c6867-2k8ww from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 29 21:45:46.120: INFO: metrics-server-f9f7549df-6xdmf from kube-system started at 2021-03-29 19:50:53 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container metrics-server ready: true, restart count 0
Mar 29 21:45:46.120: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 29 21:45:46.120: INFO: coredns-autoscaler-bff977695-9wkhn from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container autoscaler ready: true, restart count 0
Mar 29 21:45:46.120: INFO: coredns-7cc79848cf-kjg7p from kube-system started at 2021-03-29 19:57:07 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container coredns ready: true, restart count 0
Mar 29 21:45:46.120: INFO: ibm-master-proxy-static-10.189.118.208 from kube-system started at 2021-03-29 19:48:43 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 21:45:46.120: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:46.120: INFO: coredns-7cc79848cf-m5klb from kube-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container coredns ready: true, restart count 0
Mar 29 21:45:46.120: INFO: ibm-file-plugin-c76c68fd9-bpt7r from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:46.120: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node 10.189.118.194
STEP: verifying the node has the label node 10.189.118.196
STEP: verifying the node has the label node 10.189.118.208
Mar 29 21:45:46.281: INFO: Pod addon-catalog-source-lgq2p requesting resource cpu=10m on Node 10.189.118.194
Mar 29 21:45:46.282: INFO: Pod catalog-operator-7bc4c797b5-kbbnz requesting resource cpu=10m on Node 10.189.118.208
Mar 29 21:45:46.282: INFO: Pod ibm-cloud-provider-ip-169-63-130-250-65c6959456-gps6w requesting resource cpu=5m on Node 10.189.118.208
Mar 29 21:45:46.282: INFO: Pod ibm-cloud-provider-ip-169-63-130-250-65c6959456-tm4wj requesting resource cpu=5m on Node 10.189.118.194
Mar 29 21:45:46.282: INFO: Pod olm-operator-8496678794-gg5v7 requesting resource cpu=10m on Node 10.189.118.208
Mar 29 21:45:46.282: INFO: Pod calico-kube-controllers-6599f97f59-7g48l requesting resource cpu=10m on Node 10.189.118.208
Mar 29 21:45:46.282: INFO: Pod calico-node-9tz6n requesting resource cpu=250m on Node 10.189.118.194
Mar 29 21:45:46.282: INFO: Pod calico-node-mtdnd requesting resource cpu=250m on Node 10.189.118.196
Mar 29 21:45:46.282: INFO: Pod calico-node-zjnmd requesting resource cpu=250m on Node 10.189.118.208
Mar 29 21:45:46.282: INFO: Pod coredns-7cc79848cf-8xnhq requesting resource cpu=100m on Node 10.189.118.194
Mar 29 21:45:46.282: INFO: Pod coredns-7cc79848cf-kjg7p requesting resource cpu=100m on Node 10.189.118.208
Mar 29 21:45:46.282: INFO: Pod coredns-7cc79848cf-m5klb requesting resource cpu=100m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod coredns-autoscaler-bff977695-9wkhn requesting resource cpu=20m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod dashboard-metrics-scraper-b585c6867-2k8ww requesting resource cpu=1m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod ibm-file-plugin-c76c68fd9-bpt7r requesting resource cpu=50m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod ibm-keepalived-watcher-4x7cv requesting resource cpu=5m on Node 10.189.118.196
Mar 29 21:45:46.283: INFO: Pod ibm-keepalived-watcher-5j4dz requesting resource cpu=5m on Node 10.189.118.194
Mar 29 21:45:46.283: INFO: Pod ibm-keepalived-watcher-ps2vt requesting resource cpu=5m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod ibm-kubelet-monitor-cmcxj requesting resource cpu=10m on Node 10.189.118.196
Mar 29 21:45:46.283: INFO: Pod ibm-kubelet-monitor-gbcvf requesting resource cpu=10m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod ibm-kubelet-monitor-jrnhz requesting resource cpu=10m on Node 10.189.118.194
Mar 29 21:45:46.283: INFO: Pod ibm-master-proxy-static-10.189.118.194 requesting resource cpu=25m on Node 10.189.118.194
Mar 29 21:45:46.283: INFO: Pod ibm-master-proxy-static-10.189.118.196 requesting resource cpu=25m on Node 10.189.118.196
Mar 29 21:45:46.283: INFO: Pod ibm-master-proxy-static-10.189.118.208 requesting resource cpu=25m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod ibm-storage-watcher-659bdcc695-qqkhw requesting resource cpu=50m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod kubernetes-dashboard-6cf6cfdf4-kzffs requesting resource cpu=50m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod metrics-server-f9f7549df-6xdmf requesting resource cpu=121m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-2lcqz requesting resource cpu=10m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-vbjdf requesting resource cpu=10m on Node 10.189.118.194
Mar 29 21:45:46.283: INFO: Pod vpn-657cb5cdb6-v5h8m requesting resource cpu=5m on Node 10.189.118.208
Mar 29 21:45:46.283: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.189.118.194
Mar 29 21:45:46.283: INFO: Pod sonobuoy-e2e-job-95ac96a8209247c9 requesting resource cpu=0m on Node 10.189.118.194
Mar 29 21:45:46.284: INFO: Pod sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-l4nd5 requesting resource cpu=0m on Node 10.189.118.196
Mar 29 21:45:46.284: INFO: Pod sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-n7lmz requesting resource cpu=0m on Node 10.189.118.208
Mar 29 21:45:46.284: INFO: Pod sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-ngv5d requesting resource cpu=0m on Node 10.189.118.194
STEP: Starting Pods to consume most of the cluster CPU.
Mar 29 21:45:46.284: INFO: Creating a pod which consumes cpu=2446m on Node 10.189.118.194
Mar 29 21:45:46.313: INFO: Creating a pod which consumes cpu=2534m on Node 10.189.118.196
Mar 29 21:45:46.331: INFO: Creating a pod which consumes cpu=2154m on Node 10.189.118.208
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-111d5bb8-b58b-4c4c-aa53-707fd0ccbe83.1670ee5c49393f73], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7540/filler-pod-111d5bb8-b58b-4c4c-aa53-707fd0ccbe83 to 10.189.118.208]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-111d5bb8-b58b-4c4c-aa53-707fd0ccbe83.1670ee5c870816ce], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-111d5bb8-b58b-4c4c-aa53-707fd0ccbe83.1670ee5c97ae686a], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-111d5bb8-b58b-4c4c-aa53-707fd0ccbe83.1670ee5c9b83ee12], Reason = [Created], Message = [Created container filler-pod-111d5bb8-b58b-4c4c-aa53-707fd0ccbe83]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-111d5bb8-b58b-4c4c-aa53-707fd0ccbe83.1670ee5ca676f968], Reason = [Started], Message = [Started container filler-pod-111d5bb8-b58b-4c4c-aa53-707fd0ccbe83]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24a29c32-cdcb-4e9c-8577-3308ac272e14.1670ee5c4722944c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7540/filler-pod-24a29c32-cdcb-4e9c-8577-3308ac272e14 to 10.189.118.194]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24a29c32-cdcb-4e9c-8577-3308ac272e14.1670ee5c825f0564], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24a29c32-cdcb-4e9c-8577-3308ac272e14.1670ee5c9557b98c], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24a29c32-cdcb-4e9c-8577-3308ac272e14.1670ee5c986f0f85], Reason = [Created], Message = [Created container filler-pod-24a29c32-cdcb-4e9c-8577-3308ac272e14]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24a29c32-cdcb-4e9c-8577-3308ac272e14.1670ee5ca26ea6ba], Reason = [Started], Message = [Started container filler-pod-24a29c32-cdcb-4e9c-8577-3308ac272e14]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f3a020a-1ade-47b0-b8f0-0b9deb0ced11.1670ee5c48356d8e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7540/filler-pod-2f3a020a-1ade-47b0-b8f0-0b9deb0ced11 to 10.189.118.196]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f3a020a-1ade-47b0-b8f0-0b9deb0ced11.1670ee5c91841b28], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f3a020a-1ade-47b0-b8f0-0b9deb0ced11.1670ee5c95e54f6d], Reason = [Created], Message = [Created container filler-pod-2f3a020a-1ade-47b0-b8f0-0b9deb0ced11]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f3a020a-1ade-47b0-b8f0-0b9deb0ced11.1670ee5ca3578914], Reason = [Started], Message = [Started container filler-pod-2f3a020a-1ade-47b0-b8f0-0b9deb0ced11]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1670ee5d3e3139fb], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.189.118.194
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.189.118.196
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.189.118.208
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:45:51.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7540" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:6.472 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":277,"completed":156,"skipped":2647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:45:51.654: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9077
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 29 21:45:51.876: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 29 21:45:51.932: INFO: Waiting for terminating namespaces to be deleted...
Mar 29 21:45:51.942: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.194 before test
Mar 29 21:45:52.022: INFO: ibm-master-proxy-static-10.189.118.194 from kube-system started at 2021-03-29 19:48:41 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:52.022: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 21:45:52.022: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:52.022: INFO: coredns-7cc79848cf-8xnhq from kube-system started at 2021-03-29 19:57:06 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.022: INFO: 	Container coredns ready: true, restart count 0
Mar 29 21:45:52.022: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-ngv5d from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:52.022: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:45:52.022: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 21:45:52.022: INFO: ibm-kubelet-monitor-jrnhz from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.022: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:52.022: INFO: ibm-cloud-provider-ip-169-63-130-250-65c6959456-tm4wj from ibm-system started at 2021-03-29 20:12:35 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.022: INFO: 	Container ibm-cloud-provider-ip-169-63-130-250 ready: true, restart count 0
Mar 29 21:45:52.022: INFO: sonobuoy from sonobuoy started at 2021-03-29 21:07:51 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.022: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 29 21:45:52.022: INFO: filler-pod-24a29c32-cdcb-4e9c-8577-3308ac272e14 from sched-pred-7540 started at 2021-03-29 21:45:46 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.022: INFO: 	Container filler-pod-24a29c32-cdcb-4e9c-8577-3308ac272e14 ready: true, restart count 0
Mar 29 21:45:52.023: INFO: ibm-keepalived-watcher-5j4dz from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.023: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 21:45:52.023: INFO: calico-node-9tz6n from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.023: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 21:45:52.023: INFO: public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-vbjdf from kube-system started at 2021-03-29 20:11:55 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.023: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 29 21:45:52.023: INFO: addon-catalog-source-lgq2p from ibm-system started at 2021-03-29 19:53:09 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.023: INFO: 	Container configmap-registry-server ready: true, restart count 0
Mar 29 21:45:52.023: INFO: sonobuoy-e2e-job-95ac96a8209247c9 from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:52.023: INFO: 	Container e2e ready: true, restart count 0
Mar 29 21:45:52.023: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:45:52.023: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.196 before test
Mar 29 21:45:52.068: INFO: calico-node-mtdnd from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.068: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 21:45:52.068: INFO: filler-pod-2f3a020a-1ade-47b0-b8f0-0b9deb0ced11 from sched-pred-7540 started at 2021-03-29 21:45:46 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.068: INFO: 	Container filler-pod-2f3a020a-1ade-47b0-b8f0-0b9deb0ced11 ready: true, restart count 0
Mar 29 21:45:52.068: INFO: ibm-master-proxy-static-10.189.118.196 from kube-system started at 2021-03-29 19:48:58 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:52.068: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 21:45:52.068: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:52.068: INFO: ibm-kubelet-monitor-cmcxj from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.068: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:52.068: INFO: ibm-keepalived-watcher-4x7cv from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.068: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 21:45:52.068: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-l4nd5 from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:52.068: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:45:52.068: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 21:45:52.068: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.208 before test
Mar 29 21:45:52.147: INFO: calico-node-zjnmd from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 21:45:52.147: INFO: kubernetes-dashboard-6cf6cfdf4-kzffs from kube-system started at 2021-03-29 19:49:50 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 29 21:45:52.147: INFO: ibm-cloud-provider-ip-169-63-130-250-65c6959456-gps6w from ibm-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container ibm-cloud-provider-ip-169-63-130-250 ready: true, restart count 0
Mar 29 21:45:52.147: INFO: public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-2lcqz from kube-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 29 21:45:52.147: INFO: ibm-kubelet-monitor-gbcvf from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:52.147: INFO: vpn-657cb5cdb6-v5h8m from kube-system started at 2021-03-29 19:56:31 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container vpn ready: true, restart count 0
Mar 29 21:45:52.147: INFO: ibm-storage-watcher-659bdcc695-qqkhw from kube-system started at 2021-03-29 19:49:50 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 29 21:45:52.147: INFO: ibm-keepalived-watcher-ps2vt from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 21:45:52.147: INFO: olm-operator-8496678794-gg5v7 from ibm-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container olm-operator ready: true, restart count 0
Mar 29 21:45:52.147: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-n7lmz from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 21:45:52.147: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 21:45:52.147: INFO: filler-pod-111d5bb8-b58b-4c4c-aa53-707fd0ccbe83 from sched-pred-7540 started at 2021-03-29 21:45:46 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container filler-pod-111d5bb8-b58b-4c4c-aa53-707fd0ccbe83 ready: true, restart count 0
Mar 29 21:45:52.147: INFO: metrics-server-f9f7549df-6xdmf from kube-system started at 2021-03-29 19:50:53 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container metrics-server ready: true, restart count 0
Mar 29 21:45:52.147: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 29 21:45:52.147: INFO: coredns-autoscaler-bff977695-9wkhn from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container autoscaler ready: true, restart count 0
Mar 29 21:45:52.147: INFO: calico-kube-controllers-6599f97f59-7g48l from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.147: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 29 21:45:52.147: INFO: catalog-operator-7bc4c797b5-kbbnz from ibm-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.148: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 29 21:45:52.148: INFO: dashboard-metrics-scraper-b585c6867-2k8ww from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.148: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 29 21:45:52.148: INFO: ibm-master-proxy-static-10.189.118.208 from kube-system started at 2021-03-29 19:48:43 +0000 UTC (2 container statuses recorded)
Mar 29 21:45:52.148: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 21:45:52.148: INFO: 	Container pause ready: true, restart count 0
Mar 29 21:45:52.148: INFO: coredns-7cc79848cf-kjg7p from kube-system started at 2021-03-29 19:57:07 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.148: INFO: 	Container coredns ready: true, restart count 0
Mar 29 21:45:52.148: INFO: coredns-7cc79848cf-m5klb from kube-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.148: INFO: 	Container coredns ready: true, restart count 0
Mar 29 21:45:52.148: INFO: ibm-file-plugin-c76c68fd9-bpt7r from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 21:45:52.148: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-41c6cec4-00f2-40e9-a46b-ada89f470cf1 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-41c6cec4-00f2-40e9-a46b-ada89f470cf1 off the node 10.189.118.196
STEP: verifying the node doesn't have the label kubernetes.io/e2e-41c6cec4-00f2-40e9-a46b-ada89f470cf1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:50:58.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9077" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:306.768 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":277,"completed":157,"skipped":2673,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:50:58.423: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7639
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 29 21:50:58.750: INFO: Number of nodes with available pods: 0
Mar 29 21:50:58.750: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 21:50:59.781: INFO: Number of nodes with available pods: 0
Mar 29 21:50:59.781: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 21:51:00.774: INFO: Number of nodes with available pods: 0
Mar 29 21:51:00.774: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 21:51:01.775: INFO: Number of nodes with available pods: 3
Mar 29 21:51:01.775: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 29 21:51:01.833: INFO: Number of nodes with available pods: 2
Mar 29 21:51:01.834: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 21:51:02.861: INFO: Number of nodes with available pods: 2
Mar 29 21:51:02.861: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 21:51:03.858: INFO: Number of nodes with available pods: 2
Mar 29 21:51:03.858: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 21:51:04.856: INFO: Number of nodes with available pods: 3
Mar 29 21:51:04.856: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7639, will wait for the garbage collector to delete the pods
Mar 29 21:51:04.965: INFO: Deleting DaemonSet.extensions daemon-set took: 29.405277ms
Mar 29 21:51:05.065: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.269575ms
Mar 29 21:51:16.975: INFO: Number of nodes with available pods: 0
Mar 29 21:51:16.975: INFO: Number of running nodes: 0, number of available pods: 0
Mar 29 21:51:16.988: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7639/daemonsets","resourceVersion":"42338"},"items":null}

Mar 29 21:51:16.996: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7639/pods","resourceVersion":"42339"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:51:17.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7639" for this suite.

• [SLOW TEST:18.639 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":277,"completed":158,"skipped":2680,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:51:17.062: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 29 21:51:23.406: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 29 21:51:23.418: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 29 21:51:25.418: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 29 21:51:25.429: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 29 21:51:27.418: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 29 21:51:27.432: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:51:27.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3099" for this suite.

• [SLOW TEST:10.595 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":277,"completed":159,"skipped":2684,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:51:27.660: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5942
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:51:27.886: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 29 21:51:32.897: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 29 21:51:32.898: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 29 21:51:34.908: INFO: Creating deployment "test-rollover-deployment"
Mar 29 21:51:34.929: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 29 21:51:36.950: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 29 21:51:36.969: INFO: Ensure that both replica sets have 1 created replica
Mar 29 21:51:36.986: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 29 21:51:37.007: INFO: Updating deployment test-rollover-deployment
Mar 29 21:51:37.007: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 29 21:51:39.032: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 29 21:51:39.050: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 29 21:51:39.072: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 21:51:39.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651497, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 21:51:41.091: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 21:51:41.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651499, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 21:51:43.092: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 21:51:43.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651499, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 21:51:45.094: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 21:51:45.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651499, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 21:51:47.092: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 21:51:47.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651499, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 21:51:49.101: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 21:51:49.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651499, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651494, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 21:51:51.093: INFO: 
Mar 29 21:51:51.093: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 29 21:51:51.120: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5942 /apis/apps/v1/namespaces/deployment-5942/deployments/test-rollover-deployment 91970e80-339f-4128-81ae-e6a314959283 42624 2 2021-03-29 21:51:34 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-29 21:51:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-29 21:51:49 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00344db58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-29 21:51:34 +0000 UTC,LastTransitionTime:2021-03-29 21:51:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2021-03-29 21:51:49 +0000 UTC,LastTransitionTime:2021-03-29 21:51:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 29 21:51:51.129: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-5942 /apis/apps/v1/namespaces/deployment-5942/replicasets/test-rollover-deployment-84f7f6f64b 4a404df8-4898-493f-9548-8aee3814c483 42612 2 2021-03-29 21:51:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 91970e80-339f-4128-81ae-e6a314959283 0xc0049e2ab7 0xc0049e2ab8}] []  [{kube-controller-manager Update apps/v1 2021-03-29 21:51:49 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 49 57 55 48 101 56 48 45 51 51 57 102 45 52 49 50 56 45 56 49 97 101 45 101 54 97 51 49 52 57 53 57 50 56 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0049e2b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:51:51.129: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 29 21:51:51.129: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5942 /apis/apps/v1/namespaces/deployment-5942/replicasets/test-rollover-controller cf36be6f-e5ba-41c0-95a4-0baff87457b7 42622 2 2021-03-29 21:51:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 91970e80-339f-4128-81ae-e6a314959283 0xc0049e28a7 0xc0049e28a8}] []  [{e2e.test Update apps/v1 2021-03-29 21:51:27 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-29 21:51:49 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 49 57 55 48 101 56 48 45 51 51 57 102 45 52 49 50 56 45 56 49 97 101 45 101 54 97 51 49 52 57 53 57 50 56 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0049e2948 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:51:51.129: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-5942 /apis/apps/v1/namespaces/deployment-5942/replicasets/test-rollover-deployment-5686c4cfd5 761baa6e-932a-4034-b3da-6f0659d06979 42560 2 2021-03-29 21:51:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 91970e80-339f-4128-81ae-e6a314959283 0xc0049e29b7 0xc0049e29b8}] []  [{kube-controller-manager Update apps/v1 2021-03-29 21:51:37 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 49 57 55 48 101 56 48 45 51 51 57 102 45 52 49 50 56 45 56 49 97 101 45 101 54 97 51 49 52 57 53 57 50 56 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 114 101 100 105 115 45 115 108 97 118 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0049e2a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 21:51:51.139: INFO: Pod "test-rollover-deployment-84f7f6f64b-wbklj" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-wbklj test-rollover-deployment-84f7f6f64b- deployment-5942 /api/v1/namespaces/deployment-5942/pods/test-rollover-deployment-84f7f6f64b-wbklj 490b1d77-83b1-4af9-9c43-dd50aca09f28 42584 0 2021-03-29 21:51:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b 4a404df8-4898-493f-9548-8aee3814c483 0xc0048f6797 0xc0048f6798}] []  [{kube-controller-manager Update v1 2021-03-29 21:51:37 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 97 52 48 52 100 102 56 45 52 56 57 56 45 52 57 51 102 45 57 53 52 56 45 56 97 101 101 51 56 49 52 99 52 56 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-29 21:51:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 51 48 46 53 56 46 53 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dkpgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dkpgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dkpgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.189.118.196,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:51:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:51:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-29 21:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.189.118.196,PodIP:172.30.58.52,StartTime:2021-03-29 21:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-29 21:51:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:containerd://4a1e47d252c8703d7449daa38f9484d7518e1baddd6d0f94bd348d95dea8f2e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:51:51.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5942" for this suite.

• [SLOW TEST:23.507 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":277,"completed":160,"skipped":2712,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:51:51.168: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2286
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:51:51.368: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:51:51.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2286" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":277,"completed":161,"skipped":2729,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:51:51.981: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:51:52.226: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb62ab86-bf72-4dbe-b6f5-28e10f4aa83c" in namespace "downward-api-7503" to be "Succeeded or Failed"
Mar 29 21:51:52.235: INFO: Pod "downwardapi-volume-bb62ab86-bf72-4dbe-b6f5-28e10f4aa83c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.285561ms
Mar 29 21:51:54.245: INFO: Pod "downwardapi-volume-bb62ab86-bf72-4dbe-b6f5-28e10f4aa83c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019145731s
Mar 29 21:51:56.256: INFO: Pod "downwardapi-volume-bb62ab86-bf72-4dbe-b6f5-28e10f4aa83c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02961627s
STEP: Saw pod success
Mar 29 21:51:56.256: INFO: Pod "downwardapi-volume-bb62ab86-bf72-4dbe-b6f5-28e10f4aa83c" satisfied condition "Succeeded or Failed"
Mar 29 21:51:56.266: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-bb62ab86-bf72-4dbe-b6f5-28e10f4aa83c container client-container: <nil>
STEP: delete the pod
Mar 29 21:51:56.331: INFO: Waiting for pod downwardapi-volume-bb62ab86-bf72-4dbe-b6f5-28e10f4aa83c to disappear
Mar 29 21:51:56.345: INFO: Pod downwardapi-volume-bb62ab86-bf72-4dbe-b6f5-28e10f4aa83c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:51:56.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7503" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":162,"skipped":2759,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:51:56.373: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8919
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 29 21:51:56.593: INFO: Waiting up to 5m0s for pod "pod-4ef4253e-775f-4e4b-b3a5-0bfd477f0e5a" in namespace "emptydir-8919" to be "Succeeded or Failed"
Mar 29 21:51:56.605: INFO: Pod "pod-4ef4253e-775f-4e4b-b3a5-0bfd477f0e5a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.256492ms
Mar 29 21:51:58.615: INFO: Pod "pod-4ef4253e-775f-4e4b-b3a5-0bfd477f0e5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021628522s
STEP: Saw pod success
Mar 29 21:51:58.615: INFO: Pod "pod-4ef4253e-775f-4e4b-b3a5-0bfd477f0e5a" satisfied condition "Succeeded or Failed"
Mar 29 21:51:58.625: INFO: Trying to get logs from node 10.189.118.196 pod pod-4ef4253e-775f-4e4b-b3a5-0bfd477f0e5a container test-container: <nil>
STEP: delete the pod
Mar 29 21:51:58.681: INFO: Waiting for pod pod-4ef4253e-775f-4e4b-b3a5-0bfd477f0e5a to disappear
Mar 29 21:51:58.690: INFO: Pod pod-4ef4253e-775f-4e4b-b3a5-0bfd477f0e5a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:51:58.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8919" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":163,"skipped":2763,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:51:58.723: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-658
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:51:59.285: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 21:52:01.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651519, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651519, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651519, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651519, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:52:04.362: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:52:04.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-658" for this suite.
STEP: Destroying namespace "webhook-658-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.105 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":277,"completed":164,"skipped":2767,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:52:04.830: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2728
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:52:05.061: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:52:06.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2728" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":277,"completed":165,"skipped":2770,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:52:06.399: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:52:08.717: INFO: Waiting up to 5m0s for pod "client-envvars-0938144a-b37f-40a0-8433-34e499178a9a" in namespace "pods-1034" to be "Succeeded or Failed"
Mar 29 21:52:08.727: INFO: Pod "client-envvars-0938144a-b37f-40a0-8433-34e499178a9a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.582057ms
Mar 29 21:52:10.738: INFO: Pod "client-envvars-0938144a-b37f-40a0-8433-34e499178a9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021501802s
STEP: Saw pod success
Mar 29 21:52:10.738: INFO: Pod "client-envvars-0938144a-b37f-40a0-8433-34e499178a9a" satisfied condition "Succeeded or Failed"
Mar 29 21:52:10.748: INFO: Trying to get logs from node 10.189.118.196 pod client-envvars-0938144a-b37f-40a0-8433-34e499178a9a container env3cont: <nil>
STEP: delete the pod
Mar 29 21:52:10.798: INFO: Waiting for pod client-envvars-0938144a-b37f-40a0-8433-34e499178a9a to disappear
Mar 29 21:52:10.809: INFO: Pod client-envvars-0938144a-b37f-40a0-8433-34e499178a9a no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:52:10.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1034" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":277,"completed":166,"skipped":2779,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:52:10.838: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4646
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-34df6ebe-7858-4750-8aea-1deae8f2c6fd
STEP: Creating configMap with name cm-test-opt-upd-33510871-625e-463e-8a4a-abb384aa23ea
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-34df6ebe-7858-4750-8aea-1deae8f2c6fd
STEP: Updating configmap cm-test-opt-upd-33510871-625e-463e-8a4a-abb384aa23ea
STEP: Creating configMap with name cm-test-opt-create-0fced278-b854-4192-8614-61f1647ffbe2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:53:28.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4646" for this suite.

• [SLOW TEST:77.617 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":167,"skipped":2790,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:53:28.455: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9248
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 29 21:53:28.734: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9248 /api/v1/namespaces/watch-9248/configmaps/e2e-watch-test-label-changed 417f3ed5-0071-4aba-a1fe-caf63bc96826 43263 0 2021-03-29 21:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-29 21:53:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 21:53:28.735: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9248 /api/v1/namespaces/watch-9248/configmaps/e2e-watch-test-label-changed 417f3ed5-0071-4aba-a1fe-caf63bc96826 43264 0 2021-03-29 21:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-29 21:53:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 21:53:28.735: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9248 /api/v1/namespaces/watch-9248/configmaps/e2e-watch-test-label-changed 417f3ed5-0071-4aba-a1fe-caf63bc96826 43265 0 2021-03-29 21:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-29 21:53:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 29 21:53:38.849: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9248 /api/v1/namespaces/watch-9248/configmaps/e2e-watch-test-label-changed 417f3ed5-0071-4aba-a1fe-caf63bc96826 43318 0 2021-03-29 21:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-29 21:53:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 21:53:38.849: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9248 /api/v1/namespaces/watch-9248/configmaps/e2e-watch-test-label-changed 417f3ed5-0071-4aba-a1fe-caf63bc96826 43319 0 2021-03-29 21:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-29 21:53:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 21:53:38.849: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9248 /api/v1/namespaces/watch-9248/configmaps/e2e-watch-test-label-changed 417f3ed5-0071-4aba-a1fe-caf63bc96826 43320 0 2021-03-29 21:53:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-29 21:53:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:53:38.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9248" for this suite.

• [SLOW TEST:10.431 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":277,"completed":168,"skipped":2791,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:53:38.887: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-875fc943-f305-42b6-8426-397375621cb6
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:53:39.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9288" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":277,"completed":169,"skipped":2792,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:53:39.135: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 21:53:39.361: INFO: Waiting up to 5m0s for pod "downwardapi-volume-883ecb93-bc5b-48a2-8b42-74d9f586680a" in namespace "projected-5459" to be "Succeeded or Failed"
Mar 29 21:53:39.371: INFO: Pod "downwardapi-volume-883ecb93-bc5b-48a2-8b42-74d9f586680a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.661487ms
Mar 29 21:53:41.380: INFO: Pod "downwardapi-volume-883ecb93-bc5b-48a2-8b42-74d9f586680a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018984904s
STEP: Saw pod success
Mar 29 21:53:41.380: INFO: Pod "downwardapi-volume-883ecb93-bc5b-48a2-8b42-74d9f586680a" satisfied condition "Succeeded or Failed"
Mar 29 21:53:41.389: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-883ecb93-bc5b-48a2-8b42-74d9f586680a container client-container: <nil>
STEP: delete the pod
Mar 29 21:53:41.454: INFO: Waiting for pod downwardapi-volume-883ecb93-bc5b-48a2-8b42-74d9f586680a to disappear
Mar 29 21:53:41.466: INFO: Pod downwardapi-volume-883ecb93-bc5b-48a2-8b42-74d9f586680a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:53:41.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5459" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":170,"skipped":2800,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:53:41.493: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9830
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 29 21:53:41.717: INFO: Waiting up to 5m0s for pod "pod-e9393002-5856-4322-a575-596a30117d5e" in namespace "emptydir-9830" to be "Succeeded or Failed"
Mar 29 21:53:41.726: INFO: Pod "pod-e9393002-5856-4322-a575-596a30117d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.658349ms
Mar 29 21:53:43.735: INFO: Pod "pod-e9393002-5856-4322-a575-596a30117d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018075519s
Mar 29 21:53:45.747: INFO: Pod "pod-e9393002-5856-4322-a575-596a30117d5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029505066s
STEP: Saw pod success
Mar 29 21:53:45.747: INFO: Pod "pod-e9393002-5856-4322-a575-596a30117d5e" satisfied condition "Succeeded or Failed"
Mar 29 21:53:45.756: INFO: Trying to get logs from node 10.189.118.196 pod pod-e9393002-5856-4322-a575-596a30117d5e container test-container: <nil>
STEP: delete the pod
Mar 29 21:53:45.811: INFO: Waiting for pod pod-e9393002-5856-4322-a575-596a30117d5e to disappear
Mar 29 21:53:45.819: INFO: Pod pod-e9393002-5856-4322-a575-596a30117d5e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:53:45.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9830" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":171,"skipped":2815,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:53:45.848: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8740
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 21:53:46.767: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 21:53:48.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651626, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651626, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651626, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752651626, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 21:53:51.837: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:53:51.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8740" for this suite.
STEP: Destroying namespace "webhook-8740-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.354 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":277,"completed":172,"skipped":2830,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:53:52.202: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-79bs
STEP: Creating a pod to test atomic-volume-subpath
Mar 29 21:53:52.449: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-79bs" in namespace "subpath-5588" to be "Succeeded or Failed"
Mar 29 21:53:52.459: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Pending", Reason="", readiness=false. Elapsed: 9.817621ms
Mar 29 21:53:54.469: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Running", Reason="", readiness=true. Elapsed: 2.020082553s
Mar 29 21:53:56.483: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Running", Reason="", readiness=true. Elapsed: 4.033642013s
Mar 29 21:53:58.492: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Running", Reason="", readiness=true. Elapsed: 6.043540114s
Mar 29 21:54:00.504: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Running", Reason="", readiness=true. Elapsed: 8.055191576s
Mar 29 21:54:02.515: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Running", Reason="", readiness=true. Elapsed: 10.065980987s
Mar 29 21:54:04.525: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Running", Reason="", readiness=true. Elapsed: 12.076125638s
Mar 29 21:54:06.539: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Running", Reason="", readiness=true. Elapsed: 14.090109916s
Mar 29 21:54:08.549: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Running", Reason="", readiness=true. Elapsed: 16.10055221s
Mar 29 21:54:10.561: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Running", Reason="", readiness=true. Elapsed: 18.111911207s
Mar 29 21:54:12.572: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Running", Reason="", readiness=true. Elapsed: 20.123165266s
Mar 29 21:54:14.582: INFO: Pod "pod-subpath-test-projected-79bs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.133439597s
STEP: Saw pod success
Mar 29 21:54:14.582: INFO: Pod "pod-subpath-test-projected-79bs" satisfied condition "Succeeded or Failed"
Mar 29 21:54:14.592: INFO: Trying to get logs from node 10.189.118.196 pod pod-subpath-test-projected-79bs container test-container-subpath-projected-79bs: <nil>
STEP: delete the pod
Mar 29 21:54:14.654: INFO: Waiting for pod pod-subpath-test-projected-79bs to disappear
Mar 29 21:54:14.664: INFO: Pod pod-subpath-test-projected-79bs no longer exists
STEP: Deleting pod pod-subpath-test-projected-79bs
Mar 29 21:54:14.664: INFO: Deleting pod "pod-subpath-test-projected-79bs" in namespace "subpath-5588"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:54:14.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5588" for this suite.

• [SLOW TEST:22.497 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":277,"completed":173,"skipped":2843,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:54:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-7700
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar 29 21:54:14.921: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar 29 21:54:14.935: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 29 21:54:14.935: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar 29 21:54:14.959: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 29 21:54:14.959: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar 29 21:54:14.984: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 29 21:54:14.984: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar 29 21:54:22.078: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:54:22.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7700" for this suite.

• [SLOW TEST:7.425 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":277,"completed":174,"skipped":2908,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:54:22.135: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-fe8a98f3-f73d-40df-a963-81305ece965f
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:54:22.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9584" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":277,"completed":175,"skipped":2916,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:54:22.369: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2461
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Mar 29 21:54:22.597: INFO: Waiting up to 5m0s for pod "var-expansion-5227f23d-ce54-4ec0-8dbe-da9d416c3287" in namespace "var-expansion-2461" to be "Succeeded or Failed"
Mar 29 21:54:22.607: INFO: Pod "var-expansion-5227f23d-ce54-4ec0-8dbe-da9d416c3287": Phase="Pending", Reason="", readiness=false. Elapsed: 9.884607ms
Mar 29 21:54:24.618: INFO: Pod "var-expansion-5227f23d-ce54-4ec0-8dbe-da9d416c3287": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020657704s
Mar 29 21:54:26.630: INFO: Pod "var-expansion-5227f23d-ce54-4ec0-8dbe-da9d416c3287": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032120793s
STEP: Saw pod success
Mar 29 21:54:26.630: INFO: Pod "var-expansion-5227f23d-ce54-4ec0-8dbe-da9d416c3287" satisfied condition "Succeeded or Failed"
Mar 29 21:54:26.642: INFO: Trying to get logs from node 10.189.118.196 pod var-expansion-5227f23d-ce54-4ec0-8dbe-da9d416c3287 container dapi-container: <nil>
STEP: delete the pod
Mar 29 21:54:26.723: INFO: Waiting for pod var-expansion-5227f23d-ce54-4ec0-8dbe-da9d416c3287 to disappear
Mar 29 21:54:26.736: INFO: Pod var-expansion-5227f23d-ce54-4ec0-8dbe-da9d416c3287 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:54:26.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2461" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":277,"completed":176,"skipped":2923,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:54:26.781: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Mar 29 21:54:31.102: INFO: Pod pod-hostip-89b6dd6e-f185-4af1-8728-322a2e3baa5c has hostIP: 10.189.118.196
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:54:31.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-119" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":277,"completed":177,"skipped":2943,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:54:31.146: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:54:31.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1355" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":277,"completed":178,"skipped":2946,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:54:31.504: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6836.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6836.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6836.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 29 21:54:35.847: INFO: DNS probes using dns-test-5555dd0d-8625-46d4-a091-edbb55fe7b63 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6836.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6836.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6836.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 29 21:54:40.047: INFO: File wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local from pod  dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 29 21:54:40.072: INFO: Lookups using dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 failed for: [wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local]

Mar 29 21:54:45.110: INFO: File jessie_udp@dns-test-service-3.dns-6836.svc.cluster.local from pod  dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 29 21:54:45.110: INFO: Lookups using dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 failed for: [jessie_udp@dns-test-service-3.dns-6836.svc.cluster.local]

Mar 29 21:54:50.089: INFO: File wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local from pod  dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 29 21:54:50.108: INFO: Lookups using dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 failed for: [wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local]

Mar 29 21:54:55.089: INFO: File wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local from pod  dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 29 21:54:55.107: INFO: File jessie_udp@dns-test-service-3.dns-6836.svc.cluster.local from pod  dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 29 21:54:55.107: INFO: Lookups using dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 failed for: [wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local jessie_udp@dns-test-service-3.dns-6836.svc.cluster.local]

Mar 29 21:55:00.088: INFO: File wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local from pod  dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 29 21:55:00.109: INFO: File jessie_udp@dns-test-service-3.dns-6836.svc.cluster.local from pod  dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 29 21:55:00.109: INFO: Lookups using dns-6836/dns-test-503045ba-a071-43c3-9497-783ffd4da856 failed for: [wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local jessie_udp@dns-test-service-3.dns-6836.svc.cluster.local]

Mar 29 21:55:05.104: INFO: DNS probes using dns-test-503045ba-a071-43c3-9497-783ffd4da856 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6836.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6836.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6836.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6836.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 29 21:55:11.293: INFO: DNS probes using dns-test-84fd3aee-66b1-4117-84fd-10533d34639a succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:55:11.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6836" for this suite.

• [SLOW TEST:39.898 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":277,"completed":179,"skipped":2948,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:55:11.402: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1751
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:56:11.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1751" for this suite.

• [SLOW TEST:60.265 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":277,"completed":180,"skipped":2952,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:56:11.668: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-4927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar 29 21:56:11.866: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 29 21:57:11.933: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 21:57:11.944: INFO: Starting informer...
STEP: Starting pod...
Mar 29 21:57:12.185: INFO: Pod is running on 10.189.118.196. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 29 21:57:12.216: INFO: Pod wasn't evicted. Proceeding
Mar 29 21:57:12.216: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 29 21:58:27.251: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:58:27.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4927" for this suite.

• [SLOW TEST:135.614 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":277,"completed":181,"skipped":2959,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:58:27.283: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 29 21:58:30.587: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:58:31.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5534" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":277,"completed":182,"skipped":2964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:58:31.661: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1437
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Mar 29 21:58:31.884: INFO: Waiting up to 5m0s for pod "client-containers-c3c17d14-957f-4a3b-8392-eed260ff0a69" in namespace "containers-1437" to be "Succeeded or Failed"
Mar 29 21:58:31.893: INFO: Pod "client-containers-c3c17d14-957f-4a3b-8392-eed260ff0a69": Phase="Pending", Reason="", readiness=false. Elapsed: 9.019994ms
Mar 29 21:58:33.906: INFO: Pod "client-containers-c3c17d14-957f-4a3b-8392-eed260ff0a69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022048647s
Mar 29 21:58:35.917: INFO: Pod "client-containers-c3c17d14-957f-4a3b-8392-eed260ff0a69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032712379s
STEP: Saw pod success
Mar 29 21:58:35.917: INFO: Pod "client-containers-c3c17d14-957f-4a3b-8392-eed260ff0a69" satisfied condition "Succeeded or Failed"
Mar 29 21:58:35.926: INFO: Trying to get logs from node 10.189.118.196 pod client-containers-c3c17d14-957f-4a3b-8392-eed260ff0a69 container test-container: <nil>
STEP: delete the pod
Mar 29 21:58:36.022: INFO: Waiting for pod client-containers-c3c17d14-957f-4a3b-8392-eed260ff0a69 to disappear
Mar 29 21:58:36.031: INFO: Pod client-containers-c3c17d14-957f-4a3b-8392-eed260ff0a69 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:58:36.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1437" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":277,"completed":183,"skipped":2996,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:58:36.059: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:58:58.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6026" for this suite.

• [SLOW TEST:22.929 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":277,"completed":184,"skipped":2998,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:58:58.988: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6543
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-0c725cbf-1eeb-456c-b549-801086189094
STEP: Creating a pod to test consume configMaps
Mar 29 21:58:59.280: INFO: Waiting up to 5m0s for pod "pod-configmaps-b9d91548-e8f5-4030-8bb5-6238f837604d" in namespace "configmap-6543" to be "Succeeded or Failed"
Mar 29 21:58:59.291: INFO: Pod "pod-configmaps-b9d91548-e8f5-4030-8bb5-6238f837604d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.357769ms
Mar 29 21:59:01.302: INFO: Pod "pod-configmaps-b9d91548-e8f5-4030-8bb5-6238f837604d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021809619s
Mar 29 21:59:03.312: INFO: Pod "pod-configmaps-b9d91548-e8f5-4030-8bb5-6238f837604d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031651669s
STEP: Saw pod success
Mar 29 21:59:03.312: INFO: Pod "pod-configmaps-b9d91548-e8f5-4030-8bb5-6238f837604d" satisfied condition "Succeeded or Failed"
Mar 29 21:59:03.321: INFO: Trying to get logs from node 10.189.118.196 pod pod-configmaps-b9d91548-e8f5-4030-8bb5-6238f837604d container configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 21:59:03.374: INFO: Waiting for pod pod-configmaps-b9d91548-e8f5-4030-8bb5-6238f837604d to disappear
Mar 29 21:59:03.383: INFO: Pod pod-configmaps-b9d91548-e8f5-4030-8bb5-6238f837604d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:59:03.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6543" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":185,"skipped":3004,"failed":0}

------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:59:03.412: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-9225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 29 21:59:09.706: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9225 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:09.706: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:09.913: INFO: Exec stderr: ""
Mar 29 21:59:09.913: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9225 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:10.124: INFO: Exec stderr: ""
Mar 29 21:59:10.124: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9225 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:10.125: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:10.306: INFO: Exec stderr: ""
Mar 29 21:59:10.306: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9225 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:10.306: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:10.493: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 29 21:59:10.493: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9225 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:10.493: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:10.675: INFO: Exec stderr: ""
Mar 29 21:59:10.676: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9225 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:10.676: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:10.874: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 29 21:59:10.875: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9225 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:10.875: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:11.057: INFO: Exec stderr: ""
Mar 29 21:59:11.057: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9225 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:11.057: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:11.340: INFO: Exec stderr: ""
Mar 29 21:59:11.340: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9225 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:11.341: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:11.535: INFO: Exec stderr: ""
Mar 29 21:59:11.535: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9225 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:11.535: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:11.738: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:59:11.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9225" for this suite.

• [SLOW TEST:8.354 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":186,"skipped":3004,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:59:11.766: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2227
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2227.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2227.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 27.196.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.196.27_udp@PTR;check="$$(dig +tcp +noall +answer +search 27.196.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.196.27_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2227.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2227.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 27.196.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.196.27_udp@PTR;check="$$(dig +tcp +noall +answer +search 27.196.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.196.27_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 29 21:59:16.105: INFO: Unable to read wheezy_udp@dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:16.123: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:16.139: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:16.154: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:16.269: INFO: Unable to read jessie_udp@dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:16.289: INFO: Unable to read jessie_tcp@dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:16.305: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:16.320: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:16.423: INFO: Lookups using dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb failed for: [wheezy_udp@dns-test-service.dns-2227.svc.cluster.local wheezy_tcp@dns-test-service.dns-2227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_udp@dns-test-service.dns-2227.svc.cluster.local jessie_tcp@dns-test-service.dns-2227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local]

Mar 29 21:59:21.497: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:21.514: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:21.675: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:21.693: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:21.816: INFO: Lookups using dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local]

Mar 29 21:59:26.479: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:26.495: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:26.654: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:26.672: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:26.783: INFO: Lookups using dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local]

Mar 29 21:59:31.473: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:31.491: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:31.636: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:31.651: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:31.742: INFO: Lookups using dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local]

Mar 29 21:59:36.477: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:36.493: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:36.640: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:36.655: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:36.746: INFO: Lookups using dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local]

Mar 29 21:59:41.478: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:41.493: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:41.647: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:41.663: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local from pod dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb: the server could not find the requested resource (get pods dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb)
Mar 29 21:59:41.769: INFO: Lookups using dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2227.svc.cluster.local]

Mar 29 21:59:46.768: INFO: DNS probes using dns-2227/dns-test-1b716fa0-7ab7-4119-8c19-125216d334bb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:59:46.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2227" for this suite.

• [SLOW TEST:35.222 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":277,"completed":187,"skipped":3018,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:59:46.988: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-3418476a-f55d-4f9d-ad41-2520a346264d
STEP: Creating a pod to test consume configMaps
Mar 29 21:59:47.246: INFO: Waiting up to 5m0s for pod "pod-configmaps-f52a167f-6be2-4192-8710-be426e1896fe" in namespace "configmap-2864" to be "Succeeded or Failed"
Mar 29 21:59:47.257: INFO: Pod "pod-configmaps-f52a167f-6be2-4192-8710-be426e1896fe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.084291ms
Mar 29 21:59:49.268: INFO: Pod "pod-configmaps-f52a167f-6be2-4192-8710-be426e1896fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022066107s
Mar 29 21:59:51.280: INFO: Pod "pod-configmaps-f52a167f-6be2-4192-8710-be426e1896fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033990256s
STEP: Saw pod success
Mar 29 21:59:51.280: INFO: Pod "pod-configmaps-f52a167f-6be2-4192-8710-be426e1896fe" satisfied condition "Succeeded or Failed"
Mar 29 21:59:51.294: INFO: Trying to get logs from node 10.189.118.196 pod pod-configmaps-f52a167f-6be2-4192-8710-be426e1896fe container configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 21:59:51.370: INFO: Waiting for pod pod-configmaps-f52a167f-6be2-4192-8710-be426e1896fe to disappear
Mar 29 21:59:51.385: INFO: Pod pod-configmaps-f52a167f-6be2-4192-8710-be426e1896fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:59:51.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2864" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":188,"skipped":3021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:59:51.412: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6018
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar 29 21:59:53.691: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6018 PodName:pod-sharedvolume-596c7a71-eb0b-40a7-bc13-dbdec7251a33 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 21:59:53.692: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 21:59:53.898: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 21:59:53.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6018" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":277,"completed":189,"skipped":3105,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 21:59:53.926: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2672
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 29 21:59:54.865: INFO: Pod name wrapped-volume-race-2592e484-535d-4f2a-aef8-333996c04a31: Found 0 pods out of 5
Mar 29 21:59:59.885: INFO: Pod name wrapped-volume-race-2592e484-535d-4f2a-aef8-333996c04a31: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2592e484-535d-4f2a-aef8-333996c04a31 in namespace emptydir-wrapper-2672, will wait for the garbage collector to delete the pods
Mar 29 22:00:00.027: INFO: Deleting ReplicationController wrapped-volume-race-2592e484-535d-4f2a-aef8-333996c04a31 took: 25.017747ms
Mar 29 22:00:00.128: INFO: Terminating ReplicationController wrapped-volume-race-2592e484-535d-4f2a-aef8-333996c04a31 pods took: 100.318249ms
STEP: Creating RC which spawns configmap-volume pods
Mar 29 22:00:12.977: INFO: Pod name wrapped-volume-race-6310b3ee-e1b8-43b1-9e68-91f5d458070a: Found 0 pods out of 5
Mar 29 22:00:17.996: INFO: Pod name wrapped-volume-race-6310b3ee-e1b8-43b1-9e68-91f5d458070a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6310b3ee-e1b8-43b1-9e68-91f5d458070a in namespace emptydir-wrapper-2672, will wait for the garbage collector to delete the pods
Mar 29 22:00:18.132: INFO: Deleting ReplicationController wrapped-volume-race-6310b3ee-e1b8-43b1-9e68-91f5d458070a took: 24.406137ms
Mar 29 22:00:18.232: INFO: Terminating ReplicationController wrapped-volume-race-6310b3ee-e1b8-43b1-9e68-91f5d458070a pods took: 100.234191ms
STEP: Creating RC which spawns configmap-volume pods
Mar 29 22:00:32.786: INFO: Pod name wrapped-volume-race-ab90a8c5-08b6-46db-8108-94fbbab594be: Found 0 pods out of 5
Mar 29 22:00:37.807: INFO: Pod name wrapped-volume-race-ab90a8c5-08b6-46db-8108-94fbbab594be: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ab90a8c5-08b6-46db-8108-94fbbab594be in namespace emptydir-wrapper-2672, will wait for the garbage collector to delete the pods
Mar 29 22:00:37.951: INFO: Deleting ReplicationController wrapped-volume-race-ab90a8c5-08b6-46db-8108-94fbbab594be took: 26.638841ms
Mar 29 22:00:38.055: INFO: Terminating ReplicationController wrapped-volume-race-ab90a8c5-08b6-46db-8108-94fbbab594be pods took: 104.301382ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:00:52.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2672" for this suite.

• [SLOW TEST:58.547 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":277,"completed":190,"skipped":3109,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:00:52.474: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Mar 29 22:00:52.707: INFO: Waiting up to 5m0s for pod "var-expansion-5198b71c-4332-4d4f-9041-aa1708ac029e" in namespace "var-expansion-758" to be "Succeeded or Failed"
Mar 29 22:00:52.716: INFO: Pod "var-expansion-5198b71c-4332-4d4f-9041-aa1708ac029e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.292181ms
Mar 29 22:00:54.726: INFO: Pod "var-expansion-5198b71c-4332-4d4f-9041-aa1708ac029e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019239093s
Mar 29 22:00:56.740: INFO: Pod "var-expansion-5198b71c-4332-4d4f-9041-aa1708ac029e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03350572s
STEP: Saw pod success
Mar 29 22:00:56.740: INFO: Pod "var-expansion-5198b71c-4332-4d4f-9041-aa1708ac029e" satisfied condition "Succeeded or Failed"
Mar 29 22:00:56.749: INFO: Trying to get logs from node 10.189.118.196 pod var-expansion-5198b71c-4332-4d4f-9041-aa1708ac029e container dapi-container: <nil>
STEP: delete the pod
Mar 29 22:00:56.804: INFO: Waiting for pod var-expansion-5198b71c-4332-4d4f-9041-aa1708ac029e to disappear
Mar 29 22:00:56.814: INFO: Pod var-expansion-5198b71c-4332-4d4f-9041-aa1708ac029e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:00:56.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-758" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":277,"completed":191,"skipped":3110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:00:56.842: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8225
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Mar 29 22:00:57.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-8225 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 29 22:00:57.249: INFO: stderr: ""
Mar 29 22:00:57.249: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Mar 29 22:00:57.249: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 29 22:00:57.249: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8225" to be "running and ready, or succeeded"
Mar 29 22:00:57.261: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.566004ms
Mar 29 22:00:59.273: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.023792572s
Mar 29 22:00:59.273: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 29 22:00:59.273: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 29 22:00:59.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 logs logs-generator logs-generator --namespace=kubectl-8225'
Mar 29 22:00:59.430: INFO: stderr: ""
Mar 29 22:00:59.430: INFO: stdout: "I0329 22:00:58.710459       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/vh9q 467\nI0329 22:00:58.910808       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/2rt2 346\nI0329 22:00:59.110957       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/826h 245\nI0329 22:00:59.310758       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/ddhd 523\n"
STEP: limiting log lines
Mar 29 22:00:59.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 logs logs-generator logs-generator --namespace=kubectl-8225 --tail=1'
Mar 29 22:00:59.592: INFO: stderr: ""
Mar 29 22:00:59.592: INFO: stdout: "I0329 22:00:59.510846       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/q5b 297\n"
Mar 29 22:00:59.592: INFO: got output "I0329 22:00:59.510846       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/q5b 297\n"
STEP: limiting log bytes
Mar 29 22:00:59.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 logs logs-generator logs-generator --namespace=kubectl-8225 --limit-bytes=1'
Mar 29 22:00:59.736: INFO: stderr: ""
Mar 29 22:00:59.736: INFO: stdout: "I"
Mar 29 22:00:59.736: INFO: got output "I"
STEP: exposing timestamps
Mar 29 22:00:59.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 logs logs-generator logs-generator --namespace=kubectl-8225 --tail=1 --timestamps'
Mar 29 22:00:59.881: INFO: stderr: ""
Mar 29 22:00:59.881: INFO: stdout: "2021-03-29T22:00:59.711083635Z I0329 22:00:59.710861       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/qgtk 235\n"
Mar 29 22:00:59.881: INFO: got output "2021-03-29T22:00:59.711083635Z I0329 22:00:59.710861       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/qgtk 235\n"
STEP: restricting to a time range
Mar 29 22:01:02.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 logs logs-generator logs-generator --namespace=kubectl-8225 --since=1s'
Mar 29 22:01:02.515: INFO: stderr: ""
Mar 29 22:01:02.515: INFO: stdout: "I0329 22:01:01.510796       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/cjp4 297\nI0329 22:01:01.710794       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/sgq 592\nI0329 22:01:01.910833       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/rmmx 236\nI0329 22:01:02.110806       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/wz5s 426\nI0329 22:01:02.310807       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/hpq 434\n"
Mar 29 22:01:02.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 logs logs-generator logs-generator --namespace=kubectl-8225 --since=24h'
Mar 29 22:01:02.821: INFO: stderr: ""
Mar 29 22:01:02.821: INFO: stdout: "I0329 22:00:58.710459       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/vh9q 467\nI0329 22:00:58.910808       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/2rt2 346\nI0329 22:00:59.110957       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/826h 245\nI0329 22:00:59.310758       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/ddhd 523\nI0329 22:00:59.510846       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/q5b 297\nI0329 22:00:59.710861       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/qgtk 235\nI0329 22:00:59.910824       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/ljr4 320\nI0329 22:01:00.110816       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/xj6 598\nI0329 22:01:00.310791       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/66cz 557\nI0329 22:01:00.510775       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/9mt 423\nI0329 22:01:00.710757       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/fd87 363\nI0329 22:01:00.910699       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/6jrl 483\nI0329 22:01:01.110790       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/tp96 328\nI0329 22:01:01.310778       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/m2l4 454\nI0329 22:01:01.510796       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/cjp4 297\nI0329 22:01:01.710794       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/sgq 592\nI0329 22:01:01.910833       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/rmmx 236\nI0329 22:01:02.110806       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/wz5s 426\nI0329 22:01:02.310807       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/hpq 434\nI0329 22:01:02.510873       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/dsnh 323\nI0329 22:01:02.710798       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/kb49 283\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Mar 29 22:01:02.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete pod logs-generator --namespace=kubectl-8225'
Mar 29 22:01:04.960: INFO: stderr: ""
Mar 29 22:01:04.960: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:01:04.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8225" for this suite.

• [SLOW TEST:8.147 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1284
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":277,"completed":192,"skipped":3138,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:01:04.990: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 22:01:05.232: INFO: Waiting up to 5m0s for pod "downwardapi-volume-206b420b-4288-488f-a244-2564a2d85489" in namespace "downward-api-6263" to be "Succeeded or Failed"
Mar 29 22:01:05.242: INFO: Pod "downwardapi-volume-206b420b-4288-488f-a244-2564a2d85489": Phase="Pending", Reason="", readiness=false. Elapsed: 9.98559ms
Mar 29 22:01:07.253: INFO: Pod "downwardapi-volume-206b420b-4288-488f-a244-2564a2d85489": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020882322s
STEP: Saw pod success
Mar 29 22:01:07.253: INFO: Pod "downwardapi-volume-206b420b-4288-488f-a244-2564a2d85489" satisfied condition "Succeeded or Failed"
Mar 29 22:01:07.263: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-206b420b-4288-488f-a244-2564a2d85489 container client-container: <nil>
STEP: delete the pod
Mar 29 22:01:07.318: INFO: Waiting for pod downwardapi-volume-206b420b-4288-488f-a244-2564a2d85489 to disappear
Mar 29 22:01:07.327: INFO: Pod downwardapi-volume-206b420b-4288-488f-a244-2564a2d85489 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:01:07.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6263" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":193,"skipped":3156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:01:07.355: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-1059
STEP: creating replication controller nodeport-test in namespace services-1059
I0329 22:01:07.650320      26 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-1059, replica count: 2
I0329 22:01:10.700786      26 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0329 22:01:13.701091      26 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 22:01:13.701: INFO: Creating new exec pod
Mar 29 22:01:18.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-1059 execpodjmwkc -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 29 22:01:19.073: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 29 22:01:19.073: INFO: stdout: ""
Mar 29 22:01:19.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-1059 execpodjmwkc -- /bin/sh -x -c nc -zv -t -w 2 172.21.236.223 80'
Mar 29 22:01:19.353: INFO: stderr: "+ nc -zv -t -w 2 172.21.236.223 80\nConnection to 172.21.236.223 80 port [tcp/http] succeeded!\n"
Mar 29 22:01:19.353: INFO: stdout: ""
Mar 29 22:01:19.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-1059 execpodjmwkc -- /bin/sh -x -c nc -zv -t -w 2 10.189.118.194 32033'
Mar 29 22:01:19.650: INFO: stderr: "+ nc -zv -t -w 2 10.189.118.194 32033\nConnection to 10.189.118.194 32033 port [tcp/32033] succeeded!\n"
Mar 29 22:01:19.650: INFO: stdout: ""
Mar 29 22:01:19.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-1059 execpodjmwkc -- /bin/sh -x -c nc -zv -t -w 2 10.189.118.196 32033'
Mar 29 22:01:19.950: INFO: stderr: "+ nc -zv -t -w 2 10.189.118.196 32033\nConnection to 10.189.118.196 32033 port [tcp/32033] succeeded!\n"
Mar 29 22:01:19.950: INFO: stdout: ""
Mar 29 22:01:19.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-1059 execpodjmwkc -- /bin/sh -x -c nc -zv -t -w 2 169.63.164.3 32033'
Mar 29 22:01:20.250: INFO: stderr: "+ nc -zv -t -w 2 169.63.164.3 32033\nConnection to 169.63.164.3 32033 port [tcp/32033] succeeded!\n"
Mar 29 22:01:20.250: INFO: stdout: ""
Mar 29 22:01:20.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-1059 execpodjmwkc -- /bin/sh -x -c nc -zv -t -w 2 169.63.164.5 32033'
Mar 29 22:01:20.529: INFO: stderr: "+ nc -zv -t -w 2 169.63.164.5 32033\nConnection to 169.63.164.5 32033 port [tcp/32033] succeeded!\n"
Mar 29 22:01:20.529: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:01:20.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1059" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:13.205 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":277,"completed":194,"skipped":3182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:01:20.560: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1705
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 22:01:20.783: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:01:27.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1705" for this suite.

• [SLOW TEST:6.902 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":277,"completed":195,"skipped":3209,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:01:27.465: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Mar 29 22:01:27.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 api-versions'
Mar 29 22:01:27.836: INFO: stderr: ""
Mar 29 22:01:27.836: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:01:27.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1545" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":277,"completed":196,"skipped":3221,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:01:27.862: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-txl7
STEP: Creating a pod to test atomic-volume-subpath
Mar 29 22:01:28.155: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-txl7" in namespace "subpath-9548" to be "Succeeded or Failed"
Mar 29 22:01:28.163: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.563318ms
Mar 29 22:01:30.173: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Running", Reason="", readiness=true. Elapsed: 2.018226354s
Mar 29 22:01:32.185: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Running", Reason="", readiness=true. Elapsed: 4.030345902s
Mar 29 22:01:34.197: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Running", Reason="", readiness=true. Elapsed: 6.042207721s
Mar 29 22:01:36.208: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Running", Reason="", readiness=true. Elapsed: 8.053372354s
Mar 29 22:01:38.219: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Running", Reason="", readiness=true. Elapsed: 10.064072209s
Mar 29 22:01:40.232: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Running", Reason="", readiness=true. Elapsed: 12.076712229s
Mar 29 22:01:42.241: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Running", Reason="", readiness=true. Elapsed: 14.086265029s
Mar 29 22:01:44.252: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Running", Reason="", readiness=true. Elapsed: 16.097494481s
Mar 29 22:01:46.265: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Running", Reason="", readiness=true. Elapsed: 18.1104667s
Mar 29 22:01:48.276: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Running", Reason="", readiness=true. Elapsed: 20.121281915s
Mar 29 22:01:50.288: INFO: Pod "pod-subpath-test-downwardapi-txl7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.132671287s
STEP: Saw pod success
Mar 29 22:01:50.288: INFO: Pod "pod-subpath-test-downwardapi-txl7" satisfied condition "Succeeded or Failed"
Mar 29 22:01:50.298: INFO: Trying to get logs from node 10.189.118.196 pod pod-subpath-test-downwardapi-txl7 container test-container-subpath-downwardapi-txl7: <nil>
STEP: delete the pod
Mar 29 22:01:50.354: INFO: Waiting for pod pod-subpath-test-downwardapi-txl7 to disappear
Mar 29 22:01:50.363: INFO: Pod pod-subpath-test-downwardapi-txl7 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-txl7
Mar 29 22:01:50.363: INFO: Deleting pod "pod-subpath-test-downwardapi-txl7" in namespace "subpath-9548"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:01:50.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9548" for this suite.

• [SLOW TEST:22.539 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":277,"completed":197,"skipped":3222,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:01:50.402: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3293
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 29 22:01:50.620: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:01:54.351: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:02:10.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3293" for this suite.

• [SLOW TEST:19.827 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":277,"completed":198,"skipped":3223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:02:10.230: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 22:02:10.470: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eaf0fe21-8db8-4f06-9edf-4812aac7e49c" in namespace "downward-api-7930" to be "Succeeded or Failed"
Mar 29 22:02:10.481: INFO: Pod "downwardapi-volume-eaf0fe21-8db8-4f06-9edf-4812aac7e49c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.308181ms
Mar 29 22:02:12.492: INFO: Pod "downwardapi-volume-eaf0fe21-8db8-4f06-9edf-4812aac7e49c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021859752s
STEP: Saw pod success
Mar 29 22:02:12.492: INFO: Pod "downwardapi-volume-eaf0fe21-8db8-4f06-9edf-4812aac7e49c" satisfied condition "Succeeded or Failed"
Mar 29 22:02:12.505: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-eaf0fe21-8db8-4f06-9edf-4812aac7e49c container client-container: <nil>
STEP: delete the pod
Mar 29 22:02:12.568: INFO: Waiting for pod downwardapi-volume-eaf0fe21-8db8-4f06-9edf-4812aac7e49c to disappear
Mar 29 22:02:12.578: INFO: Pod downwardapi-volume-eaf0fe21-8db8-4f06-9edf-4812aac7e49c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:02:12.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7930" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":199,"skipped":3250,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:02:12.609: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-km8f
STEP: Creating a pod to test atomic-volume-subpath
Mar 29 22:02:12.889: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-km8f" in namespace "subpath-6122" to be "Succeeded or Failed"
Mar 29 22:02:12.902: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.105485ms
Mar 29 22:02:14.914: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024951581s
Mar 29 22:02:16.924: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Running", Reason="", readiness=true. Elapsed: 4.034878232s
Mar 29 22:02:18.935: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Running", Reason="", readiness=true. Elapsed: 6.046768331s
Mar 29 22:02:20.946: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Running", Reason="", readiness=true. Elapsed: 8.057274591s
Mar 29 22:02:22.957: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Running", Reason="", readiness=true. Elapsed: 10.06801358s
Mar 29 22:02:24.967: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Running", Reason="", readiness=true. Elapsed: 12.078008176s
Mar 29 22:02:26.977: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Running", Reason="", readiness=true. Elapsed: 14.088431324s
Mar 29 22:02:28.989: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Running", Reason="", readiness=true. Elapsed: 16.100520485s
Mar 29 22:02:31.000: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Running", Reason="", readiness=true. Elapsed: 18.111607739s
Mar 29 22:02:33.012: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Running", Reason="", readiness=true. Elapsed: 20.122847306s
Mar 29 22:02:35.022: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Running", Reason="", readiness=true. Elapsed: 22.132917448s
Mar 29 22:02:37.032: INFO: Pod "pod-subpath-test-configmap-km8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.143271008s
STEP: Saw pod success
Mar 29 22:02:37.032: INFO: Pod "pod-subpath-test-configmap-km8f" satisfied condition "Succeeded or Failed"
Mar 29 22:02:37.042: INFO: Trying to get logs from node 10.189.118.196 pod pod-subpath-test-configmap-km8f container test-container-subpath-configmap-km8f: <nil>
STEP: delete the pod
Mar 29 22:02:37.099: INFO: Waiting for pod pod-subpath-test-configmap-km8f to disappear
Mar 29 22:02:37.108: INFO: Pod pod-subpath-test-configmap-km8f no longer exists
STEP: Deleting pod pod-subpath-test-configmap-km8f
Mar 29 22:02:37.109: INFO: Deleting pod "pod-subpath-test-configmap-km8f" in namespace "subpath-6122"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:02:37.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6122" for this suite.

• [SLOW TEST:24.539 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":277,"completed":200,"skipped":3264,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:02:37.148: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2778
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 29 22:02:37.351: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:02:41.118: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:02:56.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2778" for this suite.

• [SLOW TEST:18.892 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":277,"completed":201,"skipped":3268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:02:56.041: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8839
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8839.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8839.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 29 22:03:00.425: INFO: DNS probes using dns-8839/dns-test-aedf6bbe-8e79-4e33-b49a-77d215f43da4 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:03:00.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8839" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":277,"completed":202,"skipped":3299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:03:00.490: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7215
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 29 22:03:00.709: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 29 22:03:15.288: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:03:19.108: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:03:34.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7215" for this suite.

• [SLOW TEST:34.081 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":277,"completed":203,"skipped":3360,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:03:34.571: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Mar 29 22:03:34.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-4405'
Mar 29 22:03:35.089: INFO: stderr: ""
Mar 29 22:03:35.089: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 29 22:03:35.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4405'
Mar 29 22:03:35.195: INFO: stderr: ""
Mar 29 22:03:35.195: INFO: stdout: "update-demo-nautilus-cn6qx update-demo-nautilus-vgtgk "
Mar 29 22:03:35.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-cn6qx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4405'
Mar 29 22:03:35.324: INFO: stderr: ""
Mar 29 22:03:35.324: INFO: stdout: ""
Mar 29 22:03:35.324: INFO: update-demo-nautilus-cn6qx is created but not running
Mar 29 22:03:40.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4405'
Mar 29 22:03:40.429: INFO: stderr: ""
Mar 29 22:03:40.429: INFO: stdout: "update-demo-nautilus-cn6qx update-demo-nautilus-vgtgk "
Mar 29 22:03:40.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-cn6qx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4405'
Mar 29 22:03:40.517: INFO: stderr: ""
Mar 29 22:03:40.517: INFO: stdout: "true"
Mar 29 22:03:40.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-cn6qx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4405'
Mar 29 22:03:40.605: INFO: stderr: ""
Mar 29 22:03:40.605: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 29 22:03:40.605: INFO: validating pod update-demo-nautilus-cn6qx
Mar 29 22:03:40.627: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 22:03:40.627: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 22:03:40.627: INFO: update-demo-nautilus-cn6qx is verified up and running
Mar 29 22:03:40.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-vgtgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4405'
Mar 29 22:03:40.717: INFO: stderr: ""
Mar 29 22:03:40.717: INFO: stdout: "true"
Mar 29 22:03:40.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-vgtgk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4405'
Mar 29 22:03:40.802: INFO: stderr: ""
Mar 29 22:03:40.802: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 29 22:03:40.802: INFO: validating pod update-demo-nautilus-vgtgk
Mar 29 22:03:40.822: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 22:03:40.822: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 22:03:40.822: INFO: update-demo-nautilus-vgtgk is verified up and running
STEP: using delete to clean up resources
Mar 29 22:03:40.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete --grace-period=0 --force -f - --namespace=kubectl-4405'
Mar 29 22:03:40.925: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 22:03:40.925: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 29 22:03:40.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4405'
Mar 29 22:03:41.044: INFO: stderr: "No resources found in kubectl-4405 namespace.\n"
Mar 29 22:03:41.044: INFO: stdout: ""
Mar 29 22:03:41.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -l name=update-demo --namespace=kubectl-4405 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 29 22:03:41.147: INFO: stderr: ""
Mar 29 22:03:41.147: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:03:41.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4405" for this suite.

• [SLOW TEST:6.609 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":277,"completed":204,"skipped":3363,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:03:41.180: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 22:03:41.792: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 22:03:43.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652221, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652221, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652221, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652221, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 22:03:46.865: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:03:59.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5547" for this suite.
STEP: Destroying namespace "webhook-5547-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.475 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":277,"completed":205,"skipped":3365,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:03:59.656: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8946
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8946
I0329 22:03:59.941399      26 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8946, replica count: 2
Mar 29 22:04:02.991: INFO: Creating new exec pod
I0329 22:04:02.991916      26 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 22:04:06.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-8946 execpod5cknl -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 29 22:04:06.342: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 29 22:04:06.342: INFO: stdout: ""
Mar 29 22:04:06.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-8946 execpod5cknl -- /bin/sh -x -c nc -zv -t -w 2 172.21.143.103 80'
Mar 29 22:04:06.649: INFO: stderr: "+ nc -zv -t -w 2 172.21.143.103 80\nConnection to 172.21.143.103 80 port [tcp/http] succeeded!\n"
Mar 29 22:04:06.649: INFO: stdout: ""
Mar 29 22:04:06.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-8946 execpod5cknl -- /bin/sh -x -c nc -zv -t -w 2 10.189.118.196 30031'
Mar 29 22:04:06.939: INFO: stderr: "+ nc -zv -t -w 2 10.189.118.196 30031\nConnection to 10.189.118.196 30031 port [tcp/30031] succeeded!\n"
Mar 29 22:04:06.939: INFO: stdout: ""
Mar 29 22:04:06.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-8946 execpod5cknl -- /bin/sh -x -c nc -zv -t -w 2 10.189.118.208 30031'
Mar 29 22:04:07.232: INFO: stderr: "+ nc -zv -t -w 2 10.189.118.208 30031\nConnection to 10.189.118.208 30031 port [tcp/30031] succeeded!\n"
Mar 29 22:04:07.232: INFO: stdout: ""
Mar 29 22:04:07.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-8946 execpod5cknl -- /bin/sh -x -c nc -zv -t -w 2 169.63.164.5 30031'
Mar 29 22:04:07.545: INFO: stderr: "+ nc -zv -t -w 2 169.63.164.5 30031\nConnection to 169.63.164.5 30031 port [tcp/30031] succeeded!\n"
Mar 29 22:04:07.545: INFO: stdout: ""
Mar 29 22:04:07.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-8946 execpod5cknl -- /bin/sh -x -c nc -zv -t -w 2 169.63.164.9 30031'
Mar 29 22:04:07.835: INFO: stderr: "+ nc -zv -t -w 2 169.63.164.9 30031\nConnection to 169.63.164.9 30031 port [tcp/30031] succeeded!\n"
Mar 29 22:04:07.835: INFO: stdout: ""
Mar 29 22:04:07.835: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:07.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8946" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:8.312 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":277,"completed":206,"skipped":3378,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:07.968: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:15.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5477" for this suite.

• [SLOW TEST:7.278 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":277,"completed":207,"skipped":3400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:15.247: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 29 22:04:15.478: INFO: Waiting up to 5m0s for pod "pod-2fe0372b-e14c-4264-ba91-c1e544e72617" in namespace "emptydir-4922" to be "Succeeded or Failed"
Mar 29 22:04:15.488: INFO: Pod "pod-2fe0372b-e14c-4264-ba91-c1e544e72617": Phase="Pending", Reason="", readiness=false. Elapsed: 9.906521ms
Mar 29 22:04:17.498: INFO: Pod "pod-2fe0372b-e14c-4264-ba91-c1e544e72617": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020556565s
Mar 29 22:04:19.513: INFO: Pod "pod-2fe0372b-e14c-4264-ba91-c1e544e72617": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035221503s
STEP: Saw pod success
Mar 29 22:04:19.513: INFO: Pod "pod-2fe0372b-e14c-4264-ba91-c1e544e72617" satisfied condition "Succeeded or Failed"
Mar 29 22:04:19.526: INFO: Trying to get logs from node 10.189.118.196 pod pod-2fe0372b-e14c-4264-ba91-c1e544e72617 container test-container: <nil>
STEP: delete the pod
Mar 29 22:04:19.618: INFO: Waiting for pod pod-2fe0372b-e14c-4264-ba91-c1e544e72617 to disappear
Mar 29 22:04:19.629: INFO: Pod pod-2fe0372b-e14c-4264-ba91-c1e544e72617 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:19.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4922" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":208,"skipped":3430,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:19.670: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 22:04:19.899: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-d586be10-002f-41fb-85b3-6ddd122825cd" in namespace "security-context-test-4621" to be "Succeeded or Failed"
Mar 29 22:04:19.909: INFO: Pod "alpine-nnp-false-d586be10-002f-41fb-85b3-6ddd122825cd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.434587ms
Mar 29 22:04:21.922: INFO: Pod "alpine-nnp-false-d586be10-002f-41fb-85b3-6ddd122825cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023547667s
Mar 29 22:04:23.935: INFO: Pod "alpine-nnp-false-d586be10-002f-41fb-85b3-6ddd122825cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036086993s
Mar 29 22:04:23.935: INFO: Pod "alpine-nnp-false-d586be10-002f-41fb-85b3-6ddd122825cd" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:23.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4621" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":209,"skipped":3444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:23.993: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4616
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:40.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4616" for this suite.

• [SLOW TEST:16.564 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":277,"completed":210,"skipped":3473,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:40.558: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:40.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6920" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":277,"completed":211,"skipped":3480,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:40.812: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 29 22:04:41.026: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 29 22:04:41.072: INFO: Waiting for terminating namespaces to be deleted...
Mar 29 22:04:41.081: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.194 before test
Mar 29 22:04:41.199: INFO: sonobuoy from sonobuoy started at 2021-03-29 21:07:51 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.199: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 29 22:04:41.199: INFO: public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-vbjdf from kube-system started at 2021-03-29 20:11:55 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.199: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 29 22:04:41.199: INFO: addon-catalog-source-lgq2p from ibm-system started at 2021-03-29 19:53:09 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.199: INFO: 	Container configmap-registry-server ready: true, restart count 0
Mar 29 22:04:41.199: INFO: sonobuoy-e2e-job-95ac96a8209247c9 from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 22:04:41.199: INFO: 	Container e2e ready: true, restart count 0
Mar 29 22:04:41.200: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 22:04:41.200: INFO: ibm-keepalived-watcher-5j4dz from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.200: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 22:04:41.200: INFO: calico-node-9tz6n from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.200: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 22:04:41.200: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-ngv5d from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 22:04:41.200: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 22:04:41.200: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 22:04:41.200: INFO: ibm-master-proxy-static-10.189.118.194 from kube-system started at 2021-03-29 19:48:41 +0000 UTC (2 container statuses recorded)
Mar 29 22:04:41.200: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 22:04:41.200: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:04:41.200: INFO: coredns-7cc79848cf-8xnhq from kube-system started at 2021-03-29 19:57:06 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.200: INFO: 	Container coredns ready: true, restart count 0
Mar 29 22:04:41.200: INFO: ibm-cloud-provider-ip-169-63-130-250-65c6959456-tm4wj from ibm-system started at 2021-03-29 20:12:35 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.200: INFO: 	Container ibm-cloud-provider-ip-169-63-130-250 ready: true, restart count 0
Mar 29 22:04:41.201: INFO: ibm-kubelet-monitor-jrnhz from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.201: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:04:41.201: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.196 before test
Mar 29 22:04:41.235: INFO: ibm-keepalived-watcher-4x7cv from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.235: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 22:04:41.235: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-l4nd5 from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 22:04:41.235: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 22:04:41.235: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 22:04:41.235: INFO: calico-node-mtdnd from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.235: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 22:04:41.235: INFO: ibm-master-proxy-static-10.189.118.196 from kube-system started at 2021-03-29 19:48:58 +0000 UTC (2 container statuses recorded)
Mar 29 22:04:41.235: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 22:04:41.235: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:04:41.235: INFO: ibm-kubelet-monitor-cmcxj from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.235: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:04:41.235: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.208 before test
Mar 29 22:04:41.341: INFO: calico-node-zjnmd from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.341: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 22:04:41.341: INFO: kubernetes-dashboard-6cf6cfdf4-kzffs from kube-system started at 2021-03-29 19:49:50 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.341: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 29 22:04:41.341: INFO: ibm-cloud-provider-ip-169-63-130-250-65c6959456-gps6w from ibm-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.341: INFO: 	Container ibm-cloud-provider-ip-169-63-130-250 ready: true, restart count 0
Mar 29 22:04:41.341: INFO: public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-2lcqz from kube-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.341: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 29 22:04:41.341: INFO: ibm-kubelet-monitor-gbcvf from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.341: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:04:41.341: INFO: vpn-657cb5cdb6-v5h8m from kube-system started at 2021-03-29 19:56:31 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.341: INFO: 	Container vpn ready: true, restart count 0
Mar 29 22:04:41.341: INFO: ibm-storage-watcher-659bdcc695-qqkhw from kube-system started at 2021-03-29 19:49:50 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.341: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 29 22:04:41.342: INFO: ibm-keepalived-watcher-ps2vt from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.342: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 22:04:41.342: INFO: olm-operator-8496678794-gg5v7 from ibm-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.342: INFO: 	Container olm-operator ready: true, restart count 0
Mar 29 22:04:41.342: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-n7lmz from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 22:04:41.342: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 22:04:41.342: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 22:04:41.342: INFO: coredns-autoscaler-bff977695-9wkhn from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.342: INFO: 	Container autoscaler ready: true, restart count 0
Mar 29 22:04:41.342: INFO: calico-kube-controllers-6599f97f59-7g48l from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.342: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 29 22:04:41.342: INFO: catalog-operator-7bc4c797b5-kbbnz from ibm-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.342: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 29 22:04:41.342: INFO: dashboard-metrics-scraper-b585c6867-2k8ww from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.342: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 29 22:04:41.342: INFO: metrics-server-f9f7549df-6xdmf from kube-system started at 2021-03-29 19:50:53 +0000 UTC (2 container statuses recorded)
Mar 29 22:04:41.342: INFO: 	Container metrics-server ready: true, restart count 0
Mar 29 22:04:41.342: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 29 22:04:41.342: INFO: ibm-master-proxy-static-10.189.118.208 from kube-system started at 2021-03-29 19:48:43 +0000 UTC (2 container statuses recorded)
Mar 29 22:04:41.343: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 22:04:41.343: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:04:41.343: INFO: coredns-7cc79848cf-kjg7p from kube-system started at 2021-03-29 19:57:07 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.343: INFO: 	Container coredns ready: true, restart count 0
Mar 29 22:04:41.343: INFO: coredns-7cc79848cf-m5klb from kube-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.343: INFO: 	Container coredns ready: true, restart count 0
Mar 29 22:04:41.343: INFO: ibm-file-plugin-c76c68fd9-bpt7r from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:04:41.343: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1670ef649038031d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:42.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-193" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":277,"completed":212,"skipped":3480,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:42.455: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-36ecaa9d-9225-4e5b-9c57-15e3854e6e87
STEP: Creating a pod to test consume secrets
Mar 29 22:04:42.696: INFO: Waiting up to 5m0s for pod "pod-secrets-3b52c0d5-417f-488f-a13d-08989bbbde48" in namespace "secrets-8062" to be "Succeeded or Failed"
Mar 29 22:04:42.711: INFO: Pod "pod-secrets-3b52c0d5-417f-488f-a13d-08989bbbde48": Phase="Pending", Reason="", readiness=false. Elapsed: 14.11281ms
Mar 29 22:04:44.723: INFO: Pod "pod-secrets-3b52c0d5-417f-488f-a13d-08989bbbde48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0266551s
Mar 29 22:04:46.738: INFO: Pod "pod-secrets-3b52c0d5-417f-488f-a13d-08989bbbde48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041671612s
STEP: Saw pod success
Mar 29 22:04:46.738: INFO: Pod "pod-secrets-3b52c0d5-417f-488f-a13d-08989bbbde48" satisfied condition "Succeeded or Failed"
Mar 29 22:04:46.754: INFO: Trying to get logs from node 10.189.118.196 pod pod-secrets-3b52c0d5-417f-488f-a13d-08989bbbde48 container secret-volume-test: <nil>
STEP: delete the pod
Mar 29 22:04:46.817: INFO: Waiting for pod pod-secrets-3b52c0d5-417f-488f-a13d-08989bbbde48 to disappear
Mar 29 22:04:46.829: INFO: Pod pod-secrets-3b52c0d5-417f-488f-a13d-08989bbbde48 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:46.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8062" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":213,"skipped":3489,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:46.862: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3304
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-878c2f70-0070-49de-93ea-92dc44313fa3
STEP: Creating a pod to test consume secrets
Mar 29 22:04:47.115: INFO: Waiting up to 5m0s for pod "pod-secrets-c2610742-799a-456e-b18a-77fbbca08366" in namespace "secrets-3304" to be "Succeeded or Failed"
Mar 29 22:04:47.132: INFO: Pod "pod-secrets-c2610742-799a-456e-b18a-77fbbca08366": Phase="Pending", Reason="", readiness=false. Elapsed: 17.042023ms
Mar 29 22:04:49.143: INFO: Pod "pod-secrets-c2610742-799a-456e-b18a-77fbbca08366": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028642174s
STEP: Saw pod success
Mar 29 22:04:49.143: INFO: Pod "pod-secrets-c2610742-799a-456e-b18a-77fbbca08366" satisfied condition "Succeeded or Failed"
Mar 29 22:04:49.153: INFO: Trying to get logs from node 10.189.118.196 pod pod-secrets-c2610742-799a-456e-b18a-77fbbca08366 container secret-volume-test: <nil>
STEP: delete the pod
Mar 29 22:04:49.218: INFO: Waiting for pod pod-secrets-c2610742-799a-456e-b18a-77fbbca08366 to disappear
Mar 29 22:04:49.228: INFO: Pod pod-secrets-c2610742-799a-456e-b18a-77fbbca08366 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:49.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3304" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":214,"skipped":3492,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:49.259: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-610
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-977216a2-c52f-4b4f-a752-02f1e8b488e5
STEP: Creating a pod to test consume configMaps
Mar 29 22:04:49.608: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7535638e-7f35-4fa6-990a-d2ac9e4d94fd" in namespace "projected-610" to be "Succeeded or Failed"
Mar 29 22:04:49.618: INFO: Pod "pod-projected-configmaps-7535638e-7f35-4fa6-990a-d2ac9e4d94fd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.710501ms
Mar 29 22:04:51.629: INFO: Pod "pod-projected-configmaps-7535638e-7f35-4fa6-990a-d2ac9e4d94fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020938745s
Mar 29 22:04:53.640: INFO: Pod "pod-projected-configmaps-7535638e-7f35-4fa6-990a-d2ac9e4d94fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032589536s
STEP: Saw pod success
Mar 29 22:04:53.641: INFO: Pod "pod-projected-configmaps-7535638e-7f35-4fa6-990a-d2ac9e4d94fd" satisfied condition "Succeeded or Failed"
Mar 29 22:04:53.651: INFO: Trying to get logs from node 10.189.118.196 pod pod-projected-configmaps-7535638e-7f35-4fa6-990a-d2ac9e4d94fd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 22:04:53.712: INFO: Waiting for pod pod-projected-configmaps-7535638e-7f35-4fa6-990a-d2ac9e4d94fd to disappear
Mar 29 22:04:53.722: INFO: Pod pod-projected-configmaps-7535638e-7f35-4fa6-990a-d2ac9e4d94fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:53.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-610" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":215,"skipped":3505,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:53.753: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2668
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-650bfe61-3783-4100-98cf-c8adc6d17b43
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-650bfe61-3783-4100-98cf-c8adc6d17b43
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:04:58.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2668" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":216,"skipped":3518,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:04:58.199: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3455
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-3455
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 29 22:04:58.403: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 29 22:04:58.490: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 22:05:00.501: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:05:02.502: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:05:04.505: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:05:06.503: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:05:08.506: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:05:10.501: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:05:12.501: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:05:14.504: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:05:16.500: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 29 22:05:16.524: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 29 22:05:16.543: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar 29 22:05:18.557: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 29 22:05:22.626: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.45:8080/dial?request=hostname&protocol=http&host=172.30.124.162&port=8080&tries=1'] Namespace:pod-network-test-3455 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:05:22.626: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:05:22.858: INFO: Waiting for responses: map[]
Mar 29 22:05:22.870: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.45:8080/dial?request=hostname&protocol=http&host=172.30.58.47&port=8080&tries=1'] Namespace:pod-network-test-3455 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:05:22.870: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:05:23.066: INFO: Waiting for responses: map[]
Mar 29 22:05:23.078: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.45:8080/dial?request=hostname&protocol=http&host=172.30.209.246&port=8080&tries=1'] Namespace:pod-network-test-3455 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:05:23.078: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:05:23.268: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:05:23.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3455" for this suite.

• [SLOW TEST:25.096 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":277,"completed":217,"skipped":3525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:05:23.297: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 22:05:24.004: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 22:05:26.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652324, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652324, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652324, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652323, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 22:05:29.078: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:05:29.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8428" for this suite.
STEP: Destroying namespace "webhook-8428-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.211 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":277,"completed":218,"skipped":3557,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:05:29.508: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2710
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 29 22:05:29.750: INFO: Waiting up to 5m0s for pod "downward-api-56a32066-4855-4008-8083-651d2068331e" in namespace "downward-api-2710" to be "Succeeded or Failed"
Mar 29 22:05:29.760: INFO: Pod "downward-api-56a32066-4855-4008-8083-651d2068331e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.218495ms
Mar 29 22:05:31.771: INFO: Pod "downward-api-56a32066-4855-4008-8083-651d2068331e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020250426s
Mar 29 22:05:33.782: INFO: Pod "downward-api-56a32066-4855-4008-8083-651d2068331e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031337962s
STEP: Saw pod success
Mar 29 22:05:33.782: INFO: Pod "downward-api-56a32066-4855-4008-8083-651d2068331e" satisfied condition "Succeeded or Failed"
Mar 29 22:05:33.792: INFO: Trying to get logs from node 10.189.118.196 pod downward-api-56a32066-4855-4008-8083-651d2068331e container dapi-container: <nil>
STEP: delete the pod
Mar 29 22:05:33.854: INFO: Waiting for pod downward-api-56a32066-4855-4008-8083-651d2068331e to disappear
Mar 29 22:05:33.866: INFO: Pod downward-api-56a32066-4855-4008-8083-651d2068331e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:05:33.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2710" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":277,"completed":219,"skipped":3573,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:05:33.903: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8419
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 29 22:05:34.135: INFO: Waiting up to 5m0s for pod "pod-e269736a-11fb-41ef-ad4c-68ff18b46a61" in namespace "emptydir-8419" to be "Succeeded or Failed"
Mar 29 22:05:34.144: INFO: Pod "pod-e269736a-11fb-41ef-ad4c-68ff18b46a61": Phase="Pending", Reason="", readiness=false. Elapsed: 9.80368ms
Mar 29 22:05:36.156: INFO: Pod "pod-e269736a-11fb-41ef-ad4c-68ff18b46a61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021099283s
Mar 29 22:05:38.166: INFO: Pod "pod-e269736a-11fb-41ef-ad4c-68ff18b46a61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031691574s
STEP: Saw pod success
Mar 29 22:05:38.166: INFO: Pod "pod-e269736a-11fb-41ef-ad4c-68ff18b46a61" satisfied condition "Succeeded or Failed"
Mar 29 22:05:38.177: INFO: Trying to get logs from node 10.189.118.196 pod pod-e269736a-11fb-41ef-ad4c-68ff18b46a61 container test-container: <nil>
STEP: delete the pod
Mar 29 22:05:38.258: INFO: Waiting for pod pod-e269736a-11fb-41ef-ad4c-68ff18b46a61 to disappear
Mar 29 22:05:38.269: INFO: Pod pod-e269736a-11fb-41ef-ad4c-68ff18b46a61 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:05:38.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8419" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":220,"skipped":3595,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:05:38.306: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4557
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 29 22:05:41.138: INFO: Successfully updated pod "labelsupdate8bf108ed-2038-4c7f-a4ba-14cdf74c6b46"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:05:43.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4557" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":221,"skipped":3616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:05:43.219: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-8859
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-8859
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8859
Mar 29 22:05:43.473: INFO: Found 0 stateful pods, waiting for 1
Mar 29 22:05:53.485: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 29 22:05:53.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 22:05:53.829: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 22:05:53.829: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 22:05:53.829: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 22:05:53.838: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 29 22:06:03.850: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 22:06:03.850: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 22:06:03.912: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:03.912: INFO: ss-0  10.189.118.196  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:03.912: INFO: 
Mar 29 22:06:03.912: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 29 22:06:04.924: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9834618s
Mar 29 22:06:05.936: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97162849s
Mar 29 22:06:06.949: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.959659912s
Mar 29 22:06:07.963: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.946805895s
Mar 29 22:06:08.975: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.932399802s
Mar 29 22:06:09.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.920492032s
Mar 29 22:06:10.998: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.908351294s
Mar 29 22:06:12.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.897509527s
Mar 29 22:06:13.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 848.05208ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8859
Mar 29 22:06:14.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:06:14.365: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 22:06:14.365: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 22:06:14.365: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 22:06:14.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:06:14.649: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 29 22:06:14.649: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 22:06:14.649: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 22:06:14.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:06:14.931: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 29 22:06:14.932: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 22:06:14.932: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 22:06:14.944: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 22:06:14.944: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 22:06:14.944: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 29 22:06:14.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 22:06:15.228: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 22:06:15.228: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 22:06:15.228: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 22:06:15.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 22:06:15.502: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 22:06:15.502: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 22:06:15.502: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 22:06:15.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 22:06:15.760: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 22:06:15.760: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 22:06:15.760: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 22:06:15.760: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 22:06:15.774: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 29 22:06:25.801: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 22:06:25.801: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 22:06:25.801: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 22:06:25.843: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:25.843: INFO: ss-0  10.189.118.196  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:25.843: INFO: ss-1  10.189.118.194  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:25.843: INFO: ss-2  10.189.118.208  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:25.843: INFO: 
Mar 29 22:06:25.843: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 29 22:06:26.856: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:26.856: INFO: ss-0  10.189.118.196  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:26.856: INFO: ss-1  10.189.118.194  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:26.856: INFO: ss-2  10.189.118.208  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:26.856: INFO: 
Mar 29 22:06:26.856: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 29 22:06:27.871: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:27.871: INFO: ss-0  10.189.118.196  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:27.871: INFO: ss-1  10.189.118.194  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:27.871: INFO: ss-2  10.189.118.208  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:27.871: INFO: 
Mar 29 22:06:27.871: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 29 22:06:28.887: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:28.887: INFO: ss-0  10.189.118.196  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:28.887: INFO: ss-1  10.189.118.194  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:28.887: INFO: ss-2  10.189.118.208  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:28.887: INFO: 
Mar 29 22:06:28.887: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 29 22:06:29.899: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:29.899: INFO: ss-0  10.189.118.196  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:29.899: INFO: ss-1  10.189.118.194  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:29.899: INFO: ss-2  10.189.118.208  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:29.899: INFO: 
Mar 29 22:06:29.899: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 29 22:06:30.911: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:30.911: INFO: ss-0  10.189.118.196  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:30.911: INFO: ss-2  10.189.118.208  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:30.911: INFO: 
Mar 29 22:06:30.911: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 29 22:06:31.951: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:31.951: INFO: ss-0  10.189.118.196  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:31.951: INFO: ss-2  10.189.118.208  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:03 +0000 UTC  }]
Mar 29 22:06:31.951: INFO: 
Mar 29 22:06:31.951: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 29 22:06:32.962: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:32.962: INFO: ss-0  10.189.118.196  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:32.962: INFO: 
Mar 29 22:06:32.962: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 29 22:06:33.977: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:33.977: INFO: ss-0  10.189.118.196  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:33.977: INFO: 
Mar 29 22:06:33.977: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 29 22:06:35.073: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 29 22:06:35.073: INFO: ss-0  10.189.118.196  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-29 22:05:43 +0000 UTC  }]
Mar 29 22:06:35.073: INFO: 
Mar 29 22:06:35.073: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8859
Mar 29 22:06:36.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:06:36.253: INFO: rc: 1
Mar 29 22:06:36.253: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar 29 22:06:46.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:06:46.368: INFO: rc: 1
Mar 29 22:06:46.368: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:06:56.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:06:56.472: INFO: rc: 1
Mar 29 22:06:56.472: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:07:06.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:07:06.579: INFO: rc: 1
Mar 29 22:07:06.579: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:07:16.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:07:16.675: INFO: rc: 1
Mar 29 22:07:16.675: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:07:26.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:07:26.784: INFO: rc: 1
Mar 29 22:07:26.784: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:07:36.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:07:36.894: INFO: rc: 1
Mar 29 22:07:36.894: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:07:46.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:07:46.993: INFO: rc: 1
Mar 29 22:07:46.993: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:07:56.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:07:57.094: INFO: rc: 1
Mar 29 22:07:57.094: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:08:07.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:08:07.190: INFO: rc: 1
Mar 29 22:08:07.190: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:08:17.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:08:17.289: INFO: rc: 1
Mar 29 22:08:17.289: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:08:27.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:08:27.380: INFO: rc: 1
Mar 29 22:08:27.380: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:08:37.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:08:37.490: INFO: rc: 1
Mar 29 22:08:37.490: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:08:47.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:08:47.590: INFO: rc: 1
Mar 29 22:08:47.590: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:08:57.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:08:57.686: INFO: rc: 1
Mar 29 22:08:57.686: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:09:07.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:09:07.782: INFO: rc: 1
Mar 29 22:09:07.783: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:09:17.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:09:17.878: INFO: rc: 1
Mar 29 22:09:17.878: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:09:27.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:09:27.994: INFO: rc: 1
Mar 29 22:09:27.994: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:09:37.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:09:38.088: INFO: rc: 1
Mar 29 22:09:38.088: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:09:48.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:09:48.186: INFO: rc: 1
Mar 29 22:09:48.186: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:09:58.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:09:58.290: INFO: rc: 1
Mar 29 22:09:58.290: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:10:08.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:10:08.387: INFO: rc: 1
Mar 29 22:10:08.387: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:10:18.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:10:18.493: INFO: rc: 1
Mar 29 22:10:18.493: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:10:28.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:10:28.591: INFO: rc: 1
Mar 29 22:10:28.591: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:10:38.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:10:38.687: INFO: rc: 1
Mar 29 22:10:38.687: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:10:48.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:10:48.786: INFO: rc: 1
Mar 29 22:10:48.787: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:10:58.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:10:58.995: INFO: rc: 1
Mar 29 22:10:58.995: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:11:08.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:11:09.095: INFO: rc: 1
Mar 29 22:11:09.095: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:11:19.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:11:19.197: INFO: rc: 1
Mar 29 22:11:19.197: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:11:29.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:11:29.308: INFO: rc: 1
Mar 29 22:11:29.308: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 29 22:11:39.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-8859 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:11:39.405: INFO: rc: 1
Mar 29 22:11:39.405: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Mar 29 22:11:39.405: INFO: Scaling statefulset ss to 0
Mar 29 22:11:39.448: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 29 22:11:39.463: INFO: Deleting all statefulset in ns statefulset-8859
Mar 29 22:11:39.474: INFO: Scaling statefulset ss to 0
Mar 29 22:11:39.515: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 22:11:39.528: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:11:39.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8859" for this suite.

• [SLOW TEST:356.397 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":277,"completed":222,"skipped":3640,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:11:39.617: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5040
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5040.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5040.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5040.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5040.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5040.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5040.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 29 22:11:44.056: INFO: DNS probes using dns-5040/dns-test-05880ba2-f95e-41f7-bb0c-e815374830e4 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:11:44.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5040" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":277,"completed":223,"skipped":3653,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:11:44.121: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 29 22:11:44.354: INFO: Waiting up to 5m0s for pod "downward-api-56dfbdc5-1ad9-4bb3-905e-5511e2e3cba4" in namespace "downward-api-4284" to be "Succeeded or Failed"
Mar 29 22:11:44.372: INFO: Pod "downward-api-56dfbdc5-1ad9-4bb3-905e-5511e2e3cba4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.859612ms
Mar 29 22:11:46.383: INFO: Pod "downward-api-56dfbdc5-1ad9-4bb3-905e-5511e2e3cba4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028680803s
Mar 29 22:11:48.393: INFO: Pod "downward-api-56dfbdc5-1ad9-4bb3-905e-5511e2e3cba4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038633192s
STEP: Saw pod success
Mar 29 22:11:48.393: INFO: Pod "downward-api-56dfbdc5-1ad9-4bb3-905e-5511e2e3cba4" satisfied condition "Succeeded or Failed"
Mar 29 22:11:48.406: INFO: Trying to get logs from node 10.189.118.196 pod downward-api-56dfbdc5-1ad9-4bb3-905e-5511e2e3cba4 container dapi-container: <nil>
STEP: delete the pod
Mar 29 22:11:48.519: INFO: Waiting for pod downward-api-56dfbdc5-1ad9-4bb3-905e-5511e2e3cba4 to disappear
Mar 29 22:11:48.527: INFO: Pod downward-api-56dfbdc5-1ad9-4bb3-905e-5511e2e3cba4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:11:48.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4284" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":277,"completed":224,"skipped":3669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:11:48.559: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 29 22:11:48.787: INFO: Waiting up to 5m0s for pod "pod-1ab550b5-6b2b-4d7d-acbb-580746f06fa3" in namespace "emptydir-9252" to be "Succeeded or Failed"
Mar 29 22:11:48.798: INFO: Pod "pod-1ab550b5-6b2b-4d7d-acbb-580746f06fa3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.859708ms
Mar 29 22:11:50.810: INFO: Pod "pod-1ab550b5-6b2b-4d7d-acbb-580746f06fa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022864728s
STEP: Saw pod success
Mar 29 22:11:50.811: INFO: Pod "pod-1ab550b5-6b2b-4d7d-acbb-580746f06fa3" satisfied condition "Succeeded or Failed"
Mar 29 22:11:50.821: INFO: Trying to get logs from node 10.189.118.196 pod pod-1ab550b5-6b2b-4d7d-acbb-580746f06fa3 container test-container: <nil>
STEP: delete the pod
Mar 29 22:11:50.886: INFO: Waiting for pod pod-1ab550b5-6b2b-4d7d-acbb-580746f06fa3 to disappear
Mar 29 22:11:50.899: INFO: Pod pod-1ab550b5-6b2b-4d7d-acbb-580746f06fa3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:11:50.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9252" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":225,"skipped":3782,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:11:50.933: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 22:11:51.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-2331'
Mar 29 22:11:51.375: INFO: stderr: ""
Mar 29 22:11:51.375: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Mar 29 22:11:51.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-2331'
Mar 29 22:11:51.734: INFO: stderr: ""
Mar 29 22:11:51.734: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 29 22:11:52.747: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 22:11:52.747: INFO: Found 0 / 1
Mar 29 22:11:53.745: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 22:11:53.745: INFO: Found 1 / 1
Mar 29 22:11:53.745: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 29 22:11:53.760: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 22:11:53.760: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 29 22:11:53.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 describe pod agnhost-master-h85fp --namespace=kubectl-2331'
Mar 29 22:11:53.893: INFO: stderr: ""
Mar 29 22:11:53.893: INFO: stdout: "Name:         agnhost-master-h85fp\nNamespace:    kubectl-2331\nPriority:     0\nNode:         10.189.118.196/10.189.118.196\nStart Time:   Mon, 29 Mar 2021 22:11:51 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.30.58.58\nIPs:\n  IP:           172.30.58.58\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   containerd://98482b5449a315861ca003e3c7c045776b7c6414459d0b230a6831a7dadcd083\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 29 Mar 2021 22:11:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nqswz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-nqswz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-nqswz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 600s\n                 node.kubernetes.io/unreachable:NoExecute for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2331/agnhost-master-h85fp to 10.189.118.196\n  Normal  Pulled     1s    kubelet            Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-master\n  Normal  Started    1s    kubelet            Started container agnhost-master\n"
Mar 29 22:11:53.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 describe rc agnhost-master --namespace=kubectl-2331'
Mar 29 22:11:54.019: INFO: stderr: ""
Mar 29 22:11:54.019: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-2331\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-h85fp\n"
Mar 29 22:11:54.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 describe service agnhost-master --namespace=kubectl-2331'
Mar 29 22:11:54.140: INFO: stderr: ""
Mar 29 22:11:54.140: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-2331\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                172.21.69.190\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.58.58:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 29 22:11:54.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 describe node 10.189.118.194'
Mar 29 22:11:54.350: INFO: stderr: ""
Mar 29 22:11:54.350: INFO: stdout: "Name:               10.189.118.194\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east\n                    failure-domain.beta.kubernetes.io/zone=wdc06\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.63.164.3\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.189.118.194\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=us-east\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-c1h2ld5w0208nq2let2g-kubee2epvga-default-00000349\n                    ibm-cloud.kubernetes.io/worker-pool-id=c1h2ld5w0208nq2let2g-a9f8010\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.18.16_1545\n                    ibm-cloud.kubernetes.io/zone=wdc06\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.189.118.194\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2722950\n                    publicVLAN=2722948\n                    topology.kubernetes.io/region=us-east\n                    topology.kubernetes.io/zone=wdc06\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 29 Mar 2021 19:48:58 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.189.118.194\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 29 Mar 2021 22:11:53 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 29 Mar 2021 22:08:47 +0000   Mon, 29 Mar 2021 19:48:58 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 29 Mar 2021 22:08:47 +0000   Mon, 29 Mar 2021 19:48:58 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 29 Mar 2021 22:08:47 +0000   Mon, 29 Mar 2021 19:48:58 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 29 Mar 2021 22:08:47 +0000   Mon, 29 Mar 2021 19:49:58 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.189.118.194\n  ExternalIP:  169.63.164.3\n  Hostname:    10.189.118.194\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102685624Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16398252Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93986994917\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13607852Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 96529a02d7fc4df9bbbde55c9899e08c\n  System UUID:                97D51A5B-5694-70A5-35C6-6B708CA24036\n  Boot ID:                    c1992a57-0f9b-4a4e-ac23-47d6e11bac8e\n  Kernel Version:             4.15.0-136-generic\n  OS Image:                   Ubuntu 18.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.10\n  Kubelet Version:            v1.18.16+IKS\n  Kube-Proxy Version:         v1.18.16+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///c1h2ld5w0208nq2let2g/kube-c1h2ld5w0208nq2let2g-kubee2epvga-default-00000349\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                  addon-catalog-source-lgq2p                                 10m (0%)      100m (2%)   50Mi (0%)        100Mi (0%)     138m\n  ibm-system                  ibm-cloud-provider-ip-169-63-130-250-65c6959456-tm4wj      5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         119m\n  kube-system                 calico-node-9tz6n                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         142m\n  kube-system                 coredns-7cc79848cf-8xnhq                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     134m\n  kube-system                 ibm-keepalived-watcher-5j4dz                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         142m\n  kube-system                 ibm-kubelet-monitor-jrnhz                                  10m (0%)      0 (0%)      10M (0%)         0 (0%)         142m\n  kube-system                 ibm-master-proxy-static-10.189.118.194                     25m (0%)      300m (7%)   32M (0%)         512M (3%)      142m\n  kube-system                 public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-vbjdf        10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         119m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\n  sonobuoy                    sonobuoy-e2e-job-95ac96a8209247c9                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-ngv5d    0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                415m (10%)      400m (10%)\n  memory             377544320 (2%)  1012000Ki (7%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
Mar 29 22:11:54.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 describe namespace kubectl-2331'
Mar 29 22:11:54.481: INFO: stderr: ""
Mar 29 22:11:54.481: INFO: stdout: "Name:         kubectl-2331\nLabels:       e2e-framework=kubectl\n              e2e-run=82791058-0d6f-4387-bc6a-2d2f137fddeb\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:11:54.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2331" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":277,"completed":226,"skipped":3784,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:11:54.510: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4470
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4470
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4470
STEP: creating replication controller externalsvc in namespace services-4470
I0329 22:11:54.818563      26 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4470, replica count: 2
I0329 22:11:57.869103      26 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 29 22:11:57.938: INFO: Creating new exec pod
Mar 29 22:11:59.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=services-4470 execpodck4lx -- /bin/sh -x -c nslookup clusterip-service'
Mar 29 22:12:00.286: INFO: stderr: "+ nslookup clusterip-service\n"
Mar 29 22:12:00.286: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-4470.svc.cluster.local\tcanonical name = externalsvc.services-4470.svc.cluster.local.\nName:\texternalsvc.services-4470.svc.cluster.local\nAddress: 172.21.24.181\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4470, will wait for the garbage collector to delete the pods
Mar 29 22:12:00.389: INFO: Deleting ReplicationController externalsvc took: 25.552847ms
Mar 29 22:12:00.489: INFO: Terminating ReplicationController externalsvc pods took: 100.245384ms
Mar 29 22:12:10.670: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:12:10.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4470" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:16.244 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":277,"completed":227,"skipped":3805,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:12:10.754: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2174
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 29 22:12:10.998: INFO: Created pod &Pod{ObjectMeta:{dns-2174  dns-2174 /api/v1/namespaces/dns-2174/pods/dns-2174 873d4377-6f81-43bb-9e8d-cb633a117d32 50173 0 2021-03-29 22:12:10 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-03-29 22:12:10 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 67 111 110 102 105 103 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 115 101 114 118 101 114 115 34 58 123 125 44 34 102 58 115 101 97 114 99 104 101 115 34 58 123 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hvmp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hvmp7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hvmp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 22:12:11.015: INFO: The status of Pod dns-2174 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 22:12:13.026: INFO: The status of Pod dns-2174 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 29 22:12:13.026: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2174 PodName:dns-2174 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:12:13.026: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Verifying customized DNS server is configured on pod...
Mar 29 22:12:13.224: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2174 PodName:dns-2174 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:12:13.224: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:12:13.441: INFO: Deleting pod dns-2174...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:12:13.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2174" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":277,"completed":228,"skipped":3814,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:12:13.510: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 22:12:13.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26ff30c0-78fb-47cc-add8-97e603b6eae4" in namespace "projected-194" to be "Succeeded or Failed"
Mar 29 22:12:13.752: INFO: Pod "downwardapi-volume-26ff30c0-78fb-47cc-add8-97e603b6eae4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.353221ms
Mar 29 22:12:15.775: INFO: Pod "downwardapi-volume-26ff30c0-78fb-47cc-add8-97e603b6eae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03503819s
STEP: Saw pod success
Mar 29 22:12:15.775: INFO: Pod "downwardapi-volume-26ff30c0-78fb-47cc-add8-97e603b6eae4" satisfied condition "Succeeded or Failed"
Mar 29 22:12:15.790: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-26ff30c0-78fb-47cc-add8-97e603b6eae4 container client-container: <nil>
STEP: delete the pod
Mar 29 22:12:15.859: INFO: Waiting for pod downwardapi-volume-26ff30c0-78fb-47cc-add8-97e603b6eae4 to disappear
Mar 29 22:12:15.875: INFO: Pod downwardapi-volume-26ff30c0-78fb-47cc-add8-97e603b6eae4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:12:15.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-194" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":229,"skipped":3831,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:12:15.904: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 29 22:12:18.724: INFO: Successfully updated pod "annotationupdateba827b55-c86e-455d-a631-11a19e36546b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:12:20.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9885" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":230,"skipped":3842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:12:20.824: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 22:12:21.059: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bea3be9-6a3c-46da-a492-6c14a984daec" in namespace "downward-api-7038" to be "Succeeded or Failed"
Mar 29 22:12:21.068: INFO: Pod "downwardapi-volume-6bea3be9-6a3c-46da-a492-6c14a984daec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.426619ms
Mar 29 22:12:23.080: INFO: Pod "downwardapi-volume-6bea3be9-6a3c-46da-a492-6c14a984daec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020849086s
STEP: Saw pod success
Mar 29 22:12:23.080: INFO: Pod "downwardapi-volume-6bea3be9-6a3c-46da-a492-6c14a984daec" satisfied condition "Succeeded or Failed"
Mar 29 22:12:23.092: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-6bea3be9-6a3c-46da-a492-6c14a984daec container client-container: <nil>
STEP: delete the pod
Mar 29 22:12:23.148: INFO: Waiting for pod downwardapi-volume-6bea3be9-6a3c-46da-a492-6c14a984daec to disappear
Mar 29 22:12:23.160: INFO: Pod downwardapi-volume-6bea3be9-6a3c-46da-a492-6c14a984daec no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:12:23.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7038" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":231,"skipped":3865,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:12:23.194: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-ca304d84-a9d9-42f4-8a35-69173710729e
STEP: Creating a pod to test consume configMaps
Mar 29 22:12:23.561: INFO: Waiting up to 5m0s for pod "pod-configmaps-74a237e7-e005-4be8-8dc3-01a71a6cce1c" in namespace "configmap-4883" to be "Succeeded or Failed"
Mar 29 22:12:23.576: INFO: Pod "pod-configmaps-74a237e7-e005-4be8-8dc3-01a71a6cce1c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.077879ms
Mar 29 22:12:25.587: INFO: Pod "pod-configmaps-74a237e7-e005-4be8-8dc3-01a71a6cce1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025736828s
Mar 29 22:12:27.598: INFO: Pod "pod-configmaps-74a237e7-e005-4be8-8dc3-01a71a6cce1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036551529s
STEP: Saw pod success
Mar 29 22:12:27.598: INFO: Pod "pod-configmaps-74a237e7-e005-4be8-8dc3-01a71a6cce1c" satisfied condition "Succeeded or Failed"
Mar 29 22:12:27.610: INFO: Trying to get logs from node 10.189.118.196 pod pod-configmaps-74a237e7-e005-4be8-8dc3-01a71a6cce1c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 22:12:27.670: INFO: Waiting for pod pod-configmaps-74a237e7-e005-4be8-8dc3-01a71a6cce1c to disappear
Mar 29 22:12:27.680: INFO: Pod pod-configmaps-74a237e7-e005-4be8-8dc3-01a71a6cce1c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:12:27.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4883" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":232,"skipped":3865,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:12:27.710: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1897
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-30f0e6fa-d16d-4790-9a54-714d0cc22be8
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-30f0e6fa-d16d-4790-9a54-714d0cc22be8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:12:32.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1897" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":233,"skipped":3883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:12:32.132: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6001
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 29 22:12:32.378: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-a 5bb2c38d-1837-489d-b6df-cefc34ad74cf 50457 0 2021-03-29 22:12:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-29 22:12:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 22:12:32.379: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-a 5bb2c38d-1837-489d-b6df-cefc34ad74cf 50457 0 2021-03-29 22:12:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-29 22:12:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 29 22:12:42.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-a 5bb2c38d-1837-489d-b6df-cefc34ad74cf 50518 0 2021-03-29 22:12:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-29 22:12:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 22:12:42.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-a 5bb2c38d-1837-489d-b6df-cefc34ad74cf 50518 0 2021-03-29 22:12:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-29 22:12:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 29 22:12:52.439: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-a 5bb2c38d-1837-489d-b6df-cefc34ad74cf 50552 0 2021-03-29 22:12:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-29 22:12:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 22:12:52.439: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-a 5bb2c38d-1837-489d-b6df-cefc34ad74cf 50552 0 2021-03-29 22:12:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-29 22:12:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 29 22:13:02.493: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-a 5bb2c38d-1837-489d-b6df-cefc34ad74cf 50583 0 2021-03-29 22:12:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-29 22:12:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 22:13:02.493: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-a 5bb2c38d-1837-489d-b6df-cefc34ad74cf 50583 0 2021-03-29 22:12:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-29 22:12:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 29 22:13:12.510: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-b 810936cf-e5fe-4052-a10f-b762777bf796 50610 0 2021-03-29 22:13:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-29 22:13:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 22:13:12.510: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-b 810936cf-e5fe-4052-a10f-b762777bf796 50610 0 2021-03-29 22:13:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-29 22:13:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 29 22:13:22.548: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-b 810936cf-e5fe-4052-a10f-b762777bf796 50639 0 2021-03-29 22:13:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-29 22:13:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 22:13:22.548: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6001 /api/v1/namespaces/watch-6001/configmaps/e2e-watch-test-configmap-b 810936cf-e5fe-4052-a10f-b762777bf796 50639 0 2021-03-29 22:13:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-29 22:13:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:13:32.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6001" for this suite.

• [SLOW TEST:60.451 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":277,"completed":234,"skipped":3907,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:13:32.583: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-72cd630d-3528-4a2f-a9d9-5670b257ae83
STEP: Creating a pod to test consume secrets
Mar 29 22:13:32.822: INFO: Waiting up to 5m0s for pod "pod-secrets-24eb8ba1-1615-4b48-a14b-9b949276c44d" in namespace "secrets-4753" to be "Succeeded or Failed"
Mar 29 22:13:32.832: INFO: Pod "pod-secrets-24eb8ba1-1615-4b48-a14b-9b949276c44d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.248344ms
Mar 29 22:13:34.846: INFO: Pod "pod-secrets-24eb8ba1-1615-4b48-a14b-9b949276c44d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024090554s
Mar 29 22:13:36.857: INFO: Pod "pod-secrets-24eb8ba1-1615-4b48-a14b-9b949276c44d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034723618s
STEP: Saw pod success
Mar 29 22:13:36.857: INFO: Pod "pod-secrets-24eb8ba1-1615-4b48-a14b-9b949276c44d" satisfied condition "Succeeded or Failed"
Mar 29 22:13:36.869: INFO: Trying to get logs from node 10.189.118.196 pod pod-secrets-24eb8ba1-1615-4b48-a14b-9b949276c44d container secret-volume-test: <nil>
STEP: delete the pod
Mar 29 22:13:36.933: INFO: Waiting for pod pod-secrets-24eb8ba1-1615-4b48-a14b-9b949276c44d to disappear
Mar 29 22:13:36.944: INFO: Pod pod-secrets-24eb8ba1-1615-4b48-a14b-9b949276c44d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:13:36.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4753" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":235,"skipped":3907,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:13:36.975: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3943
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:13:41.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3943" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":277,"completed":236,"skipped":3909,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:13:41.299: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5313
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-5313
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Mar 29 22:13:41.557: INFO: Found 0 stateful pods, waiting for 3
Mar 29 22:13:51.572: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 22:13:51.573: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 22:13:51.573: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 22:13:51.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-5313 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 22:13:51.872: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 22:13:51.872: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 22:13:51.872: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 29 22:14:01.976: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 29 22:14:12.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-5313 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:14:12.413: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 22:14:12.413: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 22:14:12.413: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 22:14:22.484: INFO: Waiting for StatefulSet statefulset-5313/ss2 to complete update
Mar 29 22:14:22.484: INFO: Waiting for Pod statefulset-5313/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 29 22:14:22.484: INFO: Waiting for Pod statefulset-5313/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 29 22:14:22.484: INFO: Waiting for Pod statefulset-5313/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 29 22:14:32.867: INFO: Waiting for StatefulSet statefulset-5313/ss2 to complete update
Mar 29 22:14:32.867: INFO: Waiting for Pod statefulset-5313/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Mar 29 22:14:42.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-5313 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 22:14:42.768: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 22:14:42.768: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 22:14:42.768: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 22:14:52.883: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 29 22:15:02.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 exec --namespace=statefulset-5313 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 22:15:03.185: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 22:15:03.185: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 22:15:03.185: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 22:15:13.261: INFO: Waiting for StatefulSet statefulset-5313/ss2 to complete update
Mar 29 22:15:13.261: INFO: Waiting for Pod statefulset-5313/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 29 22:15:13.261: INFO: Waiting for Pod statefulset-5313/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 29 22:15:23.286: INFO: Waiting for StatefulSet statefulset-5313/ss2 to complete update
Mar 29 22:15:23.286: INFO: Waiting for Pod statefulset-5313/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 29 22:15:33.283: INFO: Deleting all statefulset in ns statefulset-5313
Mar 29 22:15:33.293: INFO: Scaling statefulset ss2 to 0
Mar 29 22:16:03.341: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 22:16:03.355: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:16:03.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5313" for this suite.

• [SLOW TEST:142.162 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":277,"completed":237,"skipped":3909,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:16:03.462: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 22:16:04.217: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 29 22:16:06.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652964, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652964, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652964, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752652964, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 22:16:09.294: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:16:09.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7377" for this suite.
STEP: Destroying namespace "webhook-7377-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.169 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":277,"completed":238,"skipped":3925,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:16:09.632: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 22:16:09.902: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 29 22:16:09.936: INFO: Number of nodes with available pods: 0
Mar 29 22:16:09.936: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 22:16:10.980: INFO: Number of nodes with available pods: 0
Mar 29 22:16:10.980: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 22:16:11.961: INFO: Number of nodes with available pods: 3
Mar 29 22:16:11.961: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 29 22:16:12.042: INFO: Wrong image for pod: daemon-set-8q7h8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:12.042: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:12.042: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:13.065: INFO: Wrong image for pod: daemon-set-8q7h8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:13.065: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:13.065: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:14.065: INFO: Wrong image for pod: daemon-set-8q7h8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:14.065: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:14.065: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:15.067: INFO: Wrong image for pod: daemon-set-8q7h8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:15.067: INFO: Pod daemon-set-8q7h8 is not available
Mar 29 22:16:15.067: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:15.067: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:16.065: INFO: Wrong image for pod: daemon-set-8q7h8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:16.065: INFO: Pod daemon-set-8q7h8 is not available
Mar 29 22:16:16.065: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:16.065: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:17.067: INFO: Wrong image for pod: daemon-set-8q7h8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:17.067: INFO: Pod daemon-set-8q7h8 is not available
Mar 29 22:16:17.067: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:17.067: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:18.065: INFO: Wrong image for pod: daemon-set-8q7h8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:18.065: INFO: Pod daemon-set-8q7h8 is not available
Mar 29 22:16:18.065: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:18.065: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:19.066: INFO: Wrong image for pod: daemon-set-8q7h8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:19.066: INFO: Pod daemon-set-8q7h8 is not available
Mar 29 22:16:19.066: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:19.066: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:20.066: INFO: Wrong image for pod: daemon-set-8q7h8. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:20.066: INFO: Pod daemon-set-8q7h8 is not available
Mar 29 22:16:20.066: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:20.066: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:21.064: INFO: Pod daemon-set-6vpvn is not available
Mar 29 22:16:21.064: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:21.064: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:22.065: INFO: Pod daemon-set-6vpvn is not available
Mar 29 22:16:22.065: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:22.065: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:23.065: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:23.065: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:24.064: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:24.064: INFO: Pod daemon-set-fnfsg is not available
Mar 29 22:16:24.064: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:25.064: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:25.064: INFO: Pod daemon-set-fnfsg is not available
Mar 29 22:16:25.064: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:26.064: INFO: Wrong image for pod: daemon-set-fnfsg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:26.064: INFO: Pod daemon-set-fnfsg is not available
Mar 29 22:16:26.064: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:27.065: INFO: Pod daemon-set-9cp98 is not available
Mar 29 22:16:27.065: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:28.066: INFO: Pod daemon-set-9cp98 is not available
Mar 29 22:16:28.066: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:29.067: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:30.065: INFO: Wrong image for pod: daemon-set-vj852. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 29 22:16:30.065: INFO: Pod daemon-set-vj852 is not available
Mar 29 22:16:31.067: INFO: Pod daemon-set-2fg7d is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 29 22:16:31.104: INFO: Number of nodes with available pods: 2
Mar 29 22:16:31.105: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 22:16:32.134: INFO: Number of nodes with available pods: 2
Mar 29 22:16:32.134: INFO: Node 10.189.118.208 is running more than one daemon pod
Mar 29 22:16:33.131: INFO: Number of nodes with available pods: 3
Mar 29 22:16:33.131: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4406, will wait for the garbage collector to delete the pods
Mar 29 22:16:33.275: INFO: Deleting DaemonSet.extensions daemon-set took: 28.158109ms
Mar 29 22:16:33.376: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.278744ms
Mar 29 22:16:40.687: INFO: Number of nodes with available pods: 0
Mar 29 22:16:40.687: INFO: Number of running nodes: 0, number of available pods: 0
Mar 29 22:16:40.697: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4406/daemonsets","resourceVersion":"51963"},"items":null}

Mar 29 22:16:40.705: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4406/pods","resourceVersion":"51963"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:16:40.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4406" for this suite.

• [SLOW TEST:31.148 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":277,"completed":239,"skipped":3930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:16:40.781: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 29 22:16:41.091: INFO: Waiting up to 5m0s for pod "pod-cccae0e3-a7b4-4ffa-8f43-5142ad3a33b1" in namespace "emptydir-8991" to be "Succeeded or Failed"
Mar 29 22:16:41.101: INFO: Pod "pod-cccae0e3-a7b4-4ffa-8f43-5142ad3a33b1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.746617ms
Mar 29 22:16:43.114: INFO: Pod "pod-cccae0e3-a7b4-4ffa-8f43-5142ad3a33b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022850862s
STEP: Saw pod success
Mar 29 22:16:43.114: INFO: Pod "pod-cccae0e3-a7b4-4ffa-8f43-5142ad3a33b1" satisfied condition "Succeeded or Failed"
Mar 29 22:16:43.124: INFO: Trying to get logs from node 10.189.118.196 pod pod-cccae0e3-a7b4-4ffa-8f43-5142ad3a33b1 container test-container: <nil>
STEP: delete the pod
Mar 29 22:16:43.200: INFO: Waiting for pod pod-cccae0e3-a7b4-4ffa-8f43-5142ad3a33b1 to disappear
Mar 29 22:16:43.210: INFO: Pod pod-cccae0e3-a7b4-4ffa-8f43-5142ad3a33b1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:16:43.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8991" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":240,"skipped":3954,"failed":0}
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:16:43.239: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4786
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 22:16:43.450: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4786
I0329 22:16:43.474129      26 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4786, replica count: 1
I0329 22:16:44.524536      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0329 22:16:45.524809      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0329 22:16:46.525062      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 22:16:46.665: INFO: Created: latency-svc-z9q5h
Mar 29 22:16:46.679: INFO: Got endpoints: latency-svc-z9q5h [54.403987ms]
Mar 29 22:16:46.715: INFO: Created: latency-svc-cf9vl
Mar 29 22:16:46.726: INFO: Got endpoints: latency-svc-cf9vl [46.243453ms]
Mar 29 22:16:46.740: INFO: Created: latency-svc-pqlbd
Mar 29 22:16:46.757: INFO: Got endpoints: latency-svc-pqlbd [77.271316ms]
Mar 29 22:16:46.766: INFO: Created: latency-svc-hb8qp
Mar 29 22:16:46.783: INFO: Got endpoints: latency-svc-hb8qp [102.819561ms]
Mar 29 22:16:46.788: INFO: Created: latency-svc-6tx2h
Mar 29 22:16:46.804: INFO: Got endpoints: latency-svc-6tx2h [124.229328ms]
Mar 29 22:16:46.806: INFO: Created: latency-svc-nd2mg
Mar 29 22:16:46.825: INFO: Got endpoints: latency-svc-nd2mg [143.839599ms]
Mar 29 22:16:46.833: INFO: Created: latency-svc-fhnmb
Mar 29 22:16:46.844: INFO: Got endpoints: latency-svc-fhnmb [163.343808ms]
Mar 29 22:16:46.849: INFO: Created: latency-svc-bw54r
Mar 29 22:16:46.867: INFO: Got endpoints: latency-svc-bw54r [186.206233ms]
Mar 29 22:16:46.868: INFO: Created: latency-svc-fkx7p
Mar 29 22:16:46.889: INFO: Created: latency-svc-gwg5r
Mar 29 22:16:46.890: INFO: Got endpoints: latency-svc-fkx7p [208.52463ms]
Mar 29 22:16:46.902: INFO: Got endpoints: latency-svc-gwg5r [220.892556ms]
Mar 29 22:16:46.915: INFO: Created: latency-svc-54t6t
Mar 29 22:16:46.932: INFO: Got endpoints: latency-svc-54t6t [250.403314ms]
Mar 29 22:16:46.953: INFO: Created: latency-svc-755mc
Mar 29 22:16:46.983: INFO: Got endpoints: latency-svc-755mc [303.499442ms]
Mar 29 22:16:46.990: INFO: Created: latency-svc-7qngn
Mar 29 22:16:47.003: INFO: Got endpoints: latency-svc-7qngn [322.064556ms]
Mar 29 22:16:47.008: INFO: Created: latency-svc-mqnnc
Mar 29 22:16:47.020: INFO: Got endpoints: latency-svc-mqnnc [339.128782ms]
Mar 29 22:16:47.033: INFO: Created: latency-svc-xgs5x
Mar 29 22:16:47.055: INFO: Created: latency-svc-t96dh
Mar 29 22:16:47.057: INFO: Got endpoints: latency-svc-xgs5x [375.710972ms]
Mar 29 22:16:47.075: INFO: Got endpoints: latency-svc-t96dh [393.96491ms]
Mar 29 22:16:47.076: INFO: Created: latency-svc-gp528
Mar 29 22:16:47.117: INFO: Got endpoints: latency-svc-gp528 [390.592141ms]
Mar 29 22:16:47.124: INFO: Created: latency-svc-swsmp
Mar 29 22:16:47.142: INFO: Got endpoints: latency-svc-swsmp [384.984124ms]
Mar 29 22:16:47.162: INFO: Created: latency-svc-qmc9x
Mar 29 22:16:47.174: INFO: Got endpoints: latency-svc-qmc9x [390.929357ms]
Mar 29 22:16:47.192: INFO: Created: latency-svc-tzxw9
Mar 29 22:16:47.209: INFO: Got endpoints: latency-svc-tzxw9 [405.18818ms]
Mar 29 22:16:47.215: INFO: Created: latency-svc-dpsdp
Mar 29 22:16:47.230: INFO: Got endpoints: latency-svc-dpsdp [405.30942ms]
Mar 29 22:16:47.243: INFO: Created: latency-svc-hxzxt
Mar 29 22:16:47.259: INFO: Got endpoints: latency-svc-hxzxt [414.710397ms]
Mar 29 22:16:47.278: INFO: Created: latency-svc-8df5c
Mar 29 22:16:47.297: INFO: Got endpoints: latency-svc-8df5c [430.219654ms]
Mar 29 22:16:47.302: INFO: Created: latency-svc-m9v2w
Mar 29 22:16:47.320: INFO: Got endpoints: latency-svc-m9v2w [430.292981ms]
Mar 29 22:16:47.328: INFO: Created: latency-svc-8s2zq
Mar 29 22:16:47.339: INFO: Got endpoints: latency-svc-8s2zq [436.871107ms]
Mar 29 22:16:47.344: INFO: Created: latency-svc-gkkxw
Mar 29 22:16:47.364: INFO: Got endpoints: latency-svc-gkkxw [431.986509ms]
Mar 29 22:16:47.372: INFO: Created: latency-svc-4kjdw
Mar 29 22:16:47.386: INFO: Got endpoints: latency-svc-4kjdw [402.787307ms]
Mar 29 22:16:47.396: INFO: Created: latency-svc-2t2z9
Mar 29 22:16:47.417: INFO: Got endpoints: latency-svc-2t2z9 [413.667729ms]
Mar 29 22:16:47.426: INFO: Created: latency-svc-kkrh9
Mar 29 22:16:47.442: INFO: Got endpoints: latency-svc-kkrh9 [422.406919ms]
Mar 29 22:16:47.456: INFO: Created: latency-svc-zlnjh
Mar 29 22:16:47.464: INFO: Got endpoints: latency-svc-zlnjh [407.467173ms]
Mar 29 22:16:47.472: INFO: Created: latency-svc-jbgw7
Mar 29 22:16:47.487: INFO: Got endpoints: latency-svc-jbgw7 [411.724553ms]
Mar 29 22:16:47.509: INFO: Created: latency-svc-mx8jj
Mar 29 22:16:47.533: INFO: Got endpoints: latency-svc-mx8jj [416.345899ms]
Mar 29 22:16:47.543: INFO: Created: latency-svc-tj6db
Mar 29 22:16:47.557: INFO: Got endpoints: latency-svc-tj6db [414.421633ms]
Mar 29 22:16:47.562: INFO: Created: latency-svc-cww6w
Mar 29 22:16:47.579: INFO: Got endpoints: latency-svc-cww6w [405.370899ms]
Mar 29 22:16:47.590: INFO: Created: latency-svc-48dgc
Mar 29 22:16:47.606: INFO: Got endpoints: latency-svc-48dgc [397.243443ms]
Mar 29 22:16:47.609: INFO: Created: latency-svc-h8s5g
Mar 29 22:16:47.633: INFO: Got endpoints: latency-svc-h8s5g [402.600443ms]
Mar 29 22:16:47.649: INFO: Created: latency-svc-bj8qj
Mar 29 22:16:47.667: INFO: Got endpoints: latency-svc-bj8qj [407.823085ms]
Mar 29 22:16:47.680: INFO: Created: latency-svc-x9dhp
Mar 29 22:16:47.699: INFO: Got endpoints: latency-svc-x9dhp [401.474139ms]
Mar 29 22:16:47.704: INFO: Created: latency-svc-n8rc5
Mar 29 22:16:47.720: INFO: Got endpoints: latency-svc-n8rc5 [400.521435ms]
Mar 29 22:16:47.727: INFO: Created: latency-svc-2kfsh
Mar 29 22:16:47.740: INFO: Got endpoints: latency-svc-2kfsh [401.091106ms]
Mar 29 22:16:47.748: INFO: Created: latency-svc-kfsqj
Mar 29 22:16:47.766: INFO: Got endpoints: latency-svc-kfsqj [401.982865ms]
Mar 29 22:16:47.773: INFO: Created: latency-svc-sjcgl
Mar 29 22:16:47.794: INFO: Got endpoints: latency-svc-sjcgl [407.447966ms]
Mar 29 22:16:47.796: INFO: Created: latency-svc-zd9tp
Mar 29 22:16:47.814: INFO: Got endpoints: latency-svc-zd9tp [396.611118ms]
Mar 29 22:16:47.823: INFO: Created: latency-svc-pmwn2
Mar 29 22:16:47.835: INFO: Got endpoints: latency-svc-pmwn2 [392.254974ms]
Mar 29 22:16:47.840: INFO: Created: latency-svc-76f4z
Mar 29 22:16:47.853: INFO: Got endpoints: latency-svc-76f4z [388.293375ms]
Mar 29 22:16:47.859: INFO: Created: latency-svc-glkpb
Mar 29 22:16:47.876: INFO: Got endpoints: latency-svc-glkpb [388.758315ms]
Mar 29 22:16:47.877: INFO: Created: latency-svc-mlvnc
Mar 29 22:16:47.897: INFO: Got endpoints: latency-svc-mlvnc [363.599872ms]
Mar 29 22:16:47.899: INFO: Created: latency-svc-7jjgc
Mar 29 22:16:47.915: INFO: Got endpoints: latency-svc-7jjgc [357.907857ms]
Mar 29 22:16:47.931: INFO: Created: latency-svc-5shf6
Mar 29 22:16:47.948: INFO: Got endpoints: latency-svc-5shf6 [368.605149ms]
Mar 29 22:16:47.950: INFO: Created: latency-svc-sqbd9
Mar 29 22:16:47.967: INFO: Got endpoints: latency-svc-sqbd9 [360.757596ms]
Mar 29 22:16:47.976: INFO: Created: latency-svc-77fzs
Mar 29 22:16:47.990: INFO: Got endpoints: latency-svc-77fzs [356.868826ms]
Mar 29 22:16:47.995: INFO: Created: latency-svc-xtxr2
Mar 29 22:16:48.007: INFO: Got endpoints: latency-svc-xtxr2 [339.805104ms]
Mar 29 22:16:48.013: INFO: Created: latency-svc-6vl8z
Mar 29 22:16:48.032: INFO: Got endpoints: latency-svc-6vl8z [333.158764ms]
Mar 29 22:16:48.033: INFO: Created: latency-svc-wkr5r
Mar 29 22:16:48.045: INFO: Got endpoints: latency-svc-wkr5r [324.877777ms]
Mar 29 22:16:48.052: INFO: Created: latency-svc-qqczg
Mar 29 22:16:48.065: INFO: Got endpoints: latency-svc-qqczg [324.756317ms]
Mar 29 22:16:48.081: INFO: Created: latency-svc-dphcv
Mar 29 22:16:48.099: INFO: Got endpoints: latency-svc-dphcv [333.788353ms]
Mar 29 22:16:48.105: INFO: Created: latency-svc-2xc9m
Mar 29 22:16:48.118: INFO: Got endpoints: latency-svc-2xc9m [324.15988ms]
Mar 29 22:16:48.127: INFO: Created: latency-svc-bvpg2
Mar 29 22:16:48.139: INFO: Created: latency-svc-mpgw9
Mar 29 22:16:48.139: INFO: Got endpoints: latency-svc-bvpg2 [325.156459ms]
Mar 29 22:16:48.150: INFO: Got endpoints: latency-svc-mpgw9 [315.155048ms]
Mar 29 22:16:48.158: INFO: Created: latency-svc-nv2vl
Mar 29 22:16:48.173: INFO: Got endpoints: latency-svc-nv2vl [320.079742ms]
Mar 29 22:16:48.190: INFO: Created: latency-svc-krbh2
Mar 29 22:16:48.205: INFO: Got endpoints: latency-svc-krbh2 [329.084882ms]
Mar 29 22:16:48.210: INFO: Created: latency-svc-rc8gw
Mar 29 22:16:48.221: INFO: Got endpoints: latency-svc-rc8gw [323.856412ms]
Mar 29 22:16:48.227: INFO: Created: latency-svc-g29pg
Mar 29 22:16:48.239: INFO: Got endpoints: latency-svc-g29pg [324.535632ms]
Mar 29 22:16:48.253: INFO: Created: latency-svc-fwvg2
Mar 29 22:16:48.266: INFO: Got endpoints: latency-svc-fwvg2 [317.778993ms]
Mar 29 22:16:48.275: INFO: Created: latency-svc-mv64k
Mar 29 22:16:48.290: INFO: Got endpoints: latency-svc-mv64k [322.801114ms]
Mar 29 22:16:48.296: INFO: Created: latency-svc-fr4r4
Mar 29 22:16:48.311: INFO: Created: latency-svc-7dxq6
Mar 29 22:16:48.311: INFO: Got endpoints: latency-svc-fr4r4 [321.635988ms]
Mar 29 22:16:48.323: INFO: Got endpoints: latency-svc-7dxq6 [316.316182ms]
Mar 29 22:16:48.333: INFO: Created: latency-svc-sdwwf
Mar 29 22:16:48.345: INFO: Got endpoints: latency-svc-sdwwf [313.147358ms]
Mar 29 22:16:48.355: INFO: Created: latency-svc-tcf5b
Mar 29 22:16:48.366: INFO: Got endpoints: latency-svc-tcf5b [320.115219ms]
Mar 29 22:16:48.379: INFO: Created: latency-svc-qd7lj
Mar 29 22:16:48.390: INFO: Got endpoints: latency-svc-qd7lj [325.248643ms]
Mar 29 22:16:48.395: INFO: Created: latency-svc-km4v6
Mar 29 22:16:48.412: INFO: Got endpoints: latency-svc-km4v6 [312.189461ms]
Mar 29 22:16:48.418: INFO: Created: latency-svc-wzqpx
Mar 29 22:16:48.431: INFO: Got endpoints: latency-svc-wzqpx [313.562628ms]
Mar 29 22:16:48.443: INFO: Created: latency-svc-9z96b
Mar 29 22:16:48.458: INFO: Got endpoints: latency-svc-9z96b [319.163501ms]
Mar 29 22:16:48.470: INFO: Created: latency-svc-r69rp
Mar 29 22:16:48.491: INFO: Got endpoints: latency-svc-r69rp [341.509644ms]
Mar 29 22:16:48.499: INFO: Created: latency-svc-7rbqp
Mar 29 22:16:48.515: INFO: Got endpoints: latency-svc-7rbqp [342.075615ms]
Mar 29 22:16:48.520: INFO: Created: latency-svc-clklr
Mar 29 22:16:48.572: INFO: Created: latency-svc-lgdpj
Mar 29 22:16:48.576: INFO: Got endpoints: latency-svc-clklr [371.259619ms]
Mar 29 22:16:48.622: INFO: Got endpoints: latency-svc-lgdpj [401.170055ms]
Mar 29 22:16:48.629: INFO: Created: latency-svc-9c94h
Mar 29 22:16:48.643: INFO: Got endpoints: latency-svc-9c94h [403.945956ms]
Mar 29 22:16:48.650: INFO: Created: latency-svc-schhz
Mar 29 22:16:48.683: INFO: Created: latency-svc-s68xl
Mar 29 22:16:48.687: INFO: Got endpoints: latency-svc-schhz [420.970702ms]
Mar 29 22:16:48.724: INFO: Got endpoints: latency-svc-s68xl [433.132883ms]
Mar 29 22:16:48.733: INFO: Created: latency-svc-9kb2f
Mar 29 22:16:48.744: INFO: Got endpoints: latency-svc-9kb2f [432.792491ms]
Mar 29 22:16:48.757: INFO: Created: latency-svc-6kpx8
Mar 29 22:16:48.769: INFO: Got endpoints: latency-svc-6kpx8 [446.104318ms]
Mar 29 22:16:48.785: INFO: Created: latency-svc-vwpwr
Mar 29 22:16:48.802: INFO: Got endpoints: latency-svc-vwpwr [456.17743ms]
Mar 29 22:16:48.807: INFO: Created: latency-svc-bd4g7
Mar 29 22:16:48.819: INFO: Got endpoints: latency-svc-bd4g7 [452.870003ms]
Mar 29 22:16:48.836: INFO: Created: latency-svc-cv8m5
Mar 29 22:16:48.847: INFO: Got endpoints: latency-svc-cv8m5 [456.95232ms]
Mar 29 22:16:48.865: INFO: Created: latency-svc-xnm7l
Mar 29 22:16:48.896: INFO: Got endpoints: latency-svc-xnm7l [483.844411ms]
Mar 29 22:16:48.911: INFO: Created: latency-svc-gqv6h
Mar 29 22:16:48.924: INFO: Created: latency-svc-sxdl4
Mar 29 22:16:48.924: INFO: Got endpoints: latency-svc-gqv6h [492.946056ms]
Mar 29 22:16:48.937: INFO: Got endpoints: latency-svc-sxdl4 [478.343256ms]
Mar 29 22:16:48.947: INFO: Created: latency-svc-xw5l6
Mar 29 22:16:48.958: INFO: Got endpoints: latency-svc-xw5l6 [466.038076ms]
Mar 29 22:16:48.964: INFO: Created: latency-svc-xdbn7
Mar 29 22:16:48.984: INFO: Got endpoints: latency-svc-xdbn7 [469.234856ms]
Mar 29 22:16:48.992: INFO: Created: latency-svc-t7q88
Mar 29 22:16:49.008: INFO: Got endpoints: latency-svc-t7q88 [431.747407ms]
Mar 29 22:16:49.029: INFO: Created: latency-svc-r7x78
Mar 29 22:16:49.051: INFO: Got endpoints: latency-svc-r7x78 [428.713366ms]
Mar 29 22:16:49.065: INFO: Created: latency-svc-pbsg2
Mar 29 22:16:49.089: INFO: Created: latency-svc-zpwfz
Mar 29 22:16:49.098: INFO: Got endpoints: latency-svc-pbsg2 [454.132163ms]
Mar 29 22:16:49.114: INFO: Got endpoints: latency-svc-zpwfz [427.043967ms]
Mar 29 22:16:49.115: INFO: Created: latency-svc-gpcx9
Mar 29 22:16:49.129: INFO: Got endpoints: latency-svc-gpcx9 [405.34734ms]
Mar 29 22:16:49.133: INFO: Created: latency-svc-sg7nk
Mar 29 22:16:49.162: INFO: Got endpoints: latency-svc-sg7nk [418.180474ms]
Mar 29 22:16:49.170: INFO: Created: latency-svc-l7gmz
Mar 29 22:16:49.184: INFO: Got endpoints: latency-svc-l7gmz [414.523275ms]
Mar 29 22:16:49.193: INFO: Created: latency-svc-kdgb6
Mar 29 22:16:49.205: INFO: Got endpoints: latency-svc-kdgb6 [403.283317ms]
Mar 29 22:16:49.216: INFO: Created: latency-svc-ktpsq
Mar 29 22:16:49.229: INFO: Got endpoints: latency-svc-ktpsq [410.613135ms]
Mar 29 22:16:49.234: INFO: Created: latency-svc-27fbh
Mar 29 22:16:49.247: INFO: Got endpoints: latency-svc-27fbh [399.900796ms]
Mar 29 22:16:49.254: INFO: Created: latency-svc-p8657
Mar 29 22:16:49.266: INFO: Got endpoints: latency-svc-p8657 [370.708721ms]
Mar 29 22:16:49.274: INFO: Created: latency-svc-pvqp8
Mar 29 22:16:49.287: INFO: Got endpoints: latency-svc-pvqp8 [362.757252ms]
Mar 29 22:16:49.291: INFO: Created: latency-svc-jd2rs
Mar 29 22:16:49.311: INFO: Got endpoints: latency-svc-jd2rs [374.32515ms]
Mar 29 22:16:49.311: INFO: Created: latency-svc-tjgfw
Mar 29 22:16:49.323: INFO: Got endpoints: latency-svc-tjgfw [365.364321ms]
Mar 29 22:16:49.329: INFO: Created: latency-svc-n8pqm
Mar 29 22:16:49.344: INFO: Created: latency-svc-2k7ml
Mar 29 22:16:49.346: INFO: Got endpoints: latency-svc-n8pqm [361.098297ms]
Mar 29 22:16:49.356: INFO: Got endpoints: latency-svc-2k7ml [347.539632ms]
Mar 29 22:16:49.364: INFO: Created: latency-svc-g7mpj
Mar 29 22:16:49.387: INFO: Got endpoints: latency-svc-g7mpj [336.427189ms]
Mar 29 22:16:49.391: INFO: Created: latency-svc-xm4gf
Mar 29 22:16:49.404: INFO: Got endpoints: latency-svc-xm4gf [306.866136ms]
Mar 29 22:16:49.413: INFO: Created: latency-svc-mm4zz
Mar 29 22:16:49.425: INFO: Got endpoints: latency-svc-mm4zz [311.255977ms]
Mar 29 22:16:49.427: INFO: Created: latency-svc-k259f
Mar 29 22:16:49.438: INFO: Got endpoints: latency-svc-k259f [308.796625ms]
Mar 29 22:16:49.443: INFO: Created: latency-svc-lbtkz
Mar 29 22:16:49.459: INFO: Got endpoints: latency-svc-lbtkz [296.745915ms]
Mar 29 22:16:49.463: INFO: Created: latency-svc-cp2zf
Mar 29 22:16:49.479: INFO: Got endpoints: latency-svc-cp2zf [295.118983ms]
Mar 29 22:16:49.488: INFO: Created: latency-svc-k6p27
Mar 29 22:16:49.504: INFO: Got endpoints: latency-svc-k6p27 [299.321485ms]
Mar 29 22:16:49.506: INFO: Created: latency-svc-grntr
Mar 29 22:16:49.527: INFO: Got endpoints: latency-svc-grntr [297.197209ms]
Mar 29 22:16:49.534: INFO: Created: latency-svc-z6hht
Mar 29 22:16:49.559: INFO: Got endpoints: latency-svc-z6hht [311.168555ms]
Mar 29 22:16:49.559: INFO: Created: latency-svc-559jc
Mar 29 22:16:49.573: INFO: Got endpoints: latency-svc-559jc [306.815231ms]
Mar 29 22:16:49.582: INFO: Created: latency-svc-2k2gf
Mar 29 22:16:49.600: INFO: Got endpoints: latency-svc-2k2gf [312.043022ms]
Mar 29 22:16:49.602: INFO: Created: latency-svc-cwgqk
Mar 29 22:16:49.616: INFO: Got endpoints: latency-svc-cwgqk [304.622866ms]
Mar 29 22:16:49.626: INFO: Created: latency-svc-4spvd
Mar 29 22:16:49.637: INFO: Got endpoints: latency-svc-4spvd [313.154893ms]
Mar 29 22:16:49.647: INFO: Created: latency-svc-55s2l
Mar 29 22:16:49.666: INFO: Got endpoints: latency-svc-55s2l [320.006374ms]
Mar 29 22:16:49.669: INFO: Created: latency-svc-5sw7l
Mar 29 22:16:49.685: INFO: Got endpoints: latency-svc-5sw7l [329.719778ms]
Mar 29 22:16:49.693: INFO: Created: latency-svc-wvdpq
Mar 29 22:16:49.707: INFO: Got endpoints: latency-svc-wvdpq [320.158873ms]
Mar 29 22:16:49.713: INFO: Created: latency-svc-f4cdq
Mar 29 22:16:49.725: INFO: Got endpoints: latency-svc-f4cdq [320.742152ms]
Mar 29 22:16:49.732: INFO: Created: latency-svc-9f69r
Mar 29 22:16:49.752: INFO: Got endpoints: latency-svc-9f69r [326.122895ms]
Mar 29 22:16:49.759: INFO: Created: latency-svc-dcst4
Mar 29 22:16:49.781: INFO: Got endpoints: latency-svc-dcst4 [343.56661ms]
Mar 29 22:16:49.789: INFO: Created: latency-svc-8rz4t
Mar 29 22:16:49.825: INFO: Got endpoints: latency-svc-8rz4t [366.021612ms]
Mar 29 22:16:49.845: INFO: Created: latency-svc-4rntm
Mar 29 22:16:49.857: INFO: Got endpoints: latency-svc-4rntm [377.49853ms]
Mar 29 22:16:49.862: INFO: Created: latency-svc-dnvd7
Mar 29 22:16:49.877: INFO: Got endpoints: latency-svc-dnvd7 [372.422829ms]
Mar 29 22:16:49.883: INFO: Created: latency-svc-crnjv
Mar 29 22:16:49.915: INFO: Got endpoints: latency-svc-crnjv [388.179114ms]
Mar 29 22:16:49.923: INFO: Created: latency-svc-bx6vt
Mar 29 22:16:49.935: INFO: Got endpoints: latency-svc-bx6vt [375.887068ms]
Mar 29 22:16:49.940: INFO: Created: latency-svc-4dmxd
Mar 29 22:16:49.955: INFO: Got endpoints: latency-svc-4dmxd [381.715356ms]
Mar 29 22:16:49.962: INFO: Created: latency-svc-x5h47
Mar 29 22:16:49.972: INFO: Got endpoints: latency-svc-x5h47 [372.516078ms]
Mar 29 22:16:49.988: INFO: Created: latency-svc-v8m8b
Mar 29 22:16:50.003: INFO: Got endpoints: latency-svc-v8m8b [387.331373ms]
Mar 29 22:16:50.026: INFO: Created: latency-svc-qsqxh
Mar 29 22:16:50.038: INFO: Got endpoints: latency-svc-qsqxh [400.935822ms]
Mar 29 22:16:50.046: INFO: Created: latency-svc-lzj4k
Mar 29 22:16:50.058: INFO: Got endpoints: latency-svc-lzj4k [392.006242ms]
Mar 29 22:16:50.064: INFO: Created: latency-svc-5r2lc
Mar 29 22:16:50.078: INFO: Got endpoints: latency-svc-5r2lc [392.532916ms]
Mar 29 22:16:50.087: INFO: Created: latency-svc-nhggx
Mar 29 22:16:50.092: INFO: Got endpoints: latency-svc-nhggx [384.531993ms]
Mar 29 22:16:50.109: INFO: Created: latency-svc-2bzhl
Mar 29 22:16:50.124: INFO: Got endpoints: latency-svc-2bzhl [399.026485ms]
Mar 29 22:16:50.132: INFO: Created: latency-svc-kwpb7
Mar 29 22:16:50.149: INFO: Got endpoints: latency-svc-kwpb7 [397.214046ms]
Mar 29 22:16:50.156: INFO: Created: latency-svc-ndstd
Mar 29 22:16:50.175: INFO: Got endpoints: latency-svc-ndstd [393.198366ms]
Mar 29 22:16:50.189: INFO: Created: latency-svc-c8d7m
Mar 29 22:16:50.202: INFO: Got endpoints: latency-svc-c8d7m [376.940727ms]
Mar 29 22:16:50.203: INFO: Created: latency-svc-6qwjt
Mar 29 22:16:50.217: INFO: Got endpoints: latency-svc-6qwjt [360.066296ms]
Mar 29 22:16:50.224: INFO: Created: latency-svc-t84r5
Mar 29 22:16:50.238: INFO: Got endpoints: latency-svc-t84r5 [360.853617ms]
Mar 29 22:16:50.242: INFO: Created: latency-svc-7kbp5
Mar 29 22:16:50.254: INFO: Got endpoints: latency-svc-7kbp5 [339.181386ms]
Mar 29 22:16:50.261: INFO: Created: latency-svc-gsjhr
Mar 29 22:16:50.271: INFO: Got endpoints: latency-svc-gsjhr [336.746126ms]
Mar 29 22:16:50.279: INFO: Created: latency-svc-wmrrn
Mar 29 22:16:50.293: INFO: Got endpoints: latency-svc-wmrrn [337.229859ms]
Mar 29 22:16:50.298: INFO: Created: latency-svc-47l2d
Mar 29 22:16:50.315: INFO: Got endpoints: latency-svc-47l2d [342.322165ms]
Mar 29 22:16:50.315: INFO: Created: latency-svc-gc87m
Mar 29 22:16:50.329: INFO: Got endpoints: latency-svc-gc87m [325.46946ms]
Mar 29 22:16:50.341: INFO: Created: latency-svc-p9wcz
Mar 29 22:16:50.364: INFO: Got endpoints: latency-svc-p9wcz [326.356474ms]
Mar 29 22:16:50.371: INFO: Created: latency-svc-tbdwm
Mar 29 22:16:50.397: INFO: Got endpoints: latency-svc-tbdwm [339.68991ms]
Mar 29 22:16:50.399: INFO: Created: latency-svc-scgtw
Mar 29 22:16:50.422: INFO: Got endpoints: latency-svc-scgtw [344.22202ms]
Mar 29 22:16:50.439: INFO: Created: latency-svc-ngwqm
Mar 29 22:16:50.453: INFO: Got endpoints: latency-svc-ngwqm [361.302504ms]
Mar 29 22:16:50.468: INFO: Created: latency-svc-58kvb
Mar 29 22:16:50.488: INFO: Got endpoints: latency-svc-58kvb [363.237284ms]
Mar 29 22:16:50.489: INFO: Created: latency-svc-54xhs
Mar 29 22:16:50.510: INFO: Got endpoints: latency-svc-54xhs [360.530785ms]
Mar 29 22:16:50.515: INFO: Created: latency-svc-rnl5q
Mar 29 22:16:50.526: INFO: Got endpoints: latency-svc-rnl5q [351.276418ms]
Mar 29 22:16:50.542: INFO: Created: latency-svc-hsqfv
Mar 29 22:16:50.560: INFO: Got endpoints: latency-svc-hsqfv [357.408935ms]
Mar 29 22:16:50.561: INFO: Created: latency-svc-ls2j8
Mar 29 22:16:50.579: INFO: Got endpoints: latency-svc-ls2j8 [361.945271ms]
Mar 29 22:16:50.587: INFO: Created: latency-svc-rz2mf
Mar 29 22:16:50.609: INFO: Got endpoints: latency-svc-rz2mf [371.084807ms]
Mar 29 22:16:50.617: INFO: Created: latency-svc-2z5pq
Mar 29 22:16:50.634: INFO: Got endpoints: latency-svc-2z5pq [379.336846ms]
Mar 29 22:16:50.635: INFO: Created: latency-svc-q9m6d
Mar 29 22:16:50.651: INFO: Got endpoints: latency-svc-q9m6d [379.477595ms]
Mar 29 22:16:50.655: INFO: Created: latency-svc-mx7lk
Mar 29 22:16:50.670: INFO: Got endpoints: latency-svc-mx7lk [377.539573ms]
Mar 29 22:16:50.679: INFO: Created: latency-svc-84gjv
Mar 29 22:16:50.694: INFO: Got endpoints: latency-svc-84gjv [379.496484ms]
Mar 29 22:16:50.699: INFO: Created: latency-svc-r2gwg
Mar 29 22:16:50.711: INFO: Got endpoints: latency-svc-r2gwg [381.935907ms]
Mar 29 22:16:50.718: INFO: Created: latency-svc-8ndqq
Mar 29 22:16:50.732: INFO: Got endpoints: latency-svc-8ndqq [368.337149ms]
Mar 29 22:16:50.741: INFO: Created: latency-svc-7nbxn
Mar 29 22:16:50.757: INFO: Created: latency-svc-d9gnb
Mar 29 22:16:50.758: INFO: Got endpoints: latency-svc-7nbxn [360.043788ms]
Mar 29 22:16:50.772: INFO: Got endpoints: latency-svc-d9gnb [349.967967ms]
Mar 29 22:16:50.780: INFO: Created: latency-svc-58wj9
Mar 29 22:16:50.803: INFO: Got endpoints: latency-svc-58wj9 [350.016897ms]
Mar 29 22:16:50.806: INFO: Created: latency-svc-6g4wb
Mar 29 22:16:50.821: INFO: Got endpoints: latency-svc-6g4wb [333.050491ms]
Mar 29 22:16:50.836: INFO: Created: latency-svc-sbgsh
Mar 29 22:16:50.842: INFO: Created: latency-svc-7hqz2
Mar 29 22:16:50.847: INFO: Got endpoints: latency-svc-sbgsh [336.907056ms]
Mar 29 22:16:50.855: INFO: Got endpoints: latency-svc-7hqz2 [328.417057ms]
Mar 29 22:16:50.862: INFO: Created: latency-svc-mj4jl
Mar 29 22:16:50.874: INFO: Got endpoints: latency-svc-mj4jl [314.404563ms]
Mar 29 22:16:50.879: INFO: Created: latency-svc-6x46x
Mar 29 22:16:50.892: INFO: Got endpoints: latency-svc-6x46x [312.784413ms]
Mar 29 22:16:50.895: INFO: Created: latency-svc-tnz4k
Mar 29 22:16:50.908: INFO: Got endpoints: latency-svc-tnz4k [298.878281ms]
Mar 29 22:16:50.912: INFO: Created: latency-svc-kj9g2
Mar 29 22:16:50.930: INFO: Got endpoints: latency-svc-kj9g2 [296.605724ms]
Mar 29 22:16:50.938: INFO: Created: latency-svc-24dgw
Mar 29 22:16:50.951: INFO: Got endpoints: latency-svc-24dgw [300.093578ms]
Mar 29 22:16:50.961: INFO: Created: latency-svc-bz5lj
Mar 29 22:16:50.979: INFO: Got endpoints: latency-svc-bz5lj [309.013613ms]
Mar 29 22:16:50.986: INFO: Created: latency-svc-k7l4l
Mar 29 22:16:51.010: INFO: Got endpoints: latency-svc-k7l4l [315.638494ms]
Mar 29 22:16:51.021: INFO: Created: latency-svc-g2526
Mar 29 22:16:51.076: INFO: Got endpoints: latency-svc-g2526 [364.982142ms]
Mar 29 22:16:51.084: INFO: Created: latency-svc-dm6f2
Mar 29 22:16:51.131: INFO: Got endpoints: latency-svc-dm6f2 [398.744053ms]
Mar 29 22:16:51.175: INFO: Created: latency-svc-4l4wg
Mar 29 22:16:51.194: INFO: Got endpoints: latency-svc-4l4wg [436.783545ms]
Mar 29 22:16:51.221: INFO: Created: latency-svc-6z2dk
Mar 29 22:16:51.239: INFO: Got endpoints: latency-svc-6z2dk [466.41326ms]
Mar 29 22:16:51.263: INFO: Created: latency-svc-9jzld
Mar 29 22:16:51.317: INFO: Got endpoints: latency-svc-9jzld [513.3371ms]
Mar 29 22:16:51.324: INFO: Created: latency-svc-2hhr5
Mar 29 22:16:51.344: INFO: Got endpoints: latency-svc-2hhr5 [523.428904ms]
Mar 29 22:16:51.361: INFO: Created: latency-svc-pc48l
Mar 29 22:16:51.377: INFO: Got endpoints: latency-svc-pc48l [530.73209ms]
Mar 29 22:16:51.387: INFO: Created: latency-svc-slwlj
Mar 29 22:16:51.397: INFO: Got endpoints: latency-svc-slwlj [542.518537ms]
Mar 29 22:16:51.412: INFO: Created: latency-svc-8qffx
Mar 29 22:16:51.431: INFO: Got endpoints: latency-svc-8qffx [556.413343ms]
Mar 29 22:16:51.431: INFO: Created: latency-svc-4k8qp
Mar 29 22:16:51.455: INFO: Got endpoints: latency-svc-4k8qp [563.343318ms]
Mar 29 22:16:51.462: INFO: Created: latency-svc-gb6ss
Mar 29 22:16:51.477: INFO: Got endpoints: latency-svc-gb6ss [569.014817ms]
Mar 29 22:16:51.487: INFO: Created: latency-svc-gmxsz
Mar 29 22:16:51.498: INFO: Got endpoints: latency-svc-gmxsz [567.535803ms]
Mar 29 22:16:51.504: INFO: Created: latency-svc-tmg4x
Mar 29 22:16:51.514: INFO: Got endpoints: latency-svc-tmg4x [563.485144ms]
Mar 29 22:16:51.526: INFO: Created: latency-svc-78ws2
Mar 29 22:16:51.542: INFO: Got endpoints: latency-svc-78ws2 [562.111499ms]
Mar 29 22:16:51.549: INFO: Created: latency-svc-4vvp6
Mar 29 22:16:51.569: INFO: Got endpoints: latency-svc-4vvp6 [558.761073ms]
Mar 29 22:16:51.573: INFO: Created: latency-svc-5tvpk
Mar 29 22:16:51.585: INFO: Got endpoints: latency-svc-5tvpk [509.563497ms]
Mar 29 22:16:51.594: INFO: Created: latency-svc-5frlc
Mar 29 22:16:51.607: INFO: Got endpoints: latency-svc-5frlc [475.480102ms]
Mar 29 22:16:51.616: INFO: Created: latency-svc-b2dkc
Mar 29 22:16:51.636: INFO: Got endpoints: latency-svc-b2dkc [441.15851ms]
Mar 29 22:16:51.640: INFO: Created: latency-svc-rtb78
Mar 29 22:16:51.652: INFO: Got endpoints: latency-svc-rtb78 [413.105583ms]
Mar 29 22:16:51.658: INFO: Created: latency-svc-s9wq9
Mar 29 22:16:51.673: INFO: Got endpoints: latency-svc-s9wq9 [356.04536ms]
Mar 29 22:16:51.678: INFO: Created: latency-svc-6kclm
Mar 29 22:16:51.693: INFO: Created: latency-svc-x4ntg
Mar 29 22:16:51.693: INFO: Got endpoints: latency-svc-6kclm [348.534889ms]
Mar 29 22:16:51.704: INFO: Got endpoints: latency-svc-x4ntg [326.151546ms]
Mar 29 22:16:51.716: INFO: Created: latency-svc-tj6ks
Mar 29 22:16:51.729: INFO: Got endpoints: latency-svc-tj6ks [332.10036ms]
Mar 29 22:16:51.757: INFO: Created: latency-svc-v5pq8
Mar 29 22:16:51.771: INFO: Got endpoints: latency-svc-v5pq8 [339.931363ms]
Mar 29 22:16:51.771: INFO: Latencies: [46.243453ms 77.271316ms 102.819561ms 124.229328ms 143.839599ms 163.343808ms 186.206233ms 208.52463ms 220.892556ms 250.403314ms 295.118983ms 296.605724ms 296.745915ms 297.197209ms 298.878281ms 299.321485ms 300.093578ms 303.499442ms 304.622866ms 306.815231ms 306.866136ms 308.796625ms 309.013613ms 311.168555ms 311.255977ms 312.043022ms 312.189461ms 312.784413ms 313.147358ms 313.154893ms 313.562628ms 314.404563ms 315.155048ms 315.638494ms 316.316182ms 317.778993ms 319.163501ms 320.006374ms 320.079742ms 320.115219ms 320.158873ms 320.742152ms 321.635988ms 322.064556ms 322.801114ms 323.856412ms 324.15988ms 324.535632ms 324.756317ms 324.877777ms 325.156459ms 325.248643ms 325.46946ms 326.122895ms 326.151546ms 326.356474ms 328.417057ms 329.084882ms 329.719778ms 332.10036ms 333.050491ms 333.158764ms 333.788353ms 336.427189ms 336.746126ms 336.907056ms 337.229859ms 339.128782ms 339.181386ms 339.68991ms 339.805104ms 339.931363ms 341.509644ms 342.075615ms 342.322165ms 343.56661ms 344.22202ms 347.539632ms 348.534889ms 349.967967ms 350.016897ms 351.276418ms 356.04536ms 356.868826ms 357.408935ms 357.907857ms 360.043788ms 360.066296ms 360.530785ms 360.757596ms 360.853617ms 361.098297ms 361.302504ms 361.945271ms 362.757252ms 363.237284ms 363.599872ms 364.982142ms 365.364321ms 366.021612ms 368.337149ms 368.605149ms 370.708721ms 371.084807ms 371.259619ms 372.422829ms 372.516078ms 374.32515ms 375.710972ms 375.887068ms 376.940727ms 377.49853ms 377.539573ms 379.336846ms 379.477595ms 379.496484ms 381.715356ms 381.935907ms 384.531993ms 384.984124ms 387.331373ms 388.179114ms 388.293375ms 388.758315ms 390.592141ms 390.929357ms 392.006242ms 392.254974ms 392.532916ms 393.198366ms 393.96491ms 396.611118ms 397.214046ms 397.243443ms 398.744053ms 399.026485ms 399.900796ms 400.521435ms 400.935822ms 401.091106ms 401.170055ms 401.474139ms 401.982865ms 402.600443ms 402.787307ms 403.283317ms 403.945956ms 405.18818ms 405.30942ms 405.34734ms 405.370899ms 407.447966ms 407.467173ms 407.823085ms 410.613135ms 411.724553ms 413.105583ms 413.667729ms 414.421633ms 414.523275ms 414.710397ms 416.345899ms 418.180474ms 420.970702ms 422.406919ms 427.043967ms 428.713366ms 430.219654ms 430.292981ms 431.747407ms 431.986509ms 432.792491ms 433.132883ms 436.783545ms 436.871107ms 441.15851ms 446.104318ms 452.870003ms 454.132163ms 456.17743ms 456.95232ms 466.038076ms 466.41326ms 469.234856ms 475.480102ms 478.343256ms 483.844411ms 492.946056ms 509.563497ms 513.3371ms 523.428904ms 530.73209ms 542.518537ms 556.413343ms 558.761073ms 562.111499ms 563.343318ms 563.485144ms 567.535803ms 569.014817ms]
Mar 29 22:16:51.771: INFO: 50 %ile: 368.337149ms
Mar 29 22:16:51.771: INFO: 90 %ile: 456.95232ms
Mar 29 22:16:51.771: INFO: 99 %ile: 567.535803ms
Mar 29 22:16:51.771: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:16:51.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4786" for this suite.

• [SLOW TEST:8.566 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":277,"completed":241,"skipped":3957,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:16:51.805: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 29 22:16:55.087: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:16:55.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3653" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":242,"skipped":3971,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:16:55.162: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Mar 29 22:16:55.378: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-820232969 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:16:55.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8594" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":277,"completed":243,"skipped":3971,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:16:55.482: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7728
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 29 22:16:55.719: INFO: Waiting up to 5m0s for pod "pod-060c1577-5b06-4b1c-b996-60766b2e4e80" in namespace "emptydir-7728" to be "Succeeded or Failed"
Mar 29 22:16:55.732: INFO: Pod "pod-060c1577-5b06-4b1c-b996-60766b2e4e80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.923618ms
Mar 29 22:16:57.743: INFO: Pod "pod-060c1577-5b06-4b1c-b996-60766b2e4e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024104909s
Mar 29 22:16:59.754: INFO: Pod "pod-060c1577-5b06-4b1c-b996-60766b2e4e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034561563s
STEP: Saw pod success
Mar 29 22:16:59.754: INFO: Pod "pod-060c1577-5b06-4b1c-b996-60766b2e4e80" satisfied condition "Succeeded or Failed"
Mar 29 22:16:59.763: INFO: Trying to get logs from node 10.189.118.196 pod pod-060c1577-5b06-4b1c-b996-60766b2e4e80 container test-container: <nil>
STEP: delete the pod
Mar 29 22:16:59.852: INFO: Waiting for pod pod-060c1577-5b06-4b1c-b996-60766b2e4e80 to disappear
Mar 29 22:16:59.862: INFO: Pod pod-060c1577-5b06-4b1c-b996-60766b2e4e80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:16:59.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7728" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":244,"skipped":3980,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:16:59.891: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-1026
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 29 22:17:00.090: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 29 22:17:00.175: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 22:17:02.188: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 22:17:04.185: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:17:06.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:17:08.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:17:10.188: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:17:12.185: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:17:14.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:17:16.186: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:17:18.186: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 29 22:17:18.209: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 29 22:17:20.220: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 29 22:17:20.241: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 29 22:17:22.342: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.124.170 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1026 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:17:22.342: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:17:23.562: INFO: Found all expected endpoints: [netserver-0]
Mar 29 22:17:23.574: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.58.20 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1026 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:17:23.574: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:17:24.757: INFO: Found all expected endpoints: [netserver-1]
Mar 29 22:17:24.769: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.209.252 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1026 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:17:24.769: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:17:25.998: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:17:25.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1026" for this suite.

• [SLOW TEST:26.140 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":245,"skipped":3992,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:17:26.031: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 22:17:26.958: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 22:17:28.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653046, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653046, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653047, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653046, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 22:17:32.035: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:17:32.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8737" for this suite.
STEP: Destroying namespace "webhook-8737-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.223 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":277,"completed":246,"skipped":3994,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:17:32.255: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8196.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8196.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 29 22:17:36.562: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:36.579: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:36.594: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:36.610: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:36.661: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:36.677: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:36.694: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:36.709: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:36.748: INFO: Lookups using dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local]

Mar 29 22:17:41.781: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:41.815: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:41.831: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:41.912: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:41.933: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:41.966: INFO: Lookups using dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05 failed for: [wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8196.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local]

Mar 29 22:17:46.808: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:46.827: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:46.917: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:46.932: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:46.967: INFO: Lookups using dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05 failed for: [wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local]

Mar 29 22:17:51.800: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:51.817: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:51.898: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:51.914: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:51.945: INFO: Lookups using dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05 failed for: [wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local]

Mar 29 22:17:56.802: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:56.819: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:56.909: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:56.925: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:17:56.959: INFO: Lookups using dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05 failed for: [wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local]

Mar 29 22:18:01.800: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:18:01.819: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:18:01.922: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:18:01.938: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local from pod dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05: the server could not find the requested resource (get pods dns-test-c27f3650-b687-41db-a5fa-7414385a1b05)
Mar 29 22:18:01.972: INFO: Lookups using dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05 failed for: [wheezy_udp@dns-test-service-2.dns-8196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8196.svc.cluster.local jessie_udp@dns-test-service-2.dns-8196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8196.svc.cluster.local]

Mar 29 22:18:06.974: INFO: DNS probes using dns-8196/dns-test-c27f3650-b687-41db-a5fa-7414385a1b05 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:18:07.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8196" for this suite.

• [SLOW TEST:34.840 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":277,"completed":247,"skipped":4000,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:18:07.096: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9794
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-9794
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 29 22:18:07.370: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 29 22:18:07.457: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 22:18:09.467: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 22:18:11.469: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:18:13.467: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:18:15.469: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:18:17.472: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:18:19.468: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 29 22:18:19.491: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 29 22:18:21.505: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 29 22:18:21.525: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 29 22:18:23.604: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.27:8080/dial?request=hostname&protocol=udp&host=172.30.124.171&port=8081&tries=1'] Namespace:pod-network-test-9794 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:18:23.604: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:18:23.807: INFO: Waiting for responses: map[]
Mar 29 22:18:23.817: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.27:8080/dial?request=hostname&protocol=udp&host=172.30.58.24&port=8081&tries=1'] Namespace:pod-network-test-9794 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:18:23.818: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:18:24.043: INFO: Waiting for responses: map[]
Mar 29 22:18:24.053: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.27:8080/dial?request=hostname&protocol=udp&host=172.30.209.254&port=8081&tries=1'] Namespace:pod-network-test-9794 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:18:24.053: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:18:24.247: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:18:24.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9794" for this suite.

• [SLOW TEST:17.185 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":277,"completed":248,"skipped":4002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:18:24.285: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5513
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-5513
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 29 22:18:24.495: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 29 22:18:24.583: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 22:18:26.594: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 22:18:28.596: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:18:30.595: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:18:32.593: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:18:34.595: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 29 22:18:36.595: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 29 22:18:36.615: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 29 22:18:38.628: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 29 22:18:40.626: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 29 22:18:42.625: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 29 22:18:42.647: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar 29 22:18:44.660: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 29 22:18:46.776: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.124.168:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5513 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:18:46.776: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:18:46.956: INFO: Found all expected endpoints: [netserver-0]
Mar 29 22:18:46.971: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.58.28:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5513 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:18:46.971: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:18:47.179: INFO: Found all expected endpoints: [netserver-1]
Mar 29 22:18:47.191: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.209.255:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5513 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 29 22:18:47.191: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
Mar 29 22:18:47.400: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:18:47.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5513" for this suite.

• [SLOW TEST:23.164 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":249,"skipped":4034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:18:47.451: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 22:18:47.731: INFO: (0) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 58.650748ms)
Mar 29 22:18:47.751: INFO: (1) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 19.019733ms)
Mar 29 22:18:47.775: INFO: (2) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 24.387748ms)
Mar 29 22:18:47.804: INFO: (3) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 28.83973ms)
Mar 29 22:18:47.822: INFO: (4) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 17.687387ms)
Mar 29 22:18:47.839: INFO: (5) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 17.003238ms)
Mar 29 22:18:47.857: INFO: (6) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 17.965386ms)
Mar 29 22:18:47.879: INFO: (7) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 22.060069ms)
Mar 29 22:18:47.899: INFO: (8) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 18.313387ms)
Mar 29 22:18:47.918: INFO: (9) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 18.501773ms)
Mar 29 22:18:47.934: INFO: (10) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 15.961119ms)
Mar 29 22:18:47.954: INFO: (11) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 20.237606ms)
Mar 29 22:18:47.976: INFO: (12) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 21.386721ms)
Mar 29 22:18:47.992: INFO: (13) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 16.081103ms)
Mar 29 22:18:48.010: INFO: (14) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 17.846579ms)
Mar 29 22:18:48.026: INFO: (15) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 16.28594ms)
Mar 29 22:18:48.045: INFO: (16) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 19.083501ms)
Mar 29 22:18:48.060: INFO: (17) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 14.287298ms)
Mar 29 22:18:48.076: INFO: (18) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 16.072295ms)
Mar 29 22:18:48.092: INFO: (19) /api/v1/nodes/10.189.118.194:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 16.028111ms)
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:18:48.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2165" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":277,"completed":250,"skipped":4069,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:18:48.119: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-7048
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-7048
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7048
STEP: Deleting pre-stop pod
Mar 29 22:19:01.468: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:19:01.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7048" for this suite.

• [SLOW TEST:13.403 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":277,"completed":251,"skipped":4071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:19:01.523: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Mar 29 22:19:01.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 cluster-info'
Mar 29 22:19:01.840: INFO: stderr: ""
Mar 29 22:19:01.840: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mNodeLocalDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/node-local-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:19:01.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1900" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":277,"completed":252,"skipped":4111,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:19:01.877: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 29 22:19:02.095: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 29 22:19:02.135: INFO: Waiting for terminating namespaces to be deleted...
Mar 29 22:19:02.146: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.194 before test
Mar 29 22:19:02.197: INFO: ibm-master-proxy-static-10.189.118.194 from kube-system started at 2021-03-29 19:48:41 +0000 UTC (2 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 22:19:02.197: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:19:02.197: INFO: coredns-7cc79848cf-8xnhq from kube-system started at 2021-03-29 19:57:06 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container coredns ready: true, restart count 0
Mar 29 22:19:02.197: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-ngv5d from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Mar 29 22:19:02.197: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 22:19:02.197: INFO: ibm-kubelet-monitor-jrnhz from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:19:02.197: INFO: ibm-cloud-provider-ip-169-63-130-250-65c6959456-tm4wj from ibm-system started at 2021-03-29 20:12:35 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container ibm-cloud-provider-ip-169-63-130-250 ready: true, restart count 0
Mar 29 22:19:02.197: INFO: sonobuoy from sonobuoy started at 2021-03-29 21:07:51 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 29 22:19:02.197: INFO: ibm-keepalived-watcher-5j4dz from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 22:19:02.197: INFO: calico-node-9tz6n from kube-system started at 2021-03-29 19:49:29 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 22:19:02.197: INFO: public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-vbjdf from kube-system started at 2021-03-29 20:11:55 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 29 22:19:02.197: INFO: addon-catalog-source-lgq2p from ibm-system started at 2021-03-29 19:53:09 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container configmap-registry-server ready: true, restart count 0
Mar 29 22:19:02.197: INFO: sonobuoy-e2e-job-95ac96a8209247c9 from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 22:19:02.197: INFO: 	Container e2e ready: true, restart count 0
Mar 29 22:19:02.197: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 22:19:02.197: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.196 before test
Mar 29 22:19:02.261: INFO: calico-node-mtdnd from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.261: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 22:19:02.261: INFO: ibm-master-proxy-static-10.189.118.196 from kube-system started at 2021-03-29 19:48:58 +0000 UTC (2 container statuses recorded)
Mar 29 22:19:02.261: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 22:19:02.261: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:19:02.261: INFO: ibm-kubelet-monitor-cmcxj from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.261: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:19:02.261: INFO: server from prestop-7048 started at 2021-03-29 22:18:48 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.261: INFO: 	Container server ready: true, restart count 0
Mar 29 22:19:02.261: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-l4nd5 from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 22:19:02.261: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Mar 29 22:19:02.261: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 22:19:02.261: INFO: tester from prestop-7048 started at 2021-03-29 22:18:52 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.261: INFO: 	Container tester ready: true, restart count 0
Mar 29 22:19:02.261: INFO: ibm-keepalived-watcher-4x7cv from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.261: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 22:19:02.261: INFO: 
Logging pods the kubelet thinks is on node 10.189.118.208 before test
Mar 29 22:19:02.408: INFO: coredns-7cc79848cf-m5klb from kube-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.408: INFO: 	Container coredns ready: true, restart count 0
Mar 29 22:19:02.408: INFO: ibm-file-plugin-c76c68fd9-bpt7r from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.408: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 29 22:19:02.408: INFO: calico-node-zjnmd from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.408: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 22:19:02.408: INFO: kubernetes-dashboard-6cf6cfdf4-kzffs from kube-system started at 2021-03-29 19:49:50 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.408: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 29 22:19:02.409: INFO: ibm-cloud-provider-ip-169-63-130-250-65c6959456-gps6w from ibm-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container ibm-cloud-provider-ip-169-63-130-250 ready: true, restart count 0
Mar 29 22:19:02.409: INFO: public-crc1h2ld5w0208nq2let2g-alb1-7766bfb67d-2lcqz from kube-system started at 2021-03-29 21:27:23 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 29 22:19:02.409: INFO: ibm-kubelet-monitor-gbcvf from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:19:02.409: INFO: vpn-657cb5cdb6-v5h8m from kube-system started at 2021-03-29 19:56:31 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container vpn ready: true, restart count 0
Mar 29 22:19:02.409: INFO: ibm-storage-watcher-659bdcc695-qqkhw from kube-system started at 2021-03-29 19:49:50 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 29 22:19:02.409: INFO: ibm-keepalived-watcher-ps2vt from kube-system started at 2021-03-29 19:49:30 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 29 22:19:02.409: INFO: olm-operator-8496678794-gg5v7 from ibm-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container olm-operator ready: true, restart count 0
Mar 29 22:19:02.409: INFO: sonobuoy-systemd-logs-daemon-set-6c49ba06995549a4-n7lmz from sonobuoy started at 2021-03-29 21:07:56 +0000 UTC (2 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Mar 29 22:19:02.409: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 22:19:02.409: INFO: coredns-autoscaler-bff977695-9wkhn from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container autoscaler ready: true, restart count 0
Mar 29 22:19:02.409: INFO: calico-kube-controllers-6599f97f59-7g48l from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 29 22:19:02.409: INFO: catalog-operator-7bc4c797b5-kbbnz from ibm-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.409: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 29 22:19:02.409: INFO: dashboard-metrics-scraper-b585c6867-2k8ww from kube-system started at 2021-03-29 19:49:40 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.410: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 29 22:19:02.410: INFO: metrics-server-f9f7549df-6xdmf from kube-system started at 2021-03-29 19:50:53 +0000 UTC (2 container statuses recorded)
Mar 29 22:19:02.410: INFO: 	Container metrics-server ready: true, restart count 0
Mar 29 22:19:02.410: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 29 22:19:02.410: INFO: ibm-master-proxy-static-10.189.118.208 from kube-system started at 2021-03-29 19:48:43 +0000 UTC (2 container statuses recorded)
Mar 29 22:19:02.410: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 29 22:19:02.410: INFO: 	Container pause ready: true, restart count 0
Mar 29 22:19:02.410: INFO: coredns-7cc79848cf-kjg7p from kube-system started at 2021-03-29 19:57:07 +0000 UTC (1 container statuses recorded)
Mar 29 22:19:02.410: INFO: 	Container coredns ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9a1f3f13-d878-49f9-8285-b7063806af81 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-9a1f3f13-d878-49f9-8285-b7063806af81 off the node 10.189.118.196
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9a1f3f13-d878-49f9-8285-b7063806af81
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:19:18.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2679" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:16.849 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":277,"completed":253,"skipped":4116,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:19:18.727: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-284e7434-c3d4-4ca5-be0b-442322bd4635
STEP: Creating a pod to test consume configMaps
Mar 29 22:19:18.975: INFO: Waiting up to 5m0s for pod "pod-configmaps-e264e25c-5a07-4454-8984-016472091ef9" in namespace "configmap-8407" to be "Succeeded or Failed"
Mar 29 22:19:18.986: INFO: Pod "pod-configmaps-e264e25c-5a07-4454-8984-016472091ef9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.688097ms
Mar 29 22:19:20.999: INFO: Pod "pod-configmaps-e264e25c-5a07-4454-8984-016472091ef9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023985078s
STEP: Saw pod success
Mar 29 22:19:20.999: INFO: Pod "pod-configmaps-e264e25c-5a07-4454-8984-016472091ef9" satisfied condition "Succeeded or Failed"
Mar 29 22:19:21.009: INFO: Trying to get logs from node 10.189.118.196 pod pod-configmaps-e264e25c-5a07-4454-8984-016472091ef9 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 22:19:21.068: INFO: Waiting for pod pod-configmaps-e264e25c-5a07-4454-8984-016472091ef9 to disappear
Mar 29 22:19:21.078: INFO: Pod pod-configmaps-e264e25c-5a07-4454-8984-016472091ef9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:19:21.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8407" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":254,"skipped":4167,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:19:21.110: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 29 22:19:21.339: INFO: Waiting up to 5m0s for pod "downwardapi-volume-520b53d0-aba6-4ab9-bf94-1091f9cdcd63" in namespace "downward-api-5595" to be "Succeeded or Failed"
Mar 29 22:19:21.351: INFO: Pod "downwardapi-volume-520b53d0-aba6-4ab9-bf94-1091f9cdcd63": Phase="Pending", Reason="", readiness=false. Elapsed: 11.604853ms
Mar 29 22:19:23.362: INFO: Pod "downwardapi-volume-520b53d0-aba6-4ab9-bf94-1091f9cdcd63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02262112s
Mar 29 22:19:25.375: INFO: Pod "downwardapi-volume-520b53d0-aba6-4ab9-bf94-1091f9cdcd63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035704872s
STEP: Saw pod success
Mar 29 22:19:25.375: INFO: Pod "downwardapi-volume-520b53d0-aba6-4ab9-bf94-1091f9cdcd63" satisfied condition "Succeeded or Failed"
Mar 29 22:19:25.385: INFO: Trying to get logs from node 10.189.118.196 pod downwardapi-volume-520b53d0-aba6-4ab9-bf94-1091f9cdcd63 container client-container: <nil>
STEP: delete the pod
Mar 29 22:19:25.445: INFO: Waiting for pod downwardapi-volume-520b53d0-aba6-4ab9-bf94-1091f9cdcd63 to disappear
Mar 29 22:19:25.456: INFO: Pod downwardapi-volume-520b53d0-aba6-4ab9-bf94-1091f9cdcd63 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:19:25.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5595" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":255,"skipped":4179,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:19:25.487: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-325b47ba-22f0-4d23-be52-e0d183e80cae in namespace container-probe-8364
Mar 29 22:19:29.727: INFO: Started pod liveness-325b47ba-22f0-4d23-be52-e0d183e80cae in namespace container-probe-8364
STEP: checking the pod's current state and verifying that restartCount is present
Mar 29 22:19:29.738: INFO: Initial restart count of pod liveness-325b47ba-22f0-4d23-be52-e0d183e80cae is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:23:31.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8364" for this suite.

• [SLOW TEST:245.760 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":277,"completed":256,"skipped":4185,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:23:31.249: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:160
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:23:31.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2767" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":277,"completed":257,"skipped":4192,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:23:31.534: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5349
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-b4ng
STEP: Creating a pod to test atomic-volume-subpath
Mar 29 22:23:31.795: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-b4ng" in namespace "subpath-5349" to be "Succeeded or Failed"
Mar 29 22:23:31.805: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Pending", Reason="", readiness=false. Elapsed: 9.85559ms
Mar 29 22:23:33.816: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Running", Reason="", readiness=true. Elapsed: 2.021196186s
Mar 29 22:23:35.829: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Running", Reason="", readiness=true. Elapsed: 4.034188236s
Mar 29 22:23:37.845: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Running", Reason="", readiness=true. Elapsed: 6.050224215s
Mar 29 22:23:39.857: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Running", Reason="", readiness=true. Elapsed: 8.062076104s
Mar 29 22:23:41.868: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Running", Reason="", readiness=true. Elapsed: 10.072400706s
Mar 29 22:23:43.881: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Running", Reason="", readiness=true. Elapsed: 12.085327106s
Mar 29 22:23:45.937: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Running", Reason="", readiness=true. Elapsed: 14.141425444s
Mar 29 22:23:47.948: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Running", Reason="", readiness=true. Elapsed: 16.153224906s
Mar 29 22:23:49.958: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Running", Reason="", readiness=true. Elapsed: 18.163259868s
Mar 29 22:23:51.971: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Running", Reason="", readiness=true. Elapsed: 20.175387577s
Mar 29 22:23:53.981: INFO: Pod "pod-subpath-test-secret-b4ng": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.185819477s
STEP: Saw pod success
Mar 29 22:23:53.981: INFO: Pod "pod-subpath-test-secret-b4ng" satisfied condition "Succeeded or Failed"
Mar 29 22:23:53.992: INFO: Trying to get logs from node 10.189.118.196 pod pod-subpath-test-secret-b4ng container test-container-subpath-secret-b4ng: <nil>
STEP: delete the pod
Mar 29 22:23:54.102: INFO: Waiting for pod pod-subpath-test-secret-b4ng to disappear
Mar 29 22:23:54.114: INFO: Pod pod-subpath-test-secret-b4ng no longer exists
STEP: Deleting pod pod-subpath-test-secret-b4ng
Mar 29 22:23:54.114: INFO: Deleting pod "pod-subpath-test-secret-b4ng" in namespace "subpath-5349"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:23:54.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5349" for this suite.

• [SLOW TEST:22.619 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":277,"completed":258,"skipped":4205,"failed":0}
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:23:54.154: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 29 22:23:55.037: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 29 22:23:57.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653435, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653435, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653435, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653435, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 22:24:00.112: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 22:24:00.124: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:24:01.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1657" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.625 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":277,"completed":259,"skipped":4205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:24:01.781: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Mar 29 22:24:02.561: INFO: created pod pod-service-account-defaultsa
Mar 29 22:24:02.561: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 29 22:24:02.575: INFO: created pod pod-service-account-mountsa
Mar 29 22:24:02.575: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 29 22:24:02.589: INFO: created pod pod-service-account-nomountsa
Mar 29 22:24:02.589: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 29 22:24:02.607: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 29 22:24:02.607: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 29 22:24:02.619: INFO: created pod pod-service-account-mountsa-mountspec
Mar 29 22:24:02.619: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 29 22:24:02.633: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 29 22:24:02.633: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 29 22:24:02.648: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 29 22:24:02.648: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 29 22:24:02.665: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 29 22:24:02.665: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 29 22:24:02.679: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 29 22:24:02.679: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:24:02.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4539" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":277,"completed":260,"skipped":4261,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:24:02.713: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-7664fc43-f6d0-4234-ac46-6007f3528059
STEP: Creating a pod to test consume secrets
Mar 29 22:24:02.956: INFO: Waiting up to 5m0s for pod "pod-secrets-a48da990-0fde-4f27-8d44-47e63059c16a" in namespace "secrets-2790" to be "Succeeded or Failed"
Mar 29 22:24:02.966: INFO: Pod "pod-secrets-a48da990-0fde-4f27-8d44-47e63059c16a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.297989ms
Mar 29 22:24:04.978: INFO: Pod "pod-secrets-a48da990-0fde-4f27-8d44-47e63059c16a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021532479s
STEP: Saw pod success
Mar 29 22:24:04.978: INFO: Pod "pod-secrets-a48da990-0fde-4f27-8d44-47e63059c16a" satisfied condition "Succeeded or Failed"
Mar 29 22:24:04.989: INFO: Trying to get logs from node 10.189.118.208 pod pod-secrets-a48da990-0fde-4f27-8d44-47e63059c16a container secret-volume-test: <nil>
STEP: delete the pod
Mar 29 22:24:05.079: INFO: Waiting for pod pod-secrets-a48da990-0fde-4f27-8d44-47e63059c16a to disappear
Mar 29 22:24:05.091: INFO: Pod pod-secrets-a48da990-0fde-4f27-8d44-47e63059c16a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:24:05.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2790" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":261,"skipped":4276,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:24:05.124: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 29 22:24:09.947: INFO: Successfully updated pod "pod-update-activedeadlineseconds-41701d74-d88b-44fe-82c4-15910d12697e"
Mar 29 22:24:09.947: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-41701d74-d88b-44fe-82c4-15910d12697e" in namespace "pods-6828" to be "terminated due to deadline exceeded"
Mar 29 22:24:09.962: INFO: Pod "pod-update-activedeadlineseconds-41701d74-d88b-44fe-82c4-15910d12697e": Phase="Running", Reason="", readiness=true. Elapsed: 15.337067ms
Mar 29 22:24:11.974: INFO: Pod "pod-update-activedeadlineseconds-41701d74-d88b-44fe-82c4-15910d12697e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.027119575s
Mar 29 22:24:11.974: INFO: Pod "pod-update-activedeadlineseconds-41701d74-d88b-44fe-82c4-15910d12697e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:24:11.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6828" for this suite.

• [SLOW TEST:6.883 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":277,"completed":262,"skipped":4288,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:24:12.008: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-a2b51be0-9074-4ecc-ab18-36ed3aaa9a03
STEP: Creating a pod to test consume configMaps
Mar 29 22:24:12.287: INFO: Waiting up to 5m0s for pod "pod-configmaps-9167d88b-1aa7-4e8e-a3eb-18b609a0c211" in namespace "configmap-4496" to be "Succeeded or Failed"
Mar 29 22:24:12.297: INFO: Pod "pod-configmaps-9167d88b-1aa7-4e8e-a3eb-18b609a0c211": Phase="Pending", Reason="", readiness=false. Elapsed: 9.832596ms
Mar 29 22:24:14.311: INFO: Pod "pod-configmaps-9167d88b-1aa7-4e8e-a3eb-18b609a0c211": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023459745s
STEP: Saw pod success
Mar 29 22:24:14.311: INFO: Pod "pod-configmaps-9167d88b-1aa7-4e8e-a3eb-18b609a0c211" satisfied condition "Succeeded or Failed"
Mar 29 22:24:14.322: INFO: Trying to get logs from node 10.189.118.196 pod pod-configmaps-9167d88b-1aa7-4e8e-a3eb-18b609a0c211 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 29 22:24:14.385: INFO: Waiting for pod pod-configmaps-9167d88b-1aa7-4e8e-a3eb-18b609a0c211 to disappear
Mar 29 22:24:14.395: INFO: Pod pod-configmaps-9167d88b-1aa7-4e8e-a3eb-18b609a0c211 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:24:14.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4496" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":263,"skipped":4300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:24:14.428: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 22:24:15.150: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 22:24:17.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653455, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653455, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653455, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653455, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 22:24:20.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 22:24:20.243: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:24:21.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6382" for this suite.
STEP: Destroying namespace "webhook-6382-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.472 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":277,"completed":264,"skipped":4345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:24:21.902: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3808
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 29 22:24:22.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-3808'
Mar 29 22:24:22.350: INFO: stderr: ""
Mar 29 22:24:22.350: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 29 22:24:27.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pod e2e-test-httpd-pod --namespace=kubectl-3808 -o json'
Mar 29 22:24:27.491: INFO: stderr: ""
Mar 29 22:24:27.491: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-03-29T22:24:22Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-29T22:24:22Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.30.58.44\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-29T22:24:23Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3808\",\n        \"resourceVersion\": \"57003\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3808/pods/e2e-test-httpd-pod\",\n        \"uid\": \"2bf4c9e5-6388-43c6-9f79-cafdc99ade2e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-clx6q\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.189.118.196\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-clx6q\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-clx6q\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-29T22:24:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-29T22:24:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-29T22:24:23Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-29T22:24:22Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://83de7d160616e89e962348eaddc5b566f98bfa00bbed55c3300c22b95c36c89b\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-03-29T22:24:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.189.118.196\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.58.44\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.58.44\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-03-29T22:24:22Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 29 22:24:27.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 replace -f - --namespace=kubectl-3808'
Mar 29 22:24:27.824: INFO: stderr: ""
Mar 29 22:24:27.824: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Mar 29 22:24:27.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete pods e2e-test-httpd-pod --namespace=kubectl-3808'
Mar 29 22:24:36.956: INFO: stderr: ""
Mar 29 22:24:36.956: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:24:36.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3808" for this suite.

• [SLOW TEST:15.087 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":277,"completed":265,"skipped":4371,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:24:36.989: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4304
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-4304/configmap-test-c3365870-2f42-44af-98c4-54e817086a6a
STEP: Creating a pod to test consume configMaps
Mar 29 22:24:37.236: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e2d2d7b-7c27-4e2e-9068-afa3dc5db5e7" in namespace "configmap-4304" to be "Succeeded or Failed"
Mar 29 22:24:37.253: INFO: Pod "pod-configmaps-7e2d2d7b-7c27-4e2e-9068-afa3dc5db5e7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.405587ms
Mar 29 22:24:39.272: INFO: Pod "pod-configmaps-7e2d2d7b-7c27-4e2e-9068-afa3dc5db5e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035783693s
STEP: Saw pod success
Mar 29 22:24:39.272: INFO: Pod "pod-configmaps-7e2d2d7b-7c27-4e2e-9068-afa3dc5db5e7" satisfied condition "Succeeded or Failed"
Mar 29 22:24:39.283: INFO: Trying to get logs from node 10.189.118.196 pod pod-configmaps-7e2d2d7b-7c27-4e2e-9068-afa3dc5db5e7 container env-test: <nil>
STEP: delete the pod
Mar 29 22:24:39.347: INFO: Waiting for pod pod-configmaps-7e2d2d7b-7c27-4e2e-9068-afa3dc5db5e7 to disappear
Mar 29 22:24:39.364: INFO: Pod pod-configmaps-7e2d2d7b-7c27-4e2e-9068-afa3dc5db5e7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:24:39.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4304" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":266,"skipped":4412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:24:39.397: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3680
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-6c5f54db-f8d0-47bb-9226-842d080731b4
STEP: Creating secret with name s-test-opt-upd-bd3f8125-bfd6-46e9-a583-61fcf9f846b2
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6c5f54db-f8d0-47bb-9226-842d080731b4
STEP: Updating secret s-test-opt-upd-bd3f8125-bfd6-46e9-a583-61fcf9f846b2
STEP: Creating secret with name s-test-opt-create-766ace4c-d1ad-41f0-8ec1-0e4645424a72
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:25:52.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3680" for this suite.

• [SLOW TEST:73.582 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":267,"skipped":4451,"failed":0}
SS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:25:52.979: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-6401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:25:53.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6401" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":277,"completed":268,"skipped":4453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:25:53.268: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 29 22:25:54.087: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 29 22:25:56.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653554, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653554, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653554, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653554, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 29 22:25:59.168: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 29 22:26:03.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 attach --namespace=webhook-8189 to-be-attached-pod -i -c=container1'
Mar 29 22:26:03.414: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:26:03.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8189" for this suite.
STEP: Destroying namespace "webhook-8189-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.411 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":277,"completed":269,"skipped":4485,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:26:03.680: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 29 22:26:06.963: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:26:07.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3879" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":277,"completed":270,"skipped":4524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:26:07.041: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3514
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9073
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:26:13.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6097" for this suite.
STEP: Destroying namespace "nsdeletetest-3514" for this suite.
Mar 29 22:26:13.827: INFO: Namespace nsdeletetest-3514 was already deleted
STEP: Destroying namespace "nsdeletetest-9073" for this suite.

• [SLOW TEST:6.800 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":277,"completed":271,"skipped":4551,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:26:13.843: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Mar 29 22:26:14.048: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Mar 29 22:26:14.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-1139'
Mar 29 22:26:14.391: INFO: stderr: ""
Mar 29 22:26:14.391: INFO: stdout: "service/agnhost-slave created\n"
Mar 29 22:26:14.391: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Mar 29 22:26:14.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-1139'
Mar 29 22:26:14.735: INFO: stderr: ""
Mar 29 22:26:14.735: INFO: stdout: "service/agnhost-master created\n"
Mar 29 22:26:14.736: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 29 22:26:14.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-1139'
Mar 29 22:26:15.086: INFO: stderr: ""
Mar 29 22:26:15.086: INFO: stdout: "service/frontend created\n"
Mar 29 22:26:15.086: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 29 22:26:15.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-1139'
Mar 29 22:26:15.280: INFO: stderr: ""
Mar 29 22:26:15.280: INFO: stdout: "deployment.apps/frontend created\n"
Mar 29 22:26:15.280: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 29 22:26:15.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-1139'
Mar 29 22:26:15.643: INFO: stderr: ""
Mar 29 22:26:15.643: INFO: stdout: "deployment.apps/agnhost-master created\n"
Mar 29 22:26:15.643: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 29 22:26:15.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-1139'
Mar 29 22:26:15.974: INFO: stderr: ""
Mar 29 22:26:15.974: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Mar 29 22:26:15.974: INFO: Waiting for all frontend pods to be Running.
Mar 29 22:26:21.024: INFO: Waiting for frontend to serve content.
Mar 29 22:26:21.066: INFO: Trying to add a new entry to the guestbook.
Mar 29 22:26:21.105: INFO: Verifying that added entry can be retrieved.
Mar 29 22:26:21.136: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Mar 29 22:26:26.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete --grace-period=0 --force -f - --namespace=kubectl-1139'
Mar 29 22:26:26.309: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 22:26:26.309: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 29 22:26:26.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete --grace-period=0 --force -f - --namespace=kubectl-1139'
Mar 29 22:26:26.484: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 22:26:26.484: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 29 22:26:26.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete --grace-period=0 --force -f - --namespace=kubectl-1139'
Mar 29 22:26:26.648: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 22:26:26.648: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 29 22:26:26.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete --grace-period=0 --force -f - --namespace=kubectl-1139'
Mar 29 22:26:26.766: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 22:26:26.766: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 29 22:26:26.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete --grace-period=0 --force -f - --namespace=kubectl-1139'
Mar 29 22:26:26.877: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 22:26:26.877: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 29 22:26:26.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete --grace-period=0 --force -f - --namespace=kubectl-1139'
Mar 29 22:26:27.018: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 22:26:27.018: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:26:27.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1139" for this suite.

• [SLOW TEST:13.205 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:310
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":277,"completed":272,"skipped":4562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:26:27.048: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-1591
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 29 22:26:27.251: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Creating first CR 
Mar 29 22:26:27.905: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-29T22:26:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-29T22:26:27Z]] name:name1 resourceVersion:57904 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:307411d4-6214-4309-a729-61a949c073d8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 29 22:26:37.920: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-29T22:26:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-29T22:26:37Z]] name:name2 resourceVersion:57995 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:643249b8-686f-46d3-8665-1174949d7c2a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 29 22:26:47.939: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-29T22:26:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-29T22:26:47Z]] name:name1 resourceVersion:58028 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:307411d4-6214-4309-a729-61a949c073d8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 29 22:26:57.955: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-29T22:26:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-29T22:26:57Z]] name:name2 resourceVersion:58057 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:643249b8-686f-46d3-8665-1174949d7c2a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 29 22:27:07.983: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-29T22:26:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-29T22:26:47Z]] name:name1 resourceVersion:58083 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:307411d4-6214-4309-a729-61a949c073d8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 29 22:27:18.011: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-29T22:26:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-29T22:26:57Z]] name:name2 resourceVersion:58109 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:643249b8-686f-46d3-8665-1174949d7c2a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:27:28.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1591" for this suite.

• [SLOW TEST:61.529 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":277,"completed":273,"skipped":4604,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:27:28.577: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:27:45.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3917" for this suite.

• [SLOW TEST:17.356 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":277,"completed":274,"skipped":4621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:27:45.937: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8630
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar 29 22:27:46.154: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Mar 29 22:27:47.092: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 29 22:27:49.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 22:27:51.216: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 22:27:53.337: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 22:27:55.217: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 22:27:57.220: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752653667, loc:(*time.Location)(0x7b4c620)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 22:28:00.405: INFO: Waited 1.170408768s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:28:00.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8630" for this suite.

• [SLOW TEST:15.132 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":277,"completed":275,"skipped":4653,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:28:01.069: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 29 22:28:01.398: INFO: Number of nodes with available pods: 0
Mar 29 22:28:01.398: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 22:28:02.428: INFO: Number of nodes with available pods: 0
Mar 29 22:28:02.428: INFO: Node 10.189.118.194 is running more than one daemon pod
Mar 29 22:28:03.433: INFO: Number of nodes with available pods: 2
Mar 29 22:28:03.433: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:04.423: INFO: Number of nodes with available pods: 3
Mar 29 22:28:04.423: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 29 22:28:04.521: INFO: Number of nodes with available pods: 2
Mar 29 22:28:04.521: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:05.545: INFO: Number of nodes with available pods: 2
Mar 29 22:28:05.545: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:06.545: INFO: Number of nodes with available pods: 2
Mar 29 22:28:06.545: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:07.546: INFO: Number of nodes with available pods: 2
Mar 29 22:28:07.546: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:08.549: INFO: Number of nodes with available pods: 2
Mar 29 22:28:08.549: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:09.546: INFO: Number of nodes with available pods: 2
Mar 29 22:28:09.547: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:10.549: INFO: Number of nodes with available pods: 2
Mar 29 22:28:10.549: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:11.545: INFO: Number of nodes with available pods: 2
Mar 29 22:28:11.545: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:12.549: INFO: Number of nodes with available pods: 2
Mar 29 22:28:12.549: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:13.548: INFO: Number of nodes with available pods: 2
Mar 29 22:28:13.548: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:14.546: INFO: Number of nodes with available pods: 2
Mar 29 22:28:14.546: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:15.549: INFO: Number of nodes with available pods: 2
Mar 29 22:28:15.549: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:16.638: INFO: Number of nodes with available pods: 2
Mar 29 22:28:16.638: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:17.660: INFO: Number of nodes with available pods: 2
Mar 29 22:28:17.660: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:18.545: INFO: Number of nodes with available pods: 2
Mar 29 22:28:18.545: INFO: Node 10.189.118.196 is running more than one daemon pod
Mar 29 22:28:19.559: INFO: Number of nodes with available pods: 3
Mar 29 22:28:19.559: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-991, will wait for the garbage collector to delete the pods
Mar 29 22:28:19.656: INFO: Deleting DaemonSet.extensions daemon-set took: 25.973309ms
Mar 29 22:28:19.757: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.473917ms
Mar 29 22:28:32.569: INFO: Number of nodes with available pods: 0
Mar 29 22:28:32.569: INFO: Number of running nodes: 0, number of available pods: 0
Mar 29 22:28:32.579: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-991/daemonsets","resourceVersion":"58552"},"items":null}

Mar 29 22:28:32.589: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-991/pods","resourceVersion":"58552"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:28:32.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-991" for this suite.

• [SLOW TEST:31.595 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":277,"completed":276,"skipped":4669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 29 22:28:32.665: INFO: >>> kubeConfig: /tmp/kubeconfig-820232969
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Mar 29 22:28:32.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 create -f - --namespace=kubectl-1157'
Mar 29 22:28:33.202: INFO: stderr: ""
Mar 29 22:28:33.202: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 29 22:28:33.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1157'
Mar 29 22:28:33.308: INFO: stderr: ""
Mar 29 22:28:33.308: INFO: stdout: "update-demo-nautilus-7jgrg update-demo-nautilus-pfmd9 "
Mar 29 22:28:33.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-7jgrg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:28:33.404: INFO: stderr: ""
Mar 29 22:28:33.404: INFO: stdout: ""
Mar 29 22:28:33.404: INFO: update-demo-nautilus-7jgrg is created but not running
Mar 29 22:28:38.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1157'
Mar 29 22:28:38.503: INFO: stderr: ""
Mar 29 22:28:38.503: INFO: stdout: "update-demo-nautilus-7jgrg update-demo-nautilus-pfmd9 "
Mar 29 22:28:38.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-7jgrg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:28:38.608: INFO: stderr: ""
Mar 29 22:28:38.608: INFO: stdout: "true"
Mar 29 22:28:38.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-7jgrg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:28:38.708: INFO: stderr: ""
Mar 29 22:28:38.708: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 29 22:28:38.709: INFO: validating pod update-demo-nautilus-7jgrg
Mar 29 22:28:38.750: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 22:28:38.750: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 22:28:38.750: INFO: update-demo-nautilus-7jgrg is verified up and running
Mar 29 22:28:38.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-pfmd9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:28:38.849: INFO: stderr: ""
Mar 29 22:28:38.849: INFO: stdout: "true"
Mar 29 22:28:38.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-pfmd9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:28:38.934: INFO: stderr: ""
Mar 29 22:28:38.934: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 29 22:28:38.934: INFO: validating pod update-demo-nautilus-pfmd9
Mar 29 22:28:38.957: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 22:28:38.957: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 22:28:38.957: INFO: update-demo-nautilus-pfmd9 is verified up and running
STEP: scaling down the replication controller
Mar 29 22:28:38.960: INFO: scanned /root for discovery docs: <nil>
Mar 29 22:28:38.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1157'
Mar 29 22:28:40.100: INFO: stderr: ""
Mar 29 22:28:40.100: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 29 22:28:40.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1157'
Mar 29 22:28:40.210: INFO: stderr: ""
Mar 29 22:28:40.210: INFO: stdout: "update-demo-nautilus-7jgrg update-demo-nautilus-pfmd9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 29 22:28:45.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1157'
Mar 29 22:28:45.307: INFO: stderr: ""
Mar 29 22:28:45.307: INFO: stdout: "update-demo-nautilus-7jgrg update-demo-nautilus-pfmd9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 29 22:28:50.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1157'
Mar 29 22:28:50.402: INFO: stderr: ""
Mar 29 22:28:50.402: INFO: stdout: "update-demo-nautilus-7jgrg update-demo-nautilus-pfmd9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 29 22:28:55.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1157'
Mar 29 22:28:55.501: INFO: stderr: ""
Mar 29 22:28:55.502: INFO: stdout: "update-demo-nautilus-pfmd9 "
Mar 29 22:28:55.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-pfmd9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:28:55.585: INFO: stderr: ""
Mar 29 22:28:55.585: INFO: stdout: "true"
Mar 29 22:28:55.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-pfmd9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:28:55.721: INFO: stderr: ""
Mar 29 22:28:55.721: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 29 22:28:55.721: INFO: validating pod update-demo-nautilus-pfmd9
Mar 29 22:28:55.739: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 22:28:55.739: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 22:28:55.739: INFO: update-demo-nautilus-pfmd9 is verified up and running
STEP: scaling up the replication controller
Mar 29 22:28:55.741: INFO: scanned /root for discovery docs: <nil>
Mar 29 22:28:55.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1157'
Mar 29 22:28:56.874: INFO: stderr: ""
Mar 29 22:28:56.874: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 29 22:28:56.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1157'
Mar 29 22:28:57.093: INFO: stderr: ""
Mar 29 22:28:57.093: INFO: stdout: "update-demo-nautilus-fr5dj update-demo-nautilus-pfmd9 "
Mar 29 22:28:57.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-fr5dj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:28:57.194: INFO: stderr: ""
Mar 29 22:28:57.194: INFO: stdout: ""
Mar 29 22:28:57.194: INFO: update-demo-nautilus-fr5dj is created but not running
Mar 29 22:29:02.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1157'
Mar 29 22:29:02.290: INFO: stderr: ""
Mar 29 22:29:02.291: INFO: stdout: "update-demo-nautilus-fr5dj update-demo-nautilus-pfmd9 "
Mar 29 22:29:02.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-fr5dj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:29:02.382: INFO: stderr: ""
Mar 29 22:29:02.382: INFO: stdout: "true"
Mar 29 22:29:02.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-fr5dj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:29:02.472: INFO: stderr: ""
Mar 29 22:29:02.472: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 29 22:29:02.472: INFO: validating pod update-demo-nautilus-fr5dj
Mar 29 22:29:02.497: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 22:29:02.497: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 22:29:02.497: INFO: update-demo-nautilus-fr5dj is verified up and running
Mar 29 22:29:02.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-pfmd9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:29:02.586: INFO: stderr: ""
Mar 29 22:29:02.586: INFO: stdout: "true"
Mar 29 22:29:02.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods update-demo-nautilus-pfmd9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1157'
Mar 29 22:29:02.701: INFO: stderr: ""
Mar 29 22:29:02.701: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 29 22:29:02.701: INFO: validating pod update-demo-nautilus-pfmd9
Mar 29 22:29:02.720: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 22:29:02.720: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 22:29:02.720: INFO: update-demo-nautilus-pfmd9 is verified up and running
STEP: using delete to clean up resources
Mar 29 22:29:02.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 delete --grace-period=0 --force -f - --namespace=kubectl-1157'
Mar 29 22:29:02.880: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 22:29:02.880: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 29 22:29:02.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1157'
Mar 29 22:29:02.987: INFO: stderr: "No resources found in kubectl-1157 namespace.\n"
Mar 29 22:29:02.987: INFO: stdout: ""
Mar 29 22:29:02.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-820232969 get pods -l name=update-demo --namespace=kubectl-1157 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 29 22:29:03.099: INFO: stderr: ""
Mar 29 22:29:03.099: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 29 22:29:03.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1157" for this suite.

• [SLOW TEST:30.472 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":277,"completed":277,"skipped":4714,"failed":0}
SSSMar 29 22:29:03.137: INFO: Running AfterSuite actions on all nodes
Mar 29 22:29:03.137: INFO: Running AfterSuite actions on node 1
Mar 29 22:29:03.137: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":277,"completed":277,"skipped":4717,"failed":0}

Ran 277 of 4994 Specs in 4851.543 seconds
SUCCESS! -- 277 Passed | 0 Failed | 0 Pending | 4717 Skipped
PASS

Ginkgo ran 1 suite in 1h20m52.973394365s
Test Suite Passed
