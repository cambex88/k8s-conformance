Conformance test: not doing test setup.
Feb 21 08:25:36.707: INFO: Overriding default scale value of zero to 1
Feb 21 08:25:36.707: INFO: Overriding default milliseconds value of zero to 5000
I0221 08:25:37.288181   32452 e2e.go:304] Starting e2e run "4399f893-35b2-11e9-b7c5-a2b84e263bfe" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1550737536 - Will randomize all specs
Will run 188 of 2011 specs

Feb 21 08:25:37.562: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 08:25:37.564: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 21 08:25:37.596: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 21 08:25:37.638: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 21 08:25:37.638: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 21 08:25:37.638: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 21 08:25:37.652: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 21 08:25:37.652: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 21 08:25:37.652: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 21 08:25:37.652: INFO: e2e test version: v1.12.5
Feb 21 08:25:37.655: INFO: kube-apiserver version: v1.12.5
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:25:37.656: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
Feb 21 08:25:38.108: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 21 08:25:38.132: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-kl64f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 08:27:16.276: INFO: Container started at 2019-02-21 08:26:58 +0000 UTC, pod became ready at 2019-02-21 08:27:14 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:27:16.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-kl64f" for this suite.
Feb 21 08:27:38.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:27:38.394: INFO: namespace: e2e-tests-container-probe-kl64f, resource: bindings, ignored listing per whitelist
Feb 21 08:27:38.658: INFO: namespace e2e-tests-container-probe-kl64f deletion completed in 22.37712413s

• [SLOW TEST:121.003 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:27:38.659: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-2qg6n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0221 08:27:49.107060   32452 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 21 08:27:49.107: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:27:49.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2qg6n" for this suite.
Feb 21 08:27:55.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:27:55.185: INFO: namespace: e2e-tests-gc-2qg6n, resource: bindings, ignored listing per whitelist
Feb 21 08:27:55.325: INFO: namespace e2e-tests-gc-2qg6n deletion completed in 6.213872624s

• [SLOW TEST:16.667 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:27:55.326: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-6x6kr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 21 08:32:51.653: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-9675649a-35b2-11e9-b7c5-a2b84e263bfe,GenerateName:,Namespace:e2e-tests-events-6x6kr,SelfLink:/api/v1/namespaces/e2e-tests-events-6x6kr/pods/send-events-9675649a-35b2-11e9-b7c5-a2b84e263bfe,UID:9676ba56-35b2-11e9-890e-3aac84b8a476,ResourceVersion:4968,Generation:0,CreationTimestamp:2019-02-21 08:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 623751174,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.9/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-qrkq5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qrkq5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-qrkq5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0011b7f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0011b7fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:27:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:32:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:32:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:27:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:100.96.1.9,StartTime:2019-02-21 08:27:55 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-21 08:32:50 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://69bda13f274b5dec9597ed0a5915b8eda9ff1c49c296ac6eb9b3ca7c7177aadb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 21 08:32:53.659: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 21 08:32:55.667: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:32:55.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-6x6kr" for this suite.
Feb 21 08:33:37.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:33:38.019: INFO: namespace: e2e-tests-events-6x6kr, resource: bindings, ignored listing per whitelist
Feb 21 08:33:38.049: INFO: namespace e2e-tests-events-6x6kr deletion completed in 42.366270471s

• [SLOW TEST:342.724 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:33:38.050: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-w8f52
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-62cad4ae-35b3-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 08:33:38.453: INFO: Waiting up to 5m0s for pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-secrets-w8f52" to be "success or failure"
Feb 21 08:33:38.456: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.508147ms
Feb 21 08:33:40.474: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020834048s
Feb 21 08:33:42.481: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028036597s
Feb 21 08:33:44.487: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033992009s
Feb 21 08:33:46.496: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.043307278s
Feb 21 08:33:48.502: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.049047523s
Feb 21 08:33:50.508: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.055063395s
Feb 21 08:33:52.513: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.059893913s
Feb 21 08:33:54.518: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.06544882s
Feb 21 08:33:56.525: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 18.071836133s
Feb 21 08:33:58.535: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 20.082002524s
Feb 21 08:34:00.541: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 22.088365966s
Feb 21 08:34:02.550: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 24.096746761s
Feb 21 08:34:04.561: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 26.108270969s
Feb 21 08:34:06.567: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 28.113865753s
Feb 21 08:34:08.574: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 30.120856122s
Feb 21 08:34:10.580: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 32.127362158s
Feb 21 08:34:12.585: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 34.132519435s
Feb 21 08:34:14.591: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 36.137920795s
Feb 21 08:34:16.597: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 38.143979329s
Feb 21 08:34:18.622: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 40.169249821s
Feb 21 08:34:20.628: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 42.174727329s
Feb 21 08:34:22.633: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 44.179811464s
Feb 21 08:34:24.638: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 46.185430099s
Feb 21 08:34:26.645: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 48.192615495s
Feb 21 08:34:28.652: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 50.198854544s
Feb 21 08:34:30.658: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 52.205054658s
Feb 21 08:34:32.671: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 54.218558423s
Feb 21 08:34:34.677: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 56.224245907s
Feb 21 08:34:36.683: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 58.230577282s
Feb 21 08:34:38.689: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.236577607s
Feb 21 08:34:40.696: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.242973943s
Feb 21 08:34:42.701: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.248639679s
Feb 21 08:34:44.708: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.25474791s
Feb 21 08:34:46.715: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.261839108s
Feb 21 08:34:48.721: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.267839669s
Feb 21 08:34:50.727: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.274682762s
Feb 21 08:34:52.733: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.280562526s
Feb 21 08:34:54.741: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.288576759s
Feb 21 08:34:56.749: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.295764242s
Feb 21 08:34:58.754: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.300752633s
Feb 21 08:35:00.763: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m22.310178327s
STEP: Saw pod success
Feb 21 08:35:00.763: INFO: Pod "pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:35:00.769: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 08:35:00.933: INFO: Waiting for pod pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:35:00.937: INFO: Pod pod-secrets-62cbcdc1-35b3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:35:00.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-w8f52" for this suite.
Feb 21 08:35:06.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:35:06.988: INFO: namespace: e2e-tests-secrets-w8f52, resource: bindings, ignored listing per whitelist
Feb 21 08:35:07.385: INFO: namespace e2e-tests-secrets-w8f52 deletion completed in 6.441282384s

• [SLOW TEST:89.335 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:35:07.385: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-x2ccw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 21 08:37:45.785: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:37:45.790: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 08:37:47.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:37:47.797: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 08:37:49.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:37:49.795: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 08:37:51.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:37:51.798: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 08:37:53.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:37:53.797: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 08:37:55.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:37:55.796: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 08:37:57.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:37:57.796: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 08:37:59.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:37:59.796: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 08:38:01.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:38:01.799: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 08:38:03.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:38:03.796: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 08:38:05.792: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 08:38:05.799: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:38:05.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-x2ccw" for this suite.
Feb 21 08:38:27.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:38:28.011: INFO: namespace: e2e-tests-container-lifecycle-hook-x2ccw, resource: bindings, ignored listing per whitelist
Feb 21 08:38:28.033: INFO: namespace e2e-tests-container-lifecycle-hook-x2ccw deletion completed in 22.223377312s

• [SLOW TEST:200.648 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:38:28.034: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-c7bbp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 21 08:38:28.331: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-c7bbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c7bbp/configmaps/e2e-watch-test-watch-closed,UID:0f93ff22-35b4-11e9-890e-3aac84b8a476,ResourceVersion:5687,Generation:0,CreationTimestamp:2019-02-21 08:38:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 21 08:38:28.332: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-c7bbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c7bbp/configmaps/e2e-watch-test-watch-closed,UID:0f93ff22-35b4-11e9-890e-3aac84b8a476,ResourceVersion:5688,Generation:0,CreationTimestamp:2019-02-21 08:38:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 21 08:38:28.350: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-c7bbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c7bbp/configmaps/e2e-watch-test-watch-closed,UID:0f93ff22-35b4-11e9-890e-3aac84b8a476,ResourceVersion:5689,Generation:0,CreationTimestamp:2019-02-21 08:38:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 21 08:38:28.350: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-c7bbp,SelfLink:/api/v1/namespaces/e2e-tests-watch-c7bbp/configmaps/e2e-watch-test-watch-closed,UID:0f93ff22-35b4-11e9-890e-3aac84b8a476,ResourceVersion:5690,Generation:0,CreationTimestamp:2019-02-21 08:38:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:38:28.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-c7bbp" for this suite.
Feb 21 08:38:34.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:38:34.413: INFO: namespace: e2e-tests-watch-c7bbp, resource: bindings, ignored listing per whitelist
Feb 21 08:38:34.576: INFO: namespace e2e-tests-watch-c7bbp deletion completed in 6.219775714s

• [SLOW TEST:6.543 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:38:34.577: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-bs2xw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-1383dfee-35b4-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 08:38:34.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-1384d1f4-35b4-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-configmap-bs2xw" to be "success or failure"
Feb 21 08:38:34.950: INFO: Pod "pod-configmaps-1384d1f4-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.605745ms
Feb 21 08:38:36.956: INFO: Pod "pod-configmaps-1384d1f4-35b4-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0112305s
STEP: Saw pod success
Feb 21 08:38:36.956: INFO: Pod "pod-configmaps-1384d1f4-35b4-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:38:36.961: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-configmaps-1384d1f4-35b4-11e9-b7c5-a2b84e263bfe container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 08:38:36.985: INFO: Waiting for pod pod-configmaps-1384d1f4-35b4-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:38:36.990: INFO: Pod pod-configmaps-1384d1f4-35b4-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:38:36.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bs2xw" for this suite.
Feb 21 08:38:43.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:38:43.268: INFO: namespace: e2e-tests-configmap-bs2xw, resource: bindings, ignored listing per whitelist
Feb 21 08:38:43.420: INFO: namespace e2e-tests-configmap-bs2xw deletion completed in 6.424218061s

• [SLOW TEST:8.844 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:38:43.421: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pjd7j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-18c06595-35b4-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 08:38:43.736: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18c2316f-35b4-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-pjd7j" to be "success or failure"
Feb 21 08:38:43.742: INFO: Pod "pod-projected-secrets-18c2316f-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.842197ms
Feb 21 08:38:45.748: INFO: Pod "pod-projected-secrets-18c2316f-35b4-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012395112s
STEP: Saw pod success
Feb 21 08:38:45.748: INFO: Pod "pod-projected-secrets-18c2316f-35b4-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:38:45.758: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-secrets-18c2316f-35b4-11e9-b7c5-a2b84e263bfe container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 08:38:45.781: INFO: Waiting for pod pod-projected-secrets-18c2316f-35b4-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:38:45.785: INFO: Pod pod-projected-secrets-18c2316f-35b4-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:38:45.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pjd7j" for this suite.
Feb 21 08:38:51.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:38:52.043: INFO: namespace: e2e-tests-projected-pjd7j, resource: bindings, ignored listing per whitelist
Feb 21 08:38:52.046: INFO: namespace e2e-tests-projected-pjd7j deletion completed in 6.256106327s

• [SLOW TEST:8.626 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:38:52.047: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-6xkgr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 21 08:38:52.323: INFO: Waiting up to 5m0s for pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-var-expansion-6xkgr" to be "success or failure"
Feb 21 08:38:52.327: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.346348ms
Feb 21 08:38:54.333: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009579584s
Feb 21 08:38:56.339: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015432972s
Feb 21 08:38:58.348: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024591265s
Feb 21 08:39:00.357: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033970186s
Feb 21 08:39:02.366: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042353236s
Feb 21 08:39:04.372: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.048478682s
Feb 21 08:39:06.376: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.053186351s
Feb 21 08:39:08.383: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.059953423s
Feb 21 08:39:10.599: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 18.27616651s
Feb 21 08:39:12.604: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 20.280915808s
Feb 21 08:39:14.610: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 22.286627769s
Feb 21 08:39:16.616: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 24.292728138s
Feb 21 08:39:18.624: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 26.301056995s
Feb 21 08:39:20.631: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 28.307493939s
Feb 21 08:39:22.639: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 30.315465644s
Feb 21 08:39:24.645: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 32.321459234s
Feb 21 08:39:26.650: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 34.327189143s
Feb 21 08:39:28.657: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 36.334170473s
Feb 21 08:39:30.668: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 38.344884205s
Feb 21 08:39:32.674: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 40.350684189s
Feb 21 08:39:34.679: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 42.355482078s
Feb 21 08:39:36.686: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 44.362550497s
Feb 21 08:39:38.692: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 46.369150037s
Feb 21 08:39:40.697: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 48.374213588s
Feb 21 08:39:42.703: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 50.379820538s
Feb 21 08:39:44.708: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 52.385128538s
Feb 21 08:39:46.716: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 54.393143316s
STEP: Saw pod success
Feb 21 08:39:46.716: INFO: Pod "var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:39:46.722: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe container dapi-container: <nil>
STEP: delete the pod
Feb 21 08:39:46.748: INFO: Waiting for pod var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:39:46.751: INFO: Pod var-expansion-1de0a4a9-35b4-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:39:46.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-6xkgr" for this suite.
Feb 21 08:39:52.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:39:52.946: INFO: namespace: e2e-tests-var-expansion-6xkgr, resource: bindings, ignored listing per whitelist
Feb 21 08:39:52.962: INFO: namespace e2e-tests-var-expansion-6xkgr deletion completed in 6.204127675s

• [SLOW TEST:60.916 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:39:52.962: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-xq52t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 08:39:53.323: INFO: Creating ReplicaSet my-hostname-basic-423db041-35b4-11e9-b7c5-a2b84e263bfe
Feb 21 08:39:53.334: INFO: Pod name my-hostname-basic-423db041-35b4-11e9-b7c5-a2b84e263bfe: Found 0 pods out of 1
Feb 21 08:39:58.339: INFO: Pod name my-hostname-basic-423db041-35b4-11e9-b7c5-a2b84e263bfe: Found 1 pods out of 1
Feb 21 08:39:58.339: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-423db041-35b4-11e9-b7c5-a2b84e263bfe" is running
Feb 21 08:39:58.344: INFO: Pod "my-hostname-basic-423db041-35b4-11e9-b7c5-a2b84e263bfe-rrctf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-21 08:39:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-21 08:39:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-21 08:39:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-21 08:39:53 +0000 UTC Reason: Message:}])
Feb 21 08:39:58.344: INFO: Trying to dial the pod
Feb 21 08:40:03.441: INFO: Controller my-hostname-basic-423db041-35b4-11e9-b7c5-a2b84e263bfe: Got expected result from replica 1 [my-hostname-basic-423db041-35b4-11e9-b7c5-a2b84e263bfe-rrctf]: "my-hostname-basic-423db041-35b4-11e9-b7c5-a2b84e263bfe-rrctf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:40:03.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-xq52t" for this suite.
Feb 21 08:40:09.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:40:09.582: INFO: namespace: e2e-tests-replicaset-xq52t, resource: bindings, ignored listing per whitelist
Feb 21 08:40:09.684: INFO: namespace e2e-tests-replicaset-xq52t deletion completed in 6.235930621s

• [SLOW TEST:16.721 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:40:09.684: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ghh86
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 21 08:40:10.012: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 21 08:40:10.013: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:40:11.272: INFO: stderr: ""
Feb 21 08:40:11.273: INFO: stdout: "service/redis-slave created\n"
Feb 21 08:40:11.273: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 21 08:40:11.273: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:40:11.537: INFO: stderr: ""
Feb 21 08:40:11.537: INFO: stdout: "service/redis-master created\n"
Feb 21 08:40:11.537: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 21 08:40:11.537: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:40:11.782: INFO: stderr: ""
Feb 21 08:40:11.782: INFO: stdout: "service/frontend created\n"
Feb 21 08:40:11.782: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 21 08:40:11.782: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:40:12.057: INFO: stderr: ""
Feb 21 08:40:12.057: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 21 08:40:12.057: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 21 08:40:12.057: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:40:12.296: INFO: stderr: ""
Feb 21 08:40:12.296: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 21 08:40:12.296: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 21 08:40:12.296: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:40:12.573: INFO: stderr: ""
Feb 21 08:40:12.573: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 21 08:40:12.573: INFO: Waiting for all frontend pods to be Running.
Feb 21 08:44:22.638: INFO: Waiting for frontend to serve content.
Feb 21 08:44:27.711: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 21 08:44:37.778: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 21 08:44:47.848: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 21 08:44:57.917: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 21 08:45:07.985: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 21 08:45:13.080: INFO: Trying to add a new entry to the guestbook.
Feb 21 08:45:18.130: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-master:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-mas...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Str in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 21 08:45:28.199: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-master:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-mas...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Str in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 21 08:45:33.291: INFO: Verifying that added entry can be retrieved.
Feb 21 08:45:33.342: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 21 08:45:38.429: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 21 08:45:43.517: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 21 08:45:48.609: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 21 08:45:53.699: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 21 08:45:58.789: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 21 08:46:03.877: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Feb 21 08:46:08.971: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:46:09.157: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 08:46:09.157: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 08:46:09.157: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:46:09.286: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 08:46:09.286: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 08:46:09.286: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:46:09.408: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 08:46:09.408: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 08:46:09.408: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:46:09.521: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 08:46:09.521: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 08:46:09.521: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:46:09.625: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 08:46:09.625: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 08:46:09.625: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ghh86'
Feb 21 08:46:09.735: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 08:46:09.736: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:46:09.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ghh86" for this suite.
Feb 21 08:46:47.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:46:47.847: INFO: namespace: e2e-tests-kubectl-ghh86, resource: bindings, ignored listing per whitelist
Feb 21 08:46:47.915: INFO: namespace e2e-tests-kubectl-ghh86 deletion completed in 38.17196809s

• [SLOW TEST:398.231 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:46:47.916: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-x85qz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 08:46:48.409: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-x85qz'
Feb 21 08:46:48.560: INFO: stderr: ""
Feb 21 08:46:48.560: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 21 08:46:53.611: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-x85qz -o json'
Feb 21 08:46:53.720: INFO: stderr: ""
Feb 21 08:46:53.720: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.22/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-02-21T08:46:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-x85qz\",\n        \"resourceVersion\": \"6886\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-x85qz/pods/e2e-test-nginx-pod\",\n        \"uid\": \"39bc82a4-35b5-11e9-890e-3aac84b8a476\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8x8fz\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"nodeName\": \"shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8x8fz\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8x8fz\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-21T08:46:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-21T08:46:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-21T08:46:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-21T08:46:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://4ca286fe702a8e633c98eefd2f96d2d8851a385fa2fea67c5a087eb00f82ca2f\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-21T08:46:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.19\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.22\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-21T08:46:48Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 21 08:46:53.720: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml replace -f - --namespace=e2e-tests-kubectl-x85qz'
Feb 21 08:46:53.925: INFO: stderr: ""
Feb 21 08:46:53.926: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Feb 21 08:46:53.936: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-x85qz'
Feb 21 08:47:05.591: INFO: stderr: ""
Feb 21 08:47:05.591: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:47:05.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x85qz" for this suite.
Feb 21 08:47:11.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:47:11.717: INFO: namespace: e2e-tests-kubectl-x85qz, resource: bindings, ignored listing per whitelist
Feb 21 08:47:11.903: INFO: namespace e2e-tests-kubectl-x85qz deletion completed in 6.304735122s

• [SLOW TEST:23.988 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:47:11.904: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-mh557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-47e581b4-35b5-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 08:47:12.322: INFO: Waiting up to 5m0s for pod "pod-configmaps-47e662d8-35b5-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-configmap-mh557" to be "success or failure"
Feb 21 08:47:12.326: INFO: Pod "pod-configmaps-47e662d8-35b5-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.447249ms
Feb 21 08:47:14.332: INFO: Pod "pod-configmaps-47e662d8-35b5-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010115334s
STEP: Saw pod success
Feb 21 08:47:14.332: INFO: Pod "pod-configmaps-47e662d8-35b5-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:47:14.337: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-configmaps-47e662d8-35b5-11e9-b7c5-a2b84e263bfe container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 08:47:14.370: INFO: Waiting for pod pod-configmaps-47e662d8-35b5-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:47:14.374: INFO: Pod pod-configmaps-47e662d8-35b5-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:47:14.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mh557" for this suite.
Feb 21 08:47:20.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:47:20.482: INFO: namespace: e2e-tests-configmap-mh557, resource: bindings, ignored listing per whitelist
Feb 21 08:47:20.548: INFO: namespace e2e-tests-configmap-mh557 deletion completed in 6.168912158s

• [SLOW TEST:8.645 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:47:20.548: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vnv9v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4cf70a1c-35b5-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 08:47:20.827: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4cf7ccc2-35b5-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-vnv9v" to be "success or failure"
Feb 21 08:47:20.832: INFO: Pod "pod-projected-configmaps-4cf7ccc2-35b5-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.336834ms
Feb 21 08:47:22.838: INFO: Pod "pod-projected-configmaps-4cf7ccc2-35b5-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010514751s
STEP: Saw pod success
Feb 21 08:47:22.838: INFO: Pod "pod-projected-configmaps-4cf7ccc2-35b5-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:47:22.842: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-configmaps-4cf7ccc2-35b5-11e9-b7c5-a2b84e263bfe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 08:47:22.867: INFO: Waiting for pod pod-projected-configmaps-4cf7ccc2-35b5-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:47:22.870: INFO: Pod pod-projected-configmaps-4cf7ccc2-35b5-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:47:22.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vnv9v" for this suite.
Feb 21 08:47:28.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:47:29.096: INFO: namespace: e2e-tests-projected-vnv9v, resource: bindings, ignored listing per whitelist
Feb 21 08:47:29.100: INFO: namespace e2e-tests-projected-vnv9v deletion completed in 6.225348935s

• [SLOW TEST:8.552 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:47:29.101: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-khhln
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 08:47:29.522: INFO: (0) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 103.046679ms)
Feb 21 08:47:29.535: INFO: (1) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.183245ms)
Feb 21 08:47:29.544: INFO: (2) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.2961ms)
Feb 21 08:47:29.550: INFO: (3) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.812944ms)
Feb 21 08:47:29.557: INFO: (4) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.421065ms)
Feb 21 08:47:29.562: INFO: (5) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.246493ms)
Feb 21 08:47:29.567: INFO: (6) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.899885ms)
Feb 21 08:47:29.572: INFO: (7) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.565713ms)
Feb 21 08:47:29.578: INFO: (8) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.381183ms)
Feb 21 08:47:29.586: INFO: (9) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.667847ms)
Feb 21 08:47:29.592: INFO: (10) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.668943ms)
Feb 21 08:47:29.599: INFO: (11) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.439137ms)
Feb 21 08:47:29.605: INFO: (12) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.028003ms)
Feb 21 08:47:29.610: INFO: (13) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.988112ms)
Feb 21 08:47:29.615: INFO: (14) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.197858ms)
Feb 21 08:47:29.621: INFO: (15) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.990005ms)
Feb 21 08:47:29.627: INFO: (16) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.911726ms)
Feb 21 08:47:29.634: INFO: (17) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.393899ms)
Feb 21 08:47:29.640: INFO: (18) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.830176ms)
Feb 21 08:47:29.646: INFO: (19) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.027493ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:47:29.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-khhln" for this suite.
Feb 21 08:47:35.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:47:35.830: INFO: namespace: e2e-tests-proxy-khhln, resource: bindings, ignored listing per whitelist
Feb 21 08:47:35.867: INFO: namespace e2e-tests-proxy-khhln deletion completed in 6.216194584s

• [SLOW TEST:6.767 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:47:35.868: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-rnlt5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-slcb
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 08:47:36.140: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-slcb" in namespace "e2e-tests-subpath-rnlt5" to be "success or failure"
Feb 21 08:47:36.148: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.642834ms
Feb 21 08:47:38.155: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014975866s
Feb 21 08:47:40.162: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Running", Reason="", readiness=false. Elapsed: 4.022596915s
Feb 21 08:47:42.170: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Running", Reason="", readiness=false. Elapsed: 6.030500869s
Feb 21 08:47:44.176: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Running", Reason="", readiness=false. Elapsed: 8.036272209s
Feb 21 08:47:46.184: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Running", Reason="", readiness=false. Elapsed: 10.043852726s
Feb 21 08:47:48.191: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Running", Reason="", readiness=false. Elapsed: 12.050955662s
Feb 21 08:47:50.197: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Running", Reason="", readiness=false. Elapsed: 14.057345205s
Feb 21 08:47:52.202: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Running", Reason="", readiness=false. Elapsed: 16.062451506s
Feb 21 08:47:54.208: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Running", Reason="", readiness=false. Elapsed: 18.068478611s
Feb 21 08:47:56.215: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Running", Reason="", readiness=false. Elapsed: 20.074798112s
Feb 21 08:47:58.221: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Running", Reason="", readiness=false. Elapsed: 22.080912577s
Feb 21 08:48:00.227: INFO: Pod "pod-subpath-test-configmap-slcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.086693686s
STEP: Saw pod success
Feb 21 08:48:00.227: INFO: Pod "pod-subpath-test-configmap-slcb" satisfied condition "success or failure"
Feb 21 08:48:00.231: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-subpath-test-configmap-slcb container test-container-subpath-configmap-slcb: <nil>
STEP: delete the pod
Feb 21 08:48:00.262: INFO: Waiting for pod pod-subpath-test-configmap-slcb to disappear
Feb 21 08:48:00.267: INFO: Pod pod-subpath-test-configmap-slcb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-slcb
Feb 21 08:48:00.267: INFO: Deleting pod "pod-subpath-test-configmap-slcb" in namespace "e2e-tests-subpath-rnlt5"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:48:00.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-rnlt5" for this suite.
Feb 21 08:48:06.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:48:06.653: INFO: namespace: e2e-tests-subpath-rnlt5, resource: bindings, ignored listing per whitelist
Feb 21 08:48:06.682: INFO: namespace e2e-tests-subpath-rnlt5 deletion completed in 6.406636673s

• [SLOW TEST:30.815 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:48:06.683: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6xx8p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-6880cd83-35b5-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 08:48:07.029: INFO: Waiting up to 5m0s for pod "pod-secrets-6881ae5f-35b5-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-secrets-6xx8p" to be "success or failure"
Feb 21 08:48:07.033: INFO: Pod "pod-secrets-6881ae5f-35b5-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.820907ms
Feb 21 08:48:09.039: INFO: Pod "pod-secrets-6881ae5f-35b5-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010164601s
STEP: Saw pod success
Feb 21 08:48:09.039: INFO: Pod "pod-secrets-6881ae5f-35b5-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:48:09.044: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-secrets-6881ae5f-35b5-11e9-b7c5-a2b84e263bfe container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 08:48:09.072: INFO: Waiting for pod pod-secrets-6881ae5f-35b5-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:48:09.077: INFO: Pod pod-secrets-6881ae5f-35b5-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:48:09.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6xx8p" for this suite.
Feb 21 08:48:15.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:48:15.116: INFO: namespace: e2e-tests-secrets-6xx8p, resource: bindings, ignored listing per whitelist
Feb 21 08:48:15.318: INFO: namespace e2e-tests-secrets-6xx8p deletion completed in 6.234333057s

• [SLOW TEST:8.635 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:48:15.318: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-kkdbz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-6da0efa3-35b5-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 08:48:15.628: INFO: Waiting up to 5m0s for pod "pod-secrets-6da1ba88-35b5-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-secrets-kkdbz" to be "success or failure"
Feb 21 08:48:15.633: INFO: Pod "pod-secrets-6da1ba88-35b5-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.400532ms
Feb 21 08:48:17.642: INFO: Pod "pod-secrets-6da1ba88-35b5-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014590004s
STEP: Saw pod success
Feb 21 08:48:17.642: INFO: Pod "pod-secrets-6da1ba88-35b5-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:48:17.668: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-secrets-6da1ba88-35b5-11e9-b7c5-a2b84e263bfe container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 08:48:17.696: INFO: Waiting for pod pod-secrets-6da1ba88-35b5-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:48:17.701: INFO: Pod pod-secrets-6da1ba88-35b5-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:48:17.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kkdbz" for this suite.
Feb 21 08:48:23.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:48:23.872: INFO: namespace: e2e-tests-secrets-kkdbz, resource: bindings, ignored listing per whitelist
Feb 21 08:48:24.125: INFO: namespace e2e-tests-secrets-kkdbz deletion completed in 6.415265419s

• [SLOW TEST:8.806 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:48:24.125: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-sv8v4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-72e00f85-35b5-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 08:48:24.428: INFO: Waiting up to 5m0s for pod "pod-configmaps-72e0f279-35b5-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-configmap-sv8v4" to be "success or failure"
Feb 21 08:48:24.432: INFO: Pod "pod-configmaps-72e0f279-35b5-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.910821ms
Feb 21 08:48:26.443: INFO: Pod "pod-configmaps-72e0f279-35b5-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014684395s
Feb 21 08:48:28.449: INFO: Pod "pod-configmaps-72e0f279-35b5-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020636479s
STEP: Saw pod success
Feb 21 08:48:28.449: INFO: Pod "pod-configmaps-72e0f279-35b5-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:48:28.454: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-configmaps-72e0f279-35b5-11e9-b7c5-a2b84e263bfe container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 08:48:28.481: INFO: Waiting for pod pod-configmaps-72e0f279-35b5-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:48:28.484: INFO: Pod pod-configmaps-72e0f279-35b5-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:48:28.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sv8v4" for this suite.
Feb 21 08:48:34.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:48:34.690: INFO: namespace: e2e-tests-configmap-sv8v4, resource: bindings, ignored listing per whitelist
Feb 21 08:48:34.714: INFO: namespace e2e-tests-configmap-sv8v4 deletion completed in 6.221153942s

• [SLOW TEST:10.590 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:48:34.715: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-kphd7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 08:48:35.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79669717-35b5-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-kphd7" to be "success or failure"
Feb 21 08:48:35.376: INFO: Pod "downwardapi-volume-79669717-35b5-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.715965ms
Feb 21 08:48:37.381: INFO: Pod "downwardapi-volume-79669717-35b5-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008748244s
STEP: Saw pod success
Feb 21 08:48:37.381: INFO: Pod "downwardapi-volume-79669717-35b5-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:48:37.385: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-79669717-35b5-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 08:48:37.409: INFO: Waiting for pod downwardapi-volume-79669717-35b5-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:48:37.413: INFO: Pod downwardapi-volume-79669717-35b5-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:48:37.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kphd7" for this suite.
Feb 21 08:48:43.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:48:43.505: INFO: namespace: e2e-tests-downward-api-kphd7, resource: bindings, ignored listing per whitelist
Feb 21 08:48:43.603: INFO: namespace e2e-tests-downward-api-kphd7 deletion completed in 6.184796471s

• [SLOW TEST:8.889 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:48:43.603: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-sjslp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-fvlxk in namespace e2e-tests-proxy-sjslp
I0221 08:48:43.932235   32452 runners.go:180] Created replication controller with name: proxy-service-fvlxk, namespace: e2e-tests-proxy-sjslp, replica count: 1
I0221 08:48:44.982723   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:45.983076   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:46.983351   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:47.983787   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:48.984095   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:49.984532   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:50.984932   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:51.985271   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:52.985524   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:53.985876   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:54.986281   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:55.987538   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:56.987773   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:57.988183   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:58.988462   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:48:59.988751   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:00.989307   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:01.989718   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:02.990004   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:03.990462   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:04.991010   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:05.991240   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:06.991540   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:07.991801   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:08.992827   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:09.993100   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:10.993361   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:11.993597   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:12.994015   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:13.996039   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:14.996535   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:15.997048   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:16.997678   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:17.998015   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:18.998379   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:19.998794   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:20.999205   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:21.999617   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:23.000430   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:24.000812   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:25.001369   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:26.001643   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:27.002136   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:28.002604   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:29.002893   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:30.003213   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:31.003510   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:32.003771   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:33.003988   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:34.004310   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:35.005237   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:36.005940   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:37.006356   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:38.006814   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:39.007092   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:40.007814   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:41.008149   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:42.008508   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:43.008996   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:44.009255   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:45.009791   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:46.010084   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:47.010425   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:48.010843   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:49.011120   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:50.011385   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:51.011686   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:52.011982   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:53.012270   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:54.012603   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:55.012881   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:56.013523   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:57.013777   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:58.014133   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:49:59.014387   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 08:50:00.014688   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 08:50:01.015139   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 08:50:02.015535   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 08:50:03.015821   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 08:50:04.016192   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 08:50:05.016452   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 08:50:06.016776   32452 runners.go:180] proxy-service-fvlxk Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 08:50:06.021: INFO: setup took 1m22.107036526s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 21 08:50:06.036: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 14.729882ms)
Feb 21 08:50:06.042: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 20.756448ms)
Feb 21 08:50:06.042: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 20.711168ms)
Feb 21 08:50:06.042: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 20.747986ms)
Feb 21 08:50:06.042: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 20.852148ms)
Feb 21 08:50:06.042: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 20.896292ms)
Feb 21 08:50:06.042: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 20.792207ms)
Feb 21 08:50:06.042: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 20.985929ms)
Feb 21 08:50:06.042: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 20.822505ms)
Feb 21 08:50:06.043: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 21.426309ms)
Feb 21 08:50:06.043: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 21.555662ms)
Feb 21 08:50:06.045: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 23.23911ms)
Feb 21 08:50:06.045: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 23.530271ms)
Feb 21 08:50:06.047: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 24.971189ms)
Feb 21 08:50:06.047: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 25.02126ms)
Feb 21 08:50:06.047: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 24.984442ms)
Feb 21 08:50:06.055: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 8.174067ms)
Feb 21 08:50:06.055: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 8.35729ms)
Feb 21 08:50:06.055: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 8.325451ms)
Feb 21 08:50:06.055: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 8.299279ms)
Feb 21 08:50:06.057: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 9.938162ms)
Feb 21 08:50:06.057: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 10.137288ms)
Feb 21 08:50:06.057: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 10.098581ms)
Feb 21 08:50:06.057: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 10.18142ms)
Feb 21 08:50:06.057: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 10.211982ms)
Feb 21 08:50:06.057: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 10.452703ms)
Feb 21 08:50:06.057: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 10.37966ms)
Feb 21 08:50:06.057: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 10.377277ms)
Feb 21 08:50:06.057: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 10.461153ms)
Feb 21 08:50:06.057: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 10.403134ms)
Feb 21 08:50:06.059: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 11.921524ms)
Feb 21 08:50:06.059: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 11.930982ms)
Feb 21 08:50:06.067: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 7.651453ms)
Feb 21 08:50:06.067: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 7.669388ms)
Feb 21 08:50:06.067: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 7.934698ms)
Feb 21 08:50:06.067: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 7.67079ms)
Feb 21 08:50:06.067: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 8.373062ms)
Feb 21 08:50:06.067: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 8.417644ms)
Feb 21 08:50:06.107: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 47.703201ms)
Feb 21 08:50:06.107: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 47.951614ms)
Feb 21 08:50:06.107: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 47.772759ms)
Feb 21 08:50:06.112: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 53.361978ms)
Feb 21 08:50:06.112: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 53.269505ms)
Feb 21 08:50:06.112: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 52.970114ms)
Feb 21 08:50:06.112: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 53.228042ms)
Feb 21 08:50:06.112: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 53.550666ms)
Feb 21 08:50:06.113: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 53.446193ms)
Feb 21 08:50:06.113: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 53.500097ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 8.923834ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 9.025139ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 9.209558ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 8.987859ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 8.603666ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 8.978246ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 9.411328ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 9.294855ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 9.094758ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 9.506148ms)
Feb 21 08:50:06.122: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 9.415973ms)
Feb 21 08:50:06.123: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 10.100698ms)
Feb 21 08:50:06.123: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 10.701636ms)
Feb 21 08:50:06.123: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 10.531956ms)
Feb 21 08:50:06.124: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 10.951516ms)
Feb 21 08:50:06.124: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 10.986802ms)
Feb 21 08:50:06.132: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 7.520565ms)
Feb 21 08:50:06.134: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 10.162357ms)
Feb 21 08:50:06.134: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 9.967295ms)
Feb 21 08:50:06.134: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 10.148973ms)
Feb 21 08:50:06.134: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 10.320462ms)
Feb 21 08:50:06.134: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 10.21777ms)
Feb 21 08:50:06.135: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 11.069527ms)
Feb 21 08:50:06.135: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 11.150568ms)
Feb 21 08:50:06.135: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 11.396362ms)
Feb 21 08:50:06.135: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 11.325013ms)
Feb 21 08:50:06.135: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 11.48121ms)
Feb 21 08:50:06.135: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 11.543157ms)
Feb 21 08:50:06.136: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 11.449088ms)
Feb 21 08:50:06.136: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 12.30168ms)
Feb 21 08:50:06.138: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 14.052565ms)
Feb 21 08:50:06.138: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 13.680594ms)
Feb 21 08:50:06.149: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 10.282275ms)
Feb 21 08:50:06.149: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 10.485155ms)
Feb 21 08:50:06.149: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 10.750634ms)
Feb 21 08:50:06.149: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 10.279776ms)
Feb 21 08:50:06.149: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 10.556764ms)
Feb 21 08:50:06.150: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 11.230535ms)
Feb 21 08:50:06.150: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 10.62998ms)
Feb 21 08:50:06.150: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 11.104026ms)
Feb 21 08:50:06.150: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 10.863105ms)
Feb 21 08:50:06.150: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 11.503558ms)
Feb 21 08:50:06.150: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 10.997794ms)
Feb 21 08:50:06.151: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 12.091666ms)
Feb 21 08:50:06.151: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 11.996392ms)
Feb 21 08:50:06.151: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 12.313968ms)
Feb 21 08:50:06.151: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 12.361925ms)
Feb 21 08:50:06.151: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 12.114749ms)
Feb 21 08:50:06.158: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 6.260144ms)
Feb 21 08:50:06.158: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 7.04527ms)
Feb 21 08:50:06.160: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 7.21499ms)
Feb 21 08:50:06.160: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 8.322634ms)
Feb 21 08:50:06.160: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 7.670558ms)
Feb 21 08:50:06.160: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 8.210519ms)
Feb 21 08:50:06.160: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 7.801747ms)
Feb 21 08:50:06.160: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 8.125518ms)
Feb 21 08:50:06.160: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 8.77605ms)
Feb 21 08:50:06.160: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 8.595904ms)
Feb 21 08:50:06.160: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 8.678419ms)
Feb 21 08:50:06.160: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 7.875492ms)
Feb 21 08:50:06.161: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 8.240308ms)
Feb 21 08:50:06.161: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 9.489644ms)
Feb 21 08:50:06.161: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 9.279148ms)
Feb 21 08:50:06.161: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 8.416682ms)
Feb 21 08:50:06.168: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 6.644523ms)
Feb 21 08:50:06.168: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 6.795332ms)
Feb 21 08:50:06.168: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 6.992173ms)
Feb 21 08:50:06.168: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 7.161299ms)
Feb 21 08:50:06.168: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 6.686718ms)
Feb 21 08:50:06.168: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 6.784637ms)
Feb 21 08:50:06.168: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 6.608685ms)
Feb 21 08:50:06.169: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 7.32884ms)
Feb 21 08:50:06.169: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 7.185477ms)
Feb 21 08:50:06.169: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 6.942775ms)
Feb 21 08:50:06.169: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 7.726164ms)
Feb 21 08:50:06.170: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 7.886421ms)
Feb 21 08:50:06.170: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 8.122156ms)
Feb 21 08:50:06.170: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 8.610702ms)
Feb 21 08:50:06.170: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 8.808668ms)
Feb 21 08:50:06.171: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 8.92028ms)
Feb 21 08:50:06.180: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 8.822576ms)
Feb 21 08:50:06.180: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 8.933663ms)
Feb 21 08:50:06.180: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 9.004766ms)
Feb 21 08:50:06.180: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 9.088855ms)
Feb 21 08:50:06.180: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 8.958451ms)
Feb 21 08:50:06.180: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 8.929171ms)
Feb 21 08:50:06.180: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 9.146661ms)
Feb 21 08:50:06.180: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 9.080272ms)
Feb 21 08:50:06.180: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 9.216517ms)
Feb 21 08:50:06.180: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 9.372125ms)
Feb 21 08:50:06.181: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 9.973472ms)
Feb 21 08:50:06.182: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 11.439964ms)
Feb 21 08:50:06.182: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 11.37719ms)
Feb 21 08:50:06.183: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 12.145954ms)
Feb 21 08:50:06.183: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 12.478572ms)
Feb 21 08:50:06.184: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 13.125947ms)
Feb 21 08:50:06.217: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 32.892232ms)
Feb 21 08:50:06.218: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 34.257255ms)
Feb 21 08:50:06.218: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 34.322231ms)
Feb 21 08:50:06.218: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 34.264193ms)
Feb 21 08:50:06.221: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 36.982386ms)
Feb 21 08:50:06.221: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 37.016478ms)
Feb 21 08:50:06.221: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 37.09831ms)
Feb 21 08:50:06.221: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 37.081787ms)
Feb 21 08:50:06.221: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 37.098457ms)
Feb 21 08:50:06.221: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 37.072575ms)
Feb 21 08:50:06.221: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 37.059032ms)
Feb 21 08:50:06.223: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 38.875754ms)
Feb 21 08:50:06.223: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 38.983438ms)
Feb 21 08:50:06.223: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 39.15199ms)
Feb 21 08:50:06.224: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 40.091446ms)
Feb 21 08:50:06.225: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 40.507826ms)
Feb 21 08:50:06.236: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 11.088208ms)
Feb 21 08:50:06.236: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 11.296561ms)
Feb 21 08:50:06.236: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 11.222209ms)
Feb 21 08:50:06.236: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 11.192851ms)
Feb 21 08:50:06.236: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 11.390758ms)
Feb 21 08:50:06.236: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 11.27248ms)
Feb 21 08:50:06.236: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 11.305512ms)
Feb 21 08:50:06.236: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 11.647746ms)
Feb 21 08:50:06.238: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 13.359768ms)
Feb 21 08:50:06.238: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 13.456996ms)
Feb 21 08:50:06.239: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 14.561263ms)
Feb 21 08:50:06.239: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 14.52657ms)
Feb 21 08:50:06.240: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 14.916148ms)
Feb 21 08:50:06.240: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 14.773176ms)
Feb 21 08:50:06.240: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 15.080504ms)
Feb 21 08:50:06.240: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 15.105498ms)
Feb 21 08:50:06.248: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 7.59096ms)
Feb 21 08:50:06.248: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 7.761135ms)
Feb 21 08:50:06.249: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 8.868518ms)
Feb 21 08:50:06.249: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 8.634931ms)
Feb 21 08:50:06.249: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 8.576137ms)
Feb 21 08:50:06.249: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 8.641552ms)
Feb 21 08:50:06.249: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 8.726112ms)
Feb 21 08:50:06.249: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 8.763201ms)
Feb 21 08:50:06.249: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 8.778821ms)
Feb 21 08:50:06.250: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 9.780437ms)
Feb 21 08:50:06.250: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 9.947236ms)
Feb 21 08:50:06.250: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 10.037499ms)
Feb 21 08:50:06.250: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 9.812006ms)
Feb 21 08:50:06.250: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 10.054272ms)
Feb 21 08:50:06.250: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 10.242169ms)
Feb 21 08:50:06.251: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 10.3581ms)
Feb 21 08:50:06.260: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 9.260709ms)
Feb 21 08:50:06.260: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 9.424913ms)
Feb 21 08:50:06.260: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 9.156775ms)
Feb 21 08:50:06.260: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 9.220146ms)
Feb 21 08:50:06.260: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 9.033433ms)
Feb 21 08:50:06.260: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 9.007165ms)
Feb 21 08:50:06.260: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 8.924327ms)
Feb 21 08:50:06.261: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 8.973671ms)
Feb 21 08:50:06.261: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 9.060291ms)
Feb 21 08:50:06.261: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 9.703802ms)
Feb 21 08:50:06.261: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 9.60696ms)
Feb 21 08:50:06.261: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 9.476117ms)
Feb 21 08:50:06.262: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 10.573847ms)
Feb 21 08:50:06.262: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 10.685646ms)
Feb 21 08:50:06.262: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 10.718245ms)
Feb 21 08:50:06.262: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 11.3486ms)
Feb 21 08:50:06.272: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 9.434965ms)
Feb 21 08:50:06.272: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 9.692746ms)
Feb 21 08:50:06.272: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 9.598837ms)
Feb 21 08:50:06.272: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 9.57602ms)
Feb 21 08:50:06.272: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 9.732082ms)
Feb 21 08:50:06.272: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 9.785582ms)
Feb 21 08:50:06.272: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 9.733224ms)
Feb 21 08:50:06.273: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 10.309761ms)
Feb 21 08:50:06.273: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 10.43667ms)
Feb 21 08:50:06.273: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 10.363451ms)
Feb 21 08:50:06.273: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 10.375851ms)
Feb 21 08:50:06.273: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 10.460014ms)
Feb 21 08:50:06.274: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 11.009972ms)
Feb 21 08:50:06.274: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 10.917194ms)
Feb 21 08:50:06.274: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 11.036274ms)
Feb 21 08:50:06.274: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 11.516795ms)
Feb 21 08:50:06.323: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 48.466605ms)
Feb 21 08:50:06.323: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 49.17351ms)
Feb 21 08:50:06.324: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 49.82179ms)
Feb 21 08:50:06.324: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 49.969439ms)
Feb 21 08:50:06.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 50.354752ms)
Feb 21 08:50:06.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 49.999275ms)
Feb 21 08:50:06.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 50.032383ms)
Feb 21 08:50:06.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 50.349887ms)
Feb 21 08:50:06.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 50.78536ms)
Feb 21 08:50:06.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 50.82022ms)
Feb 21 08:50:06.326: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 51.474436ms)
Feb 21 08:50:06.327: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 52.822799ms)
Feb 21 08:50:06.328: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 53.02093ms)
Feb 21 08:50:06.328: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 53.070828ms)
Feb 21 08:50:06.328: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 53.122838ms)
Feb 21 08:50:06.328: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 53.786532ms)
Feb 21 08:50:06.338: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 9.464477ms)
Feb 21 08:50:06.339: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 10.56957ms)
Feb 21 08:50:06.339: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 10.54122ms)
Feb 21 08:50:06.339: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 10.768442ms)
Feb 21 08:50:06.339: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 10.798048ms)
Feb 21 08:50:06.339: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 10.70452ms)
Feb 21 08:50:06.340: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 10.934192ms)
Feb 21 08:50:06.340: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 11.019436ms)
Feb 21 08:50:06.340: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 11.11253ms)
Feb 21 08:50:06.340: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 11.271968ms)
Feb 21 08:50:06.341: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 11.900011ms)
Feb 21 08:50:06.341: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 12.117289ms)
Feb 21 08:50:06.341: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 12.232738ms)
Feb 21 08:50:06.341: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 12.231021ms)
Feb 21 08:50:06.341: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 12.755903ms)
Feb 21 08:50:06.343: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 13.998701ms)
Feb 21 08:50:06.352: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 8.952754ms)
Feb 21 08:50:06.352: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 8.952047ms)
Feb 21 08:50:06.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 10.881735ms)
Feb 21 08:50:06.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 10.995158ms)
Feb 21 08:50:06.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 11.144068ms)
Feb 21 08:50:06.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 11.104918ms)
Feb 21 08:50:06.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 11.243887ms)
Feb 21 08:50:06.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 11.326589ms)
Feb 21 08:50:06.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 11.147947ms)
Feb 21 08:50:06.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 11.180196ms)
Feb 21 08:50:06.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 11.160361ms)
Feb 21 08:50:06.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 11.246732ms)
Feb 21 08:50:06.356: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 13.079964ms)
Feb 21 08:50:06.356: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 13.481135ms)
Feb 21 08:50:06.356: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 13.598733ms)
Feb 21 08:50:06.356: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 13.438382ms)
Feb 21 08:50:06.364: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 7.532122ms)
Feb 21 08:50:06.366: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 9.887615ms)
Feb 21 08:50:06.366: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 9.821013ms)
Feb 21 08:50:06.366: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 9.741321ms)
Feb 21 08:50:06.366: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 10.043649ms)
Feb 21 08:50:06.366: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 9.778184ms)
Feb 21 08:50:06.366: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 9.77416ms)
Feb 21 08:50:06.368: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 11.811727ms)
Feb 21 08:50:06.368: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 11.733421ms)
Feb 21 08:50:06.368: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 11.686942ms)
Feb 21 08:50:06.368: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 11.674236ms)
Feb 21 08:50:06.368: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 11.787211ms)
Feb 21 08:50:06.369: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 12.06103ms)
Feb 21 08:50:06.369: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 12.000803ms)
Feb 21 08:50:06.369: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 12.171272ms)
Feb 21 08:50:06.370: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 13.694229ms)
Feb 21 08:50:06.380: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 9.425564ms)
Feb 21 08:50:06.382: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 11.201077ms)
Feb 21 08:50:06.382: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 11.273666ms)
Feb 21 08:50:06.382: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 11.22978ms)
Feb 21 08:50:06.382: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 11.65385ms)
Feb 21 08:50:06.382: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 11.774808ms)
Feb 21 08:50:06.382: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 11.817469ms)
Feb 21 08:50:06.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 12.203288ms)
Feb 21 08:50:06.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 12.121474ms)
Feb 21 08:50:06.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 12.138746ms)
Feb 21 08:50:06.384: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 13.757661ms)
Feb 21 08:50:06.403: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 32.903246ms)
Feb 21 08:50:06.404: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 32.978963ms)
Feb 21 08:50:06.404: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 32.994314ms)
Feb 21 08:50:06.404: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 33.062754ms)
Feb 21 08:50:06.404: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 33.100833ms)
Feb 21 08:50:06.415: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 11.207117ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h/proxy/rewriteme"... (200; 12.132647ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:460/proxy/: tls baz (200; 12.079906ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname2/proxy/: tls qux (200; 12.158768ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:1080/proxy/... (200; 12.135275ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/https:proxy-service-fvlxk:tlsportname1/proxy/: tls baz (200; 12.311572ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 12.291579ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:443/proxy/... (200; 12.210765ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/https:proxy-service-fvlxk-bjc8h:462/proxy/: tls qux (200; 12.229114ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname1/proxy/: foo (200; 12.420586ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-sjslp/pods/proxy-service-fvlxk-bjc8h:1080/proxy/rewri... (200; 12.225771ms)
Feb 21 08:50:06.416: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:162/proxy/: bar (200; 12.326626ms)
Feb 21 08:50:06.417: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/http:proxy-service-fvlxk:portname2/proxy/: bar (200; 12.771339ms)
Feb 21 08:50:06.418: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/pods/http:proxy-service-fvlxk-bjc8h:160/proxy/: foo (200; 14.227928ms)
Feb 21 08:50:06.419: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname2/proxy/: bar (200; 14.757693ms)
Feb 21 08:50:06.419: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-sjslp/services/proxy-service-fvlxk:portname1/proxy/: foo (200; 14.705908ms)
STEP: deleting { ReplicationController} proxy-service-fvlxk in namespace e2e-tests-proxy-sjslp, will wait for the garbage collector to delete the pods
Feb 21 08:50:06.479: INFO: Deleting { ReplicationController} proxy-service-fvlxk took: 6.983154ms
Feb 21 08:50:06.580: INFO: Terminating { ReplicationController} proxy-service-fvlxk pods took: 100.254621ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:50:08.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-sjslp" for this suite.
Feb 21 08:50:14.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:50:14.320: INFO: namespace: e2e-tests-proxy-sjslp, resource: bindings, ignored listing per whitelist
Feb 21 08:50:14.441: INFO: namespace e2e-tests-proxy-sjslp deletion completed in 6.255396543s

• [SLOW TEST:90.838 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:50:14.441: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2zvcb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 08:50:14.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b49ecf9c-35b5-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-2zvcb" to be "success or failure"
Feb 21 08:50:14.731: INFO: Pod "downwardapi-volume-b49ecf9c-35b5-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292616ms
Feb 21 08:50:16.737: INFO: Pod "downwardapi-volume-b49ecf9c-35b5-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009973328s
STEP: Saw pod success
Feb 21 08:50:16.737: INFO: Pod "downwardapi-volume-b49ecf9c-35b5-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:50:16.742: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-b49ecf9c-35b5-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 08:50:16.768: INFO: Waiting for pod downwardapi-volume-b49ecf9c-35b5-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:50:16.773: INFO: Pod downwardapi-volume-b49ecf9c-35b5-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:50:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2zvcb" for this suite.
Feb 21 08:50:22.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:50:23.007: INFO: namespace: e2e-tests-projected-2zvcb, resource: bindings, ignored listing per whitelist
Feb 21 08:50:23.017: INFO: namespace e2e-tests-projected-2zvcb deletion completed in 6.240455493s

• [SLOW TEST:8.576 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:50:23.018: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6mzsh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1042
STEP: creating the pod
Feb 21 08:50:23.313: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-6mzsh'
Feb 21 08:50:24.429: INFO: stderr: ""
Feb 21 08:50:24.429: INFO: stdout: "pod/pause created\n"
Feb 21 08:50:24.429: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 21 08:50:24.429: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-6mzsh" to be "running and ready"
Feb 21 08:50:24.436: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.948391ms
Feb 21 08:50:26.447: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018098278s
Feb 21 08:50:28.454: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024336662s
Feb 21 08:50:30.459: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030310984s
Feb 21 08:50:32.466: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037026482s
Feb 21 08:50:34.472: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042573428s
Feb 21 08:50:36.479: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.049774235s
Feb 21 08:50:38.486: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.056764032s
Feb 21 08:50:40.493: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 16.063732521s
Feb 21 08:50:42.500: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.07072181s
Feb 21 08:50:44.508: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 20.079118959s
Feb 21 08:50:46.514: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 22.084511302s
Feb 21 08:50:48.521: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 24.091645989s
Feb 21 08:50:50.527: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 26.098212465s
Feb 21 08:50:52.533: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 28.103909982s
Feb 21 08:50:54.539: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 30.10947533s
Feb 21 08:50:56.544: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 32.114656035s
Feb 21 08:50:58.549: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 34.120143701s
Feb 21 08:51:00.556: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 36.126472487s
Feb 21 08:51:02.561: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 38.131672399s
Feb 21 08:51:04.567: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 40.138033692s
Feb 21 08:51:06.572: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 42.142745244s
Feb 21 08:51:08.577: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 44.148328006s
Feb 21 08:51:10.583: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 46.153963855s
Feb 21 08:51:12.589: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 48.159391951s
Feb 21 08:51:14.594: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 50.164604546s
Feb 21 08:51:16.599: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 52.169717975s
Feb 21 08:51:18.604: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 54.175174118s
Feb 21 08:51:20.610: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 56.180529355s
Feb 21 08:51:22.615: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 58.186231328s
Feb 21 08:51:24.622: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 1m0.192606985s
Feb 21 08:51:24.622: INFO: Pod "pause" satisfied condition "running and ready"
Feb 21 08:51:24.622: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 21 08:51:24.622: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-6mzsh'
Feb 21 08:51:24.776: INFO: stderr: ""
Feb 21 08:51:24.776: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 21 08:51:24.776: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-6mzsh'
Feb 21 08:51:24.897: INFO: stderr: ""
Feb 21 08:51:24.897: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          60s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 21 08:51:24.897: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml label pods pause testing-label- --namespace=e2e-tests-kubectl-6mzsh'
Feb 21 08:51:25.035: INFO: stderr: ""
Feb 21 08:51:25.035: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 21 08:51:25.035: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-6mzsh'
Feb 21 08:51:25.140: INFO: stderr: ""
Feb 21 08:51:25.140: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          61s   \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1048
STEP: using delete to clean up resources
Feb 21 08:51:25.140: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6mzsh'
Feb 21 08:51:25.249: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 08:51:25.249: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 21 08:51:25.249: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-6mzsh'
Feb 21 08:51:25.359: INFO: stderr: "No resources found.\n"
Feb 21 08:51:25.359: INFO: stdout: ""
Feb 21 08:51:25.359: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=pause --namespace=e2e-tests-kubectl-6mzsh -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 08:51:25.458: INFO: stderr: ""
Feb 21 08:51:25.458: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:51:25.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6mzsh" for this suite.
Feb 21 08:51:31.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:51:31.699: INFO: namespace: e2e-tests-kubectl-6mzsh, resource: bindings, ignored listing per whitelist
Feb 21 08:51:31.715: INFO: namespace e2e-tests-kubectl-6mzsh deletion completed in 6.251509585s

• [SLOW TEST:68.698 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:51:31.716: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-2g4j2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:51:32.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-2g4j2" for this suite.
Feb 21 08:51:54.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:51:54.148: INFO: namespace: e2e-tests-pods-2g4j2, resource: bindings, ignored listing per whitelist
Feb 21 08:51:54.274: INFO: namespace e2e-tests-pods-2g4j2 deletion completed in 22.244182849s

• [SLOW TEST:22.559 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:51:54.274: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-zv7x4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 08:51:54.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f02aff58-35b5-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-zv7x4" to be "success or failure"
Feb 21 08:51:54.636: INFO: Pod "downwardapi-volume-f02aff58-35b5-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.138914ms
Feb 21 08:51:56.642: INFO: Pod "downwardapi-volume-f02aff58-35b5-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009996847s
STEP: Saw pod success
Feb 21 08:51:56.643: INFO: Pod "downwardapi-volume-f02aff58-35b5-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:51:56.647: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-f02aff58-35b5-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 08:51:56.669: INFO: Waiting for pod downwardapi-volume-f02aff58-35b5-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:51:56.674: INFO: Pod downwardapi-volume-f02aff58-35b5-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:51:56.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zv7x4" for this suite.
Feb 21 08:52:02.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:52:02.749: INFO: namespace: e2e-tests-downward-api-zv7x4, resource: bindings, ignored listing per whitelist
Feb 21 08:52:02.909: INFO: namespace e2e-tests-downward-api-zv7x4 deletion completed in 6.231603012s

• [SLOW TEST:8.635 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:52:02.910: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-fd747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 21 08:52:03.244: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 21 08:52:03.254: INFO: Waiting for terminating namespaces to be deleted...
Feb 21 08:52:03.259: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw before test
Feb 21 08:52:03.280: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-j8nvn from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.280: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 21 08:52:03.280: INFO: coredns-5f4748c5f-zzvr9 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.280: INFO: 	Container coredns ready: true, restart count 0
Feb 21 08:52:03.280: INFO: vpn-shoot-597d7f599b-56cgp from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.280: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 21 08:52:03.280: INFO: kube-proxy-pxd78 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.280: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 08:52:03.280: INFO: addons-kube-lego-648f8c9f5c-88qt5 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.280: INFO: 	Container kube-lego ready: true, restart count 0
Feb 21 08:52:03.280: INFO: addons-nginx-ingress-controller-55d976867d-zqg87 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.280: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 08:52:03.280: INFO: node-exporter-7nlxq from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.280: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 08:52:03.280: INFO: calico-node-bbf5b from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.281: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 08:52:03.281: INFO: metrics-server-5c5588c85c-lwhcv from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.281: INFO: 	Container metrics-server ready: true, restart count 0
Feb 21 08:52:03.281: INFO: addons-kubernetes-dashboard-5f64f76bd-gld9p from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.281: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 21 08:52:03.281: INFO: blackbox-exporter-64f6f7f998-frlvk from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.281: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 21 08:52:03.281: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl before test
Feb 21 08:52:03.324: INFO: calico-node-gfth2 from kube-system started at 2019-02-21 08:00:47 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.324: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 08:52:03.324: INFO: kube-proxy-2jctn from kube-system started at 2019-02-21 08:00:47 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.324: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 08:52:03.324: INFO: node-exporter-j5s5c from kube-system started at 2019-02-21 08:00:48 +0000 UTC (1 container statuses recorded)
Feb 21 08:52:03.324: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
STEP: verifying the node has the label node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl
Feb 21 08:52:03.379: INFO: Pod addons-kube-lego-648f8c9f5c-88qt5 requesting resource cpu=20m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
Feb 21 08:52:03.379: INFO: Pod addons-kubernetes-dashboard-5f64f76bd-gld9p requesting resource cpu=50m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
Feb 21 08:52:03.379: INFO: Pod addons-nginx-ingress-controller-55d976867d-zqg87 requesting resource cpu=100m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
Feb 21 08:52:03.379: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-j8nvn requesting resource cpu=0m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
Feb 21 08:52:03.379: INFO: Pod blackbox-exporter-64f6f7f998-frlvk requesting resource cpu=5m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
Feb 21 08:52:03.379: INFO: Pod calico-node-bbf5b requesting resource cpu=100m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
Feb 21 08:52:03.379: INFO: Pod calico-node-gfth2 requesting resource cpu=100m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl
Feb 21 08:52:03.379: INFO: Pod coredns-5f4748c5f-zzvr9 requesting resource cpu=50m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
Feb 21 08:52:03.379: INFO: Pod kube-proxy-2jctn requesting resource cpu=20m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl
Feb 21 08:52:03.379: INFO: Pod kube-proxy-pxd78 requesting resource cpu=20m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
Feb 21 08:52:03.379: INFO: Pod metrics-server-5c5588c85c-lwhcv requesting resource cpu=20m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
Feb 21 08:52:03.379: INFO: Pod node-exporter-7nlxq requesting resource cpu=5m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
Feb 21 08:52:03.379: INFO: Pod node-exporter-j5s5c requesting resource cpu=5m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl
Feb 21 08:52:03.379: INFO: Pod vpn-shoot-597d7f599b-56cgp requesting resource cpu=50m on Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f5637005-35b5-11e9-b7c5-a2b84e263bfe.158555043d560968], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-fd747/filler-pod-f5637005-35b5-11e9-b7c5-a2b84e263bfe to shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f5637005-35b5-11e9-b7c5-a2b84e263bfe.15855504689eb7a0], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f5637005-35b5-11e9-b7c5-a2b84e263bfe.158555119ea9886b], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f5637005-35b5-11e9-b7c5-a2b84e263bfe.15855511a19835d4], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f5637005-35b5-11e9-b7c5-a2b84e263bfe.15855511a8385339], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f564eb88-35b5-11e9-b7c5-a2b84e263bfe.158555043dbd1b26], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-fd747/filler-pod-f564eb88-35b5-11e9-b7c5-a2b84e263bfe to shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f564eb88-35b5-11e9-b7c5-a2b84e263bfe.1585550468676051], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f564eb88-35b5-11e9-b7c5-a2b84e263bfe.158555046aef49a9], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f564eb88-35b5-11e9-b7c5-a2b84e263bfe.158555047195e441], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.158555123812035e], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:53:04.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-fd747" for this suite.
Feb 21 08:53:10.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:53:10.560: INFO: namespace: e2e-tests-sched-pred-fd747, resource: bindings, ignored listing per whitelist
Feb 21 08:53:10.896: INFO: namespace e2e-tests-sched-pred-fd747 deletion completed in 6.413801955s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:67.987 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:53:10.897: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-qdbzx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-qdbzx/configmap-test-1df074cd-35b6-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 08:53:11.427: INFO: Waiting up to 5m0s for pod "pod-configmaps-1df16627-35b6-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-configmap-qdbzx" to be "success or failure"
Feb 21 08:53:11.431: INFO: Pod "pod-configmaps-1df16627-35b6-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815903ms
Feb 21 08:53:13.438: INFO: Pod "pod-configmaps-1df16627-35b6-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010296797s
STEP: Saw pod success
Feb 21 08:53:13.438: INFO: Pod "pod-configmaps-1df16627-35b6-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:53:13.442: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-configmaps-1df16627-35b6-11e9-b7c5-a2b84e263bfe container env-test: <nil>
STEP: delete the pod
Feb 21 08:53:13.464: INFO: Waiting for pod pod-configmaps-1df16627-35b6-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:53:13.468: INFO: Pod pod-configmaps-1df16627-35b6-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:53:13.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-qdbzx" for this suite.
Feb 21 08:53:19.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:53:19.553: INFO: namespace: e2e-tests-configmap-qdbzx, resource: bindings, ignored listing per whitelist
Feb 21 08:53:19.771: INFO: namespace e2e-tests-configmap-qdbzx deletion completed in 6.296719774s

• [SLOW TEST:8.874 seconds]
[sig-api-machinery] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:53:19.772: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-hxbvq
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-2313b3e7-35b6-11e9-b7c5-a2b84e263bfe
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:53:22.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hxbvq" for this suite.
Feb 21 08:53:44.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:53:44.210: INFO: namespace: e2e-tests-configmap-hxbvq, resource: bindings, ignored listing per whitelist
Feb 21 08:53:44.367: INFO: namespace e2e-tests-configmap-hxbvq deletion completed in 22.206204968s

• [SLOW TEST:24.595 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:53:44.367: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-wlhhn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 08:53:44.625: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31bb1f19-35b6-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-wlhhn" to be "success or failure"
Feb 21 08:53:44.632: INFO: Pod "downwardapi-volume-31bb1f19-35b6-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.814452ms
Feb 21 08:53:46.638: INFO: Pod "downwardapi-volume-31bb1f19-35b6-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013333443s
STEP: Saw pod success
Feb 21 08:53:46.639: INFO: Pod "downwardapi-volume-31bb1f19-35b6-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:53:46.643: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-31bb1f19-35b6-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 08:53:46.671: INFO: Waiting for pod downwardapi-volume-31bb1f19-35b6-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:53:46.675: INFO: Pod downwardapi-volume-31bb1f19-35b6-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:53:46.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wlhhn" for this suite.
Feb 21 08:53:52.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:53:52.987: INFO: namespace: e2e-tests-downward-api-wlhhn, resource: bindings, ignored listing per whitelist
Feb 21 08:53:53.066: INFO: namespace e2e-tests-downward-api-wlhhn deletion completed in 6.386637144s

• [SLOW TEST:8.699 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:53:53.066: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-k5prk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 21 08:53:53.418: INFO: Waiting up to 5m0s for pod "pod-36f8cafb-35b6-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-k5prk" to be "success or failure"
Feb 21 08:53:53.422: INFO: Pod "pod-36f8cafb-35b6-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.879295ms
Feb 21 08:53:55.427: INFO: Pod "pod-36f8cafb-35b6-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008972719s
STEP: Saw pod success
Feb 21 08:53:55.427: INFO: Pod "pod-36f8cafb-35b6-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:53:55.431: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-36f8cafb-35b6-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 08:53:55.453: INFO: Waiting for pod pod-36f8cafb-35b6-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:53:55.457: INFO: Pod pod-36f8cafb-35b6-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:53:55.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-k5prk" for this suite.
Feb 21 08:54:01.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:54:01.626: INFO: namespace: e2e-tests-emptydir-k5prk, resource: bindings, ignored listing per whitelist
Feb 21 08:54:01.639: INFO: namespace e2e-tests-emptydir-k5prk deletion completed in 6.176926323s

• [SLOW TEST:8.574 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:54:01.640: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-p848k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 08:54:02.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c1a183f-35b6-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-p848k" to be "success or failure"
Feb 21 08:54:02.032: INFO: Pod "downwardapi-volume-3c1a183f-35b6-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.368363ms
Feb 21 08:54:04.038: INFO: Pod "downwardapi-volume-3c1a183f-35b6-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011331018s
STEP: Saw pod success
Feb 21 08:54:04.038: INFO: Pod "downwardapi-volume-3c1a183f-35b6-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:54:04.042: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-3c1a183f-35b6-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 08:54:04.065: INFO: Waiting for pod downwardapi-volume-3c1a183f-35b6-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:54:04.069: INFO: Pod downwardapi-volume-3c1a183f-35b6-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:54:04.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-p848k" for this suite.
Feb 21 08:54:10.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:54:10.311: INFO: namespace: e2e-tests-downward-api-p848k, resource: bindings, ignored listing per whitelist
Feb 21 08:54:10.337: INFO: namespace e2e-tests-downward-api-p848k deletion completed in 6.263296596s

• [SLOW TEST:8.698 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:54:10.338: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-5xqxh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-413a15bd-35b6-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 08:54:10.629: INFO: Waiting up to 5m0s for pod "pod-configmaps-413adf94-35b6-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-configmap-5xqxh" to be "success or failure"
Feb 21 08:54:10.635: INFO: Pod "pod-configmaps-413adf94-35b6-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.362524ms
Feb 21 08:54:12.640: INFO: Pod "pod-configmaps-413adf94-35b6-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010261191s
STEP: Saw pod success
Feb 21 08:54:12.640: INFO: Pod "pod-configmaps-413adf94-35b6-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:54:12.644: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-configmaps-413adf94-35b6-11e9-b7c5-a2b84e263bfe container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 08:54:12.664: INFO: Waiting for pod pod-configmaps-413adf94-35b6-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:54:12.669: INFO: Pod pod-configmaps-413adf94-35b6-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:54:12.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5xqxh" for this suite.
Feb 21 08:54:18.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:54:18.949: INFO: namespace: e2e-tests-configmap-5xqxh, resource: bindings, ignored listing per whitelist
Feb 21 08:54:19.034: INFO: namespace e2e-tests-configmap-5xqxh deletion completed in 6.358299323s

• [SLOW TEST:8.697 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:54:19.035: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-rtxdg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-466c214e-35b6-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 08:54:19.347: INFO: Waiting up to 5m0s for pod "pod-configmaps-466d3725-35b6-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-configmap-rtxdg" to be "success or failure"
Feb 21 08:54:19.351: INFO: Pod "pod-configmaps-466d3725-35b6-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.128612ms
Feb 21 08:54:21.356: INFO: Pod "pod-configmaps-466d3725-35b6-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009116879s
STEP: Saw pod success
Feb 21 08:54:21.356: INFO: Pod "pod-configmaps-466d3725-35b6-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 08:54:21.360: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-configmaps-466d3725-35b6-11e9-b7c5-a2b84e263bfe container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 08:54:21.422: INFO: Waiting for pod pod-configmaps-466d3725-35b6-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 08:54:21.430: INFO: Pod pod-configmaps-466d3725-35b6-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:54:21.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rtxdg" for this suite.
Feb 21 08:54:27.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:54:27.544: INFO: namespace: e2e-tests-configmap-rtxdg, resource: bindings, ignored listing per whitelist
Feb 21 08:54:27.645: INFO: namespace e2e-tests-configmap-rtxdg deletion completed in 6.209577285s

• [SLOW TEST:8.610 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:54:27.645: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-k2ts2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 08:54:27.908: INFO: Creating deployment "nginx-deployment"
Feb 21 08:54:27.912: INFO: Waiting for observed generation 1
Feb 21 08:54:29.924: INFO: Waiting for all required pods to come up
Feb 21 08:54:29.938: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 21 08:54:31.957: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 21 08:54:31.967: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 21 08:54:31.978: INFO: Updating deployment nginx-deployment
Feb 21 08:54:31.978: INFO: Waiting for observed generation 2
Feb 21 08:54:33.990: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 21 08:54:33.994: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 21 08:54:33.997: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 21 08:54:34.232: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 21 08:54:34.232: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 21 08:54:34.236: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 21 08:54:34.243: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 21 08:54:34.243: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 21 08:54:34.253: INFO: Updating deployment nginx-deployment
Feb 21 08:54:34.253: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 21 08:54:34.261: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 21 08:54:34.264: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 21 08:54:34.275: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-k2ts2/deployments/nginx-deployment,UID:4b8a1cd0-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8371,Generation:3,CreationTimestamp:2019-02-21 08:54:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-02-21 08:54:32 +0000 UTC 2019-02-21 08:54:27 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-7dc8f79789" is progressing.} {Available False 2019-02-21 08:54:34 +0000 UTC 2019-02-21 08:54:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 21 08:54:34.281: INFO: New ReplicaSet "nginx-deployment-7dc8f79789" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789,GenerateName:,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-k2ts2/replicasets/nginx-deployment-7dc8f79789,UID:4df7289e-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8367,Generation:3,CreationTimestamp:2019-02-21 08:54:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4b8a1cd0-35b6-11e9-890e-3aac84b8a476 0xc0010b6907 0xc0010b6908}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 08:54:34.281: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 21 08:54:34.281: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b,GenerateName:,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-k2ts2/replicasets/nginx-deployment-7f9675fb8b,UID:4b8b79a6-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8365,Generation:3,CreationTimestamp:2019-02-21 08:54:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4b8a1cd0-35b6-11e9-890e-3aac84b8a476 0xc0010b6a27 0xc0010b6a28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 21 08:54:34.295: INFO: Pod "nginx-deployment-7dc8f79789-7stfr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-7stfr,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7dc8f79789-7stfr,UID:4df80707-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8359,Generation:0,CreationTimestamp:2019-02-21 08:54:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.21/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 4df7289e-35b6-11e9-890e-3aac84b8a476 0xc00275f770 0xc00275f771}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00275f7f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00275f810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-21 08:54:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.295: INFO: Pod "nginx-deployment-7dc8f79789-f2k4v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-f2k4v,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7dc8f79789-f2k4v,UID:4df8fd10-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8363,Generation:0,CreationTimestamp:2019-02-21 08:54:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.51/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 4df7289e-35b6-11e9-890e-3aac84b8a476 0xc00275f8e0 0xc00275f8e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00275fb80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00275fba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:,StartTime:2019-02-21 08:54:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.295: INFO: Pod "nginx-deployment-7dc8f79789-g57pn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-g57pn,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7dc8f79789-g57pn,UID:4e01020a-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8360,Generation:0,CreationTimestamp:2019-02-21 08:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.22/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 4df7289e-35b6-11e9-890e-3aac84b8a476 0xc00275fc70 0xc00275fc71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00275fce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001778010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-21 08:54:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.296: INFO: Pod "nginx-deployment-7dc8f79789-g7v6x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-g7v6x,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7dc8f79789-g7v6x,UID:4f54fd3f-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8377,Generation:0,CreationTimestamp:2019-02-21 08:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 4df7289e-35b6-11e9-890e-3aac84b8a476 0xc0017780d0 0xc0017780d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017781b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017781d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.296: INFO: Pod "nginx-deployment-7dc8f79789-lxvxz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-lxvxz,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7dc8f79789-lxvxz,UID:4df8d30f-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8361,Generation:0,CreationTimestamp:2019-02-21 08:54:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.49/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 4df7289e-35b6-11e9-890e-3aac84b8a476 0xc001778247 0xc001778248}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017782b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017782d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:,StartTime:2019-02-21 08:54:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.296: INFO: Pod "nginx-deployment-7dc8f79789-pn229" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-pn229,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7dc8f79789-pn229,UID:4f54f025-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8376,Generation:0,CreationTimestamp:2019-02-21 08:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 4df7289e-35b6-11e9-890e-3aac84b8a476 0xc0017783a0 0xc0017783a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001778410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001778440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.296: INFO: Pod "nginx-deployment-7dc8f79789-tgtbl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-tgtbl,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7dc8f79789-tgtbl,UID:4dfe1001-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8362,Generation:0,CreationTimestamp:2019-02-21 08:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.50/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 4df7289e-35b6-11e9-890e-3aac84b8a476 0xc0017784a7 0xc0017784a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001778520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001778540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:,StartTime:2019-02-21 08:54:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.296: INFO: Pod "nginx-deployment-7dc8f79789-zt8sb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-zt8sb,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7dc8f79789-zt8sb,UID:4f53cd70-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8373,Generation:0,CreationTimestamp:2019-02-21 08:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 4df7289e-35b6-11e9-890e-3aac84b8a476 0xc001778600 0xc001778601}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001778670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001778690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.296: INFO: Pod "nginx-deployment-7f9675fb8b-2k72w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-2k72w,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-2k72w,UID:4b8e6e5f-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8291,Generation:0,CreationTimestamp:2019-02-21 08:54:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.20/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001778720 0xc001778721}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001778780} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017787a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.0.20,StartTime:2019-02-21 08:54:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-21 08:54:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://bb0a7b348bb416f406d3705748195311aa47568c5743116b240fe8e91e86e08a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.297: INFO: Pod "nginx-deployment-7f9675fb8b-5gn7w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-5gn7w,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-5gn7w,UID:4f552785-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8378,Generation:0,CreationTimestamp:2019-02-21 08:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001778860 0xc001778861}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017788c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017788e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.297: INFO: Pod "nginx-deployment-7f9675fb8b-8zgn5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-8zgn5,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-8zgn5,UID:4f554ad8-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8380,Generation:0,CreationTimestamp:2019-02-21 08:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001778937 0xc001778938}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017789a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017789c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.297: INFO: Pod "nginx-deployment-7f9675fb8b-9nww4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-9nww4,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-9nww4,UID:4b8ff170-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8305,Generation:0,CreationTimestamp:2019-02-21 08:54:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.47/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001778a27 0xc001778a28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001778a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001778ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:100.96.1.47,StartTime:2019-02-21 08:54:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-21 08:54:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://bb95b914b55fc9842a8837a5b45e1f8cc880ed54b069cb44bbbb8c89f850baa1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.297: INFO: Pod "nginx-deployment-7f9675fb8b-bbg27" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-bbg27,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-bbg27,UID:4f544569-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8375,Generation:0,CreationTimestamp:2019-02-21 08:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001778b70 0xc001778b71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001778bd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001778bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.297: INFO: Pod "nginx-deployment-7f9675fb8b-bqxbq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-bqxbq,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-bqxbq,UID:4b90032f-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8288,Generation:0,CreationTimestamp:2019-02-21 08:54:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.18/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001778c70 0xc001778c71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001778cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001778cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.0.18,StartTime:2019-02-21 08:54:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-21 08:54:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://7374b08f53af12ef47946b48eb30e20b5669be1c9b3df6cc3333e571d52523ce}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.297: INFO: Pod "nginx-deployment-7f9675fb8b-fln6x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-fln6x,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-fln6x,UID:4b8d926f-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8308,Generation:0,CreationTimestamp:2019-02-21 08:54:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.44/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001778dc0 0xc001778dc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001778e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001778e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:100.96.1.44,StartTime:2019-02-21 08:54:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-21 08:54:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://dbfec3995e67a1fc9c2969523169df4e242e7c3eddfba003bed2f5138e903ae6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.297: INFO: Pod "nginx-deployment-7f9675fb8b-g7gdk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-g7gdk,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-g7gdk,UID:4f5562f8-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8382,Generation:0,CreationTimestamp:2019-02-21 08:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001778f00 0xc001778f01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001778f60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001778f80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.298: INFO: Pod "nginx-deployment-7f9675fb8b-glg9z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-glg9z,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-glg9z,UID:4f531b5d-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8383,Generation:0,CreationTimestamp:2019-02-21 08:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001778fd7 0xc001778fd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001779040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001779060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:34 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:,StartTime:2019-02-21 08:54:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.298: INFO: Pod "nginx-deployment-7f9675fb8b-hscqk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-hscqk,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-hscqk,UID:4b900ba9-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8317,Generation:0,CreationTimestamp:2019-02-21 08:54:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.45/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001779120 0xc001779121}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001779180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017791a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:100.96.1.45,StartTime:2019-02-21 08:54:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-21 08:54:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://ed0f697deff46d5d1423c97c7c50785d3edb5444823f567c05bdb59bcb1b3a57}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.298: INFO: Pod "nginx-deployment-7f9675fb8b-jtmbl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-jtmbl,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-jtmbl,UID:4b8d6487-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8297,Generation:0,CreationTimestamp:2019-02-21 08:54:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.17/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001779350 0xc001779351}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017793b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017793d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.0.17,StartTime:2019-02-21 08:54:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-21 08:54:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://027d217091cf72a0423562f9f18e94e09a1d86e427a43e34efff27498fb8529f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.298: INFO: Pod "nginx-deployment-7f9675fb8b-k765t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-k765t,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-k765t,UID:4f556555-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8381,Generation:0,CreationTimestamp:2019-02-21 08:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001779490 0xc001779491}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017794f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001779510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.298: INFO: Pod "nginx-deployment-7f9675fb8b-rwrpg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-rwrpg,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-rwrpg,UID:4b8ea4ea-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8294,Generation:0,CreationTimestamp:2019-02-21 08:54:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.19/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001779577 0xc001779578}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017795e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001779600}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.0.19,StartTime:2019-02-21 08:54:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-21 08:54:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://067abcb7f9964899e0a150bb93ebb4b2df86220ee669d28256e31ba3d2734296}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.298: INFO: Pod "nginx-deployment-7f9675fb8b-s4f2k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-s4f2k,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-s4f2k,UID:4b8e9db1-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8311,Generation:0,CreationTimestamp:2019-02-21 08:54:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.48/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc0017796d0 0xc0017796d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001779730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001779750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:100.96.1.48,StartTime:2019-02-21 08:54:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-21 08:54:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://451db818d403f383a9320009f87dfcf9a3381829838a3dbb85f358586f9cc450}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 08:54:34.298: INFO: Pod "nginx-deployment-7f9675fb8b-x8pjw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-x8pjw,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k2ts2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k2ts2/pods/nginx-deployment-7f9675fb8b-x8pjw,UID:4f53fbcb-35b6-11e9-890e-3aac84b8a476,ResourceVersion:8374,Generation:0,CreationTimestamp:2019-02-21 08:54:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4b8b79a6-35b6-11e9-890e-3aac84b8a476 0xc001779810 0xc001779811}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6ttjg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6ttjg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6ttjg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001779870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001779890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 08:54:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:54:34.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-k2ts2" for this suite.
Feb 21 08:54:40.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:54:40.555: INFO: namespace: e2e-tests-deployment-k2ts2, resource: bindings, ignored listing per whitelist
Feb 21 08:54:40.559: INFO: namespace e2e-tests-deployment-k2ts2 deletion completed in 6.237100863s

• [SLOW TEST:12.914 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:54:40.560: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-vc52l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 08:54:40.845: INFO: Number of nodes with available pods: 0
Feb 21 08:54:40.845: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:41.855: INFO: Number of nodes with available pods: 0
Feb 21 08:54:41.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:42.858: INFO: Number of nodes with available pods: 0
Feb 21 08:54:42.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:43.856: INFO: Number of nodes with available pods: 0
Feb 21 08:54:43.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:44.857: INFO: Number of nodes with available pods: 1
Feb 21 08:54:44.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:45.861: INFO: Number of nodes with available pods: 1
Feb 21 08:54:45.861: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:46.856: INFO: Number of nodes with available pods: 1
Feb 21 08:54:46.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:47.859: INFO: Number of nodes with available pods: 1
Feb 21 08:54:47.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:48.857: INFO: Number of nodes with available pods: 1
Feb 21 08:54:48.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:49.856: INFO: Number of nodes with available pods: 1
Feb 21 08:54:49.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:50.856: INFO: Number of nodes with available pods: 1
Feb 21 08:54:50.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:51.856: INFO: Number of nodes with available pods: 1
Feb 21 08:54:51.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:52.856: INFO: Number of nodes with available pods: 1
Feb 21 08:54:52.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:53.856: INFO: Number of nodes with available pods: 1
Feb 21 08:54:53.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:54.856: INFO: Number of nodes with available pods: 1
Feb 21 08:54:54.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:55.856: INFO: Number of nodes with available pods: 1
Feb 21 08:54:55.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:56.857: INFO: Number of nodes with available pods: 1
Feb 21 08:54:56.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:57.856: INFO: Number of nodes with available pods: 1
Feb 21 08:54:57.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:58.855: INFO: Number of nodes with available pods: 1
Feb 21 08:54:58.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:54:59.857: INFO: Number of nodes with available pods: 1
Feb 21 08:54:59.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:00.861: INFO: Number of nodes with available pods: 1
Feb 21 08:55:00.861: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:01.861: INFO: Number of nodes with available pods: 1
Feb 21 08:55:01.861: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:02.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:02.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:03.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:03.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:04.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:04.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:05.858: INFO: Number of nodes with available pods: 1
Feb 21 08:55:05.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:06.853: INFO: Number of nodes with available pods: 1
Feb 21 08:55:06.853: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:07.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:07.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:08.862: INFO: Number of nodes with available pods: 1
Feb 21 08:55:08.863: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:09.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:09.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:10.859: INFO: Number of nodes with available pods: 1
Feb 21 08:55:10.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:11.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:11.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:12.858: INFO: Number of nodes with available pods: 1
Feb 21 08:55:12.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:13.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:13.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:14.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:14.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:15.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:15.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:16.887: INFO: Number of nodes with available pods: 1
Feb 21 08:55:16.887: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:17.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:17.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:18.865: INFO: Number of nodes with available pods: 1
Feb 21 08:55:18.866: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:19.858: INFO: Number of nodes with available pods: 1
Feb 21 08:55:19.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:20.859: INFO: Number of nodes with available pods: 1
Feb 21 08:55:20.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:21.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:21.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:22.855: INFO: Number of nodes with available pods: 1
Feb 21 08:55:22.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:23.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:23.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:24.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:24.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:25.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:25.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:26.859: INFO: Number of nodes with available pods: 1
Feb 21 08:55:26.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:27.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:27.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:28.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:28.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:29.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:29.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:30.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:30.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:31.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:31.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:32.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:32.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:33.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:33.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:34.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:34.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:35.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:35.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:36.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:36.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:37.858: INFO: Number of nodes with available pods: 1
Feb 21 08:55:37.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:38.858: INFO: Number of nodes with available pods: 1
Feb 21 08:55:38.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:39.860: INFO: Number of nodes with available pods: 1
Feb 21 08:55:39.860: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:40.855: INFO: Number of nodes with available pods: 1
Feb 21 08:55:40.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:41.861: INFO: Number of nodes with available pods: 1
Feb 21 08:55:41.861: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:42.855: INFO: Number of nodes with available pods: 1
Feb 21 08:55:42.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:43.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:43.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:44.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:44.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:45.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:45.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:46.855: INFO: Number of nodes with available pods: 1
Feb 21 08:55:46.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:47.855: INFO: Number of nodes with available pods: 1
Feb 21 08:55:47.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:48.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:48.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:49.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:49.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:50.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:50.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:51.855: INFO: Number of nodes with available pods: 1
Feb 21 08:55:51.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:52.876: INFO: Number of nodes with available pods: 1
Feb 21 08:55:52.876: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:53.860: INFO: Number of nodes with available pods: 1
Feb 21 08:55:53.860: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:54.856: INFO: Number of nodes with available pods: 1
Feb 21 08:55:54.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:55.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:55.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:56.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:56.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:57.860: INFO: Number of nodes with available pods: 1
Feb 21 08:55:57.860: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:58.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:58.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:55:59.857: INFO: Number of nodes with available pods: 1
Feb 21 08:55:59.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:00.858: INFO: Number of nodes with available pods: 1
Feb 21 08:56:00.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:01.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:01.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:02.859: INFO: Number of nodes with available pods: 1
Feb 21 08:56:02.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:03.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:03.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:04.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:04.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:05.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:05.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:06.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:06.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:07.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:07.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:08.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:08.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:09.859: INFO: Number of nodes with available pods: 1
Feb 21 08:56:09.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:10.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:10.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:11.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:11.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:12.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:12.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:13.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:13.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:14.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:14.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:15.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:15.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:16.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:16.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:17.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:17.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:18.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:18.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:19.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:19.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:20.859: INFO: Number of nodes with available pods: 1
Feb 21 08:56:20.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:21.858: INFO: Number of nodes with available pods: 1
Feb 21 08:56:21.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:22.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:22.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:23.858: INFO: Number of nodes with available pods: 1
Feb 21 08:56:23.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:24.859: INFO: Number of nodes with available pods: 1
Feb 21 08:56:24.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:25.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:25.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:26.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:26.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:27.858: INFO: Number of nodes with available pods: 1
Feb 21 08:56:27.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:28.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:28.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:29.858: INFO: Number of nodes with available pods: 1
Feb 21 08:56:29.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:30.860: INFO: Number of nodes with available pods: 1
Feb 21 08:56:30.860: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:31.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:31.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:32.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:32.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:33.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:33.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:34.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:34.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:35.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:35.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:36.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:36.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:37.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:37.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:38.858: INFO: Number of nodes with available pods: 1
Feb 21 08:56:38.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:39.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:39.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:40.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:40.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:41.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:41.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:42.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:42.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:43.865: INFO: Number of nodes with available pods: 1
Feb 21 08:56:43.865: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:44.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:44.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:45.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:45.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:46.861: INFO: Number of nodes with available pods: 1
Feb 21 08:56:46.861: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:47.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:47.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:48.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:48.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:49.855: INFO: Number of nodes with available pods: 1
Feb 21 08:56:49.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:50.861: INFO: Number of nodes with available pods: 1
Feb 21 08:56:50.861: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:51.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:51.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:52.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:52.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:53.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:53.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:54.859: INFO: Number of nodes with available pods: 1
Feb 21 08:56:54.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:55.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:55.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:56.856: INFO: Number of nodes with available pods: 1
Feb 21 08:56:56.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:57.858: INFO: Number of nodes with available pods: 1
Feb 21 08:56:57.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:58.858: INFO: Number of nodes with available pods: 1
Feb 21 08:56:58.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:56:59.857: INFO: Number of nodes with available pods: 1
Feb 21 08:56:59.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:00.860: INFO: Number of nodes with available pods: 1
Feb 21 08:57:00.860: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:01.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:01.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:02.855: INFO: Number of nodes with available pods: 1
Feb 21 08:57:02.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:03.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:03.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:04.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:04.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:05.859: INFO: Number of nodes with available pods: 1
Feb 21 08:57:05.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:06.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:06.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:07.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:07.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:08.858: INFO: Number of nodes with available pods: 1
Feb 21 08:57:08.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:09.855: INFO: Number of nodes with available pods: 1
Feb 21 08:57:09.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:10.855: INFO: Number of nodes with available pods: 1
Feb 21 08:57:10.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:11.864: INFO: Number of nodes with available pods: 1
Feb 21 08:57:11.864: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:12.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:12.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:13.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:13.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:14.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:14.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:15.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:15.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:16.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:16.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:17.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:17.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:18.855: INFO: Number of nodes with available pods: 1
Feb 21 08:57:18.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:19.858: INFO: Number of nodes with available pods: 1
Feb 21 08:57:19.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:20.855: INFO: Number of nodes with available pods: 1
Feb 21 08:57:20.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:21.855: INFO: Number of nodes with available pods: 1
Feb 21 08:57:21.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:22.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:22.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:23.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:23.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:24.858: INFO: Number of nodes with available pods: 1
Feb 21 08:57:24.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:25.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:25.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:26.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:26.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:27.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:27.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:28.858: INFO: Number of nodes with available pods: 1
Feb 21 08:57:28.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:29.861: INFO: Number of nodes with available pods: 1
Feb 21 08:57:29.861: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:30.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:30.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:31.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:31.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:32.861: INFO: Number of nodes with available pods: 1
Feb 21 08:57:32.861: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:33.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:33.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:34.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:34.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:35.859: INFO: Number of nodes with available pods: 1
Feb 21 08:57:35.859: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:36.855: INFO: Number of nodes with available pods: 1
Feb 21 08:57:36.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:37.855: INFO: Number of nodes with available pods: 1
Feb 21 08:57:37.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:38.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:38.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:39.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:39.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:40.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:40.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:41.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:41.861: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:42.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:42.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:43.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:43.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:44.860: INFO: Number of nodes with available pods: 1
Feb 21 08:57:44.860: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:45.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:45.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:46.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:46.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:47.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:47.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:48.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:48.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:49.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:49.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:50.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:50.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:51.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:51.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:52.863: INFO: Number of nodes with available pods: 1
Feb 21 08:57:52.863: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:53.858: INFO: Number of nodes with available pods: 1
Feb 21 08:57:53.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:54.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:54.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:55.855: INFO: Number of nodes with available pods: 1
Feb 21 08:57:55.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:56.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:56.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:57.856: INFO: Number of nodes with available pods: 1
Feb 21 08:57:57.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:58.857: INFO: Number of nodes with available pods: 1
Feb 21 08:57:58.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:57:59.860: INFO: Number of nodes with available pods: 1
Feb 21 08:57:59.860: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:00.856: INFO: Number of nodes with available pods: 1
Feb 21 08:58:00.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:01.869: INFO: Number of nodes with available pods: 1
Feb 21 08:58:01.869: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:02.858: INFO: Number of nodes with available pods: 1
Feb 21 08:58:02.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:03.855: INFO: Number of nodes with available pods: 1
Feb 21 08:58:03.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:04.856: INFO: Number of nodes with available pods: 1
Feb 21 08:58:04.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:05.856: INFO: Number of nodes with available pods: 1
Feb 21 08:58:05.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:06.856: INFO: Number of nodes with available pods: 1
Feb 21 08:58:06.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:07.855: INFO: Number of nodes with available pods: 1
Feb 21 08:58:07.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:08.856: INFO: Number of nodes with available pods: 1
Feb 21 08:58:08.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:09.857: INFO: Number of nodes with available pods: 1
Feb 21 08:58:09.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:10.857: INFO: Number of nodes with available pods: 1
Feb 21 08:58:10.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:11.861: INFO: Number of nodes with available pods: 1
Feb 21 08:58:11.861: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:12.855: INFO: Number of nodes with available pods: 1
Feb 21 08:58:12.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:13.854: INFO: Number of nodes with available pods: 1
Feb 21 08:58:13.854: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:14.855: INFO: Number of nodes with available pods: 1
Feb 21 08:58:14.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:15.857: INFO: Number of nodes with available pods: 1
Feb 21 08:58:15.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:16.860: INFO: Number of nodes with available pods: 1
Feb 21 08:58:16.860: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:17.858: INFO: Number of nodes with available pods: 1
Feb 21 08:58:17.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:18.855: INFO: Number of nodes with available pods: 1
Feb 21 08:58:18.855: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:19.857: INFO: Number of nodes with available pods: 1
Feb 21 08:58:19.857: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:20.856: INFO: Number of nodes with available pods: 1
Feb 21 08:58:20.856: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:21.858: INFO: Number of nodes with available pods: 1
Feb 21 08:58:21.858: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:22.856: INFO: Number of nodes with available pods: 2
Feb 21 08:58:22.856: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 21 08:58:22.895: INFO: Number of nodes with available pods: 1
Feb 21 08:58:22.895: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:23.906: INFO: Number of nodes with available pods: 1
Feb 21 08:58:23.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:24.911: INFO: Number of nodes with available pods: 1
Feb 21 08:58:24.911: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:25.908: INFO: Number of nodes with available pods: 1
Feb 21 08:58:25.908: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:26.907: INFO: Number of nodes with available pods: 1
Feb 21 08:58:26.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:27.908: INFO: Number of nodes with available pods: 1
Feb 21 08:58:27.908: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:28.906: INFO: Number of nodes with available pods: 1
Feb 21 08:58:28.906: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:29.910: INFO: Number of nodes with available pods: 1
Feb 21 08:58:29.910: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:30.906: INFO: Number of nodes with available pods: 1
Feb 21 08:58:30.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:31.906: INFO: Number of nodes with available pods: 1
Feb 21 08:58:31.906: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:32.910: INFO: Number of nodes with available pods: 1
Feb 21 08:58:32.910: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:33.910: INFO: Number of nodes with available pods: 1
Feb 21 08:58:33.910: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:34.907: INFO: Number of nodes with available pods: 1
Feb 21 08:58:34.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:35.907: INFO: Number of nodes with available pods: 1
Feb 21 08:58:35.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:36.907: INFO: Number of nodes with available pods: 1
Feb 21 08:58:36.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:37.906: INFO: Number of nodes with available pods: 1
Feb 21 08:58:37.906: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:38.904: INFO: Number of nodes with available pods: 1
Feb 21 08:58:38.904: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:39.905: INFO: Number of nodes with available pods: 1
Feb 21 08:58:39.905: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:40.906: INFO: Number of nodes with available pods: 1
Feb 21 08:58:40.906: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:41.905: INFO: Number of nodes with available pods: 1
Feb 21 08:58:41.905: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:42.908: INFO: Number of nodes with available pods: 1
Feb 21 08:58:42.908: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:43.907: INFO: Number of nodes with available pods: 1
Feb 21 08:58:43.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:44.906: INFO: Number of nodes with available pods: 1
Feb 21 08:58:44.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:45.909: INFO: Number of nodes with available pods: 1
Feb 21 08:58:45.909: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:46.906: INFO: Number of nodes with available pods: 1
Feb 21 08:58:46.906: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:47.905: INFO: Number of nodes with available pods: 1
Feb 21 08:58:47.905: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:48.905: INFO: Number of nodes with available pods: 1
Feb 21 08:58:48.905: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:49.909: INFO: Number of nodes with available pods: 1
Feb 21 08:58:49.909: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:50.905: INFO: Number of nodes with available pods: 1
Feb 21 08:58:50.905: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:51.913: INFO: Number of nodes with available pods: 1
Feb 21 08:58:51.913: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:52.920: INFO: Number of nodes with available pods: 1
Feb 21 08:58:52.920: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:53.909: INFO: Number of nodes with available pods: 1
Feb 21 08:58:53.909: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:54.906: INFO: Number of nodes with available pods: 1
Feb 21 08:58:54.906: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:55.906: INFO: Number of nodes with available pods: 1
Feb 21 08:58:55.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:56.905: INFO: Number of nodes with available pods: 1
Feb 21 08:58:56.905: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:57.907: INFO: Number of nodes with available pods: 1
Feb 21 08:58:57.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:58.905: INFO: Number of nodes with available pods: 1
Feb 21 08:58:58.905: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:58:59.909: INFO: Number of nodes with available pods: 1
Feb 21 08:58:59.909: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:59:00.907: INFO: Number of nodes with available pods: 1
Feb 21 08:59:00.907: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:59:01.906: INFO: Number of nodes with available pods: 1
Feb 21 08:59:01.906: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:59:02.908: INFO: Number of nodes with available pods: 1
Feb 21 08:59:02.908: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:59:03.906: INFO: Number of nodes with available pods: 1
Feb 21 08:59:03.906: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:59:04.906: INFO: Number of nodes with available pods: 1
Feb 21 08:59:04.906: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:59:05.909: INFO: Number of nodes with available pods: 1
Feb 21 08:59:05.909: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:59:06.904: INFO: Number of nodes with available pods: 1
Feb 21 08:59:06.904: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:59:07.909: INFO: Number of nodes with available pods: 1
Feb 21 08:59:07.909: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 08:59:08.924: INFO: Number of nodes with available pods: 2
Feb 21 08:59:08.924: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-vc52l, will wait for the garbage collector to delete the pods
Feb 21 08:59:09.003: INFO: Deleting {extensions DaemonSet} daemon-set took: 12.393936ms
Feb 21 08:59:09.104: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.372784ms
Feb 21 08:59:45.612: INFO: Number of nodes with available pods: 0
Feb 21 08:59:45.612: INFO: Number of running nodes: 0, number of available pods: 0
Feb 21 08:59:45.617: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-vc52l/daemonsets","resourceVersion":"9227"},"items":null}

Feb 21 08:59:45.625: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-vc52l/pods","resourceVersion":"9227"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:59:45.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-vc52l" for this suite.
Feb 21 08:59:51.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:59:51.717: INFO: namespace: e2e-tests-daemonsets-vc52l, resource: bindings, ignored listing per whitelist
Feb 21 08:59:51.835: INFO: namespace e2e-tests-daemonsets-vc52l deletion completed in 6.189867008s

• [SLOW TEST:311.275 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:59:51.835: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fmgnk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 21 08:59:52.212: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml api-versions'
Feb 21 08:59:52.427: INFO: stderr: ""
Feb 21 08:59:52.427: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 08:59:52.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fmgnk" for this suite.
Feb 21 08:59:58.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 08:59:58.763: INFO: namespace: e2e-tests-kubectl-fmgnk, resource: bindings, ignored listing per whitelist
Feb 21 08:59:58.828: INFO: namespace e2e-tests-kubectl-fmgnk deletion completed in 6.395203335s

• [SLOW TEST:6.994 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 08:59:58.829: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-gfpfb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 08:59:59.128: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10f37fac-35b7-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-gfpfb" to be "success or failure"
Feb 21 08:59:59.134: INFO: Pod "downwardapi-volume-10f37fac-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.923977ms
Feb 21 09:00:01.140: INFO: Pod "downwardapi-volume-10f37fac-35b7-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011489123s
STEP: Saw pod success
Feb 21 09:00:01.140: INFO: Pod "downwardapi-volume-10f37fac-35b7-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:00:01.146: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-10f37fac-35b7-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 09:00:01.180: INFO: Waiting for pod downwardapi-volume-10f37fac-35b7-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:00:01.184: INFO: Pod downwardapi-volume-10f37fac-35b7-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:00:01.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gfpfb" for this suite.
Feb 21 09:00:07.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:00:07.268: INFO: namespace: e2e-tests-downward-api-gfpfb, resource: bindings, ignored listing per whitelist
Feb 21 09:00:07.434: INFO: namespace e2e-tests-downward-api-gfpfb deletion completed in 6.244975226s

• [SLOW TEST:8.605 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:00:07.435: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-zn9zt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0221 09:00:38.263504   32452 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 21 09:00:38.263: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:00:38.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-zn9zt" for this suite.
Feb 21 09:00:44.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:00:44.306: INFO: namespace: e2e-tests-gc-zn9zt, resource: bindings, ignored listing per whitelist
Feb 21 09:00:44.471: INFO: namespace e2e-tests-gc-zn9zt deletion completed in 6.202480403s

• [SLOW TEST:37.036 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:00:44.471: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-zhzbv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-zhzbv/configmap-test-2c302009-35b7-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 09:00:44.835: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c318417-35b7-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-configmap-zhzbv" to be "success or failure"
Feb 21 09:00:44.840: INFO: Pod "pod-configmaps-2c318417-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.211159ms
Feb 21 09:00:46.847: INFO: Pod "pod-configmaps-2c318417-35b7-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011620614s
STEP: Saw pod success
Feb 21 09:00:46.847: INFO: Pod "pod-configmaps-2c318417-35b7-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:00:46.852: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-configmaps-2c318417-35b7-11e9-b7c5-a2b84e263bfe container env-test: <nil>
STEP: delete the pod
Feb 21 09:00:46.876: INFO: Waiting for pod pod-configmaps-2c318417-35b7-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:00:46.878: INFO: Pod pod-configmaps-2c318417-35b7-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:00:46.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zhzbv" for this suite.
Feb 21 09:00:52.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:00:53.052: INFO: namespace: e2e-tests-configmap-zhzbv, resource: bindings, ignored listing per whitelist
Feb 21 09:00:53.125: INFO: namespace e2e-tests-configmap-zhzbv deletion completed in 6.238806361s

• [SLOW TEST:8.655 seconds]
[sig-api-machinery] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:00:53.126: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gzp9t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 09:00:53.424: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3150505e-35b7-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-gzp9t" to be "success or failure"
Feb 21 09:00:53.429: INFO: Pod "downwardapi-volume-3150505e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.752959ms
Feb 21 09:00:55.435: INFO: Pod "downwardapi-volume-3150505e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011595274s
STEP: Saw pod success
Feb 21 09:00:55.435: INFO: Pod "downwardapi-volume-3150505e-35b7-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:00:55.441: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-3150505e-35b7-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 09:00:55.467: INFO: Waiting for pod downwardapi-volume-3150505e-35b7-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:00:55.472: INFO: Pod downwardapi-volume-3150505e-35b7-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:00:55.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gzp9t" for this suite.
Feb 21 09:01:01.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:01:01.571: INFO: namespace: e2e-tests-projected-gzp9t, resource: bindings, ignored listing per whitelist
Feb 21 09:01:01.967: INFO: namespace e2e-tests-projected-gzp9t deletion completed in 6.484969177s

• [SLOW TEST:8.841 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:01:01.967: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-bj9mm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 09:01:04.557: INFO: Waiting up to 5m0s for pod "client-envvars-37f3b3be-35b7-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-pods-bj9mm" to be "success or failure"
Feb 21 09:01:04.560: INFO: Pod "client-envvars-37f3b3be-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.343323ms
Feb 21 09:01:06.572: INFO: Pod "client-envvars-37f3b3be-35b7-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015560889s
STEP: Saw pod success
Feb 21 09:01:06.572: INFO: Pod "client-envvars-37f3b3be-35b7-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:01:06.577: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod client-envvars-37f3b3be-35b7-11e9-b7c5-a2b84e263bfe container env3cont: <nil>
STEP: delete the pod
Feb 21 09:01:06.605: INFO: Waiting for pod client-envvars-37f3b3be-35b7-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:01:06.611: INFO: Pod client-envvars-37f3b3be-35b7-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:01:06.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bj9mm" for this suite.
Feb 21 09:01:46.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:01:46.780: INFO: namespace: e2e-tests-pods-bj9mm, resource: bindings, ignored listing per whitelist
Feb 21 09:01:46.844: INFO: namespace e2e-tests-pods-bj9mm deletion completed in 40.221002582s

• [SLOW TEST:44.878 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:01:46.847: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wc9ld
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 21 09:01:47.140: INFO: Waiting up to 5m0s for pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-wc9ld" to be "success or failure"
Feb 21 09:01:47.146: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.637507ms
Feb 21 09:01:49.156: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015675567s
Feb 21 09:01:51.164: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02316731s
Feb 21 09:01:53.169: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028840866s
Feb 21 09:01:55.176: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036079779s
Feb 21 09:01:57.183: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042612623s
Feb 21 09:01:59.191: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.050225817s
Feb 21 09:02:01.198: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.057162382s
Feb 21 09:02:03.204: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.064027934s
Feb 21 09:02:05.211: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 18.070773102s
Feb 21 09:02:07.217: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 20.076113208s
Feb 21 09:02:09.222: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 22.081247606s
Feb 21 09:02:11.227: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 24.086961413s
Feb 21 09:02:13.233: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 26.092414973s
Feb 21 09:02:15.241: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 28.100847363s
Feb 21 09:02:17.246: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 30.105570173s
Feb 21 09:02:19.252: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 32.111851255s
Feb 21 09:02:21.258: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 34.118054611s
Feb 21 09:02:23.264: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 36.123502798s
Feb 21 09:02:25.269: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 38.128839733s
Feb 21 09:02:27.274: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 40.133807409s
Feb 21 09:02:29.279: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 42.138722358s
Feb 21 09:02:31.284: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 44.144061759s
Feb 21 09:02:33.290: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 46.149758226s
Feb 21 09:02:35.298: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 48.157472143s
Feb 21 09:02:37.310: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 50.170050334s
Feb 21 09:02:39.318: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 52.177402301s
Feb 21 09:02:41.324: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 54.183546487s
Feb 21 09:02:43.331: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 56.190536711s
Feb 21 09:02:45.337: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 58.196841015s
Feb 21 09:02:47.343: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.202404039s
Feb 21 09:02:49.348: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.207334347s
Feb 21 09:02:51.357: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.216449914s
Feb 21 09:02:53.364: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.223328947s
Feb 21 09:02:55.371: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.231031358s
Feb 21 09:02:57.379: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.238215267s
Feb 21 09:02:59.385: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.244906634s
Feb 21 09:03:01.391: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.250664792s
Feb 21 09:03:03.398: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m16.257794079s
STEP: Saw pod success
Feb 21 09:03:03.398: INFO: Pod "pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:03:03.404: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 09:03:03.426: INFO: Waiting for pod pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:03:03.430: INFO: Pod pod-5154d74f-35b7-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:03:03.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wc9ld" for this suite.
Feb 21 09:03:09.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:03:09.469: INFO: namespace: e2e-tests-emptydir-wc9ld, resource: bindings, ignored listing per whitelist
Feb 21 09:03:09.662: INFO: namespace e2e-tests-emptydir-wc9ld deletion completed in 6.225460161s

• [SLOW TEST:82.816 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:03:09.662: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6qqlp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-82ad3be6-35b7-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 09:03:09.942: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-82ae1165-35b7-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-6qqlp" to be "success or failure"
Feb 21 09:03:09.949: INFO: Pod "pod-projected-configmaps-82ae1165-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.728761ms
Feb 21 09:03:11.956: INFO: Pod "pod-projected-configmaps-82ae1165-35b7-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013970856s
STEP: Saw pod success
Feb 21 09:03:11.956: INFO: Pod "pod-projected-configmaps-82ae1165-35b7-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:03:11.960: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-configmaps-82ae1165-35b7-11e9-b7c5-a2b84e263bfe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 09:03:11.990: INFO: Waiting for pod pod-projected-configmaps-82ae1165-35b7-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:03:11.994: INFO: Pod pod-projected-configmaps-82ae1165-35b7-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:03:11.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6qqlp" for this suite.
Feb 21 09:03:18.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:03:18.086: INFO: namespace: e2e-tests-projected-6qqlp, resource: bindings, ignored listing per whitelist
Feb 21 09:03:18.232: INFO: namespace e2e-tests-projected-6qqlp deletion completed in 6.214634073s

• [SLOW TEST:8.570 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:03:18.233: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-7lthb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 21 09:03:18.641: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-7lthb,SelfLink:/api/v1/namespaces/e2e-tests-watch-7lthb/configmaps/e2e-watch-test-label-changed,UID:87de57ab-35b7-11e9-890e-3aac84b8a476,ResourceVersion:9838,Generation:0,CreationTimestamp:2019-02-21 09:03:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 21 09:03:18.642: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-7lthb,SelfLink:/api/v1/namespaces/e2e-tests-watch-7lthb/configmaps/e2e-watch-test-label-changed,UID:87de57ab-35b7-11e9-890e-3aac84b8a476,ResourceVersion:9839,Generation:0,CreationTimestamp:2019-02-21 09:03:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 21 09:03:18.642: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-7lthb,SelfLink:/api/v1/namespaces/e2e-tests-watch-7lthb/configmaps/e2e-watch-test-label-changed,UID:87de57ab-35b7-11e9-890e-3aac84b8a476,ResourceVersion:9840,Generation:0,CreationTimestamp:2019-02-21 09:03:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 21 09:03:28.675: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-7lthb,SelfLink:/api/v1/namespaces/e2e-tests-watch-7lthb/configmaps/e2e-watch-test-label-changed,UID:87de57ab-35b7-11e9-890e-3aac84b8a476,ResourceVersion:9861,Generation:0,CreationTimestamp:2019-02-21 09:03:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 21 09:03:28.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-7lthb,SelfLink:/api/v1/namespaces/e2e-tests-watch-7lthb/configmaps/e2e-watch-test-label-changed,UID:87de57ab-35b7-11e9-890e-3aac84b8a476,ResourceVersion:9862,Generation:0,CreationTimestamp:2019-02-21 09:03:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 21 09:03:28.676: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-7lthb,SelfLink:/api/v1/namespaces/e2e-tests-watch-7lthb/configmaps/e2e-watch-test-label-changed,UID:87de57ab-35b7-11e9-890e-3aac84b8a476,ResourceVersion:9863,Generation:0,CreationTimestamp:2019-02-21 09:03:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:03:28.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-7lthb" for this suite.
Feb 21 09:03:34.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:03:34.720: INFO: namespace: e2e-tests-watch-7lthb, resource: bindings, ignored listing per whitelist
Feb 21 09:03:34.845: INFO: namespace e2e-tests-watch-7lthb deletion completed in 6.163489131s

• [SLOW TEST:16.612 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:03:34.845: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-v9v5g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 09:03:35.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91d0b60b-35b7-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-v9v5g" to be "success or failure"
Feb 21 09:03:35.333: INFO: Pod "downwardapi-volume-91d0b60b-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.49632ms
Feb 21 09:03:37.340: INFO: Pod "downwardapi-volume-91d0b60b-35b7-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012137435s
STEP: Saw pod success
Feb 21 09:03:37.340: INFO: Pod "downwardapi-volume-91d0b60b-35b7-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:03:37.345: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-91d0b60b-35b7-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 09:03:37.371: INFO: Waiting for pod downwardapi-volume-91d0b60b-35b7-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:03:37.375: INFO: Pod downwardapi-volume-91d0b60b-35b7-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:03:37.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-v9v5g" for this suite.
Feb 21 09:03:43.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:03:43.420: INFO: namespace: e2e-tests-downward-api-v9v5g, resource: bindings, ignored listing per whitelist
Feb 21 09:03:43.602: INFO: namespace e2e-tests-downward-api-v9v5g deletion completed in 6.221437018s

• [SLOW TEST:8.757 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:03:43.602: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-tlkfd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 21 09:03:46.470: INFO: Successfully updated pod "pod-update-96f36fa3-35b7-11e9-b7c5-a2b84e263bfe"
STEP: verifying the updated pod is in kubernetes
Feb 21 09:03:46.477: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:03:46.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-tlkfd" for this suite.
Feb 21 09:04:08.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:04:08.590: INFO: namespace: e2e-tests-pods-tlkfd, resource: bindings, ignored listing per whitelist
Feb 21 09:04:08.664: INFO: namespace e2e-tests-pods-tlkfd deletion completed in 22.181371692s

• [SLOW TEST:25.062 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:04:08.664: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-d76t5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 21 09:04:09.020: INFO: Waiting up to 5m0s for pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-containers-d76t5" to be "success or failure"
Feb 21 09:04:09.034: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 13.75748ms
Feb 21 09:04:11.041: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020655208s
Feb 21 09:04:13.047: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026545076s
Feb 21 09:04:15.053: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03229266s
Feb 21 09:04:17.059: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038468548s
Feb 21 09:04:19.120: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.09996892s
Feb 21 09:04:21.128: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.107354566s
Feb 21 09:04:23.155: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.134573361s
Feb 21 09:04:25.160: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.139812704s
Feb 21 09:04:27.165: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 18.144444399s
Feb 21 09:04:29.172: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 20.151400705s
Feb 21 09:04:31.177: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 22.156404336s
Feb 21 09:04:33.194: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 24.173683772s
Feb 21 09:04:35.200: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 26.180033777s
Feb 21 09:04:37.205: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 28.185170303s
Feb 21 09:04:39.212: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 30.191852609s
Feb 21 09:04:41.217: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 32.196886081s
Feb 21 09:04:43.224: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 34.203360264s
Feb 21 09:04:45.229: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 36.208901831s
Feb 21 09:04:47.235: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 38.21430129s
Feb 21 09:04:49.242: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 40.221850142s
Feb 21 09:04:51.249: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 42.228296276s
Feb 21 09:04:53.256: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 44.23541045s
Feb 21 09:04:55.262: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 46.241487376s
Feb 21 09:04:57.268: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 48.247597818s
Feb 21 09:04:59.274: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 50.253237465s
Feb 21 09:05:01.285: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 52.264989899s
Feb 21 09:05:03.292: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 54.271997012s
Feb 21 09:05:05.299: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 56.278305454s
Feb 21 09:05:07.307: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 58.286432147s
Feb 21 09:05:09.313: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.29284092s
Feb 21 09:05:11.320: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.299539356s
Feb 21 09:05:13.325: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.304855089s
Feb 21 09:05:15.330: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.310005549s
Feb 21 09:05:17.336: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.316027655s
Feb 21 09:05:19.343: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.323101516s
Feb 21 09:05:21.349: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.328538941s
Feb 21 09:05:23.353: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.332990276s
Feb 21 09:05:25.359: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m16.338372167s
STEP: Saw pod success
Feb 21 09:05:25.359: INFO: Pod "client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:05:25.364: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 09:05:25.393: INFO: Waiting for pod client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:05:25.397: INFO: Pod client-containers-a5e6559e-35b7-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:05:25.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-d76t5" for this suite.
Feb 21 09:05:31.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:05:31.872: INFO: namespace: e2e-tests-containers-d76t5, resource: bindings, ignored listing per whitelist
Feb 21 09:05:31.877: INFO: namespace e2e-tests-containers-d76t5 deletion completed in 6.472957413s

• [SLOW TEST:83.213 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:05:31.877: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-q8l5f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-q8l5f
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 09:05:32.214: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 21 09:07:08.339: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.75:8080/dial?request=hostName&protocol=http&host=100.96.1.74&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-q8l5f PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 09:07:08.340: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 09:07:08.912: INFO: Waiting for endpoints: map[]
Feb 21 09:07:08.918: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.75:8080/dial?request=hostName&protocol=http&host=100.96.0.36&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-q8l5f PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 09:07:08.918: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 09:07:09.404: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:07:09.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-q8l5f" for this suite.
Feb 21 09:07:31.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:07:31.534: INFO: namespace: e2e-tests-pod-network-test-q8l5f, resource: bindings, ignored listing per whitelist
Feb 21 09:07:31.659: INFO: namespace e2e-tests-pod-network-test-q8l5f deletion completed in 22.247063347s

• [SLOW TEST:119.782 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:07:31.659: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vgm4x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-1ee5b83f-35b8-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 09:07:32.046: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1ee8e179-35b8-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-vgm4x" to be "success or failure"
Feb 21 09:07:32.060: INFO: Pod "pod-projected-configmaps-1ee8e179-35b8-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 13.979478ms
Feb 21 09:07:34.067: INFO: Pod "pod-projected-configmaps-1ee8e179-35b8-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020748266s
STEP: Saw pod success
Feb 21 09:07:34.067: INFO: Pod "pod-projected-configmaps-1ee8e179-35b8-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:07:34.071: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-configmaps-1ee8e179-35b8-11e9-b7c5-a2b84e263bfe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 09:07:34.095: INFO: Waiting for pod pod-projected-configmaps-1ee8e179-35b8-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:07:34.101: INFO: Pod pod-projected-configmaps-1ee8e179-35b8-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:07:34.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vgm4x" for this suite.
Feb 21 09:07:40.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:07:40.409: INFO: namespace: e2e-tests-projected-vgm4x, resource: bindings, ignored listing per whitelist
Feb 21 09:07:40.855: INFO: namespace e2e-tests-projected-vgm4x deletion completed in 6.746122282s

• [SLOW TEST:9.196 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:07:40.856: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-9vzpg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 21 09:07:41.440: INFO: Waiting up to 5m0s for pod "downward-api-2482c395-35b8-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-9vzpg" to be "success or failure"
Feb 21 09:07:41.445: INFO: Pod "downward-api-2482c395-35b8-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.346077ms
Feb 21 09:07:43.451: INFO: Pod "downward-api-2482c395-35b8-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010784579s
STEP: Saw pod success
Feb 21 09:07:43.451: INFO: Pod "downward-api-2482c395-35b8-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:07:43.457: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downward-api-2482c395-35b8-11e9-b7c5-a2b84e263bfe container dapi-container: <nil>
STEP: delete the pod
Feb 21 09:07:43.480: INFO: Waiting for pod downward-api-2482c395-35b8-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:07:43.485: INFO: Pod downward-api-2482c395-35b8-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:07:43.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9vzpg" for this suite.
Feb 21 09:07:49.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:07:49.616: INFO: namespace: e2e-tests-downward-api-9vzpg, resource: bindings, ignored listing per whitelist
Feb 21 09:07:49.789: INFO: namespace e2e-tests-downward-api-9vzpg deletion completed in 6.291717382s

• [SLOW TEST:8.933 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:07:49.789: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-c2rcj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-52fm
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 09:07:50.144: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-52fm" in namespace "e2e-tests-subpath-c2rcj" to be "success or failure"
Feb 21 09:07:50.151: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.636528ms
Feb 21 09:07:52.160: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015981385s
Feb 21 09:07:54.169: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Running", Reason="", readiness=false. Elapsed: 4.02497921s
Feb 21 09:07:56.177: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Running", Reason="", readiness=false. Elapsed: 6.033693865s
Feb 21 09:07:58.184: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Running", Reason="", readiness=false. Elapsed: 8.040547083s
Feb 21 09:08:00.191: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Running", Reason="", readiness=false. Elapsed: 10.047221797s
Feb 21 09:08:02.207: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Running", Reason="", readiness=false. Elapsed: 12.063492924s
Feb 21 09:08:04.214: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Running", Reason="", readiness=false. Elapsed: 14.070734569s
Feb 21 09:08:06.221: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Running", Reason="", readiness=false. Elapsed: 16.07786883s
Feb 21 09:08:08.231: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Running", Reason="", readiness=false. Elapsed: 18.087675872s
Feb 21 09:08:10.238: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Running", Reason="", readiness=false. Elapsed: 20.094241611s
Feb 21 09:08:12.245: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Running", Reason="", readiness=false. Elapsed: 22.101890221s
Feb 21 09:08:14.255: INFO: Pod "pod-subpath-test-configmap-52fm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.111406115s
STEP: Saw pod success
Feb 21 09:08:14.255: INFO: Pod "pod-subpath-test-configmap-52fm" satisfied condition "success or failure"
Feb 21 09:08:14.261: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-subpath-test-configmap-52fm container test-container-subpath-configmap-52fm: <nil>
STEP: delete the pod
Feb 21 09:08:14.288: INFO: Waiting for pod pod-subpath-test-configmap-52fm to disappear
Feb 21 09:08:14.293: INFO: Pod pod-subpath-test-configmap-52fm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-52fm
Feb 21 09:08:14.293: INFO: Deleting pod "pod-subpath-test-configmap-52fm" in namespace "e2e-tests-subpath-c2rcj"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:08:14.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-c2rcj" for this suite.
Feb 21 09:08:20.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:08:20.485: INFO: namespace: e2e-tests-subpath-c2rcj, resource: bindings, ignored listing per whitelist
Feb 21 09:08:20.544: INFO: namespace e2e-tests-subpath-c2rcj deletion completed in 6.238701977s

• [SLOW TEST:30.755 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:08:20.544: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-sl2nn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 09:08:20.923: INFO: Creating deployment "test-recreate-deployment"
Feb 21 09:08:20.941: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 21 09:08:20.957: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 21 09:08:22.965: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 21 09:08:22.970: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 21 09:08:22.978: INFO: Updating deployment test-recreate-deployment
Feb 21 09:08:22.978: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 21 09:08:23.040: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-sl2nn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sl2nn/deployments/test-recreate-deployment,UID:3c0fead6-35b8-11e9-890e-3aac84b8a476,ResourceVersion:10638,Generation:2,CreationTimestamp:2019-02-21 09:08:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-21 09:08:23 +0000 UTC 2019-02-21 09:08:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-21 09:08:23 +0000 UTC 2019-02-21 09:08:20 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-7cf749666b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 21 09:08:23.058: INFO: New ReplicaSet "test-recreate-deployment-7cf749666b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b,GenerateName:,Namespace:e2e-tests-deployment-sl2nn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sl2nn/replicasets/test-recreate-deployment-7cf749666b,UID:3d4afea0-35b8-11e9-890e-3aac84b8a476,ResourceVersion:10637,Generation:1,CreationTimestamp:2019-02-21 09:08:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 3c0fead6-35b8-11e9-890e-3aac84b8a476 0xc0027e5af7 0xc0027e5af8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 09:08:23.058: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 21 09:08:23.058: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-79f694ff59,GenerateName:,Namespace:e2e-tests-deployment-sl2nn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sl2nn/replicasets/test-recreate-deployment-79f694ff59,UID:3c11700e-35b8-11e9-890e-3aac84b8a476,ResourceVersion:10630,Generation:2,CreationTimestamp:2019-02-21 09:08:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 3c0fead6-35b8-11e9-890e-3aac84b8a476 0xc0027e5a37 0xc0027e5a38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 09:08:23.063: INFO: Pod "test-recreate-deployment-7cf749666b-ckdr5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b-ckdr5,GenerateName:test-recreate-deployment-7cf749666b-,Namespace:e2e-tests-deployment-sl2nn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sl2nn/pods/test-recreate-deployment-7cf749666b-ckdr5,UID:3d4ba83d-35b8-11e9-890e-3aac84b8a476,ResourceVersion:10639,Generation:0,CreationTimestamp:2019-02-21 09:08:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-7cf749666b 3d4afea0-35b8-11e9-890e-3aac84b8a476 0xc001aa6347 0xc001aa6348}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-ggllp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ggllp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ggllp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001aa63b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001aa63d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:08:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:08:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:08:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:08:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:,StartTime:2019-02-21 09:08:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:08:23.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-sl2nn" for this suite.
Feb 21 09:08:29.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:08:29.240: INFO: namespace: e2e-tests-deployment-sl2nn, resource: bindings, ignored listing per whitelist
Feb 21 09:08:29.548: INFO: namespace e2e-tests-deployment-sl2nn deletion completed in 6.477971812s

• [SLOW TEST:9.004 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:08:29.548: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-fxm5s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-fxm5s
Feb 21 09:09:45.847: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-fxm5s
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 09:09:45.853: INFO: Initial restart count of pod liveness-http is 0
Feb 21 09:10:07.929: INFO: Restart count of pod e2e-tests-container-probe-fxm5s/liveness-http is now 1 (22.075724237s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:10:07.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fxm5s" for this suite.
Feb 21 09:10:13.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:10:14.055: INFO: namespace: e2e-tests-container-probe-fxm5s, resource: bindings, ignored listing per whitelist
Feb 21 09:10:14.213: INFO: namespace e2e-tests-container-probe-fxm5s deletion completed in 6.261148819s

• [SLOW TEST:104.665 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:10:14.213: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-tzr5q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 21 09:10:14.717: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:15.544: INFO: stderr: ""
Feb 21 09:10:15.544: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 09:10:15.544: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:15.664: INFO: stderr: ""
Feb 21 09:10:15.664: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:10:15.665: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:15.773: INFO: stderr: ""
Feb 21 09:10:15.773: INFO: stdout: ""
Feb 21 09:10:15.773: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:10:20.773: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:20.899: INFO: stderr: ""
Feb 21 09:10:20.900: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:10:20.900: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:22.025: INFO: stderr: ""
Feb 21 09:10:22.025: INFO: stdout: ""
Feb 21 09:10:22.025: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:10:27.025: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:27.152: INFO: stderr: ""
Feb 21 09:10:27.152: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:10:27.152: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:27.268: INFO: stderr: ""
Feb 21 09:10:27.268: INFO: stdout: ""
Feb 21 09:10:27.268: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:10:32.274: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:32.439: INFO: stderr: ""
Feb 21 09:10:32.439: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:10:32.439: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:32.593: INFO: stderr: ""
Feb 21 09:10:32.593: INFO: stdout: ""
Feb 21 09:10:32.593: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:10:37.594: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:37.703: INFO: stderr: ""
Feb 21 09:10:37.703: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:10:37.703: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:37.806: INFO: stderr: ""
Feb 21 09:10:37.806: INFO: stdout: ""
Feb 21 09:10:37.806: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:10:42.807: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:42.968: INFO: stderr: ""
Feb 21 09:10:42.968: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:10:42.968: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:43.073: INFO: stderr: ""
Feb 21 09:10:43.073: INFO: stdout: ""
Feb 21 09:10:43.073: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:10:48.073: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:48.253: INFO: stderr: ""
Feb 21 09:10:48.253: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:10:48.253: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:48.355: INFO: stderr: ""
Feb 21 09:10:48.355: INFO: stdout: ""
Feb 21 09:10:48.355: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:10:53.357: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:53.478: INFO: stderr: ""
Feb 21 09:10:53.478: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:10:53.478: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:53.596: INFO: stderr: ""
Feb 21 09:10:53.596: INFO: stdout: ""
Feb 21 09:10:53.596: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:10:58.596: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:58.757: INFO: stderr: ""
Feb 21 09:10:58.757: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:10:58.757: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:10:58.990: INFO: stderr: ""
Feb 21 09:10:58.990: INFO: stdout: ""
Feb 21 09:10:58.990: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:11:03.990: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:04.106: INFO: stderr: ""
Feb 21 09:11:04.106: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:11:04.106: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:04.221: INFO: stderr: ""
Feb 21 09:11:04.221: INFO: stdout: ""
Feb 21 09:11:04.221: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:11:09.221: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:09.332: INFO: stderr: ""
Feb 21 09:11:09.332: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:11:09.332: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:09.447: INFO: stderr: ""
Feb 21 09:11:09.447: INFO: stdout: ""
Feb 21 09:11:09.447: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:11:14.448: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:14.585: INFO: stderr: ""
Feb 21 09:11:14.585: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:11:14.585: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:14.693: INFO: stderr: ""
Feb 21 09:11:14.693: INFO: stdout: ""
Feb 21 09:11:14.693: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:11:19.693: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:19.822: INFO: stderr: ""
Feb 21 09:11:19.822: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:11:19.822: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:19.949: INFO: stderr: ""
Feb 21 09:11:19.949: INFO: stdout: ""
Feb 21 09:11:19.949: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:11:24.949: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:25.104: INFO: stderr: ""
Feb 21 09:11:25.104: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:11:25.104: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:25.221: INFO: stderr: ""
Feb 21 09:11:25.221: INFO: stdout: ""
Feb 21 09:11:25.221: INFO: update-demo-nautilus-7zbkb is created but not running
Feb 21 09:11:30.222: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:30.397: INFO: stderr: ""
Feb 21 09:11:30.397: INFO: stdout: "update-demo-nautilus-7zbkb update-demo-nautilus-ggr8k "
Feb 21 09:11:30.397: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:30.546: INFO: stderr: ""
Feb 21 09:11:30.546: INFO: stdout: "true"
Feb 21 09:11:30.546: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-7zbkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:30.666: INFO: stderr: ""
Feb 21 09:11:30.666: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 09:11:30.666: INFO: validating pod update-demo-nautilus-7zbkb
Feb 21 09:11:30.772: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 09:11:30.772: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 09:11:30.772: INFO: update-demo-nautilus-7zbkb is verified up and running
Feb 21 09:11:30.772: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-ggr8k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:30.907: INFO: stderr: ""
Feb 21 09:11:30.907: INFO: stdout: "true"
Feb 21 09:11:30.907: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-ggr8k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:11:31.041: INFO: stderr: ""
Feb 21 09:11:31.041: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 09:11:31.041: INFO: validating pod update-demo-nautilus-ggr8k
Feb 21 09:11:31.129: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 09:11:31.129: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 09:11:31.129: INFO: update-demo-nautilus-ggr8k is verified up and running
STEP: rolling-update to new replication controller
Feb 21 09:11:31.138: INFO: scanned /root for discovery docs: <nil>
Feb 21 09:11:31.138: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:14:16.636: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 21 09:14:16.636: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 09:14:16.637: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:14:16.840: INFO: stderr: ""
Feb 21 09:14:16.840: INFO: stdout: "update-demo-kitten-2fbpj update-demo-kitten-hcmfw "
Feb 21 09:14:16.840: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-2fbpj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:14:16.948: INFO: stderr: ""
Feb 21 09:14:16.948: INFO: stdout: "true"
Feb 21 09:14:16.948: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-2fbpj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:14:17.060: INFO: stderr: ""
Feb 21 09:14:17.060: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 21 09:14:17.060: INFO: validating pod update-demo-kitten-2fbpj
Feb 21 09:14:17.168: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 21 09:14:17.168: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 21 09:14:17.168: INFO: update-demo-kitten-2fbpj is verified up and running
Feb 21 09:14:17.168: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-hcmfw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:14:17.352: INFO: stderr: ""
Feb 21 09:14:17.352: INFO: stdout: "true"
Feb 21 09:14:17.352: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-hcmfw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tzr5q'
Feb 21 09:14:17.454: INFO: stderr: ""
Feb 21 09:14:17.454: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 21 09:14:17.454: INFO: validating pod update-demo-kitten-hcmfw
Feb 21 09:14:17.539: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 21 09:14:17.539: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 21 09:14:17.539: INFO: update-demo-kitten-hcmfw is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:14:17.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tzr5q" for this suite.
Feb 21 09:14:39.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:14:39.961: INFO: namespace: e2e-tests-kubectl-tzr5q, resource: bindings, ignored listing per whitelist
Feb 21 09:14:39.970: INFO: namespace e2e-tests-kubectl-tzr5q deletion completed in 22.425253661s

• [SLOW TEST:265.756 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:14:39.970: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-w2vqn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 09:14:40.357: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 09:14:40.374: INFO: Number of nodes with available pods: 0
Feb 21 09:14:40.374: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:14:41.387: INFO: Number of nodes with available pods: 0
Feb 21 09:14:41.387: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:14:42.387: INFO: Number of nodes with available pods: 2
Feb 21 09:14:42.387: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 21 09:14:42.425: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:42.425: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:43.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:43.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:44.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:44.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:45.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:45.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:46.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:46.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:47.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:47.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:48.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:48.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:49.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:49.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:50.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:50.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:51.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:51.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:52.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:52.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:53.435: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:53.435: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:54.435: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:54.435: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:55.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:55.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:56.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:56.436: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:57.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:57.439: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:58.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:58.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:59.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:14:59.436: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:00.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:00.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:01.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:01.439: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:02.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:02.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:03.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:03.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:04.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:04.436: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:05.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:05.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:06.441: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:06.441: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:07.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:07.439: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:08.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:08.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:09.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:09.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:10.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:10.439: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:11.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:11.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:12.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:12.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:13.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:13.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:14.440: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:14.440: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:15.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:15.439: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:15.439: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:16.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:16.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:16.437: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:17.440: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:17.440: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:17.440: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:18.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:18.437: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:18.437: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:19.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:19.439: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:19.439: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:20.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:20.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:20.438: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:21.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:21.439: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:21.439: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:22.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:22.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:22.438: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:23.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:23.439: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:23.439: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:24.440: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:24.440: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:24.440: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:25.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:25.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:25.438: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:26.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:26.438: INFO: Wrong image for pod: daemon-set-prwqv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:26.438: INFO: Pod daemon-set-prwqv is not available
Feb 21 09:15:27.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:27.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:28.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:28.439: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:29.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:29.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:30.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:30.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:31.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:31.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:32.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:32.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:33.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:33.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:34.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:34.439: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:35.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:35.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:36.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:36.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:37.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:37.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:38.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:38.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:39.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:39.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:40.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:40.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:41.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:41.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:42.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:42.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:43.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:43.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:44.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:44.439: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:45.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:45.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:46.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:46.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:47.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:47.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:48.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:48.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:49.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:49.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:50.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:50.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:51.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:51.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:52.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:52.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:53.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:53.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:54.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:54.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:55.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:55.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:56.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:56.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:57.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:57.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:58.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:58.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:15:59.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:15:59.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:00.440: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:00.440: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:01.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:01.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:02.440: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:02.440: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:03.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:03.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:04.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:04.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:05.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:05.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:06.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:06.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:07.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:07.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:08.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:08.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:09.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:09.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:10.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:10.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:11.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:11.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:12.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:12.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:13.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:13.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:14.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:14.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:15.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:15.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:16.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:16.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:17.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:17.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:18.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:18.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:19.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:19.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:20.440: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:20.440: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:21.442: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:21.442: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:22.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:22.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:23.440: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:23.440: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:24.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:24.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:25.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:25.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:26.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:26.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:27.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:27.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:28.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:28.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:29.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:29.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:30.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:30.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:31.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:31.439: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:32.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:32.439: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:33.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:33.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:34.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:34.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:35.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:35.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:36.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:36.436: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:37.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:37.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:38.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:38.437: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:39.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:39.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:40.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:40.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:41.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:41.438: INFO: Pod daemon-set-gl8dw is not available
Feb 21 09:16:42.440: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:43.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:44.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:45.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:46.443: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:47.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:48.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:49.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:50.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:51.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:52.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:53.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:54.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:55.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:56.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:57.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:58.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:16:59.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:00.442: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:01.469: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:02.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:03.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:04.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:05.439: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:06.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:07.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:08.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:09.438: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:10.441: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:11.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:12.437: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:13.436: INFO: Wrong image for pod: daemon-set-4w4hj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 21 09:17:13.436: INFO: Pod daemon-set-4w4hj is not available
Feb 21 09:17:14.441: INFO: Pod daemon-set-qtp6c is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 21 09:17:14.464: INFO: Number of nodes with available pods: 1
Feb 21 09:17:14.464: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl is running more than one daemon pod
Feb 21 09:17:15.479: INFO: Number of nodes with available pods: 2
Feb 21 09:17:15.479: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-w2vqn, will wait for the garbage collector to delete the pods
Feb 21 09:17:15.573: INFO: Deleting {extensions DaemonSet} daemon-set took: 7.684694ms
Feb 21 09:17:15.673: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.317799ms
Feb 21 09:17:25.679: INFO: Number of nodes with available pods: 0
Feb 21 09:17:25.679: INFO: Number of running nodes: 0, number of available pods: 0
Feb 21 09:17:25.686: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-w2vqn/daemonsets","resourceVersion":"11874"},"items":null}

Feb 21 09:17:25.691: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-w2vqn/pods","resourceVersion":"11874"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:17:25.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-w2vqn" for this suite.
Feb 21 09:17:31.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:17:31.888: INFO: namespace: e2e-tests-daemonsets-w2vqn, resource: bindings, ignored listing per whitelist
Feb 21 09:17:31.937: INFO: namespace e2e-tests-daemonsets-w2vqn deletion completed in 6.219140344s

• [SLOW TEST:171.967 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:17:31.937: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-pxqds
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1475
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 09:17:32.420: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-pxqds'
Feb 21 09:17:32.607: INFO: stderr: ""
Feb 21 09:17:32.607: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
Feb 21 09:17:32.611: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-pxqds'
Feb 21 09:17:35.590: INFO: stderr: ""
Feb 21 09:17:35.590: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:17:35.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pxqds" for this suite.
Feb 21 09:17:41.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:17:41.865: INFO: namespace: e2e-tests-kubectl-pxqds, resource: bindings, ignored listing per whitelist
Feb 21 09:17:41.991: INFO: namespace e2e-tests-kubectl-pxqds deletion completed in 6.393886874s

• [SLOW TEST:10.053 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:17:41.991: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-lccrq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:18:42.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-lccrq" for this suite.
Feb 21 09:19:04.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:19:04.495: INFO: namespace: e2e-tests-container-probe-lccrq, resource: bindings, ignored listing per whitelist
Feb 21 09:19:04.593: INFO: namespace e2e-tests-container-probe-lccrq deletion completed in 22.216996924s

• [SLOW TEST:82.602 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:19:04.593: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-hvkfm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-bbe5c4b4-35b9-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 09:19:04.925: INFO: Waiting up to 5m0s for pod "pod-secrets-bbe669ab-35b9-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-secrets-hvkfm" to be "success or failure"
Feb 21 09:19:04.929: INFO: Pod "pod-secrets-bbe669ab-35b9-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815334ms
Feb 21 09:19:06.939: INFO: Pod "pod-secrets-bbe669ab-35b9-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014161428s
STEP: Saw pod success
Feb 21 09:19:06.939: INFO: Pod "pod-secrets-bbe669ab-35b9-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:19:06.943: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-secrets-bbe669ab-35b9-11e9-b7c5-a2b84e263bfe container secret-env-test: <nil>
STEP: delete the pod
Feb 21 09:19:06.975: INFO: Waiting for pod pod-secrets-bbe669ab-35b9-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:19:06.979: INFO: Pod pod-secrets-bbe669ab-35b9-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:19:06.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hvkfm" for this suite.
Feb 21 09:19:13.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:19:13.165: INFO: namespace: e2e-tests-secrets-hvkfm, resource: bindings, ignored listing per whitelist
Feb 21 09:19:13.210: INFO: namespace e2e-tests-secrets-hvkfm deletion completed in 6.224375077s

• [SLOW TEST:8.617 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:19:13.210: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-fnszd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 21 09:19:13.527: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-a,UID:c1080ed7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12156,Generation:0,CreationTimestamp:2019-02-21 09:19:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 21 09:19:13.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-a,UID:c1080ed7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12156,Generation:0,CreationTimestamp:2019-02-21 09:19:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 21 09:19:23.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-a,UID:c1080ed7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12176,Generation:0,CreationTimestamp:2019-02-21 09:19:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 21 09:19:23.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-a,UID:c1080ed7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12176,Generation:0,CreationTimestamp:2019-02-21 09:19:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 21 09:19:33.550: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-a,UID:c1080ed7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12196,Generation:0,CreationTimestamp:2019-02-21 09:19:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 21 09:19:33.551: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-a,UID:c1080ed7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12196,Generation:0,CreationTimestamp:2019-02-21 09:19:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 21 09:19:43.558: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-a,UID:c1080ed7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12218,Generation:0,CreationTimestamp:2019-02-21 09:19:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 21 09:19:43.558: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-a,UID:c1080ed7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12218,Generation:0,CreationTimestamp:2019-02-21 09:19:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 21 09:19:53.568: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-b,UID:d8e5cee7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12238,Generation:0,CreationTimestamp:2019-02-21 09:19:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 21 09:19:53.568: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-b,UID:d8e5cee7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12238,Generation:0,CreationTimestamp:2019-02-21 09:19:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 21 09:20:03.576: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-b,UID:d8e5cee7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12258,Generation:0,CreationTimestamp:2019-02-21 09:19:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 21 09:20:03.576: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-fnszd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fnszd/configmaps/e2e-watch-test-configmap-b,UID:d8e5cee7-35b9-11e9-890e-3aac84b8a476,ResourceVersion:12258,Generation:0,CreationTimestamp:2019-02-21 09:19:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:20:13.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-fnszd" for this suite.
Feb 21 09:20:19.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:20:19.846: INFO: namespace: e2e-tests-watch-fnszd, resource: bindings, ignored listing per whitelist
Feb 21 09:20:19.895: INFO: namespace e2e-tests-watch-fnszd deletion completed in 6.311307614s

• [SLOW TEST:66.685 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:20:19.896: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-m5q8m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-m5q8m
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-m5q8m
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-m5q8m
Feb 21 09:20:20.233: INFO: Found 0 stateful pods, waiting for 1
Feb 21 09:20:30.242: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 21 09:20:30.248: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 09:20:30.856: INFO: stderr: ""
Feb 21 09:20:30.856: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 09:20:30.856: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 09:20:30.862: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 21 09:20:40.869: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 09:20:40.869: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 09:20:40.892: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:20:40.892: INFO: ss-0  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:20 +0000 UTC  }]
Feb 21 09:20:40.892: INFO: 
Feb 21 09:20:40.892: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 21 09:20:41.901: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993502595s
Feb 21 09:20:42.908: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985223895s
Feb 21 09:20:43.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977857339s
Feb 21 09:20:44.941: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.953050459s
Feb 21 09:20:45.950: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.945085291s
Feb 21 09:20:46.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.936094842s
Feb 21 09:20:47.967: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.926156285s
Feb 21 09:20:48.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.919268028s
Feb 21 09:20:49.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 913.175819ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-m5q8m
Feb 21 09:20:50.986: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:20:51.525: INFO: stderr: ""
Feb 21 09:20:51.525: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 09:20:51.525: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 09:20:51.525: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:20:52.117: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 21 09:20:52.117: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 09:20:52.117: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 09:20:52.117: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:20:52.631: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 21 09:20:52.631: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 09:20:52.631: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 09:20:52.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:20:52.640: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:20:52.640: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 21 09:20:52.646: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 09:20:53.179: INFO: stderr: ""
Feb 21 09:20:53.179: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 09:20:53.179: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 09:20:53.179: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 09:20:53.689: INFO: stderr: ""
Feb 21 09:20:53.689: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 09:20:53.689: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 09:20:53.689: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 09:20:54.197: INFO: stderr: ""
Feb 21 09:20:54.197: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 09:20:54.197: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 09:20:54.197: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 09:20:54.204: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 21 09:21:04.213: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 09:21:04.213: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 09:21:04.213: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 09:21:04.225: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:21:04.225: INFO: ss-0  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:20 +0000 UTC  }]
Feb 21 09:21:04.225: INFO: ss-1  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:04.225: INFO: ss-2  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:04.225: INFO: 
Feb 21 09:21:04.225: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 09:21:05.231: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:21:05.232: INFO: ss-0  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:20 +0000 UTC  }]
Feb 21 09:21:05.232: INFO: ss-1  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:05.232: INFO: ss-2  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:05.232: INFO: 
Feb 21 09:21:05.232: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 09:21:06.239: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:21:06.239: INFO: ss-0  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:20 +0000 UTC  }]
Feb 21 09:21:06.239: INFO: ss-2  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:06.239: INFO: 
Feb 21 09:21:06.239: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 21 09:21:07.248: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:21:07.248: INFO: ss-0  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:20 +0000 UTC  }]
Feb 21 09:21:07.248: INFO: ss-2  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:07.248: INFO: 
Feb 21 09:21:07.248: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 21 09:21:08.254: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:21:08.254: INFO: ss-2  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:08.254: INFO: 
Feb 21 09:21:08.254: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 21 09:21:09.260: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:21:09.260: INFO: ss-2  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:09.260: INFO: 
Feb 21 09:21:09.260: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 21 09:21:10.269: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:21:10.269: INFO: ss-2  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:10.269: INFO: 
Feb 21 09:21:10.269: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 21 09:21:11.276: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:21:11.276: INFO: ss-2  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:11.276: INFO: 
Feb 21 09:21:11.276: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 21 09:21:12.282: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:21:12.282: INFO: ss-2  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:12.282: INFO: 
Feb 21 09:21:12.282: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 21 09:21:13.289: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 21 09:21:13.289: INFO: ss-2  shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:20:40 +0000 UTC  }]
Feb 21 09:21:13.289: INFO: 
Feb 21 09:21:13.289: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-m5q8m
Feb 21 09:21:14.297: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:21:14.546: INFO: rc: 1
Feb 21 09:21:14.546: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002452420 exit status 1 <nil> <nil> true [0xc000c65ea8 0xc000c65ec0 0xc000c65ed8] [0xc000c65ea8 0xc000c65ec0 0xc000c65ed8] [0xc000c65eb8 0xc000c65ed0] [0x932420 0x932420] 0xc0020457a0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 21 09:21:24.547: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:21:24.651: INFO: rc: 1
Feb 21 09:21:24.651: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020b5920 exit status 1 <nil> <nil> true [0xc001fc4558 0xc001fc4598 0xc001fc45e8] [0xc001fc4558 0xc001fc4598 0xc001fc45e8] [0xc001fc4580 0xc001fc45d0] [0x932420 0x932420] 0xc0015ba720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:21:34.652: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:21:34.749: INFO: rc: 1
Feb 21 09:21:34.749: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020b5bc0 exit status 1 <nil> <nil> true [0xc001fc45f0 0xc001fc4608 0xc001fc4640] [0xc001fc45f0 0xc001fc4608 0xc001fc4640] [0xc001fc4600 0xc001fc4628] [0x932420 0x932420] 0xc0015baa20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:21:44.749: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:21:44.874: INFO: rc: 1
Feb 21 09:21:44.874: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0024526c0 exit status 1 <nil> <nil> true [0xc000c65ee0 0xc000c65ef8 0xc000c65f20] [0xc000c65ee0 0xc000c65ef8 0xc000c65f20] [0xc000c65ef0 0xc000c65f10] [0x932420 0x932420] 0xc000f2a180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:21:54.874: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:21:54.976: INFO: rc: 1
Feb 21 09:21:54.976: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020b5e90 exit status 1 <nil> <nil> true [0xc001fc4660 0xc001fc46b0 0xc001fc4700] [0xc001fc4660 0xc001fc46b0 0xc001fc4700] [0xc001fc4698 0xc001fc46e8] [0x932420 0x932420] 0xc0015bad20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:22:04.976: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:22:05.085: INFO: rc: 1
Feb 21 09:22:05.085: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001adc3c0 exit status 1 <nil> <nil> true [0xc00000e0e8 0xc00000e650 0xc00000e970] [0xc00000e0e8 0xc00000e650 0xc00000e970] [0xc00000e448 0xc00000e8a0] [0x932420 0x932420] 0xc0020449c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:22:15.086: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:22:15.227: INFO: rc: 1
Feb 21 09:22:15.227: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001adc9f0 exit status 1 <nil> <nil> true [0xc00000e9c0 0xc00000ecc8 0xc00000ef10] [0xc00000e9c0 0xc00000ecc8 0xc00000ef10] [0xc00000ec18 0xc00000ee48] [0x932420 0x932420] 0xc002045440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:22:25.228: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:22:25.334: INFO: rc: 1
Feb 21 09:22:25.334: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b742d0 exit status 1 <nil> <nil> true [0xc001fc4010 0xc001fc4058 0xc001fc4080] [0xc001fc4010 0xc001fc4058 0xc001fc4080] [0xc001fc4040 0xc001fc4068] [0x932420 0x932420] 0xc0025ce0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:22:35.334: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:22:35.532: INFO: rc: 1
Feb 21 09:22:35.532: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001adcd50 exit status 1 <nil> <nil> true [0xc00000f060 0xc00000f270 0xc00000f5f0] [0xc00000f060 0xc00000f270 0xc00000f5f0] [0xc00000f210 0xc00000f4a0] [0x932420 0x932420] 0xc002045f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:22:45.532: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:22:45.649: INFO: rc: 1
Feb 21 09:22:45.649: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b74600 exit status 1 <nil> <nil> true [0xc001fc40a0 0xc001fc40d0 0xc001fc4130] [0xc001fc40a0 0xc001fc40d0 0xc001fc4130] [0xc001fc40c0 0xc001fc4110] [0x932420 0x932420] 0xc0025ce4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:22:55.649: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:22:55.755: INFO: rc: 1
Feb 21 09:22:55.755: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0010fc270 exit status 1 <nil> <nil> true [0xc00025c000 0xc00025c540 0xc00025c570] [0xc00025c000 0xc00025c540 0xc00025c570] [0xc00025c050 0xc00025c568] [0x932420 0x932420] 0xc0018c3ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:23:05.755: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:23:05.844: INFO: rc: 1
Feb 21 09:23:05.844: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0010fc540 exit status 1 <nil> <nil> true [0xc00025c580 0xc00025c780 0xc00025c7a8] [0xc00025c580 0xc00025c780 0xc00025c7a8] [0xc00025c778 0xc00025c7a0] [0x932420 0x932420] 0xc001899b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:23:15.847: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:23:15.946: INFO: rc: 1
Feb 21 09:23:15.946: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b748a0 exit status 1 <nil> <nil> true [0xc001fc4148 0xc001fc4160 0xc001fc4188] [0xc001fc4148 0xc001fc4160 0xc001fc4188] [0xc001fc4158 0xc001fc4170] [0x932420 0x932420] 0xc0025ce9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:23:25.946: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:23:26.125: INFO: rc: 1
Feb 21 09:23:26.125: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001adcff0 exit status 1 <nil> <nil> true [0xc00000f638 0xc00000f6b0 0xc00000f7b8] [0xc00000f638 0xc00000f6b0 0xc00000f7b8] [0xc00000f688 0xc00000f760] [0x932420 0x932420] 0xc001c48c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:23:36.126: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:23:36.249: INFO: rc: 1
Feb 21 09:23:36.249: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b74b40 exit status 1 <nil> <nil> true [0xc001fc41a0 0xc001fc41d8 0xc001fc4230] [0xc001fc41a0 0xc001fc41d8 0xc001fc4230] [0xc001fc41b8 0xc001fc4218] [0x932420 0x932420] 0xc0025cf0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:23:46.250: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:23:46.364: INFO: rc: 1
Feb 21 09:23:46.365: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b74e40 exit status 1 <nil> <nil> true [0xc001fc4238 0xc001fc4250 0xc001fc4288] [0xc001fc4238 0xc001fc4250 0xc001fc4288] [0xc001fc4248 0xc001fc4280] [0x932420 0x932420] 0xc0025cf8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:23:56.366: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:23:56.507: INFO: rc: 1
Feb 21 09:23:56.507: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b750e0 exit status 1 <nil> <nil> true [0xc001fc4290 0xc001fc42a8 0xc001fc42c0] [0xc001fc4290 0xc001fc42a8 0xc001fc42c0] [0xc001fc42a0 0xc001fc42b8] [0x932420 0x932420] 0xc0025cfe60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:24:06.507: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:24:06.609: INFO: rc: 1
Feb 21 09:24:06.609: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0010fc2a0 exit status 1 <nil> <nil> true [0xc00025c008 0xc00025c558 0xc00025c580] [0xc00025c008 0xc00025c558 0xc00025c580] [0xc00025c540 0xc00025c570] [0x932420 0x932420] 0xc0018998c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:24:16.610: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:24:16.713: INFO: rc: 1
Feb 21 09:24:16.713: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001adc3f0 exit status 1 <nil> <nil> true [0xc00000e098 0xc00000e448 0xc00000e8a0] [0xc00000e098 0xc00000e448 0xc00000e8a0] [0xc00000e120 0xc00000e6c8] [0x932420 0x932420] 0xc0018c3ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:24:26.713: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:24:26.853: INFO: rc: 1
Feb 21 09:24:26.853: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0010fc570 exit status 1 <nil> <nil> true [0xc00025c768 0xc00025c790 0xc00025c7c0] [0xc00025c768 0xc00025c790 0xc00025c7c0] [0xc00025c780 0xc00025c7a8] [0x932420 0x932420] 0xc002044960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:24:36.854: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:24:37.024: INFO: rc: 1
Feb 21 09:24:37.024: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b742a0 exit status 1 <nil> <nil> true [0xc001fc4010 0xc001fc4058 0xc001fc4080] [0xc001fc4010 0xc001fc4058 0xc001fc4080] [0xc001fc4040 0xc001fc4068] [0x932420 0x932420] 0xc001c48a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:24:47.025: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:24:47.121: INFO: rc: 1
Feb 21 09:24:47.121: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001adca20 exit status 1 <nil> <nil> true [0xc00000e970 0xc00000ec18 0xc00000ee48] [0xc00000e970 0xc00000ec18 0xc00000ee48] [0xc00000ea88 0xc00000edb8] [0x932420 0x932420] 0xc0025ce180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:24:57.121: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:24:57.270: INFO: rc: 1
Feb 21 09:24:57.270: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b74690 exit status 1 <nil> <nil> true [0xc001fc40a0 0xc001fc40d0 0xc001fc4130] [0xc001fc40a0 0xc001fc40d0 0xc001fc4130] [0xc001fc40c0 0xc001fc4110] [0x932420 0x932420] 0xc001c49620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:25:07.270: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:25:07.368: INFO: rc: 1
Feb 21 09:25:07.368: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016702a0 exit status 1 <nil> <nil> true [0xc000f2e000 0xc000f2e018 0xc000f2e038] [0xc000f2e000 0xc000f2e018 0xc000f2e038] [0xc000f2e010 0xc000f2e028] [0x932420 0x932420] 0xc0012f23c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:25:17.369: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:25:17.469: INFO: rc: 1
Feb 21 09:25:17.469: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001adccf0 exit status 1 <nil> <nil> true [0xc00000ef10 0xc00000f210 0xc00000f4a0] [0xc00000ef10 0xc00000f210 0xc00000f4a0] [0xc00000f098 0xc00000f3a8] [0x932420 0x932420] 0xc0025ce5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:25:27.470: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:25:27.573: INFO: rc: 1
Feb 21 09:25:27.573: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b74930 exit status 1 <nil> <nil> true [0xc001fc4148 0xc001fc4160 0xc001fc4188] [0xc001fc4148 0xc001fc4160 0xc001fc4188] [0xc001fc4158 0xc001fc4170] [0x932420 0x932420] 0xc001002a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:25:37.573: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:25:37.705: INFO: rc: 1
Feb 21 09:25:37.706: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001adcfc0 exit status 1 <nil> <nil> true [0xc00000f5f0 0xc00000f688 0xc00000f760] [0xc00000f5f0 0xc00000f688 0xc00000f760] [0xc00000f668 0xc00000f6f0] [0x932420 0x932420] 0xc0025cea20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:25:47.707: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:25:47.813: INFO: rc: 1
Feb 21 09:25:47.813: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001670540 exit status 1 <nil> <nil> true [0xc000f2e040 0xc000f2e058 0xc000f2e070] [0xc000f2e040 0xc000f2e058 0xc000f2e070] [0xc000f2e050 0xc000f2e068] [0x932420 0x932420] 0xc0010ae3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:25:57.814: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:25:57.977: INFO: rc: 1
Feb 21 09:25:57.978: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016709c0 exit status 1 <nil> <nil> true [0xc000f2e078 0xc000f2e090 0xc000f2e0a8] [0xc000f2e078 0xc000f2e090 0xc000f2e0a8] [0xc000f2e088 0xc000f2e0a0] [0x932420 0x932420] 0xc0012b9740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:26:07.979: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:26:08.090: INFO: rc: 1
Feb 21 09:26:08.090: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016702d0 exit status 1 <nil> <nil> true [0xc001fc4028 0xc001fc4060 0xc001fc40a0] [0xc001fc4028 0xc001fc4060 0xc001fc40a0] [0xc001fc4058 0xc001fc4080] [0x932420 0x932420] 0xc001c482a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 09:26:18.091: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-m5q8m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:26:18.199: INFO: rc: 1
Feb 21 09:26:18.199: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 21 09:26:18.199: INFO: Scaling statefulset ss to 0
Feb 21 09:26:18.232: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 21 09:26:18.235: INFO: Deleting all statefulset in ns e2e-tests-statefulset-m5q8m
Feb 21 09:26:18.239: INFO: Scaling statefulset ss to 0
Feb 21 09:26:18.252: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 09:26:18.256: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:26:18.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-m5q8m" for this suite.
Feb 21 09:26:24.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:26:24.304: INFO: namespace: e2e-tests-statefulset-m5q8m, resource: bindings, ignored listing per whitelist
Feb 21 09:26:24.451: INFO: namespace e2e-tests-statefulset-m5q8m deletion completed in 6.176694101s

• [SLOW TEST:364.555 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:26:24.451: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xppjh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 21 09:26:24.723: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml --namespace=e2e-tests-kubectl-xppjh run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 21 09:26:27.991: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 21 09:26:27.991: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:26:30.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xppjh" for this suite.
Feb 21 09:26:36.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:26:36.256: INFO: namespace: e2e-tests-kubectl-xppjh, resource: bindings, ignored listing per whitelist
Feb 21 09:26:36.394: INFO: namespace e2e-tests-kubectl-xppjh deletion completed in 6.173327527s

• [SLOW TEST:11.943 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:26:36.395: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-5jdzw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-5jdzw
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 21 09:26:36.725: INFO: Found 0 stateful pods, waiting for 3
Feb 21 09:26:46.734: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:26:46.734: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:26:46.734: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 21 09:26:46.771: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 21 09:26:56.816: INFO: Updating stateful set ss2
Feb 21 09:26:56.832: INFO: Waiting for Pod e2e-tests-statefulset-5jdzw/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 21 09:27:06.870: INFO: Found 1 stateful pods, waiting for 3
Feb 21 09:27:16.876: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:27:16.876: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:27:16.876: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 21 09:27:26.879: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:27:26.879: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:27:26.879: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 21 09:27:36.877: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:27:36.877: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:27:36.877: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 21 09:27:46.876: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:27:46.877: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:27:46.877: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 21 09:27:56.877: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:27:56.877: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:27:56.877: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 21 09:28:06.877: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:28:06.877: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:28:06.877: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 21 09:28:16.876: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:28:16.876: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:28:16.876: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 21 09:28:26.877: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:28:26.877: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:28:26.877: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 21 09:28:36.877: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:28:36.877: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:28:36.877: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 21 09:28:36.910: INFO: Updating stateful set ss2
Feb 21 09:28:36.919: INFO: Waiting for Pod e2e-tests-statefulset-5jdzw/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 21 09:28:46.941: INFO: Waiting for Pod e2e-tests-statefulset-5jdzw/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 21 09:28:56.950: INFO: Updating stateful set ss2
Feb 21 09:28:56.960: INFO: Waiting for StatefulSet e2e-tests-statefulset-5jdzw/ss2 to complete update
Feb 21 09:28:56.960: INFO: Waiting for Pod e2e-tests-statefulset-5jdzw/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 21 09:29:06.979: INFO: Waiting for StatefulSet e2e-tests-statefulset-5jdzw/ss2 to complete update
Feb 21 09:29:06.980: INFO: Waiting for Pod e2e-tests-statefulset-5jdzw/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 21 09:29:16.977: INFO: Waiting for StatefulSet e2e-tests-statefulset-5jdzw/ss2 to complete update
Feb 21 09:29:16.977: INFO: Waiting for Pod e2e-tests-statefulset-5jdzw/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 21 09:29:26.984: INFO: Waiting for StatefulSet e2e-tests-statefulset-5jdzw/ss2 to complete update
Feb 21 09:29:26.984: INFO: Waiting for Pod e2e-tests-statefulset-5jdzw/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 21 09:29:36.972: INFO: Waiting for StatefulSet e2e-tests-statefulset-5jdzw/ss2 to complete update
Feb 21 09:29:36.972: INFO: Waiting for Pod e2e-tests-statefulset-5jdzw/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 21 09:29:46.975: INFO: Deleting all statefulset in ns e2e-tests-statefulset-5jdzw
Feb 21 09:29:46.980: INFO: Scaling statefulset ss2 to 0
Feb 21 09:30:07.006: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 09:30:07.011: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:30:07.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-5jdzw" for this suite.
Feb 21 09:30:13.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:30:13.719: INFO: namespace: e2e-tests-statefulset-5jdzw, resource: bindings, ignored listing per whitelist
Feb 21 09:30:13.936: INFO: namespace e2e-tests-statefulset-5jdzw deletion completed in 6.701331258s

• [SLOW TEST:217.541 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:30:13.937: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-475nv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0221 09:30:15.581821   32452 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 21 09:30:15.581: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:30:15.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-475nv" for this suite.
Feb 21 09:30:21.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:30:22.071: INFO: namespace: e2e-tests-gc-475nv, resource: bindings, ignored listing per whitelist
Feb 21 09:30:22.094: INFO: namespace e2e-tests-gc-475nv deletion completed in 6.505749867s

• [SLOW TEST:8.158 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:30:22.095: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-wf6fv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 09:30:22.621: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4fd6473a-35bb-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-wf6fv" to be "success or failure"
Feb 21 09:30:22.626: INFO: Pod "downwardapi-volume-4fd6473a-35bb-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741606ms
Feb 21 09:30:24.632: INFO: Pod "downwardapi-volume-4fd6473a-35bb-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011137787s
STEP: Saw pod success
Feb 21 09:30:24.632: INFO: Pod "downwardapi-volume-4fd6473a-35bb-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:30:24.637: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-4fd6473a-35bb-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 09:30:24.663: INFO: Waiting for pod downwardapi-volume-4fd6473a-35bb-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:30:24.668: INFO: Pod downwardapi-volume-4fd6473a-35bb-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:30:24.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wf6fv" for this suite.
Feb 21 09:30:30.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:30:30.824: INFO: namespace: e2e-tests-downward-api-wf6fv, resource: bindings, ignored listing per whitelist
Feb 21 09:30:30.905: INFO: namespace e2e-tests-downward-api-wf6fv deletion completed in 6.230209272s

• [SLOW TEST:8.810 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:30:30.905: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6ws4v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 09:30:31.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54f7df25-35bb-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-6ws4v" to be "success or failure"
Feb 21 09:30:31.240: INFO: Pod "downwardapi-volume-54f7df25-35bb-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 9.824617ms
Feb 21 09:30:33.247: INFO: Pod "downwardapi-volume-54f7df25-35bb-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016821413s
STEP: Saw pod success
Feb 21 09:30:33.247: INFO: Pod "downwardapi-volume-54f7df25-35bb-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:30:33.253: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-54f7df25-35bb-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 09:30:33.276: INFO: Waiting for pod downwardapi-volume-54f7df25-35bb-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:30:33.283: INFO: Pod downwardapi-volume-54f7df25-35bb-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:30:33.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6ws4v" for this suite.
Feb 21 09:30:39.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:30:39.498: INFO: namespace: e2e-tests-projected-6ws4v, resource: bindings, ignored listing per whitelist
Feb 21 09:30:39.567: INFO: namespace e2e-tests-projected-6ws4v deletion completed in 6.276612482s

• [SLOW TEST:8.662 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:30:39.568: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-rqsvl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 21 09:30:39.915: INFO: PodSpec: initContainers in spec.initContainers
Feb 21 09:31:26.572: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-5a26bc91-35bb-11e9-b7c5-a2b84e263bfe", GenerateName:"", Namespace:"e2e-tests-init-container-rqsvl", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-rqsvl/pods/pod-init-5a26bc91-35bb-11e9-b7c5-a2b84e263bfe", UID:"5a2808a8-35bb-11e9-890e-3aac84b8a476", ResourceVersion:"13935", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686338239, loc:(*time.Location)(0x78fbda0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"915541040"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.99/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-g7rmc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001acfac0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g7rmc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g7rmc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-g7rmc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001e98408), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0027aa3c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001e98480)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001e984a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001e984a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338239, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338239, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338239, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338239, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.19", PodIP:"100.96.1.99", StartTime:(*v1.Time)(0xc0025f9980), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000745f80)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000ca2070)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://608d237434798b5163c32e5482e726b56b3fe283c842f5315c675cadf0c764d5"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0025f99c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0025f99a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:31:26.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-rqsvl" for this suite.
Feb 21 09:31:48.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:31:48.773: INFO: namespace: e2e-tests-init-container-rqsvl, resource: bindings, ignored listing per whitelist
Feb 21 09:31:48.803: INFO: namespace e2e-tests-init-container-rqsvl deletion completed in 22.215901327s

• [SLOW TEST:69.235 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:31:48.803: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-dvjl2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 09:31:49.127: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 21 09:31:54.134: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 21 09:31:54.134: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 21 09:31:56.140: INFO: Creating deployment "test-rollover-deployment"
Feb 21 09:31:56.149: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 21 09:31:58.158: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 21 09:31:58.166: INFO: Ensure that both replica sets have 1 created replica
Feb 21 09:31:58.174: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 21 09:31:58.186: INFO: Updating deployment test-rollover-deployment
Feb 21 09:31:58.186: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 21 09:32:00.197: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 21 09:32:00.206: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 21 09:32:00.215: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 09:32:00.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338319, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 09:32:02.227: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 09:32:02.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338319, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 09:32:04.245: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 09:32:04.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338319, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 09:32:06.228: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 09:32:06.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338319, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 09:32:08.229: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 09:32:08.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338319, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338316, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 09:32:10.231: INFO: 
Feb 21 09:32:10.231: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 21 09:32:10.250: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-dvjl2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dvjl2/deployments/test-rollover-deployment,UID:8796a044-35bb-11e9-890e-3aac84b8a476,ResourceVersion:14087,Generation:2,CreationTimestamp:2019-02-21 09:31:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-21 09:31:56 +0000 UTC 2019-02-21 09:31:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-21 09:32:09 +0000 UTC 2019-02-21 09:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5b76ff8c4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 21 09:32:10.257: INFO: New ReplicaSet "test-rollover-deployment-5b76ff8c4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4,GenerateName:,Namespace:e2e-tests-deployment-dvjl2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dvjl2/replicasets/test-rollover-deployment-5b76ff8c4,UID:88ceacf6-35bb-11e9-890e-3aac84b8a476,ResourceVersion:14080,Generation:2,CreationTimestamp:2019-02-21 09:31:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 8796a044-35bb-11e9-890e-3aac84b8a476 0xc002039710 0xc002039711}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 21 09:32:10.258: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 21 09:32:10.258: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-dvjl2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dvjl2/replicasets/test-rollover-controller,UID:8366a675-35bb-11e9-890e-3aac84b8a476,ResourceVersion:14086,Generation:2,CreationTimestamp:2019-02-21 09:31:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 8796a044-35bb-11e9-890e-3aac84b8a476 0xc002039657 0xc002039658}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 09:32:10.258: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6975f4fb87,GenerateName:,Namespace:e2e-tests-deployment-dvjl2,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dvjl2/replicasets/test-rollover-deployment-6975f4fb87,UID:8798d40d-35bb-11e9-890e-3aac84b8a476,ResourceVersion:14045,Generation:2,CreationTimestamp:2019-02-21 09:31:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 8796a044-35bb-11e9-890e-3aac84b8a476 0xc0020397d7 0xc0020397d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 09:32:10.267: INFO: Pod "test-rollover-deployment-5b76ff8c4-rjxs5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4-rjxs5,GenerateName:test-rollover-deployment-5b76ff8c4-,Namespace:e2e-tests-deployment-dvjl2,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dvjl2/pods/test-rollover-deployment-5b76ff8c4-rjxs5,UID:88d286d6-35bb-11e9-890e-3aac84b8a476,ResourceVersion:14058,Generation:0,CreationTimestamp:2019-02-21 09:31:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.102/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5b76ff8c4 88ceacf6-35bb-11e9-890e-3aac84b8a476 0xc0026a4310 0xc0026a4311}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-5ctrc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5ctrc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-5ctrc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026a4370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026a4390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:31:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:31:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:31:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 09:31:58 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:100.96.1.102,StartTime:2019-02-21 09:31:58 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-21 09:31:59 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://df2b3ecbe137595c87c742d02605e5cf095f0892576288ef23f53077a8dcf85d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:32:10.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-dvjl2" for this suite.
Feb 21 09:32:16.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:32:16.445: INFO: namespace: e2e-tests-deployment-dvjl2, resource: bindings, ignored listing per whitelist
Feb 21 09:32:16.480: INFO: namespace e2e-tests-deployment-dvjl2 deletion completed in 6.204181164s

• [SLOW TEST:27.677 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:32:16.480: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-ps65f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-ps65f A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-ps65f;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-ps65f A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-ps65f;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-ps65f.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-ps65f.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-ps65f.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-ps65f.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-ps65f.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-ps65f.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-ps65f.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-ps65f.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-ps65f.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 210.145.70.100.in-addr.arpa. PTR)" && echo OK > /results/100.70.145.210_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 210.145.70.100.in-addr.arpa. PTR)" && echo OK > /results/100.70.145.210_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-ps65f A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-ps65f;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-ps65f A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-ps65f;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-ps65f.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-ps65f.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-ps65f.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-ps65f.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-ps65f.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-ps65f.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-ps65f.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-ps65f.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-ps65f.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 210.145.70.100.in-addr.arpa. PTR)" && echo OK > /results/100.70.145.210_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 210.145.70.100.in-addr.arpa. PTR)" && echo OK > /results/100.70.145.210_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 09:35:33.085: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.093: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-ps65f from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.100: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-ps65f from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.108: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-ps65f.svc from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.115: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-ps65f.svc from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.205: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.703: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.711: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.718: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-ps65f from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.727: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-ps65f from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.734: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-ps65f.svc from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.740: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-ps65f.svc from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.747: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.754: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.761: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-ps65f.svc from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.770: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-ps65f.svc from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.777: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.783: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe)
Feb 21 09:35:33.906: INFO: Lookups using e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe failed for: [wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-ps65f wheezy_tcp@dns-test-service.e2e-tests-dns-ps65f wheezy_udp@dns-test-service.e2e-tests-dns-ps65f.svc wheezy_tcp@dns-test-service.e2e-tests-dns-ps65f.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-ps65f jessie_tcp@dns-test-service.e2e-tests-dns-ps65f jessie_udp@dns-test-service.e2e-tests-dns-ps65f.svc jessie_tcp@dns-test-service.e2e-tests-dns-ps65f.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-ps65f.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-ps65f.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-ps65f.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 21 09:35:45.155: INFO: DNS probes using e2e-tests-dns-ps65f/dns-test-93ede475-35bb-11e9-b7c5-a2b84e263bfe succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:35:45.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-ps65f" for this suite.
Feb 21 09:35:51.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:35:51.263: INFO: namespace: e2e-tests-dns-ps65f, resource: bindings, ignored listing per whitelist
Feb 21 09:35:51.390: INFO: namespace e2e-tests-dns-ps65f deletion completed in 6.179805229s

• [SLOW TEST:214.910 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:35:51.391: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-mhc8q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 21 09:35:51.716: INFO: Waiting up to 5m0s for pod "pod-13fe5f49-35bc-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-mhc8q" to be "success or failure"
Feb 21 09:35:51.720: INFO: Pod "pod-13fe5f49-35bc-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048257ms
Feb 21 09:35:53.726: INFO: Pod "pod-13fe5f49-35bc-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009743132s
STEP: Saw pod success
Feb 21 09:35:53.726: INFO: Pod "pod-13fe5f49-35bc-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:35:53.730: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-13fe5f49-35bc-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 09:35:53.754: INFO: Waiting for pod pod-13fe5f49-35bc-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:35:53.758: INFO: Pod pod-13fe5f49-35bc-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:35:53.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mhc8q" for this suite.
Feb 21 09:35:59.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:35:59.879: INFO: namespace: e2e-tests-emptydir-mhc8q, resource: bindings, ignored listing per whitelist
Feb 21 09:35:59.939: INFO: namespace e2e-tests-emptydir-mhc8q deletion completed in 6.176306699s

• [SLOW TEST:8.549 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:35:59.940: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-zgtlp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 21 09:36:04.308: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 09:36:04.313: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 09:36:06.314: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 09:36:06.320: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 09:36:08.314: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 09:36:08.320: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:36:08.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-zgtlp" for this suite.
Feb 21 09:36:30.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:36:30.443: INFO: namespace: e2e-tests-container-lifecycle-hook-zgtlp, resource: bindings, ignored listing per whitelist
Feb 21 09:36:30.570: INFO: namespace e2e-tests-container-lifecycle-hook-zgtlp deletion completed in 22.229735492s

• [SLOW TEST:30.631 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:36:30.571: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-w66tm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-2b5c880d-35bc-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 09:36:30.925: INFO: Waiting up to 5m0s for pod "pod-configmaps-2b5d3738-35bc-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-configmap-w66tm" to be "success or failure"
Feb 21 09:36:30.929: INFO: Pod "pod-configmaps-2b5d3738-35bc-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.752631ms
Feb 21 09:36:32.935: INFO: Pod "pod-configmaps-2b5d3738-35bc-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010518754s
STEP: Saw pod success
Feb 21 09:36:32.935: INFO: Pod "pod-configmaps-2b5d3738-35bc-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:36:32.940: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-configmaps-2b5d3738-35bc-11e9-b7c5-a2b84e263bfe container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 09:36:32.963: INFO: Waiting for pod pod-configmaps-2b5d3738-35bc-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:36:32.967: INFO: Pod pod-configmaps-2b5d3738-35bc-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:36:32.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-w66tm" for this suite.
Feb 21 09:36:38.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:36:39.162: INFO: namespace: e2e-tests-configmap-w66tm, resource: bindings, ignored listing per whitelist
Feb 21 09:36:39.217: INFO: namespace e2e-tests-configmap-w66tm deletion completed in 6.242221029s

• [SLOW TEST:8.647 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:36:39.218: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-5zplh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 21 09:36:39.519: INFO: Waiting up to 5m0s for pod "client-containers-307ccd7f-35bc-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-containers-5zplh" to be "success or failure"
Feb 21 09:36:39.522: INFO: Pod "client-containers-307ccd7f-35bc-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513121ms
Feb 21 09:36:41.533: INFO: Pod "client-containers-307ccd7f-35bc-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013792652s
STEP: Saw pod success
Feb 21 09:36:41.533: INFO: Pod "client-containers-307ccd7f-35bc-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:36:41.537: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod client-containers-307ccd7f-35bc-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 09:36:41.559: INFO: Waiting for pod client-containers-307ccd7f-35bc-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:36:41.562: INFO: Pod client-containers-307ccd7f-35bc-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:36:41.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-5zplh" for this suite.
Feb 21 09:36:47.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:36:47.597: INFO: namespace: e2e-tests-containers-5zplh, resource: bindings, ignored listing per whitelist
Feb 21 09:36:47.792: INFO: namespace e2e-tests-containers-5zplh deletion completed in 6.22356293s

• [SLOW TEST:8.574 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:36:47.792: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-6585n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 21 09:36:48.321: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:36:51.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-6585n" for this suite.
Feb 21 09:36:57.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:36:57.467: INFO: namespace: e2e-tests-init-container-6585n, resource: bindings, ignored listing per whitelist
Feb 21 09:36:57.669: INFO: namespace e2e-tests-init-container-6585n deletion completed in 6.251283684s

• [SLOW TEST:9.876 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:36:57.669: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-r7wtq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 21 09:36:58.018: INFO: Waiting up to 5m0s for pod "pod-3b830ac4-35bc-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-r7wtq" to be "success or failure"
Feb 21 09:36:58.025: INFO: Pod "pod-3b830ac4-35bc-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.163168ms
Feb 21 09:37:00.031: INFO: Pod "pod-3b830ac4-35bc-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013336694s
STEP: Saw pod success
Feb 21 09:37:00.032: INFO: Pod "pod-3b830ac4-35bc-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:37:00.038: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-3b830ac4-35bc-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 09:37:00.058: INFO: Waiting for pod pod-3b830ac4-35bc-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:37:00.062: INFO: Pod pod-3b830ac4-35bc-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:37:00.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-r7wtq" for this suite.
Feb 21 09:37:06.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:37:06.150: INFO: namespace: e2e-tests-emptydir-r7wtq, resource: bindings, ignored listing per whitelist
Feb 21 09:37:06.283: INFO: namespace e2e-tests-emptydir-r7wtq deletion completed in 6.21532817s

• [SLOW TEST:8.615 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:37:06.284: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hjj8r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-40c37f4c-35bc-11e9-b7c5-a2b84e263bfe
STEP: Creating configMap with name cm-test-opt-upd-40c37f87-35bc-11e9-b7c5-a2b84e263bfe
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-40c37f4c-35bc-11e9-b7c5-a2b84e263bfe
STEP: Updating configmap cm-test-opt-upd-40c37f87-35bc-11e9-b7c5-a2b84e263bfe
STEP: Creating configMap with name cm-test-opt-create-40c37f99-35bc-11e9-b7c5-a2b84e263bfe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:37:13.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hjj8r" for this suite.
Feb 21 09:37:35.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:37:35.581: INFO: namespace: e2e-tests-projected-hjj8r, resource: bindings, ignored listing per whitelist
Feb 21 09:37:35.640: INFO: namespace e2e-tests-projected-hjj8r deletion completed in 22.193244964s

• [SLOW TEST:29.357 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:37:35.640: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-q2v79
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-q2v79
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 09:37:35.924: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 21 09:37:58.056: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 100.96.1.112 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-q2v79 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 09:37:58.056: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 09:37:59.507: INFO: Found all expected endpoints: [netserver-0]
Feb 21 09:37:59.514: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 100.96.0.44 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-q2v79 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 09:37:59.514: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 09:38:00.962: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:38:00.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-q2v79" for this suite.
Feb 21 09:38:24.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:38:25.027: INFO: namespace: e2e-tests-pod-network-test-q2v79, resource: bindings, ignored listing per whitelist
Feb 21 09:38:25.207: INFO: namespace e2e-tests-pod-network-test-q2v79 deletion completed in 24.236605473s

• [SLOW TEST:49.566 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:38:25.207: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-sqvmq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 21 09:38:25.516: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml cluster-info'
Feb 21 09:38:26.044: INFO: stderr: ""
Feb 21 09:38:26.044: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:38:26.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sqvmq" for this suite.
Feb 21 09:38:32.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:38:32.424: INFO: namespace: e2e-tests-kubectl-sqvmq, resource: bindings, ignored listing per whitelist
Feb 21 09:38:32.952: INFO: namespace e2e-tests-kubectl-sqvmq deletion completed in 6.900936949s

• [SLOW TEST:7.745 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:38:32.952: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-745dq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0221 09:39:13.546743   32452 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 21 09:39:13.546: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:39:13.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-745dq" for this suite.
Feb 21 09:39:19.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:39:19.630: INFO: namespace: e2e-tests-gc-745dq, resource: bindings, ignored listing per whitelist
Feb 21 09:39:19.929: INFO: namespace e2e-tests-gc-745dq deletion completed in 6.376310043s

• [SLOW TEST:46.977 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:39:19.929: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-p62kv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 21 09:39:20.331: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-p62kv'
Feb 21 09:39:20.648: INFO: stderr: ""
Feb 21 09:39:20.648: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 09:39:20.648: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-p62kv'
Feb 21 09:39:20.835: INFO: stderr: ""
Feb 21 09:39:20.835: INFO: stdout: "update-demo-nautilus-5tfz4 update-demo-nautilus-nv58c "
Feb 21 09:39:20.835: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5tfz4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p62kv'
Feb 21 09:39:20.939: INFO: stderr: ""
Feb 21 09:39:20.939: INFO: stdout: ""
Feb 21 09:39:20.939: INFO: update-demo-nautilus-5tfz4 is created but not running
Feb 21 09:39:25.939: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-p62kv'
Feb 21 09:39:26.110: INFO: stderr: ""
Feb 21 09:39:26.110: INFO: stdout: "update-demo-nautilus-5tfz4 update-demo-nautilus-nv58c "
Feb 21 09:39:26.110: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5tfz4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p62kv'
Feb 21 09:39:26.212: INFO: stderr: ""
Feb 21 09:39:26.212: INFO: stdout: "true"
Feb 21 09:39:26.212: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5tfz4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p62kv'
Feb 21 09:39:26.336: INFO: stderr: ""
Feb 21 09:39:26.336: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 09:39:26.336: INFO: validating pod update-demo-nautilus-5tfz4
Feb 21 09:39:26.427: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 09:39:26.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 09:39:26.427: INFO: update-demo-nautilus-5tfz4 is verified up and running
Feb 21 09:39:26.428: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-nv58c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p62kv'
Feb 21 09:39:26.537: INFO: stderr: ""
Feb 21 09:39:26.537: INFO: stdout: "true"
Feb 21 09:39:26.537: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-nv58c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p62kv'
Feb 21 09:39:26.639: INFO: stderr: ""
Feb 21 09:39:26.639: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 09:39:26.639: INFO: validating pod update-demo-nautilus-nv58c
Feb 21 09:39:26.727: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 09:39:26.727: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 09:39:26.727: INFO: update-demo-nautilus-nv58c is verified up and running
STEP: using delete to clean up resources
Feb 21 09:39:26.727: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-p62kv'
Feb 21 09:39:26.842: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 09:39:26.842: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 21 09:39:26.842: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-p62kv'
Feb 21 09:39:26.970: INFO: stderr: "No resources found.\n"
Feb 21 09:39:26.970: INFO: stdout: ""
Feb 21 09:39:26.970: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-p62kv -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 09:39:27.098: INFO: stderr: ""
Feb 21 09:39:27.099: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:39:27.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-p62kv" for this suite.
Feb 21 09:39:49.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:39:49.171: INFO: namespace: e2e-tests-kubectl-p62kv, resource: bindings, ignored listing per whitelist
Feb 21 09:39:49.359: INFO: namespace e2e-tests-kubectl-p62kv deletion completed in 22.253190139s

• [SLOW TEST:29.430 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:39:49.359: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-rq724
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 21 09:39:49.746: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-rq724,SelfLink:/api/v1/namespaces/e2e-tests-watch-rq724/configmaps/e2e-watch-test-resource-version,UID:a1dc120d-35bc-11e9-890e-3aac84b8a476,ResourceVersion:15388,Generation:0,CreationTimestamp:2019-02-21 09:39:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 21 09:39:49.747: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-rq724,SelfLink:/api/v1/namespaces/e2e-tests-watch-rq724/configmaps/e2e-watch-test-resource-version,UID:a1dc120d-35bc-11e9-890e-3aac84b8a476,ResourceVersion:15389,Generation:0,CreationTimestamp:2019-02-21 09:39:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:39:49.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-rq724" for this suite.
Feb 21 09:39:55.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:39:55.890: INFO: namespace: e2e-tests-watch-rq724, resource: bindings, ignored listing per whitelist
Feb 21 09:39:56.005: INFO: namespace e2e-tests-watch-rq724 deletion completed in 6.25390065s

• [SLOW TEST:6.646 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:39:56.006: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-gl8hm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 09:39:56.414: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5d8543b-35bc-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-gl8hm" to be "success or failure"
Feb 21 09:39:56.419: INFO: Pod "downwardapi-volume-a5d8543b-35bc-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.763979ms
Feb 21 09:39:58.425: INFO: Pod "downwardapi-volume-a5d8543b-35bc-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010396378s
STEP: Saw pod success
Feb 21 09:39:58.425: INFO: Pod "downwardapi-volume-a5d8543b-35bc-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:39:58.429: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-a5d8543b-35bc-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 09:39:58.491: INFO: Waiting for pod downwardapi-volume-a5d8543b-35bc-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:39:58.495: INFO: Pod downwardapi-volume-a5d8543b-35bc-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:39:58.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gl8hm" for this suite.
Feb 21 09:40:04.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:40:04.743: INFO: namespace: e2e-tests-downward-api-gl8hm, resource: bindings, ignored listing per whitelist
Feb 21 09:40:04.775: INFO: namespace e2e-tests-downward-api-gl8hm deletion completed in 6.274020901s

• [SLOW TEST:8.769 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:40:04.775: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bws9f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-ab46b75e-35bc-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 09:40:05.530: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ab476b85-35bc-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-bws9f" to be "success or failure"
Feb 21 09:40:05.534: INFO: Pod "pod-projected-secrets-ab476b85-35bc-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.170317ms
Feb 21 09:40:07.544: INFO: Pod "pod-projected-secrets-ab476b85-35bc-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014063895s
STEP: Saw pod success
Feb 21 09:40:07.544: INFO: Pod "pod-projected-secrets-ab476b85-35bc-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:40:07.549: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-secrets-ab476b85-35bc-11e9-b7c5-a2b84e263bfe container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 09:40:07.571: INFO: Waiting for pod pod-projected-secrets-ab476b85-35bc-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:40:07.575: INFO: Pod pod-projected-secrets-ab476b85-35bc-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:40:07.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bws9f" for this suite.
Feb 21 09:40:13.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:40:13.706: INFO: namespace: e2e-tests-projected-bws9f, resource: bindings, ignored listing per whitelist
Feb 21 09:40:13.882: INFO: namespace e2e-tests-projected-bws9f deletion completed in 6.2981036s

• [SLOW TEST:9.107 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:40:13.882: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-zqz2g
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 09:40:14.234: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:40:15.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-zqz2g" for this suite.
Feb 21 09:40:21.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:40:21.467: INFO: namespace: e2e-tests-custom-resource-definition-zqz2g, resource: bindings, ignored listing per whitelist
Feb 21 09:40:21.603: INFO: namespace e2e-tests-custom-resource-definition-zqz2g deletion completed in 6.288874134s

• [SLOW TEST:7.721 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:40:21.603: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-6ns64
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 21 09:40:24.056: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-b51c187e-35bc-11e9-b7c5-a2b84e263bfe", GenerateName:"", Namespace:"e2e-tests-pods-6ns64", SelfLink:"/api/v1/namespaces/e2e-tests-pods-6ns64/pods/pod-submit-remove-b51c187e-35bc-11e9-b7c5-a2b84e263bfe", UID:"b51f0d91-35bc-11e9-890e-3aac84b8a476", ResourceVersion:"15531", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686338822, loc:(*time.Location)(0x78fbda0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"15198565"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.123/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xbxgg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001fba780), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xbxgg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001a252b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0023bc1e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a252f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a25310)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001a25318), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338822, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338823, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338823, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686338822, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.19", PodIP:"100.96.1.123", StartTime:(*v1.Time)(0xc001dbac20), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001dbac40), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://885cd441f62a1a3dacf408d9ec875f69297f25ea15374607dd5182df9fc7c6b4"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:40:35.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6ns64" for this suite.
Feb 21 09:40:41.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:40:41.703: INFO: namespace: e2e-tests-pods-6ns64, resource: bindings, ignored listing per whitelist
Feb 21 09:40:41.898: INFO: namespace e2e-tests-pods-6ns64 deletion completed in 6.302684641s

• [SLOW TEST:20.295 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:40:41.899: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-6jzv8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-6jzv8
Feb 21 09:40:44.740: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-6jzv8
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 09:40:44.745: INFO: Initial restart count of pod liveness-http is 0
Feb 21 09:40:58.804: INFO: Restart count of pod e2e-tests-container-probe-6jzv8/liveness-http is now 1 (14.059417654s elapsed)
Feb 21 09:41:18.878: INFO: Restart count of pod e2e-tests-container-probe-6jzv8/liveness-http is now 2 (34.132601697s elapsed)
Feb 21 09:41:38.958: INFO: Restart count of pod e2e-tests-container-probe-6jzv8/liveness-http is now 3 (54.212571316s elapsed)
Feb 21 09:41:59.033: INFO: Restart count of pod e2e-tests-container-probe-6jzv8/liveness-http is now 4 (1m14.287968907s elapsed)
Feb 21 09:43:11.330: INFO: Restart count of pod e2e-tests-container-probe-6jzv8/liveness-http is now 5 (2m26.584703619s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:43:11.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-6jzv8" for this suite.
Feb 21 09:43:17.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:43:18.167: INFO: namespace: e2e-tests-container-probe-6jzv8, resource: bindings, ignored listing per whitelist
Feb 21 09:43:18.230: INFO: namespace e2e-tests-container-probe-6jzv8 deletion completed in 6.879365618s

• [SLOW TEST:156.332 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:43:18.231: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-89vxv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-1e594580-35bd-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 09:43:18.591: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1e5a061b-35bd-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-89vxv" to be "success or failure"
Feb 21 09:43:18.595: INFO: Pod "pod-projected-configmaps-1e5a061b-35bd-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046534ms
Feb 21 09:43:20.606: INFO: Pod "pod-projected-configmaps-1e5a061b-35bd-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014953444s
STEP: Saw pod success
Feb 21 09:43:20.606: INFO: Pod "pod-projected-configmaps-1e5a061b-35bd-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:43:20.612: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-configmaps-1e5a061b-35bd-11e9-b7c5-a2b84e263bfe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 09:43:20.639: INFO: Waiting for pod pod-projected-configmaps-1e5a061b-35bd-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:43:20.645: INFO: Pod pod-projected-configmaps-1e5a061b-35bd-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:43:20.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-89vxv" for this suite.
Feb 21 09:43:26.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:43:26.879: INFO: namespace: e2e-tests-projected-89vxv, resource: bindings, ignored listing per whitelist
Feb 21 09:43:26.935: INFO: namespace e2e-tests-projected-89vxv deletion completed in 6.278108982s

• [SLOW TEST:8.704 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:43:26.935: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-lqskz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 09:43:27.354: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 21 09:43:27.363: INFO: Number of nodes with available pods: 0
Feb 21 09:43:27.363: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 21 09:43:27.393: INFO: Number of nodes with available pods: 0
Feb 21 09:43:27.393: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:28.401: INFO: Number of nodes with available pods: 0
Feb 21 09:43:28.401: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:29.400: INFO: Number of nodes with available pods: 1
Feb 21 09:43:29.400: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 21 09:43:29.425: INFO: Number of nodes with available pods: 1
Feb 21 09:43:29.425: INFO: Number of running nodes: 0, number of available pods: 1
Feb 21 09:43:30.431: INFO: Number of nodes with available pods: 0
Feb 21 09:43:30.431: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 21 09:43:30.441: INFO: Number of nodes with available pods: 0
Feb 21 09:43:30.442: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:31.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:31.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:32.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:32.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:33.449: INFO: Number of nodes with available pods: 0
Feb 21 09:43:33.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:34.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:34.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:35.449: INFO: Number of nodes with available pods: 0
Feb 21 09:43:35.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:36.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:36.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:37.449: INFO: Number of nodes with available pods: 0
Feb 21 09:43:37.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:38.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:38.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:39.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:39.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:40.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:40.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:41.449: INFO: Number of nodes with available pods: 0
Feb 21 09:43:41.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:42.451: INFO: Number of nodes with available pods: 0
Feb 21 09:43:42.451: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:43.449: INFO: Number of nodes with available pods: 0
Feb 21 09:43:43.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:44.450: INFO: Number of nodes with available pods: 0
Feb 21 09:43:44.450: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:45.450: INFO: Number of nodes with available pods: 0
Feb 21 09:43:45.450: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:46.449: INFO: Number of nodes with available pods: 0
Feb 21 09:43:46.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:47.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:47.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:48.451: INFO: Number of nodes with available pods: 0
Feb 21 09:43:48.451: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:49.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:49.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:50.449: INFO: Number of nodes with available pods: 0
Feb 21 09:43:50.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:51.452: INFO: Number of nodes with available pods: 0
Feb 21 09:43:51.452: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:52.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:52.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:53.450: INFO: Number of nodes with available pods: 0
Feb 21 09:43:53.450: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:54.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:54.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:55.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:55.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:56.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:56.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:57.449: INFO: Number of nodes with available pods: 0
Feb 21 09:43:57.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:58.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:58.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:43:59.448: INFO: Number of nodes with available pods: 0
Feb 21 09:43:59.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:44:00.449: INFO: Number of nodes with available pods: 0
Feb 21 09:44:00.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:44:01.448: INFO: Number of nodes with available pods: 0
Feb 21 09:44:01.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:44:02.449: INFO: Number of nodes with available pods: 0
Feb 21 09:44:02.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:44:03.448: INFO: Number of nodes with available pods: 0
Feb 21 09:44:03.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:44:04.449: INFO: Number of nodes with available pods: 0
Feb 21 09:44:04.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:44:05.451: INFO: Number of nodes with available pods: 0
Feb 21 09:44:05.451: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:44:06.448: INFO: Number of nodes with available pods: 0
Feb 21 09:44:06.448: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:44:07.449: INFO: Number of nodes with available pods: 0
Feb 21 09:44:07.449: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:44:08.450: INFO: Number of nodes with available pods: 0
Feb 21 09:44:08.450: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 09:44:09.448: INFO: Number of nodes with available pods: 1
Feb 21 09:44:09.448: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-lqskz, will wait for the garbage collector to delete the pods
Feb 21 09:44:09.520: INFO: Deleting {extensions DaemonSet} daemon-set took: 8.935967ms
Feb 21 09:44:09.620: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.291377ms
Feb 21 09:44:43.529: INFO: Number of nodes with available pods: 0
Feb 21 09:44:43.529: INFO: Number of running nodes: 0, number of available pods: 0
Feb 21 09:44:43.532: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-lqskz/daemonsets","resourceVersion":"16132"},"items":null}

Feb 21 09:44:43.536: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-lqskz/pods","resourceVersion":"16132"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:44:43.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-lqskz" for this suite.
Feb 21 09:44:49.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:44:49.766: INFO: namespace: e2e-tests-daemonsets-lqskz, resource: bindings, ignored listing per whitelist
Feb 21 09:44:49.819: INFO: namespace e2e-tests-daemonsets-lqskz deletion completed in 6.227208745s

• [SLOW TEST:82.885 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:44:49.820: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-qrbdl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 21 09:44:52.700: INFO: Successfully updated pod "pod-update-activedeadlineseconds-54ec9921-35bd-11e9-b7c5-a2b84e263bfe"
Feb 21 09:44:52.700: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-54ec9921-35bd-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-pods-qrbdl" to be "terminated due to deadline exceeded"
Feb 21 09:44:52.708: INFO: Pod "pod-update-activedeadlineseconds-54ec9921-35bd-11e9-b7c5-a2b84e263bfe": Phase="Running", Reason="", readiness=true. Elapsed: 7.905309ms
Feb 21 09:44:54.719: INFO: Pod "pod-update-activedeadlineseconds-54ec9921-35bd-11e9-b7c5-a2b84e263bfe": Phase="Running", Reason="", readiness=true. Elapsed: 2.019133696s
Feb 21 09:44:56.726: INFO: Pod "pod-update-activedeadlineseconds-54ec9921-35bd-11e9-b7c5-a2b84e263bfe": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.026232572s
Feb 21 09:44:56.726: INFO: Pod "pod-update-activedeadlineseconds-54ec9921-35bd-11e9-b7c5-a2b84e263bfe" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:44:56.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-qrbdl" for this suite.
Feb 21 09:45:02.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:45:02.848: INFO: namespace: e2e-tests-pods-qrbdl, resource: bindings, ignored listing per whitelist
Feb 21 09:45:02.974: INFO: namespace e2e-tests-pods-qrbdl deletion completed in 6.240083112s

• [SLOW TEST:13.154 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:45:02.975: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-jjwn8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 21 09:45:03.322: INFO: Waiting up to 5m0s for pod "var-expansion-5cc6a3d7-35bd-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-var-expansion-jjwn8" to be "success or failure"
Feb 21 09:45:03.328: INFO: Pod "var-expansion-5cc6a3d7-35bd-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.114311ms
Feb 21 09:45:05.339: INFO: Pod "var-expansion-5cc6a3d7-35bd-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017358765s
STEP: Saw pod success
Feb 21 09:45:05.340: INFO: Pod "var-expansion-5cc6a3d7-35bd-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:45:05.345: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod var-expansion-5cc6a3d7-35bd-11e9-b7c5-a2b84e263bfe container dapi-container: <nil>
STEP: delete the pod
Feb 21 09:45:05.371: INFO: Waiting for pod var-expansion-5cc6a3d7-35bd-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:45:05.376: INFO: Pod var-expansion-5cc6a3d7-35bd-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:45:05.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-jjwn8" for this suite.
Feb 21 09:45:11.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:45:11.598: INFO: namespace: e2e-tests-var-expansion-jjwn8, resource: bindings, ignored listing per whitelist
Feb 21 09:45:11.629: INFO: namespace e2e-tests-var-expansion-jjwn8 deletion completed in 6.246233825s

• [SLOW TEST:8.654 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:45:11.629: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-t2thd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 21 09:45:11.916: INFO: Waiting up to 5m0s for pod "pod-61e6310e-35bd-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-t2thd" to be "success or failure"
Feb 21 09:45:11.920: INFO: Pod "pod-61e6310e-35bd-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.333182ms
Feb 21 09:45:13.928: INFO: Pod "pod-61e6310e-35bd-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011808732s
STEP: Saw pod success
Feb 21 09:45:13.928: INFO: Pod "pod-61e6310e-35bd-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:45:13.941: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-61e6310e-35bd-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 09:45:13.963: INFO: Waiting for pod pod-61e6310e-35bd-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:45:13.967: INFO: Pod pod-61e6310e-35bd-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:45:13.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-t2thd" for this suite.
Feb 21 09:45:19.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:45:20.159: INFO: namespace: e2e-tests-emptydir-t2thd, resource: bindings, ignored listing per whitelist
Feb 21 09:45:20.436: INFO: namespace e2e-tests-emptydir-t2thd deletion completed in 6.463944318s

• [SLOW TEST:8.807 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:45:20.436: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-h7n9j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 09:45:20.818: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml version'
Feb 21 09:45:20.924: INFO: stderr: ""
Feb 21 09:45:20.925: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"archive\", BuildDate:\"2019-02-21T08:23:41Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"clean\", BuildDate:\"2019-01-16T18:14:49Z\", GoVersion:\"go1.10.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:45:20.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-h7n9j" for this suite.
Feb 21 09:45:26.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:45:27.029: INFO: namespace: e2e-tests-kubectl-h7n9j, resource: bindings, ignored listing per whitelist
Feb 21 09:45:27.145: INFO: namespace e2e-tests-kubectl-h7n9j deletion completed in 6.212757592s

• [SLOW TEST:6.709 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:45:27.146: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-9nrdv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-9nrdv
Feb 21 09:45:29.432: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-9nrdv
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 09:45:29.436: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:49:30.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-9nrdv" for this suite.
Feb 21 09:49:36.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:49:36.799: INFO: namespace: e2e-tests-container-probe-9nrdv, resource: bindings, ignored listing per whitelist
Feb 21 09:49:36.908: INFO: namespace e2e-tests-container-probe-9nrdv deletion completed in 6.311531592s

• [SLOW TEST:249.762 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:49:36.908: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-w2lt6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 09:49:37.265: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"000f2022-35be-11e9-890e-3aac84b8a476", Controller:(*bool)(0xc00141d67e), BlockOwnerDeletion:(*bool)(0xc00141d67f)}}
Feb 21 09:49:37.270: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"000b6e7e-35be-11e9-890e-3aac84b8a476", Controller:(*bool)(0xc000fcdf9e), BlockOwnerDeletion:(*bool)(0xc000fcdf9f)}}
Feb 21 09:49:37.275: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"000dd3aa-35be-11e9-890e-3aac84b8a476", Controller:(*bool)(0xc00125a4de), BlockOwnerDeletion:(*bool)(0xc00125a4df)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:49:42.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-w2lt6" for this suite.
Feb 21 09:49:48.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:49:48.416: INFO: namespace: e2e-tests-gc-w2lt6, resource: bindings, ignored listing per whitelist
Feb 21 09:49:48.626: INFO: namespace e2e-tests-gc-w2lt6 deletion completed in 6.311128523s

• [SLOW TEST:11.718 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:49:48.627: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-hxsp4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-071204ea-35be-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 09:49:49.032: INFO: Waiting up to 5m0s for pod "pod-secrets-0712d303-35be-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-secrets-hxsp4" to be "success or failure"
Feb 21 09:49:49.036: INFO: Pod "pod-secrets-0712d303-35be-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91419ms
Feb 21 09:49:51.047: INFO: Pod "pod-secrets-0712d303-35be-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014583432s
STEP: Saw pod success
Feb 21 09:49:51.047: INFO: Pod "pod-secrets-0712d303-35be-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:49:51.055: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-secrets-0712d303-35be-11e9-b7c5-a2b84e263bfe container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 09:49:51.086: INFO: Waiting for pod pod-secrets-0712d303-35be-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:49:51.093: INFO: Pod pod-secrets-0712d303-35be-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:49:51.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hxsp4" for this suite.
Feb 21 09:49:57.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:49:57.446: INFO: namespace: e2e-tests-secrets-hxsp4, resource: bindings, ignored listing per whitelist
Feb 21 09:49:57.460: INFO: namespace e2e-tests-secrets-hxsp4 deletion completed in 6.356274966s

• [SLOW TEST:8.834 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:49:57.461: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-f5d5w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 21 09:49:57.821: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:50:00.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-f5d5w" for this suite.
Feb 21 09:50:07.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:50:07.318: INFO: namespace: e2e-tests-init-container-f5d5w, resource: bindings, ignored listing per whitelist
Feb 21 09:50:07.332: INFO: namespace e2e-tests-init-container-f5d5w deletion completed in 6.389824002s

• [SLOW TEST:9.871 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:50:07.333: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-gfhj6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 21 09:50:07.636: INFO: Waiting up to 5m0s for pod "pod-12297fa7-35be-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-gfhj6" to be "success or failure"
Feb 21 09:50:07.640: INFO: Pod "pod-12297fa7-35be-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.966585ms
Feb 21 09:50:09.646: INFO: Pod "pod-12297fa7-35be-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009813767s
STEP: Saw pod success
Feb 21 09:50:09.646: INFO: Pod "pod-12297fa7-35be-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:50:09.649: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-12297fa7-35be-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 09:50:09.670: INFO: Waiting for pod pod-12297fa7-35be-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:50:09.673: INFO: Pod pod-12297fa7-35be-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:50:09.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gfhj6" for this suite.
Feb 21 09:50:15.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:50:16.295: INFO: namespace: e2e-tests-emptydir-gfhj6, resource: bindings, ignored listing per whitelist
Feb 21 09:50:16.350: INFO: namespace e2e-tests-emptydir-gfhj6 deletion completed in 6.669770412s

• [SLOW TEST:9.017 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:50:16.350: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-nk5gt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1402
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 09:50:16.717: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-nk5gt'
Feb 21 09:50:17.427: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 21 09:50:17.427: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1407
Feb 21 09:50:17.432: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-nk5gt'
Feb 21 09:50:17.553: INFO: stderr: ""
Feb 21 09:50:17.553: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:50:17.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nk5gt" for this suite.
Feb 21 09:50:39.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:50:39.688: INFO: namespace: e2e-tests-kubectl-nk5gt, resource: bindings, ignored listing per whitelist
Feb 21 09:50:39.813: INFO: namespace e2e-tests-kubectl-nk5gt deletion completed in 22.253168375s

• [SLOW TEST:23.463 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:50:39.814: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-6l46p
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 21 09:50:40.332: INFO: Waiting up to 5m0s for pod "pod-25a63227-35be-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-6l46p" to be "success or failure"
Feb 21 09:50:40.339: INFO: Pod "pod-25a63227-35be-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.634244ms
Feb 21 09:50:42.345: INFO: Pod "pod-25a63227-35be-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012675176s
STEP: Saw pod success
Feb 21 09:50:42.345: INFO: Pod "pod-25a63227-35be-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:50:42.349: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-25a63227-35be-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 09:50:42.372: INFO: Waiting for pod pod-25a63227-35be-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:50:42.378: INFO: Pod pod-25a63227-35be-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:50:42.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6l46p" for this suite.
Feb 21 09:50:48.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:50:48.527: INFO: namespace: e2e-tests-emptydir-6l46p, resource: bindings, ignored listing per whitelist
Feb 21 09:50:48.653: INFO: namespace e2e-tests-emptydir-6l46p deletion completed in 6.267648615s

• [SLOW TEST:8.840 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:50:48.653: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-7nlh2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 21 09:50:48.914: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 21 09:50:48.928: INFO: Waiting for terminating namespaces to be deleted...
Feb 21 09:50:48.937: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw before test
Feb 21 09:50:48.958: INFO: coredns-5f4748c5f-zzvr9 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container coredns ready: true, restart count 0
Feb 21 09:50:48.958: INFO: node-exporter-7nlxq from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 09:50:48.958: INFO: addons-kube-lego-648f8c9f5c-88qt5 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container kube-lego ready: true, restart count 0
Feb 21 09:50:48.958: INFO: addons-nginx-ingress-controller-55d976867d-zqg87 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 09:50:48.958: INFO: metrics-server-5c5588c85c-lwhcv from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container metrics-server ready: true, restart count 0
Feb 21 09:50:48.958: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-j8nvn from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 21 09:50:48.958: INFO: vpn-shoot-597d7f599b-56cgp from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 21 09:50:48.958: INFO: kube-proxy-pxd78 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 09:50:48.958: INFO: calico-node-bbf5b from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 09:50:48.958: INFO: addons-kubernetes-dashboard-5f64f76bd-gld9p from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 21 09:50:48.958: INFO: blackbox-exporter-64f6f7f998-frlvk from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:48.958: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 21 09:50:48.958: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl before test
Feb 21 09:50:49.047: INFO: calico-node-gfth2 from kube-system started at 2019-02-21 08:00:47 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:49.047: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 09:50:49.047: INFO: node-exporter-j5s5c from kube-system started at 2019-02-21 08:00:48 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:49.047: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 09:50:49.047: INFO: kube-proxy-2jctn from kube-system started at 2019-02-21 08:00:47 +0000 UTC (1 container statuses recorded)
Feb 21 09:50:49.047: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2c0ffc12-35be-11e9-b7c5-a2b84e263bfe 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2c0ffc12-35be-11e9-b7c5-a2b84e263bfe off the node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2c0ffc12-35be-11e9-b7c5-a2b84e263bfe
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:50:53.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-7nlh2" for this suite.
Feb 21 09:51:01.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:51:01.404: INFO: namespace: e2e-tests-sched-pred-7nlh2, resource: bindings, ignored listing per whitelist
Feb 21 09:51:01.519: INFO: namespace e2e-tests-sched-pred-7nlh2 deletion completed in 8.297941542s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:12.866 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:51:01.519: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-wdbdh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 21 09:51:04.696: INFO: Successfully updated pod "annotationupdate3284472f-35be-11e9-b7c5-a2b84e263bfe"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:51:08.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wdbdh" for this suite.
Feb 21 09:51:30.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:51:30.957: INFO: namespace: e2e-tests-downward-api-wdbdh, resource: bindings, ignored listing per whitelist
Feb 21 09:51:31.235: INFO: namespace e2e-tests-downward-api-wdbdh deletion completed in 22.489695058s

• [SLOW TEST:29.715 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:51:31.235: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-v6nw9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 09:51:31.820: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44571302-35be-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-v6nw9" to be "success or failure"
Feb 21 09:51:31.825: INFO: Pod "downwardapi-volume-44571302-35be-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.401536ms
Feb 21 09:51:33.831: INFO: Pod "downwardapi-volume-44571302-35be-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01153438s
Feb 21 09:51:35.839: INFO: Pod "downwardapi-volume-44571302-35be-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018957675s
STEP: Saw pod success
Feb 21 09:51:35.839: INFO: Pod "downwardapi-volume-44571302-35be-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:51:35.843: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-44571302-35be-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 09:51:35.870: INFO: Waiting for pod downwardapi-volume-44571302-35be-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:51:35.874: INFO: Pod downwardapi-volume-44571302-35be-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:51:35.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-v6nw9" for this suite.
Feb 21 09:51:41.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:51:42.096: INFO: namespace: e2e-tests-downward-api-v6nw9, resource: bindings, ignored listing per whitelist
Feb 21 09:51:42.202: INFO: namespace e2e-tests-downward-api-v6nw9 deletion completed in 6.320408323s

• [SLOW TEST:10.967 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:51:42.203: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-xqc5b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-jbcf
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 09:51:42.943: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jbcf" in namespace "e2e-tests-subpath-xqc5b" to be "success or failure"
Feb 21 09:51:42.948: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742075ms
Feb 21 09:51:44.960: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016853697s
Feb 21 09:51:46.972: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Running", Reason="", readiness=false. Elapsed: 4.028843534s
Feb 21 09:51:48.983: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Running", Reason="", readiness=false. Elapsed: 6.039226911s
Feb 21 09:51:50.994: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Running", Reason="", readiness=false. Elapsed: 8.0509551s
Feb 21 09:51:53.219: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Running", Reason="", readiness=false. Elapsed: 10.275418066s
Feb 21 09:51:55.229: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Running", Reason="", readiness=false. Elapsed: 12.286086856s
Feb 21 09:51:57.235: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Running", Reason="", readiness=false. Elapsed: 14.291743647s
Feb 21 09:51:59.241: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Running", Reason="", readiness=false. Elapsed: 16.298077644s
Feb 21 09:52:01.248: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Running", Reason="", readiness=false. Elapsed: 18.30414703s
Feb 21 09:52:03.253: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Running", Reason="", readiness=false. Elapsed: 20.310115944s
Feb 21 09:52:05.261: INFO: Pod "pod-subpath-test-secret-jbcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.317768104s
STEP: Saw pod success
Feb 21 09:52:05.261: INFO: Pod "pod-subpath-test-secret-jbcf" satisfied condition "success or failure"
Feb 21 09:52:05.267: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-subpath-test-secret-jbcf container test-container-subpath-secret-jbcf: <nil>
STEP: delete the pod
Feb 21 09:52:05.296: INFO: Waiting for pod pod-subpath-test-secret-jbcf to disappear
Feb 21 09:52:05.303: INFO: Pod pod-subpath-test-secret-jbcf no longer exists
STEP: Deleting pod pod-subpath-test-secret-jbcf
Feb 21 09:52:05.303: INFO: Deleting pod "pod-subpath-test-secret-jbcf" in namespace "e2e-tests-subpath-xqc5b"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:52:05.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-xqc5b" for this suite.
Feb 21 09:52:11.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:52:11.402: INFO: namespace: e2e-tests-subpath-xqc5b, resource: bindings, ignored listing per whitelist
Feb 21 09:52:11.875: INFO: namespace e2e-tests-subpath-xqc5b deletion completed in 6.5572859s

• [SLOW TEST:29.674 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:52:11.884: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-tcktl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 09:52:12.847: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5cab7d13-35be-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-tcktl" to be "success or failure"
Feb 21 09:52:12.857: INFO: Pod "downwardapi-volume-5cab7d13-35be-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 9.811891ms
Feb 21 09:52:14.875: INFO: Pod "downwardapi-volume-5cab7d13-35be-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02763243s
STEP: Saw pod success
Feb 21 09:52:14.875: INFO: Pod "downwardapi-volume-5cab7d13-35be-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:52:14.887: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-5cab7d13-35be-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 09:52:14.915: INFO: Waiting for pod downwardapi-volume-5cab7d13-35be-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:52:14.923: INFO: Pod downwardapi-volume-5cab7d13-35be-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:52:14.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tcktl" for this suite.
Feb 21 09:52:20.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:52:21.419: INFO: namespace: e2e-tests-projected-tcktl, resource: bindings, ignored listing per whitelist
Feb 21 09:52:21.512: INFO: namespace e2e-tests-projected-tcktl deletion completed in 6.571676979s

• [SLOW TEST:9.629 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:52:21.512: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fp56k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1347
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 09:52:21.919: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-fp56k'
Feb 21 09:52:22.322: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 21 09:52:22.322: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1352
Feb 21 09:52:24.342: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-fp56k'
Feb 21 09:52:24.476: INFO: stderr: ""
Feb 21 09:52:24.477: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:52:24.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fp56k" for this suite.
Feb 21 09:52:46.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:52:46.550: INFO: namespace: e2e-tests-kubectl-fp56k, resource: bindings, ignored listing per whitelist
Feb 21 09:52:46.754: INFO: namespace e2e-tests-kubectl-fp56k deletion completed in 22.257562589s

• [SLOW TEST:25.242 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:52:46.756: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xqk75
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 21 09:52:47.313: INFO: namespace e2e-tests-kubectl-xqk75
Feb 21 09:52:47.313: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-xqk75'
Feb 21 09:52:47.639: INFO: stderr: ""
Feb 21 09:52:47.639: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 21 09:52:48.648: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 09:52:48.648: INFO: Found 0 / 1
Feb 21 09:52:49.650: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 09:52:49.650: INFO: Found 1 / 1
Feb 21 09:52:49.650: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 21 09:52:49.658: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 09:52:49.658: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 21 09:52:49.658: INFO: wait on redis-master startup in e2e-tests-kubectl-xqk75 
Feb 21 09:52:49.658: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs redis-master-pvwvl redis-master --namespace=e2e-tests-kubectl-xqk75'
Feb 21 09:52:49.825: INFO: stderr: ""
Feb 21 09:52:49.825: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Feb 09:52:48.618 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Feb 09:52:48.618 # Server started, Redis version 3.2.12\n1:M 21 Feb 09:52:48.618 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Feb 09:52:48.618 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 21 09:52:49.825: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-xqk75'
Feb 21 09:52:50.003: INFO: stderr: ""
Feb 21 09:52:50.003: INFO: stdout: "service/rm2 exposed\n"
Feb 21 09:52:50.010: INFO: Service rm2 in namespace e2e-tests-kubectl-xqk75 found.
STEP: exposing service
Feb 21 09:52:52.024: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-xqk75'
Feb 21 09:52:52.169: INFO: stderr: ""
Feb 21 09:52:52.169: INFO: stdout: "service/rm3 exposed\n"
Feb 21 09:52:52.176: INFO: Service rm3 in namespace e2e-tests-kubectl-xqk75 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:52:54.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xqk75" for this suite.
Feb 21 09:53:16.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:53:16.476: INFO: namespace: e2e-tests-kubectl-xqk75, resource: bindings, ignored listing per whitelist
Feb 21 09:53:16.510: INFO: namespace e2e-tests-kubectl-xqk75 deletion completed in 22.310051239s

• [SLOW TEST:29.755 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:53:16.511: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-n55w7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 21 09:53:16.825: INFO: Waiting up to 5m0s for pod "client-containers-82ed49a8-35be-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-containers-n55w7" to be "success or failure"
Feb 21 09:53:16.832: INFO: Pod "client-containers-82ed49a8-35be-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.642478ms
Feb 21 09:53:18.839: INFO: Pod "client-containers-82ed49a8-35be-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013555844s
Feb 21 09:53:20.844: INFO: Pod "client-containers-82ed49a8-35be-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01944759s
STEP: Saw pod success
Feb 21 09:53:20.844: INFO: Pod "client-containers-82ed49a8-35be-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:53:20.851: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod client-containers-82ed49a8-35be-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 09:53:20.873: INFO: Waiting for pod client-containers-82ed49a8-35be-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:53:20.877: INFO: Pod client-containers-82ed49a8-35be-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:53:20.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-n55w7" for this suite.
Feb 21 09:53:26.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:53:27.264: INFO: namespace: e2e-tests-containers-n55w7, resource: bindings, ignored listing per whitelist
Feb 21 09:53:27.335: INFO: namespace e2e-tests-containers-n55w7 deletion completed in 6.450254066s

• [SLOW TEST:10.825 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:53:27.335: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-sjnv4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-sjnv4
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-sjnv4
STEP: Deleting pre-stop pod
Feb 21 09:54:56.889: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:54:56.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-sjnv4" for this suite.
Feb 21 09:55:34.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:55:35.439: INFO: namespace: e2e-tests-prestop-sjnv4, resource: bindings, ignored listing per whitelist
Feb 21 09:55:35.454: INFO: namespace e2e-tests-prestop-sjnv4 deletion completed in 38.543136268s

• [SLOW TEST:128.119 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:55:35.455: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jbh8t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-d5c7f996-35be-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 09:55:35.839: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d5c912cc-35be-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-jbh8t" to be "success or failure"
Feb 21 09:55:35.845: INFO: Pod "pod-projected-configmaps-d5c912cc-35be-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.942564ms
Feb 21 09:55:37.851: INFO: Pod "pod-projected-configmaps-d5c912cc-35be-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01177796s
Feb 21 09:55:39.858: INFO: Pod "pod-projected-configmaps-d5c912cc-35be-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018841196s
STEP: Saw pod success
Feb 21 09:55:39.858: INFO: Pod "pod-projected-configmaps-d5c912cc-35be-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:55:39.864: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-configmaps-d5c912cc-35be-11e9-b7c5-a2b84e263bfe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 09:55:39.896: INFO: Waiting for pod pod-projected-configmaps-d5c912cc-35be-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:55:39.903: INFO: Pod pod-projected-configmaps-d5c912cc-35be-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:55:39.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jbh8t" for this suite.
Feb 21 09:55:45.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:55:46.180: INFO: namespace: e2e-tests-projected-jbh8t, resource: bindings, ignored listing per whitelist
Feb 21 09:55:46.194: INFO: namespace e2e-tests-projected-jbh8t deletion completed in 6.279842265s

• [SLOW TEST:10.740 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:55:46.194: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-jpjgh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-jpjgh
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 21 09:55:46.532: INFO: Found 0 stateful pods, waiting for 3
Feb 21 09:55:56.540: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:55:56.540: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:55:56.540: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 09:55:56.554: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jpjgh ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 09:55:57.389: INFO: stderr: ""
Feb 21 09:55:57.389: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 09:55:57.389: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 21 09:56:07.444: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 21 09:56:17.484: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jpjgh ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:56:18.043: INFO: stderr: ""
Feb 21 09:56:18.043: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 09:56:18.043: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
Feb 21 09:56:38.082: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jpjgh ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 09:56:38.564: INFO: stderr: ""
Feb 21 09:56:38.564: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 09:56:38.564: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 09:56:48.612: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 21 09:56:58.640: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jpjgh ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 09:56:59.164: INFO: stderr: ""
Feb 21 09:56:59.164: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 09:56:59.164: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 09:57:19.204: INFO: Waiting for StatefulSet e2e-tests-statefulset-jpjgh/ss2 to complete update
Feb 21 09:57:19.204: INFO: Waiting for Pod e2e-tests-statefulset-jpjgh/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 21 09:57:29.218: INFO: Deleting all statefulset in ns e2e-tests-statefulset-jpjgh
Feb 21 09:57:29.224: INFO: Scaling statefulset ss2 to 0
Feb 21 09:57:49.252: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 09:57:49.257: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:57:49.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-jpjgh" for this suite.
Feb 21 09:57:55.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:57:55.556: INFO: namespace: e2e-tests-statefulset-jpjgh, resource: bindings, ignored listing per whitelist
Feb 21 09:57:55.588: INFO: namespace e2e-tests-statefulset-jpjgh deletion completed in 6.305992715s

• [SLOW TEST:129.394 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:57:55.588: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-sfz62
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 09:57:56.441: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml version --client'
Feb 21 09:57:56.518: INFO: stderr: ""
Feb 21 09:57:56.519: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"archive\", BuildDate:\"2019-02-21T08:23:41Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 21 09:57:56.523: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-sfz62'
Feb 21 09:57:57.846: INFO: stderr: ""
Feb 21 09:57:57.847: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 21 09:57:57.847: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-sfz62'
Feb 21 09:57:58.279: INFO: stderr: ""
Feb 21 09:57:58.279: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 21 09:57:59.290: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 09:57:59.290: INFO: Found 0 / 1
Feb 21 09:58:00.286: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 09:58:00.286: INFO: Found 1 / 1
Feb 21 09:58:00.286: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 21 09:58:00.291: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 09:58:00.291: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 21 09:58:00.291: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe pod redis-master-5vvhn --namespace=e2e-tests-kubectl-sfz62'
Feb 21 09:58:01.444: INFO: stderr: ""
Feb 21 09:58:01.445: INFO: stdout: "Name:               redis-master-5vvhn\nNamespace:          e2e-tests-kubectl-sfz62\nPriority:           0\nPriorityClassName:  <none>\nNode:               shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl/10.250.0.19\nStart Time:         Thu, 21 Feb 2019 09:57:57 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.156/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.156\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://8a4e642c27718fa72d892930c5ae2c83fc1592525c9d707b96501356c817a615\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 21 Feb 2019 09:57:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5j7zs (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-5j7zs:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-5j7zs\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                             Message\n  ----    ------     ----  ----                                                             -------\n  Normal  Scheduled  4s    default-scheduler                                                Successfully assigned e2e-tests-kubectl-sfz62/redis-master-5vvhn to shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl\n  Normal  Pulled     3s    kubelet, shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    3s    kubelet, shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Created container\n  Normal  Started    3s    kubelet, shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl  Started container\n"
Feb 21 09:58:01.445: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe rc redis-master --namespace=e2e-tests-kubectl-sfz62'
Feb 21 09:58:01.584: INFO: stderr: ""
Feb 21 09:58:01.585: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-sfz62\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-5vvhn\n"
Feb 21 09:58:01.585: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe service redis-master --namespace=e2e-tests-kubectl-sfz62'
Feb 21 09:58:01.773: INFO: stderr: ""
Feb 21 09:58:01.773: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-sfz62\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.66.41.29\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.156:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 21 09:58:01.781: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw'
Feb 21 09:58:01.923: INFO: stderr: ""
Feb 21 09:58:01.923: INFO: stdout: "Name:               shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecf69421-6238-4433-a0a0-70bb24198e09\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/zone=rot_1_1\n                    kubernetes.io/hostname=shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.18/19\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 21 Feb 2019 08:00:23 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  OutOfDisk        False   Thu, 21 Feb 2019 09:57:54 +0000   Thu, 21 Feb 2019 08:00:23 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n  MemoryPressure   False   Thu, 21 Feb 2019 09:57:54 +0000   Thu, 21 Feb 2019 08:00:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 21 Feb 2019 09:57:54 +0000   Thu, 21 Feb 2019 08:00:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 21 Feb 2019 09:57:54 +0000   Thu, 21 Feb 2019 08:00:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 21 Feb 2019 09:57:54 +0000   Thu, 21 Feb 2019 08:03:13 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.0.18\nCapacity:\n cpu:                2\n ephemeral-storage:  38216108Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             4042360Ki\n pods:               110\nAllocatable:\n cpu:                1920m\n ephemeral-storage:  37176629834\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             2858665981\n pods:               110\nSystem Info:\n Machine ID:                 f8b84dd22e2f448f97fbbbade8d820b3\n System UUID:                F8B84DD2-2E2F-448F-97FB-BBADE8D820B3\n Boot ID:                    7a1e4299-b432-4dd9-81c5-8bf4ffc68a10\n Kernel Version:             4.14.96-coreos\n OS Image:                   Container Linux by CoreOS 1967.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.12.5\n Kube-Proxy Version:         v1.12.5\nPodCIDR:                     100.96.0.0/24\nProviderID:                  openstack:///f8b84dd2-2e2f-448f-97fb-bbade8d820b3\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------\n  kube-system                addons-kube-lego-648f8c9f5c-88qt5                                  20m (1%)      50m (2%)    8Mi (0%)         32Mi (1%)\n  kube-system                addons-kubernetes-dashboard-5f64f76bd-gld9p                        50m (2%)      100m (5%)   50Mi (1%)        256Mi (9%)\n  kube-system                addons-nginx-ingress-controller-55d976867d-zqg87                   100m (5%)     2 (104%)    100Mi (3%)       800Mi (29%)\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-j8nvn    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                blackbox-exporter-64f6f7f998-frlvk                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (1%)\n  kube-system                calico-node-bbf5b                                                  100m (5%)     500m (26%)  100Mi (3%)       700Mi (25%)\n  kube-system                coredns-5f4748c5f-zzvr9                                            50m (2%)      100m (5%)   15Mi (0%)        100Mi (3%)\n  kube-system                kube-proxy-pxd78                                                   20m (1%)      900m (46%)  64Mi (2%)        200Mi (7%)\n  kube-system                metrics-server-5c5588c85c-lwhcv                                    20m (1%)      80m (4%)    100Mi (3%)       400Mi (14%)\n  kube-system                node-exporter-7nlxq                                                5m (0%)       15m (0%)    10Mi (0%)        50Mi (1%)\n  kube-system                vpn-shoot-597d7f599b-56cgp                                         50m (2%)      100m (5%)   50Mi (1%)        100Mi (3%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource  Requests     Limits\n  --------  --------     ------\n  cpu       420m (21%)   3855m (200%)\n  memory    502Mi (18%)  2673Mi (98%)\nEvents:     <none>\n"
Feb 21 09:58:01.924: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe namespace e2e-tests-kubectl-sfz62'
Feb 21 09:58:02.058: INFO: stderr: ""
Feb 21 09:58:02.058: INFO: stdout: "Name:         e2e-tests-kubectl-sfz62\nLabels:       e2e-framework=kubectl\n              e2e-run=4399f893-35b2-11e9-b7c5-a2b84e263bfe\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:58:02.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sfz62" for this suite.
Feb 21 09:58:24.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:58:24.110: INFO: namespace: e2e-tests-kubectl-sfz62, resource: bindings, ignored listing per whitelist
Feb 21 09:58:24.737: INFO: namespace e2e-tests-kubectl-sfz62 deletion completed in 22.670242476s

• [SLOW TEST:29.148 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:58:24.737: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mb4pq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 09:58:25.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b0ba86c-35bf-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-mb4pq" to be "success or failure"
Feb 21 09:58:25.731: INFO: Pod "downwardapi-volume-3b0ba86c-35bf-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.798133ms
Feb 21 09:58:27.737: INFO: Pod "downwardapi-volume-3b0ba86c-35bf-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011143103s
STEP: Saw pod success
Feb 21 09:58:27.737: INFO: Pod "downwardapi-volume-3b0ba86c-35bf-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:58:27.741: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-3b0ba86c-35bf-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 09:58:27.768: INFO: Waiting for pod downwardapi-volume-3b0ba86c-35bf-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:58:27.772: INFO: Pod downwardapi-volume-3b0ba86c-35bf-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:58:27.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mb4pq" for this suite.
Feb 21 09:58:33.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:58:33.829: INFO: namespace: e2e-tests-projected-mb4pq, resource: bindings, ignored listing per whitelist
Feb 21 09:58:33.970: INFO: namespace e2e-tests-projected-mb4pq deletion completed in 6.187359262s

• [SLOW TEST:9.233 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:58:33.971: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-bhq7r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 21 09:58:34.378: INFO: Waiting up to 5m0s for pod "downward-api-4033f0f1-35bf-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-bhq7r" to be "success or failure"
Feb 21 09:58:34.386: INFO: Pod "downward-api-4033f0f1-35bf-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.941192ms
Feb 21 09:58:36.394: INFO: Pod "downward-api-4033f0f1-35bf-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016003887s
STEP: Saw pod success
Feb 21 09:58:36.394: INFO: Pod "downward-api-4033f0f1-35bf-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 09:58:36.399: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downward-api-4033f0f1-35bf-11e9-b7c5-a2b84e263bfe container dapi-container: <nil>
STEP: delete the pod
Feb 21 09:58:36.425: INFO: Waiting for pod downward-api-4033f0f1-35bf-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 09:58:36.438: INFO: Pod downward-api-4033f0f1-35bf-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 09:58:36.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bhq7r" for this suite.
Feb 21 09:58:42.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 09:58:42.493: INFO: namespace: e2e-tests-downward-api-bhq7r, resource: bindings, ignored listing per whitelist
Feb 21 09:58:42.667: INFO: namespace e2e-tests-downward-api-bhq7r deletion completed in 6.220279142s

• [SLOW TEST:8.696 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 09:58:42.667: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-vphs4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-vphs4
Feb 21 09:58:44.949: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-vphs4
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 09:58:44.954: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:02:46.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-vphs4" for this suite.
Feb 21 10:02:52.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:02:52.493: INFO: namespace: e2e-tests-container-probe-vphs4, resource: bindings, ignored listing per whitelist
Feb 21 10:02:52.741: INFO: namespace e2e-tests-container-probe-vphs4 deletion completed in 6.350277562s

• [SLOW TEST:250.074 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:02:52.741: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-4npnz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0221 10:02:59.366365   32452 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 21 10:02:59.366: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:02:59.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4npnz" for this suite.
Feb 21 10:03:05.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:03:05.465: INFO: namespace: e2e-tests-gc-4npnz, resource: bindings, ignored listing per whitelist
Feb 21 10:03:05.832: INFO: namespace e2e-tests-gc-4npnz deletion completed in 6.459759744s

• [SLOW TEST:13.091 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:03:05.833: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-j9jzh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-j9jzh
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j9jzh to expose endpoints map[]
Feb 21 10:03:06.234: INFO: Get endpoints failed (3.880991ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 21 10:03:07.241: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j9jzh exposes endpoints map[] (1.010303343s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-j9jzh
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j9jzh to expose endpoints map[pod1:[100]]
Feb 21 10:03:09.287: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j9jzh exposes endpoints map[pod1:[100]] (2.035844318s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-j9jzh
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j9jzh to expose endpoints map[pod1:[100] pod2:[101]]
Feb 21 10:03:11.366: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j9jzh exposes endpoints map[pod1:[100] pod2:[101]] (2.067438548s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-j9jzh
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j9jzh to expose endpoints map[pod2:[101]]
Feb 21 10:03:11.594: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j9jzh exposes endpoints map[pod2:[101]] (216.845928ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-j9jzh
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-j9jzh to expose endpoints map[]
Feb 21 10:03:11.612: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-j9jzh exposes endpoints map[] (5.236515ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:03:11.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-j9jzh" for this suite.
Feb 21 10:03:17.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:03:17.792: INFO: namespace: e2e-tests-services-j9jzh, resource: bindings, ignored listing per whitelist
Feb 21 10:03:17.868: INFO: namespace e2e-tests-services-j9jzh deletion completed in 6.223188034s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:12.036 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:03:17.868: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-hj8vb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-e9820a91-35bf-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 10:03:18.433: INFO: Waiting up to 5m0s for pod "pod-secrets-e982ecee-35bf-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-secrets-hj8vb" to be "success or failure"
Feb 21 10:03:18.439: INFO: Pod "pod-secrets-e982ecee-35bf-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.847668ms
Feb 21 10:03:20.445: INFO: Pod "pod-secrets-e982ecee-35bf-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011656688s
STEP: Saw pod success
Feb 21 10:03:20.445: INFO: Pod "pod-secrets-e982ecee-35bf-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:03:20.449: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-secrets-e982ecee-35bf-11e9-b7c5-a2b84e263bfe container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 10:03:20.477: INFO: Waiting for pod pod-secrets-e982ecee-35bf-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:03:20.481: INFO: Pod pod-secrets-e982ecee-35bf-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:03:20.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hj8vb" for this suite.
Feb 21 10:03:26.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:03:26.591: INFO: namespace: e2e-tests-secrets-hj8vb, resource: bindings, ignored listing per whitelist
Feb 21 10:03:26.967: INFO: namespace e2e-tests-secrets-hj8vb deletion completed in 6.478998855s

• [SLOW TEST:9.099 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:03:26.967: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-rk656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-rk656
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-rk656 to expose endpoints map[]
Feb 21 10:03:27.543: INFO: Get endpoints failed (4.930894ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 21 10:03:28.551: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-rk656 exposes endpoints map[] (1.012116107s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-rk656
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-rk656 to expose endpoints map[pod1:[80]]
Feb 21 10:03:29.786: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-rk656 exposes endpoints map[pod1:[80]] (1.225635313s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-rk656
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-rk656 to expose endpoints map[pod2:[80] pod1:[80]]
Feb 21 10:03:31.840: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-rk656 exposes endpoints map[pod1:[80] pod2:[80]] (2.048186271s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-rk656
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-rk656 to expose endpoints map[pod2:[80]]
Feb 21 10:03:31.855: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-rk656 exposes endpoints map[pod2:[80]] (8.38024ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-rk656
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-rk656 to expose endpoints map[]
Feb 21 10:03:31.865: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-rk656 exposes endpoints map[] (3.818013ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:03:31.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-rk656" for this suite.
Feb 21 10:03:37.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:03:38.418: INFO: namespace: e2e-tests-services-rk656, resource: bindings, ignored listing per whitelist
Feb 21 10:03:38.741: INFO: namespace e2e-tests-services-rk656 deletion completed in 6.848746915s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:11.774 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:03:38.741: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-2prbs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-2prbs
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 10:03:39.128: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 21 10:03:59.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.170:8080/dial?request=hostName&protocol=udp&host=100.96.1.169&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-2prbs PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:03:59.253: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:03:59.798: INFO: Waiting for endpoints: map[]
Feb 21 10:03:59.805: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.170:8080/dial?request=hostName&protocol=udp&host=100.96.0.61&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-2prbs PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:03:59.805: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:04:00.268: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:04:00.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-2prbs" for this suite.
Feb 21 10:04:22.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:04:22.368: INFO: namespace: e2e-tests-pod-network-test-2prbs, resource: bindings, ignored listing per whitelist
Feb 21 10:04:22.534: INFO: namespace e2e-tests-pod-network-test-2prbs deletion completed in 22.253868446s

• [SLOW TEST:43.793 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:04:22.534: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-t7fz8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-t7fz8.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-t7fz8.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-t7fz8.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-t7fz8.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-t7fz8.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-t7fz8.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 10:04:37.422: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe)
Feb 21 10:04:37.430: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe)
Feb 21 10:04:37.439: INFO: Unable to read wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-t7fz8.svc.cluster.local from pod e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe)
Feb 21 10:04:37.449: INFO: Unable to read wheezy_hosts@dns-querier-1 from pod e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe)
Feb 21 10:04:37.457: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe)
Feb 21 10:04:37.466: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe)
Feb 21 10:04:38.041: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe)
Feb 21 10:04:38.050: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe)
Feb 21 10:04:38.057: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe: the server could not find the requested resource (get pods dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe)
Feb 21 10:04:38.057: INFO: Lookups using e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-t7fz8.svc.cluster.local wheezy_hosts@dns-querier-1 wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 21 10:04:48.581: INFO: DNS probes using e2e-tests-dns-t7fz8/dns-test-1002862c-35c0-11e9-b7c5-a2b84e263bfe succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:04:48.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-t7fz8" for this suite.
Feb 21 10:04:54.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:04:55.268: INFO: namespace: e2e-tests-dns-t7fz8, resource: bindings, ignored listing per whitelist
Feb 21 10:04:55.396: INFO: namespace e2e-tests-dns-t7fz8 deletion completed in 6.773369975s

• [SLOW TEST:32.862 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:04:55.396: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-6t4hf
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-23936a7a-35c0-11e9-b7c5-a2b84e263bfe
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-23936a7a-35c0-11e9-b7c5-a2b84e263bfe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:05:00.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-6t4hf" for this suite.
Feb 21 10:05:22.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:05:22.271: INFO: namespace: e2e-tests-configmap-6t4hf, resource: bindings, ignored listing per whitelist
Feb 21 10:05:22.552: INFO: namespace e2e-tests-configmap-6t4hf deletion completed in 22.515894134s

• [SLOW TEST:27.156 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:05:22.552: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-8rwhw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 21 10:05:27.381: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8rwhw PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:05:27.381: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:05:27.784: INFO: Exec stderr: ""
Feb 21 10:05:27.784: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8rwhw PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:05:27.784: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:05:28.139: INFO: Exec stderr: ""
Feb 21 10:05:28.139: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8rwhw PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:05:28.139: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:05:28.567: INFO: Exec stderr: ""
Feb 21 10:05:28.567: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8rwhw PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:05:28.567: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:05:29.021: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 21 10:05:29.021: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8rwhw PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:05:29.021: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:05:29.605: INFO: Exec stderr: ""
Feb 21 10:05:29.605: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8rwhw PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:05:29.605: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:05:30.024: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 21 10:05:30.024: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8rwhw PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:05:30.024: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:05:30.354: INFO: Exec stderr: ""
Feb 21 10:05:30.354: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8rwhw PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:05:30.354: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:05:30.810: INFO: Exec stderr: ""
Feb 21 10:05:30.810: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8rwhw PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:05:30.810: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:05:31.227: INFO: Exec stderr: ""
Feb 21 10:05:31.227: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8rwhw PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:05:31.227: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:05:31.587: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:05:31.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-8rwhw" for this suite.
Feb 21 10:06:19.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:06:20.161: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-8rwhw, resource: bindings, ignored listing per whitelist
Feb 21 10:06:20.180: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-8rwhw deletion completed in 48.580726129s

• [SLOW TEST:57.628 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:06:20.180: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-m8k7m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-m8k7m
I0221 10:06:20.622070   32452 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-m8k7m, replica count: 1
I0221 10:06:21.672466   32452 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 10:06:22.672857   32452 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 10:06:22.787: INFO: Created: latency-svc-5z45q
Feb 21 10:06:22.794: INFO: Got endpoints: latency-svc-5z45q [21.500999ms]
Feb 21 10:06:22.808: INFO: Created: latency-svc-mc8vg
Feb 21 10:06:22.812: INFO: Got endpoints: latency-svc-mc8vg [18.072718ms]
Feb 21 10:06:22.818: INFO: Created: latency-svc-696st
Feb 21 10:06:22.830: INFO: Got endpoints: latency-svc-696st [36.091577ms]
Feb 21 10:06:22.831: INFO: Created: latency-svc-9nlj7
Feb 21 10:06:22.834: INFO: Got endpoints: latency-svc-9nlj7 [39.550634ms]
Feb 21 10:06:22.840: INFO: Created: latency-svc-7cp6d
Feb 21 10:06:22.844: INFO: Got endpoints: latency-svc-7cp6d [49.274541ms]
Feb 21 10:06:22.847: INFO: Created: latency-svc-hrpkm
Feb 21 10:06:22.864: INFO: Created: latency-svc-462p6
Feb 21 10:06:22.864: INFO: Created: latency-svc-wz8nt
Feb 21 10:06:22.864: INFO: Got endpoints: latency-svc-wz8nt [69.266908ms]
Feb 21 10:06:22.864: INFO: Got endpoints: latency-svc-hrpkm [69.278028ms]
Feb 21 10:06:22.871: INFO: Created: latency-svc-sgvj4
Feb 21 10:06:22.871: INFO: Got endpoints: latency-svc-462p6 [76.755864ms]
Feb 21 10:06:22.875: INFO: Got endpoints: latency-svc-sgvj4 [80.513349ms]
Feb 21 10:06:22.881: INFO: Created: latency-svc-qgdgb
Feb 21 10:06:22.885: INFO: Got endpoints: latency-svc-qgdgb [90.519351ms]
Feb 21 10:06:22.889: INFO: Created: latency-svc-jrtsl
Feb 21 10:06:22.894: INFO: Got endpoints: latency-svc-jrtsl [99.139147ms]
Feb 21 10:06:22.898: INFO: Created: latency-svc-lg8kl
Feb 21 10:06:22.904: INFO: Got endpoints: latency-svc-lg8kl [108.961864ms]
Feb 21 10:06:22.910: INFO: Created: latency-svc-jzx56
Feb 21 10:06:22.913: INFO: Got endpoints: latency-svc-jzx56 [118.698005ms]
Feb 21 10:06:22.922: INFO: Created: latency-svc-kzn95
Feb 21 10:06:22.925: INFO: Got endpoints: latency-svc-kzn95 [130.045184ms]
Feb 21 10:06:22.933: INFO: Created: latency-svc-92xkd
Feb 21 10:06:22.934: INFO: Got endpoints: latency-svc-92xkd [138.787358ms]
Feb 21 10:06:22.941: INFO: Created: latency-svc-j78bl
Feb 21 10:06:22.944: INFO: Got endpoints: latency-svc-j78bl [149.159995ms]
Feb 21 10:06:22.949: INFO: Created: latency-svc-jj5kx
Feb 21 10:06:22.951: INFO: Got endpoints: latency-svc-jj5kx [138.781398ms]
Feb 21 10:06:22.956: INFO: Created: latency-svc-bjtfn
Feb 21 10:06:22.958: INFO: Got endpoints: latency-svc-bjtfn [127.702207ms]
Feb 21 10:06:22.965: INFO: Created: latency-svc-hqtgl
Feb 21 10:06:22.969: INFO: Got endpoints: latency-svc-hqtgl [134.595358ms]
Feb 21 10:06:22.975: INFO: Created: latency-svc-9xgbl
Feb 21 10:06:22.977: INFO: Got endpoints: latency-svc-9xgbl [133.395078ms]
Feb 21 10:06:22.981: INFO: Created: latency-svc-sncdr
Feb 21 10:06:22.985: INFO: Got endpoints: latency-svc-sncdr [121.54385ms]
Feb 21 10:06:22.992: INFO: Created: latency-svc-gmsds
Feb 21 10:06:23.000: INFO: Got endpoints: latency-svc-gmsds [135.7483ms]
Feb 21 10:06:23.001: INFO: Created: latency-svc-qpchv
Feb 21 10:06:23.002: INFO: Got endpoints: latency-svc-qpchv [130.507773ms]
Feb 21 10:06:23.009: INFO: Created: latency-svc-96z5s
Feb 21 10:06:23.011: INFO: Got endpoints: latency-svc-96z5s [136.13591ms]
Feb 21 10:06:23.029: INFO: Created: latency-svc-9vdml
Feb 21 10:06:23.030: INFO: Got endpoints: latency-svc-9vdml [145.008876ms]
Feb 21 10:06:23.035: INFO: Created: latency-svc-94z26
Feb 21 10:06:23.039: INFO: Got endpoints: latency-svc-94z26 [144.769861ms]
Feb 21 10:06:23.041: INFO: Created: latency-svc-6rlts
Feb 21 10:06:23.046: INFO: Got endpoints: latency-svc-6rlts [132.710683ms]
Feb 21 10:06:23.048: INFO: Created: latency-svc-dwhlb
Feb 21 10:06:23.051: INFO: Got endpoints: latency-svc-dwhlb [147.790953ms]
Feb 21 10:06:23.057: INFO: Created: latency-svc-flx5z
Feb 21 10:06:23.060: INFO: Got endpoints: latency-svc-flx5z [135.549267ms]
Feb 21 10:06:23.065: INFO: Created: latency-svc-4wfgr
Feb 21 10:06:23.068: INFO: Got endpoints: latency-svc-4wfgr [134.5333ms]
Feb 21 10:06:23.073: INFO: Created: latency-svc-t5mbh
Feb 21 10:06:23.077: INFO: Got endpoints: latency-svc-t5mbh [132.824036ms]
Feb 21 10:06:23.083: INFO: Created: latency-svc-kd6cp
Feb 21 10:06:23.086: INFO: Got endpoints: latency-svc-kd6cp [134.890335ms]
Feb 21 10:06:23.091: INFO: Created: latency-svc-kpmx2
Feb 21 10:06:23.094: INFO: Got endpoints: latency-svc-kpmx2 [136.129016ms]
Feb 21 10:06:23.098: INFO: Created: latency-svc-46p25
Feb 21 10:06:23.102: INFO: Got endpoints: latency-svc-46p25 [133.545603ms]
Feb 21 10:06:23.106: INFO: Created: latency-svc-fvdh8
Feb 21 10:06:23.111: INFO: Got endpoints: latency-svc-fvdh8 [132.980564ms]
Feb 21 10:06:23.116: INFO: Created: latency-svc-s488s
Feb 21 10:06:23.124: INFO: Got endpoints: latency-svc-s488s [138.676968ms]
Feb 21 10:06:23.125: INFO: Created: latency-svc-ksb64
Feb 21 10:06:23.137: INFO: Created: latency-svc-r74g9
Feb 21 10:06:23.144: INFO: Got endpoints: latency-svc-ksb64 [142.478089ms]
Feb 21 10:06:23.145: INFO: Created: latency-svc-p9p4f
Feb 21 10:06:23.162: INFO: Created: latency-svc-29kgx
Feb 21 10:06:23.169: INFO: Created: latency-svc-lxz9d
Feb 21 10:06:23.178: INFO: Created: latency-svc-glxnh
Feb 21 10:06:23.193: INFO: Created: latency-svc-xms5w
Feb 21 10:06:23.196: INFO: Got endpoints: latency-svc-r74g9 [196.678281ms]
Feb 21 10:06:23.200: INFO: Created: latency-svc-pwk7z
Feb 21 10:06:23.212: INFO: Created: latency-svc-tdsnl
Feb 21 10:06:23.222: INFO: Created: latency-svc-bn9gl
Feb 21 10:06:23.224: INFO: Created: latency-svc-h59m4
Feb 21 10:06:23.249: INFO: Created: latency-svc-5pvqs
Feb 21 10:06:23.249: INFO: Got endpoints: latency-svc-p9p4f [237.414015ms]
Feb 21 10:06:23.254: INFO: Created: latency-svc-kbjw7
Feb 21 10:06:23.263: INFO: Created: latency-svc-g5jwl
Feb 21 10:06:23.269: INFO: Created: latency-svc-v88xr
Feb 21 10:06:23.297: INFO: Got endpoints: latency-svc-29kgx [266.417912ms]
Feb 21 10:06:23.297: INFO: Created: latency-svc-f5xzp
Feb 21 10:06:23.297: INFO: Created: latency-svc-576cs
Feb 21 10:06:23.297: INFO: Created: latency-svc-grwqv
Feb 21 10:06:23.312: INFO: Created: latency-svc-vfvpx
Feb 21 10:06:23.342: INFO: Got endpoints: latency-svc-lxz9d [303.179203ms]
Feb 21 10:06:23.360: INFO: Created: latency-svc-6wkd8
Feb 21 10:06:23.393: INFO: Got endpoints: latency-svc-glxnh [346.94044ms]
Feb 21 10:06:23.411: INFO: Created: latency-svc-9fwh8
Feb 21 10:06:23.442: INFO: Got endpoints: latency-svc-xms5w [390.373383ms]
Feb 21 10:06:23.462: INFO: Created: latency-svc-bmtgv
Feb 21 10:06:23.492: INFO: Got endpoints: latency-svc-pwk7z [431.307464ms]
Feb 21 10:06:23.511: INFO: Created: latency-svc-rt4rq
Feb 21 10:06:23.541: INFO: Got endpoints: latency-svc-tdsnl [473.056927ms]
Feb 21 10:06:23.561: INFO: Created: latency-svc-86sg8
Feb 21 10:06:23.592: INFO: Got endpoints: latency-svc-bn9gl [515.339938ms]
Feb 21 10:06:23.605: INFO: Created: latency-svc-dv9wz
Feb 21 10:06:23.643: INFO: Got endpoints: latency-svc-h59m4 [556.344629ms]
Feb 21 10:06:23.656: INFO: Created: latency-svc-2rwlx
Feb 21 10:06:23.693: INFO: Got endpoints: latency-svc-5pvqs [598.524214ms]
Feb 21 10:06:23.707: INFO: Created: latency-svc-srxjx
Feb 21 10:06:23.741: INFO: Got endpoints: latency-svc-kbjw7 [639.108322ms]
Feb 21 10:06:23.757: INFO: Created: latency-svc-zrb4s
Feb 21 10:06:23.791: INFO: Got endpoints: latency-svc-g5jwl [680.172167ms]
Feb 21 10:06:23.805: INFO: Created: latency-svc-pmfcm
Feb 21 10:06:23.849: INFO: Got endpoints: latency-svc-v88xr [724.634852ms]
Feb 21 10:06:23.863: INFO: Created: latency-svc-g2v44
Feb 21 10:06:23.891: INFO: Got endpoints: latency-svc-f5xzp [746.913495ms]
Feb 21 10:06:23.906: INFO: Created: latency-svc-rvn2t
Feb 21 10:06:23.942: INFO: Got endpoints: latency-svc-576cs [745.453784ms]
Feb 21 10:06:23.956: INFO: Created: latency-svc-hq84z
Feb 21 10:06:23.992: INFO: Got endpoints: latency-svc-grwqv [742.974995ms]
Feb 21 10:06:24.005: INFO: Created: latency-svc-x4btp
Feb 21 10:06:24.193: INFO: Got endpoints: latency-svc-vfvpx [896.584795ms]
Feb 21 10:06:24.194: INFO: Got endpoints: latency-svc-9fwh8 [800.955934ms]
Feb 21 10:06:24.194: INFO: Got endpoints: latency-svc-6wkd8 [852.131602ms]
Feb 21 10:06:24.194: INFO: Got endpoints: latency-svc-bmtgv [752.56501ms]
Feb 21 10:06:24.221: INFO: Created: latency-svc-vztft
Feb 21 10:06:24.260: INFO: Created: latency-svc-kldj7
Feb 21 10:06:24.260: INFO: Created: latency-svc-hlpxk
Feb 21 10:06:24.260: INFO: Created: latency-svc-mgd88
Feb 21 10:06:24.260: INFO: Got endpoints: latency-svc-rt4rq [767.990917ms]
Feb 21 10:06:24.292: INFO: Got endpoints: latency-svc-86sg8 [750.366069ms]
Feb 21 10:06:24.307: INFO: Created: latency-svc-dgvb5
Feb 21 10:06:24.316: INFO: Created: latency-svc-t4pgx
Feb 21 10:06:24.341: INFO: Got endpoints: latency-svc-dv9wz [749.290186ms]
Feb 21 10:06:24.444: INFO: Created: latency-svc-fpk2g
Feb 21 10:06:24.445: INFO: Got endpoints: latency-svc-srxjx [751.462126ms]
Feb 21 10:06:24.445: INFO: Got endpoints: latency-svc-2rwlx [801.920958ms]
Feb 21 10:06:24.466: INFO: Created: latency-svc-9jtnf
Feb 21 10:06:24.476: INFO: Created: latency-svc-hb25d
Feb 21 10:06:24.494: INFO: Got endpoints: latency-svc-zrb4s [752.430622ms]
Feb 21 10:06:24.510: INFO: Created: latency-svc-727wf
Feb 21 10:06:24.542: INFO: Got endpoints: latency-svc-pmfcm [750.996574ms]
Feb 21 10:06:24.559: INFO: Created: latency-svc-d5zsw
Feb 21 10:06:24.592: INFO: Got endpoints: latency-svc-g2v44 [743.242868ms]
Feb 21 10:06:24.609: INFO: Created: latency-svc-8k8w5
Feb 21 10:06:24.642: INFO: Got endpoints: latency-svc-rvn2t [750.520084ms]
Feb 21 10:06:24.660: INFO: Created: latency-svc-vt5wk
Feb 21 10:06:24.692: INFO: Got endpoints: latency-svc-hq84z [749.668702ms]
Feb 21 10:06:24.715: INFO: Created: latency-svc-9425p
Feb 21 10:06:24.754: INFO: Got endpoints: latency-svc-x4btp [762.248712ms]
Feb 21 10:06:24.771: INFO: Created: latency-svc-j9gzx
Feb 21 10:06:24.791: INFO: Got endpoints: latency-svc-vztft [597.771313ms]
Feb 21 10:06:24.813: INFO: Created: latency-svc-hlbxk
Feb 21 10:06:24.841: INFO: Got endpoints: latency-svc-mgd88 [646.798772ms]
Feb 21 10:06:24.856: INFO: Created: latency-svc-wrnbz
Feb 21 10:06:24.892: INFO: Got endpoints: latency-svc-hlpxk [697.640568ms]
Feb 21 10:06:24.908: INFO: Created: latency-svc-9kfct
Feb 21 10:06:24.942: INFO: Got endpoints: latency-svc-kldj7 [747.368624ms]
Feb 21 10:06:24.958: INFO: Created: latency-svc-rf7gm
Feb 21 10:06:24.992: INFO: Got endpoints: latency-svc-dgvb5 [732.247394ms]
Feb 21 10:06:25.011: INFO: Created: latency-svc-7pw2x
Feb 21 10:06:25.043: INFO: Got endpoints: latency-svc-t4pgx [751.685275ms]
Feb 21 10:06:25.057: INFO: Created: latency-svc-ddr7s
Feb 21 10:06:25.091: INFO: Got endpoints: latency-svc-fpk2g [749.536277ms]
Feb 21 10:06:25.115: INFO: Created: latency-svc-xqclf
Feb 21 10:06:25.142: INFO: Got endpoints: latency-svc-9jtnf [697.164747ms]
Feb 21 10:06:25.161: INFO: Created: latency-svc-cqpjw
Feb 21 10:06:25.193: INFO: Got endpoints: latency-svc-hb25d [748.213656ms]
Feb 21 10:06:25.210: INFO: Created: latency-svc-lxtdt
Feb 21 10:06:25.243: INFO: Got endpoints: latency-svc-727wf [748.820601ms]
Feb 21 10:06:25.262: INFO: Created: latency-svc-gqbtc
Feb 21 10:06:25.292: INFO: Got endpoints: latency-svc-d5zsw [749.200226ms]
Feb 21 10:06:25.309: INFO: Created: latency-svc-pv6t7
Feb 21 10:06:25.342: INFO: Got endpoints: latency-svc-8k8w5 [749.229984ms]
Feb 21 10:06:25.362: INFO: Created: latency-svc-tjlf9
Feb 21 10:06:25.390: INFO: Got endpoints: latency-svc-vt5wk [747.724216ms]
Feb 21 10:06:25.405: INFO: Created: latency-svc-q8grw
Feb 21 10:06:25.442: INFO: Got endpoints: latency-svc-9425p [750.011442ms]
Feb 21 10:06:25.460: INFO: Created: latency-svc-px5x7
Feb 21 10:06:25.493: INFO: Got endpoints: latency-svc-j9gzx [739.005308ms]
Feb 21 10:06:25.512: INFO: Created: latency-svc-k6kt8
Feb 21 10:06:25.544: INFO: Got endpoints: latency-svc-hlbxk [752.337374ms]
Feb 21 10:06:25.563: INFO: Created: latency-svc-6pxpk
Feb 21 10:06:25.592: INFO: Got endpoints: latency-svc-wrnbz [750.913266ms]
Feb 21 10:06:25.610: INFO: Created: latency-svc-wv54h
Feb 21 10:06:25.643: INFO: Got endpoints: latency-svc-9kfct [751.410985ms]
Feb 21 10:06:25.666: INFO: Created: latency-svc-wh52q
Feb 21 10:06:25.694: INFO: Got endpoints: latency-svc-rf7gm [751.920375ms]
Feb 21 10:06:25.712: INFO: Created: latency-svc-nf2xj
Feb 21 10:06:25.742: INFO: Got endpoints: latency-svc-7pw2x [749.361138ms]
Feb 21 10:06:25.761: INFO: Created: latency-svc-frkpv
Feb 21 10:06:25.792: INFO: Got endpoints: latency-svc-ddr7s [749.036233ms]
Feb 21 10:06:25.807: INFO: Created: latency-svc-876p4
Feb 21 10:06:25.842: INFO: Got endpoints: latency-svc-xqclf [750.809841ms]
Feb 21 10:06:25.861: INFO: Created: latency-svc-9zsrr
Feb 21 10:06:25.891: INFO: Got endpoints: latency-svc-cqpjw [749.362975ms]
Feb 21 10:06:25.907: INFO: Created: latency-svc-lx94w
Feb 21 10:06:25.944: INFO: Got endpoints: latency-svc-lxtdt [751.293588ms]
Feb 21 10:06:25.961: INFO: Created: latency-svc-kll99
Feb 21 10:06:25.992: INFO: Got endpoints: latency-svc-gqbtc [748.715019ms]
Feb 21 10:06:26.097: INFO: Created: latency-svc-dpsmg
Feb 21 10:06:26.097: INFO: Got endpoints: latency-svc-pv6t7 [805.123966ms]
Feb 21 10:06:26.097: INFO: Got endpoints: latency-svc-tjlf9 [755.221467ms]
Feb 21 10:06:26.118: INFO: Created: latency-svc-grz7f
Feb 21 10:06:26.124: INFO: Created: latency-svc-2x2b5
Feb 21 10:06:26.146: INFO: Got endpoints: latency-svc-q8grw [756.017931ms]
Feb 21 10:06:26.161: INFO: Created: latency-svc-dmv6k
Feb 21 10:06:26.195: INFO: Got endpoints: latency-svc-px5x7 [753.104389ms]
Feb 21 10:06:26.217: INFO: Created: latency-svc-b4mbw
Feb 21 10:06:26.244: INFO: Got endpoints: latency-svc-k6kt8 [750.88731ms]
Feb 21 10:06:26.265: INFO: Created: latency-svc-z4l79
Feb 21 10:06:26.296: INFO: Got endpoints: latency-svc-6pxpk [752.356058ms]
Feb 21 10:06:26.309: INFO: Created: latency-svc-sxtfs
Feb 21 10:06:26.342: INFO: Got endpoints: latency-svc-wv54h [749.844369ms]
Feb 21 10:06:26.358: INFO: Created: latency-svc-88wmg
Feb 21 10:06:26.393: INFO: Got endpoints: latency-svc-wh52q [749.559873ms]
Feb 21 10:06:26.406: INFO: Created: latency-svc-tfp6g
Feb 21 10:06:26.443: INFO: Got endpoints: latency-svc-nf2xj [748.936941ms]
Feb 21 10:06:26.457: INFO: Created: latency-svc-zv7sf
Feb 21 10:06:26.491: INFO: Got endpoints: latency-svc-frkpv [748.997021ms]
Feb 21 10:06:26.505: INFO: Created: latency-svc-ckkpx
Feb 21 10:06:26.544: INFO: Got endpoints: latency-svc-876p4 [751.232027ms]
Feb 21 10:06:26.558: INFO: Created: latency-svc-69wbv
Feb 21 10:06:26.592: INFO: Got endpoints: latency-svc-9zsrr [749.891992ms]
Feb 21 10:06:26.606: INFO: Created: latency-svc-57jqk
Feb 21 10:06:26.641: INFO: Got endpoints: latency-svc-lx94w [749.965421ms]
Feb 21 10:06:26.655: INFO: Created: latency-svc-x7zn9
Feb 21 10:06:26.696: INFO: Got endpoints: latency-svc-kll99 [752.059477ms]
Feb 21 10:06:26.708: INFO: Created: latency-svc-695p4
Feb 21 10:06:26.743: INFO: Got endpoints: latency-svc-dpsmg [751.012102ms]
Feb 21 10:06:26.759: INFO: Created: latency-svc-7qdf8
Feb 21 10:06:26.792: INFO: Got endpoints: latency-svc-grz7f [694.789709ms]
Feb 21 10:06:26.809: INFO: Created: latency-svc-79nsn
Feb 21 10:06:26.842: INFO: Got endpoints: latency-svc-2x2b5 [745.448811ms]
Feb 21 10:06:26.856: INFO: Created: latency-svc-4hfkd
Feb 21 10:06:26.891: INFO: Got endpoints: latency-svc-dmv6k [744.835316ms]
Feb 21 10:06:26.907: INFO: Created: latency-svc-4lk4l
Feb 21 10:06:26.943: INFO: Got endpoints: latency-svc-b4mbw [747.991692ms]
Feb 21 10:06:26.959: INFO: Created: latency-svc-88pbq
Feb 21 10:06:26.991: INFO: Got endpoints: latency-svc-z4l79 [746.999823ms]
Feb 21 10:06:27.015: INFO: Created: latency-svc-p8gs9
Feb 21 10:06:27.042: INFO: Got endpoints: latency-svc-sxtfs [746.095899ms]
Feb 21 10:06:27.061: INFO: Created: latency-svc-5jpwm
Feb 21 10:06:27.093: INFO: Got endpoints: latency-svc-88wmg [750.427761ms]
Feb 21 10:06:27.108: INFO: Created: latency-svc-v7crz
Feb 21 10:06:27.142: INFO: Got endpoints: latency-svc-tfp6g [748.687902ms]
Feb 21 10:06:27.158: INFO: Created: latency-svc-dltr9
Feb 21 10:06:27.191: INFO: Got endpoints: latency-svc-zv7sf [748.35855ms]
Feb 21 10:06:27.208: INFO: Created: latency-svc-zh5qr
Feb 21 10:06:27.243: INFO: Got endpoints: latency-svc-ckkpx [751.907962ms]
Feb 21 10:06:27.260: INFO: Created: latency-svc-mfxl8
Feb 21 10:06:27.307: INFO: Got endpoints: latency-svc-69wbv [763.396255ms]
Feb 21 10:06:27.328: INFO: Created: latency-svc-klglt
Feb 21 10:06:27.342: INFO: Got endpoints: latency-svc-57jqk [750.165977ms]
Feb 21 10:06:27.357: INFO: Created: latency-svc-pcskc
Feb 21 10:06:27.393: INFO: Got endpoints: latency-svc-x7zn9 [751.615701ms]
Feb 21 10:06:27.410: INFO: Created: latency-svc-lwklw
Feb 21 10:06:27.442: INFO: Got endpoints: latency-svc-695p4 [745.548138ms]
Feb 21 10:06:27.465: INFO: Created: latency-svc-lxjsn
Feb 21 10:06:27.492: INFO: Got endpoints: latency-svc-7qdf8 [749.512343ms]
Feb 21 10:06:27.511: INFO: Created: latency-svc-p2878
Feb 21 10:06:27.544: INFO: Got endpoints: latency-svc-79nsn [752.516571ms]
Feb 21 10:06:27.565: INFO: Created: latency-svc-72gxs
Feb 21 10:06:27.592: INFO: Got endpoints: latency-svc-4hfkd [749.543553ms]
Feb 21 10:06:27.611: INFO: Created: latency-svc-hxc44
Feb 21 10:06:27.642: INFO: Got endpoints: latency-svc-4lk4l [750.490923ms]
Feb 21 10:06:27.664: INFO: Created: latency-svc-sc9vw
Feb 21 10:06:27.691: INFO: Got endpoints: latency-svc-88pbq [748.310916ms]
Feb 21 10:06:27.713: INFO: Created: latency-svc-z8dgq
Feb 21 10:06:27.742: INFO: Got endpoints: latency-svc-p8gs9 [750.112673ms]
Feb 21 10:06:27.761: INFO: Created: latency-svc-psl76
Feb 21 10:06:27.791: INFO: Got endpoints: latency-svc-5jpwm [748.39439ms]
Feb 21 10:06:27.809: INFO: Created: latency-svc-rbkrs
Feb 21 10:06:27.844: INFO: Got endpoints: latency-svc-v7crz [751.324825ms]
Feb 21 10:06:27.869: INFO: Created: latency-svc-6p866
Feb 21 10:06:27.891: INFO: Got endpoints: latency-svc-dltr9 [749.207537ms]
Feb 21 10:06:27.906: INFO: Created: latency-svc-dpr5x
Feb 21 10:06:27.943: INFO: Got endpoints: latency-svc-zh5qr [751.752159ms]
Feb 21 10:06:27.960: INFO: Created: latency-svc-ldllf
Feb 21 10:06:27.991: INFO: Got endpoints: latency-svc-mfxl8 [748.393684ms]
Feb 21 10:06:28.013: INFO: Created: latency-svc-r92s4
Feb 21 10:06:28.041: INFO: Got endpoints: latency-svc-klglt [733.755947ms]
Feb 21 10:06:28.063: INFO: Created: latency-svc-26wgs
Feb 21 10:06:28.123: INFO: Got endpoints: latency-svc-pcskc [780.975258ms]
Feb 21 10:06:28.143: INFO: Got endpoints: latency-svc-lwklw [750.407758ms]
Feb 21 10:06:28.144: INFO: Created: latency-svc-sd8ls
Feb 21 10:06:28.225: INFO: Got endpoints: latency-svc-lxjsn [782.832831ms]
Feb 21 10:06:28.229: INFO: Created: latency-svc-m2d7r
Feb 21 10:06:28.242: INFO: Got endpoints: latency-svc-p2878 [749.43893ms]
Feb 21 10:06:28.244: INFO: Created: latency-svc-p4z6j
Feb 21 10:06:28.267: INFO: Created: latency-svc-8xgrb
Feb 21 10:06:28.322: INFO: Got endpoints: latency-svc-72gxs [777.388422ms]
Feb 21 10:06:28.336: INFO: Created: latency-svc-djs9l
Feb 21 10:06:28.341: INFO: Got endpoints: latency-svc-hxc44 [749.275585ms]
Feb 21 10:06:28.365: INFO: Created: latency-svc-gznbc
Feb 21 10:06:28.399: INFO: Got endpoints: latency-svc-sc9vw [757.154726ms]
Feb 21 10:06:28.414: INFO: Created: latency-svc-25srw
Feb 21 10:06:28.441: INFO: Got endpoints: latency-svc-z8dgq [749.692953ms]
Feb 21 10:06:28.459: INFO: Created: latency-svc-j8bsg
Feb 21 10:06:28.497: INFO: Got endpoints: latency-svc-psl76 [755.011384ms]
Feb 21 10:06:28.513: INFO: Created: latency-svc-h9gsp
Feb 21 10:06:28.542: INFO: Got endpoints: latency-svc-rbkrs [750.654515ms]
Feb 21 10:06:28.555: INFO: Created: latency-svc-g8sm7
Feb 21 10:06:28.591: INFO: Got endpoints: latency-svc-6p866 [747.346931ms]
Feb 21 10:06:28.604: INFO: Created: latency-svc-v4l4m
Feb 21 10:06:28.646: INFO: Got endpoints: latency-svc-dpr5x [754.563205ms]
Feb 21 10:06:28.662: INFO: Created: latency-svc-5p2g8
Feb 21 10:06:28.700: INFO: Got endpoints: latency-svc-ldllf [756.904235ms]
Feb 21 10:06:28.727: INFO: Created: latency-svc-sgjw9
Feb 21 10:06:28.741: INFO: Got endpoints: latency-svc-r92s4 [750.300031ms]
Feb 21 10:06:28.760: INFO: Created: latency-svc-4pqr4
Feb 21 10:06:28.792: INFO: Got endpoints: latency-svc-26wgs [750.933378ms]
Feb 21 10:06:28.820: INFO: Created: latency-svc-vsgmb
Feb 21 10:06:28.843: INFO: Got endpoints: latency-svc-sd8ls [719.502972ms]
Feb 21 10:06:28.859: INFO: Created: latency-svc-bcmmc
Feb 21 10:06:28.891: INFO: Got endpoints: latency-svc-m2d7r [747.762695ms]
Feb 21 10:06:28.927: INFO: Created: latency-svc-c9hhj
Feb 21 10:06:28.943: INFO: Got endpoints: latency-svc-p4z6j [718.209993ms]
Feb 21 10:06:28.961: INFO: Created: latency-svc-9b7jc
Feb 21 10:06:28.991: INFO: Got endpoints: latency-svc-8xgrb [749.447961ms]
Feb 21 10:06:29.095: INFO: Created: latency-svc-bmfkh
Feb 21 10:06:29.095: INFO: Got endpoints: latency-svc-djs9l [773.314741ms]
Feb 21 10:06:29.095: INFO: Got endpoints: latency-svc-gznbc [753.761585ms]
Feb 21 10:06:29.118: INFO: Created: latency-svc-58v58
Feb 21 10:06:29.121: INFO: Created: latency-svc-xp4x4
Feb 21 10:06:29.142: INFO: Got endpoints: latency-svc-25srw [742.950479ms]
Feb 21 10:06:29.161: INFO: Created: latency-svc-wj7rq
Feb 21 10:06:29.192: INFO: Got endpoints: latency-svc-j8bsg [750.994114ms]
Feb 21 10:06:29.209: INFO: Created: latency-svc-8qwrl
Feb 21 10:06:29.241: INFO: Got endpoints: latency-svc-h9gsp [744.243045ms]
Feb 21 10:06:29.258: INFO: Created: latency-svc-fs2kj
Feb 21 10:06:29.292: INFO: Got endpoints: latency-svc-g8sm7 [750.542036ms]
Feb 21 10:06:29.306: INFO: Created: latency-svc-qnqqq
Feb 21 10:06:29.343: INFO: Got endpoints: latency-svc-v4l4m [750.999719ms]
Feb 21 10:06:29.355: INFO: Created: latency-svc-9ctjw
Feb 21 10:06:29.391: INFO: Got endpoints: latency-svc-5p2g8 [745.314171ms]
Feb 21 10:06:29.405: INFO: Created: latency-svc-5v2r8
Feb 21 10:06:29.442: INFO: Got endpoints: latency-svc-sgjw9 [741.880442ms]
Feb 21 10:06:29.456: INFO: Created: latency-svc-6dpss
Feb 21 10:06:29.491: INFO: Got endpoints: latency-svc-4pqr4 [749.628093ms]
Feb 21 10:06:29.503: INFO: Created: latency-svc-lxm5g
Feb 21 10:06:29.542: INFO: Got endpoints: latency-svc-vsgmb [749.861596ms]
Feb 21 10:06:29.556: INFO: Created: latency-svc-m7vhq
Feb 21 10:06:29.591: INFO: Got endpoints: latency-svc-bcmmc [748.353231ms]
Feb 21 10:06:29.605: INFO: Created: latency-svc-mp22f
Feb 21 10:06:29.642: INFO: Got endpoints: latency-svc-c9hhj [750.578585ms]
Feb 21 10:06:29.659: INFO: Created: latency-svc-qb447
Feb 21 10:06:29.693: INFO: Got endpoints: latency-svc-9b7jc [749.501464ms]
Feb 21 10:06:29.716: INFO: Created: latency-svc-274zk
Feb 21 10:06:29.741: INFO: Got endpoints: latency-svc-bmfkh [749.68474ms]
Feb 21 10:06:29.763: INFO: Created: latency-svc-mvqfh
Feb 21 10:06:29.791: INFO: Got endpoints: latency-svc-58v58 [696.324747ms]
Feb 21 10:06:29.807: INFO: Created: latency-svc-568hc
Feb 21 10:06:30.210: INFO: Got endpoints: latency-svc-fs2kj [968.726334ms]
Feb 21 10:06:30.210: INFO: Got endpoints: latency-svc-8qwrl [1.017734351s]
Feb 21 10:06:30.210: INFO: Got endpoints: latency-svc-xp4x4 [1.114884358s]
Feb 21 10:06:30.210: INFO: Got endpoints: latency-svc-wj7rq [1.068053445s]
Feb 21 10:06:30.211: INFO: Got endpoints: latency-svc-6dpss [769.069794ms]
Feb 21 10:06:30.211: INFO: Got endpoints: latency-svc-qnqqq [919.204877ms]
Feb 21 10:06:30.211: INFO: Got endpoints: latency-svc-9ctjw [868.818202ms]
Feb 21 10:06:30.212: INFO: Got endpoints: latency-svc-5v2r8 [820.474428ms]
Feb 21 10:06:30.235: INFO: Created: latency-svc-nw742
Feb 21 10:06:30.239: INFO: Created: latency-svc-rp4x7
Feb 21 10:06:30.244: INFO: Got endpoints: latency-svc-lxm5g [752.607767ms]
Feb 21 10:06:30.248: INFO: Created: latency-svc-2l2lr
Feb 21 10:06:30.260: INFO: Created: latency-svc-kc9rc
Feb 21 10:06:30.267: INFO: Created: latency-svc-hmf6b
Feb 21 10:06:30.283: INFO: Created: latency-svc-t9szs
Feb 21 10:06:30.291: INFO: Created: latency-svc-k2wnk
Feb 21 10:06:30.295: INFO: Got endpoints: latency-svc-m7vhq [753.330141ms]
Feb 21 10:06:30.299: INFO: Created: latency-svc-52s5b
Feb 21 10:06:30.307: INFO: Created: latency-svc-9w52v
Feb 21 10:06:30.315: INFO: Created: latency-svc-wwrvx
Feb 21 10:06:30.341: INFO: Got endpoints: latency-svc-mp22f [749.571947ms]
Feb 21 10:06:30.363: INFO: Created: latency-svc-m2wj8
Feb 21 10:06:30.391: INFO: Got endpoints: latency-svc-qb447 [749.403612ms]
Feb 21 10:06:30.408: INFO: Created: latency-svc-zw5xx
Feb 21 10:06:30.443: INFO: Got endpoints: latency-svc-274zk [750.144834ms]
Feb 21 10:06:30.459: INFO: Created: latency-svc-wmzqg
Feb 21 10:06:30.492: INFO: Got endpoints: latency-svc-mvqfh [751.016114ms]
Feb 21 10:06:30.508: INFO: Created: latency-svc-4b6sm
Feb 21 10:06:30.541: INFO: Got endpoints: latency-svc-568hc [749.90206ms]
Feb 21 10:06:30.555: INFO: Created: latency-svc-rgrvw
Feb 21 10:06:30.591: INFO: Got endpoints: latency-svc-nw742 [381.155238ms]
Feb 21 10:06:30.798: INFO: Got endpoints: latency-svc-hmf6b [588.158394ms]
Feb 21 10:06:30.798: INFO: Got endpoints: latency-svc-kc9rc [586.954901ms]
Feb 21 10:06:30.798: INFO: Got endpoints: latency-svc-rp4x7 [588.057688ms]
Feb 21 10:06:30.798: INFO: Got endpoints: latency-svc-2l2lr [588.32644ms]
Feb 21 10:06:30.817: INFO: Created: latency-svc-xcm2z
Feb 21 10:06:30.842: INFO: Got endpoints: latency-svc-t9szs [630.364448ms]
Feb 21 10:06:31.044: INFO: Got endpoints: latency-svc-wwrvx [748.816056ms]
Feb 21 10:06:31.044: INFO: Got endpoints: latency-svc-9w52v [800.591601ms]
Feb 21 10:06:31.044: INFO: Got endpoints: latency-svc-k2wnk [832.824589ms]
Feb 21 10:06:31.044: INFO: Got endpoints: latency-svc-52s5b [832.892123ms]
Feb 21 10:06:31.093: INFO: Got endpoints: latency-svc-m2wj8 [752.387836ms]
Feb 21 10:06:31.142: INFO: Got endpoints: latency-svc-zw5xx [750.519637ms]
Feb 21 10:06:31.192: INFO: Got endpoints: latency-svc-wmzqg [748.359616ms]
Feb 21 10:06:31.242: INFO: Got endpoints: latency-svc-4b6sm [749.969094ms]
Feb 21 10:06:31.292: INFO: Got endpoints: latency-svc-rgrvw [750.003846ms]
Feb 21 10:06:31.342: INFO: Got endpoints: latency-svc-xcm2z [750.550115ms]
Feb 21 10:06:31.342: INFO: Latencies: [18.072718ms 36.091577ms 39.550634ms 49.274541ms 69.266908ms 69.278028ms 76.755864ms 80.513349ms 90.519351ms 99.139147ms 108.961864ms 118.698005ms 121.54385ms 127.702207ms 130.045184ms 130.507773ms 132.710683ms 132.824036ms 132.980564ms 133.395078ms 133.545603ms 134.5333ms 134.595358ms 134.890335ms 135.549267ms 135.7483ms 136.129016ms 136.13591ms 138.676968ms 138.781398ms 138.787358ms 142.478089ms 144.769861ms 145.008876ms 147.790953ms 149.159995ms 196.678281ms 237.414015ms 266.417912ms 303.179203ms 346.94044ms 381.155238ms 390.373383ms 431.307464ms 473.056927ms 515.339938ms 556.344629ms 586.954901ms 588.057688ms 588.158394ms 588.32644ms 597.771313ms 598.524214ms 630.364448ms 639.108322ms 646.798772ms 680.172167ms 694.789709ms 696.324747ms 697.164747ms 697.640568ms 718.209993ms 719.502972ms 724.634852ms 732.247394ms 733.755947ms 739.005308ms 741.880442ms 742.950479ms 742.974995ms 743.242868ms 744.243045ms 744.835316ms 745.314171ms 745.448811ms 745.453784ms 745.548138ms 746.095899ms 746.913495ms 746.999823ms 747.346931ms 747.368624ms 747.724216ms 747.762695ms 747.991692ms 748.213656ms 748.310916ms 748.353231ms 748.35855ms 748.359616ms 748.393684ms 748.39439ms 748.687902ms 748.715019ms 748.816056ms 748.820601ms 748.936941ms 748.997021ms 749.036233ms 749.200226ms 749.207537ms 749.229984ms 749.275585ms 749.290186ms 749.361138ms 749.362975ms 749.403612ms 749.43893ms 749.447961ms 749.501464ms 749.512343ms 749.536277ms 749.543553ms 749.559873ms 749.571947ms 749.628093ms 749.668702ms 749.68474ms 749.692953ms 749.844369ms 749.861596ms 749.891992ms 749.90206ms 749.965421ms 749.969094ms 750.003846ms 750.011442ms 750.112673ms 750.144834ms 750.165977ms 750.300031ms 750.366069ms 750.407758ms 750.427761ms 750.490923ms 750.519637ms 750.520084ms 750.542036ms 750.550115ms 750.578585ms 750.654515ms 750.809841ms 750.88731ms 750.913266ms 750.933378ms 750.994114ms 750.996574ms 750.999719ms 751.012102ms 751.016114ms 751.232027ms 751.293588ms 751.324825ms 751.410985ms 751.462126ms 751.615701ms 751.685275ms 751.752159ms 751.907962ms 751.920375ms 752.059477ms 752.337374ms 752.356058ms 752.387836ms 752.430622ms 752.516571ms 752.56501ms 752.607767ms 753.104389ms 753.330141ms 753.761585ms 754.563205ms 755.011384ms 755.221467ms 756.017931ms 756.904235ms 757.154726ms 762.248712ms 763.396255ms 767.990917ms 769.069794ms 773.314741ms 777.388422ms 780.975258ms 782.832831ms 800.591601ms 800.955934ms 801.920958ms 805.123966ms 820.474428ms 832.824589ms 832.892123ms 852.131602ms 868.818202ms 896.584795ms 919.204877ms 968.726334ms 1.017734351s 1.068053445s 1.114884358s]
Feb 21 10:06:31.342: INFO: 50 %ile: 749.207537ms
Feb 21 10:06:31.342: INFO: 90 %ile: 769.069794ms
Feb 21 10:06:31.342: INFO: 99 %ile: 1.068053445s
Feb 21 10:06:31.342: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:06:31.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-m8k7m" for this suite.
Feb 21 10:06:49.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:06:49.409: INFO: namespace: e2e-tests-svc-latency-m8k7m, resource: bindings, ignored listing per whitelist
Feb 21 10:06:49.605: INFO: namespace e2e-tests-svc-latency-m8k7m deletion completed in 18.250530679s

• [SLOW TEST:29.425 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:06:49.605: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-h85sk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
STEP: creating an rc
Feb 21 10:06:50.220: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-h85sk'
Feb 21 10:06:51.143: INFO: stderr: ""
Feb 21 10:06:51.143: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 21 10:06:52.149: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 10:06:52.149: INFO: Found 0 / 1
Feb 21 10:06:53.149: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 10:06:53.149: INFO: Found 1 / 1
Feb 21 10:06:53.149: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 21 10:06:53.154: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 10:06:53.154: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 21 10:06:53.154: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs redis-master-z65pv redis-master --namespace=e2e-tests-kubectl-h85sk'
Feb 21 10:06:53.503: INFO: stderr: ""
Feb 21 10:06:53.503: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Feb 10:06:52.116 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Feb 10:06:52.116 # Server started, Redis version 3.2.12\n1:M 21 Feb 10:06:52.116 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Feb 10:06:52.116 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 21 10:06:53.503: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-z65pv redis-master --namespace=e2e-tests-kubectl-h85sk --tail=1'
Feb 21 10:06:53.644: INFO: stderr: ""
Feb 21 10:06:53.644: INFO: stdout: "1:M 21 Feb 10:06:52.116 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 21 10:06:53.644: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-z65pv redis-master --namespace=e2e-tests-kubectl-h85sk --limit-bytes=1'
Feb 21 10:06:53.766: INFO: stderr: ""
Feb 21 10:06:53.766: INFO: stdout: " "
STEP: exposing timestamps
Feb 21 10:06:53.766: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-z65pv redis-master --namespace=e2e-tests-kubectl-h85sk --tail=1 --timestamps'
Feb 21 10:06:53.919: INFO: stderr: ""
Feb 21 10:06:53.919: INFO: stdout: "2019-02-21T10:06:52.116878842Z 1:M 21 Feb 10:06:52.116 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 21 10:06:56.420: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-z65pv redis-master --namespace=e2e-tests-kubectl-h85sk --since=1s'
Feb 21 10:06:56.571: INFO: stderr: ""
Feb 21 10:06:56.571: INFO: stdout: ""
Feb 21 10:06:56.571: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-z65pv redis-master --namespace=e2e-tests-kubectl-h85sk --since=24h'
Feb 21 10:06:56.711: INFO: stderr: ""
Feb 21 10:06:56.711: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Feb 10:06:52.116 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Feb 10:06:52.116 # Server started, Redis version 3.2.12\n1:M 21 Feb 10:06:52.116 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Feb 10:06:52.116 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1088
STEP: using delete to clean up resources
Feb 21 10:06:56.712: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-h85sk'
Feb 21 10:06:56.819: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 10:06:56.819: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 21 10:06:56.819: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-h85sk'
Feb 21 10:06:56.945: INFO: stderr: "No resources found.\n"
Feb 21 10:06:56.945: INFO: stdout: ""
Feb 21 10:06:56.945: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=nginx --namespace=e2e-tests-kubectl-h85sk -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 10:06:57.053: INFO: stderr: ""
Feb 21 10:06:57.053: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:06:57.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-h85sk" for this suite.
Feb 21 10:07:19.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:07:19.293: INFO: namespace: e2e-tests-kubectl-h85sk, resource: bindings, ignored listing per whitelist
Feb 21 10:07:19.765: INFO: namespace e2e-tests-kubectl-h85sk deletion completed in 22.703646795s

• [SLOW TEST:30.160 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:07:19.766: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-zq2pf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 10:07:20.627: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79dea9f4-35c0-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-zq2pf" to be "success or failure"
Feb 21 10:07:20.633: INFO: Pod "downwardapi-volume-79dea9f4-35c0-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.492101ms
Feb 21 10:07:22.641: INFO: Pod "downwardapi-volume-79dea9f4-35c0-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013820334s
STEP: Saw pod success
Feb 21 10:07:22.641: INFO: Pod "downwardapi-volume-79dea9f4-35c0-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:07:22.645: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-79dea9f4-35c0-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 10:07:22.670: INFO: Waiting for pod downwardapi-volume-79dea9f4-35c0-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:07:22.673: INFO: Pod downwardapi-volume-79dea9f4-35c0-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:07:22.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zq2pf" for this suite.
Feb 21 10:07:28.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:07:28.755: INFO: namespace: e2e-tests-projected-zq2pf, resource: bindings, ignored listing per whitelist
Feb 21 10:07:29.137: INFO: namespace e2e-tests-projected-zq2pf deletion completed in 6.45654086s

• [SLOW TEST:9.371 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:07:29.137: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-mcmww
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-7f35a53c-35c0-11e9-b7c5-a2b84e263bfe
STEP: Creating configMap with name cm-test-opt-upd-7f35a581-35c0-11e9-b7c5-a2b84e263bfe
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7f35a53c-35c0-11e9-b7c5-a2b84e263bfe
STEP: Updating configmap cm-test-opt-upd-7f35a581-35c0-11e9-b7c5-a2b84e263bfe
STEP: Creating configMap with name cm-test-opt-create-7f35a59b-35c0-11e9-b7c5-a2b84e263bfe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:07:34.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mcmww" for this suite.
Feb 21 10:07:56.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:07:56.291: INFO: namespace: e2e-tests-configmap-mcmww, resource: bindings, ignored listing per whitelist
Feb 21 10:07:56.386: INFO: namespace e2e-tests-configmap-mcmww deletion completed in 22.311987535s

• [SLOW TEST:27.248 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:07:56.387: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-p2cmk
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-8f923d60-35c0-11e9-b7c5-a2b84e263bfe
STEP: Creating secret with name s-test-opt-upd-8f923dad-35c0-11e9-b7c5-a2b84e263bfe
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8f923d60-35c0-11e9-b7c5-a2b84e263bfe
STEP: Updating secret s-test-opt-upd-8f923dad-35c0-11e9-b7c5-a2b84e263bfe
STEP: Creating secret with name s-test-opt-create-8f923dcf-35c0-11e9-b7c5-a2b84e263bfe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:08:01.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-p2cmk" for this suite.
Feb 21 10:08:23.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:08:23.897: INFO: namespace: e2e-tests-secrets-p2cmk, resource: bindings, ignored listing per whitelist
Feb 21 10:08:23.972: INFO: namespace e2e-tests-secrets-p2cmk deletion completed in 22.226589069s

• [SLOW TEST:27.586 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:08:23.972: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rtn8w
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 21 10:08:24.521: INFO: Waiting up to 5m0s for pod "downward-api-9ff4cb67-35c0-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-rtn8w" to be "success or failure"
Feb 21 10:08:24.525: INFO: Pod "downward-api-9ff4cb67-35c0-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.812859ms
Feb 21 10:08:26.532: INFO: Pod "downward-api-9ff4cb67-35c0-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010354488s
STEP: Saw pod success
Feb 21 10:08:26.532: INFO: Pod "downward-api-9ff4cb67-35c0-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:08:26.537: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downward-api-9ff4cb67-35c0-11e9-b7c5-a2b84e263bfe container dapi-container: <nil>
STEP: delete the pod
Feb 21 10:08:26.588: INFO: Waiting for pod downward-api-9ff4cb67-35c0-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:08:26.593: INFO: Pod downward-api-9ff4cb67-35c0-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:08:26.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rtn8w" for this suite.
Feb 21 10:08:32.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:08:32.651: INFO: namespace: e2e-tests-downward-api-rtn8w, resource: bindings, ignored listing per whitelist
Feb 21 10:08:33.054: INFO: namespace e2e-tests-downward-api-rtn8w deletion completed in 6.452737993s

• [SLOW TEST:9.082 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:08:33.055: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mxxfh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 21 10:08:33.613: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:33.863: INFO: stderr: ""
Feb 21 10:08:33.863: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 10:08:33.863: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:34.027: INFO: stderr: ""
Feb 21 10:08:34.027: INFO: stdout: "update-demo-nautilus-sdhn8 update-demo-nautilus-xnlpf "
Feb 21 10:08:34.027: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-sdhn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:34.184: INFO: stderr: ""
Feb 21 10:08:34.184: INFO: stdout: ""
Feb 21 10:08:34.184: INFO: update-demo-nautilus-sdhn8 is created but not running
Feb 21 10:08:39.184: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:39.287: INFO: stderr: ""
Feb 21 10:08:39.287: INFO: stdout: "update-demo-nautilus-sdhn8 update-demo-nautilus-xnlpf "
Feb 21 10:08:39.287: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-sdhn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:39.404: INFO: stderr: ""
Feb 21 10:08:39.404: INFO: stdout: "true"
Feb 21 10:08:39.404: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-sdhn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:39.509: INFO: stderr: ""
Feb 21 10:08:39.509: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 10:08:39.509: INFO: validating pod update-demo-nautilus-sdhn8
Feb 21 10:08:39.602: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 10:08:39.602: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 10:08:39.602: INFO: update-demo-nautilus-sdhn8 is verified up and running
Feb 21 10:08:39.602: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-xnlpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:39.709: INFO: stderr: ""
Feb 21 10:08:39.709: INFO: stdout: "true"
Feb 21 10:08:39.709: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-xnlpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:39.816: INFO: stderr: ""
Feb 21 10:08:39.816: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 10:08:39.816: INFO: validating pod update-demo-nautilus-xnlpf
Feb 21 10:08:39.906: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 10:08:39.906: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 10:08:39.906: INFO: update-demo-nautilus-xnlpf is verified up and running
STEP: scaling down the replication controller
Feb 21 10:08:39.910: INFO: scanned /root for discovery docs: <nil>
Feb 21 10:08:39.910: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:41.068: INFO: stderr: ""
Feb 21 10:08:41.068: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 10:08:41.068: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:41.172: INFO: stderr: ""
Feb 21 10:08:41.172: INFO: stdout: "update-demo-nautilus-sdhn8 update-demo-nautilus-xnlpf "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 21 10:08:46.172: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:46.302: INFO: stderr: ""
Feb 21 10:08:46.303: INFO: stdout: "update-demo-nautilus-sdhn8 "
Feb 21 10:08:46.303: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-sdhn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:46.455: INFO: stderr: ""
Feb 21 10:08:46.456: INFO: stdout: "true"
Feb 21 10:08:46.456: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-sdhn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:46.586: INFO: stderr: ""
Feb 21 10:08:46.586: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 10:08:46.586: INFO: validating pod update-demo-nautilus-sdhn8
Feb 21 10:08:46.593: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 10:08:46.593: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 10:08:46.593: INFO: update-demo-nautilus-sdhn8 is verified up and running
STEP: scaling up the replication controller
Feb 21 10:08:46.597: INFO: scanned /root for discovery docs: <nil>
Feb 21 10:08:46.597: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:47.765: INFO: stderr: ""
Feb 21 10:08:47.765: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 10:08:47.765: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:47.915: INFO: stderr: ""
Feb 21 10:08:47.915: INFO: stdout: "update-demo-nautilus-bqltq update-demo-nautilus-sdhn8 "
Feb 21 10:08:47.915: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bqltq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:48.069: INFO: stderr: ""
Feb 21 10:08:48.069: INFO: stdout: "true"
Feb 21 10:08:48.070: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-bqltq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:48.210: INFO: stderr: ""
Feb 21 10:08:48.210: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 10:08:48.210: INFO: validating pod update-demo-nautilus-bqltq
Feb 21 10:08:48.296: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 10:08:48.296: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 10:08:48.296: INFO: update-demo-nautilus-bqltq is verified up and running
Feb 21 10:08:48.296: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-sdhn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:48.437: INFO: stderr: ""
Feb 21 10:08:48.437: INFO: stdout: "true"
Feb 21 10:08:48.437: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-sdhn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:48.556: INFO: stderr: ""
Feb 21 10:08:48.557: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 10:08:48.557: INFO: validating pod update-demo-nautilus-sdhn8
Feb 21 10:08:48.564: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 10:08:48.564: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 10:08:48.564: INFO: update-demo-nautilus-sdhn8 is verified up and running
STEP: using delete to clean up resources
Feb 21 10:08:48.564: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:48.710: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 10:08:48.710: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 21 10:08:48.710: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-mxxfh'
Feb 21 10:08:48.854: INFO: stderr: "No resources found.\n"
Feb 21 10:08:48.854: INFO: stdout: ""
Feb 21 10:08:48.854: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-mxxfh -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 10:08:48.989: INFO: stderr: ""
Feb 21 10:08:48.989: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:08:48.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mxxfh" for this suite.
Feb 21 10:09:11.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:09:11.177: INFO: namespace: e2e-tests-kubectl-mxxfh, resource: bindings, ignored listing per whitelist
Feb 21 10:09:11.713: INFO: namespace e2e-tests-kubectl-mxxfh deletion completed in 22.704811201s

• [SLOW TEST:38.658 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:09:11.713: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-jzmvv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 21 10:09:12.033: INFO: Waiting up to 5m0s for pod "pod-bc46aa7e-35c0-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-jzmvv" to be "success or failure"
Feb 21 10:09:12.039: INFO: Pod "pod-bc46aa7e-35c0-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.704221ms
Feb 21 10:09:14.077: INFO: Pod "pod-bc46aa7e-35c0-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044035013s
STEP: Saw pod success
Feb 21 10:09:14.077: INFO: Pod "pod-bc46aa7e-35c0-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:09:14.083: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-bc46aa7e-35c0-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 10:09:14.108: INFO: Waiting for pod pod-bc46aa7e-35c0-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:09:14.113: INFO: Pod pod-bc46aa7e-35c0-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:09:14.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jzmvv" for this suite.
Feb 21 10:09:20.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:09:20.256: INFO: namespace: e2e-tests-emptydir-jzmvv, resource: bindings, ignored listing per whitelist
Feb 21 10:09:21.052: INFO: namespace e2e-tests-emptydir-jzmvv deletion completed in 6.932241905s

• [SLOW TEST:9.339 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:09:21.052: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-p22nt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-p22nt
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-p22nt
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-p22nt
Feb 21 10:09:21.442: INFO: Found 0 stateful pods, waiting for 1
Feb 21 10:09:31.456: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 21 10:09:31.464: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 10:09:32.069: INFO: stderr: ""
Feb 21 10:09:32.069: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 10:09:32.069: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 10:09:32.075: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 21 10:09:42.319: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 10:09:42.319: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 10:09:42.339: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999524s
Feb 21 10:09:43.346: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993900824s
Feb 21 10:09:44.352: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987526631s
Feb 21 10:09:45.359: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980617565s
Feb 21 10:09:46.374: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.973519713s
Feb 21 10:09:47.384: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.959055877s
Feb 21 10:09:48.392: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.948939831s
Feb 21 10:09:49.402: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.940886023s
Feb 21 10:09:50.408: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.931218531s
Feb 21 10:09:51.418: INFO: Verifying statefulset ss doesn't scale past 1 for another 925.06722ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-p22nt
Feb 21 10:09:52.425: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:09:52.986: INFO: stderr: ""
Feb 21 10:09:52.986: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 10:09:52.986: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 10:09:52.995: INFO: Found 1 stateful pods, waiting for 3
Feb 21 10:10:03.008: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 10:10:03.008: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 10:10:03.008: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 21 10:10:03.020: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 10:10:03.613: INFO: stderr: ""
Feb 21 10:10:03.613: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 10:10:03.613: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 10:10:03.613: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 10:10:04.202: INFO: stderr: ""
Feb 21 10:10:04.202: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 10:10:04.202: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 10:10:04.202: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 10:10:04.776: INFO: stderr: ""
Feb 21 10:10:04.776: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 10:10:04.776: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 10:10:04.777: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 10:10:04.784: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 21 10:10:14.797: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 10:10:14.797: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 10:10:14.797: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 10:10:14.814: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999685s
Feb 21 10:10:15.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994640893s
Feb 21 10:10:16.829: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988015374s
Feb 21 10:10:17.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978998741s
Feb 21 10:10:18.850: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967099293s
Feb 21 10:10:19.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.958308131s
Feb 21 10:10:20.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.949478873s
Feb 21 10:10:21.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.942136536s
Feb 21 10:10:22.889: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.936049786s
Feb 21 10:10:23.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 919.533746ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-p22nt
Feb 21 10:10:24.904: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:10:28.714: INFO: stderr: ""
Feb 21 10:10:28.714: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 10:10:28.714: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 10:10:28.714: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:10:29.279: INFO: stderr: ""
Feb 21 10:10:29.279: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 10:10:29.279: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 10:10:29.279: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:10:29.552: INFO: rc: 1
Feb 21 10:10:29.552: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server: 
 [] <nil> 0xc001291290 exit status 1 <nil> <nil> true [0xc000c649f0 0xc000c64a80 0xc000c64ac0] [0xc000c649f0 0xc000c64a80 0xc000c64ac0] [0xc000c64a30 0xc000c64ab0] [0x932420 0x932420] 0xc001591bc0 <nil>}:
Command stdout:

stderr:
Error from server: 

error:
exit status 1

Feb 21 10:10:39.553: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:10:39.716: INFO: rc: 1
Feb 21 10:10:39.716: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001291590 exit status 1 <nil> <nil> true [0xc000c64ac8 0xc000c64b08 0xc000c64b70] [0xc000c64ac8 0xc000c64b08 0xc000c64b70] [0xc000c64ae8 0xc000c64b60] [0x932420 0x932420] 0xc001fbc1e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:10:49.716: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:10:49.851: INFO: rc: 1
Feb 21 10:10:49.851: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0012b7050 exit status 1 <nil> <nil> true [0xc00025c988 0xc00025c9c0 0xc00025ca48] [0xc00025c988 0xc00025c9c0 0xc00025ca48] [0xc00025c9a8 0xc00025ca30] [0x932420 0x932420] 0xc0027adc20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:10:59.852: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:10:59.998: INFO: rc: 1
Feb 21 10:10:59.998: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0012918c0 exit status 1 <nil> <nil> true [0xc000c64b88 0xc000c64be8 0xc000c64c78] [0xc000c64b88 0xc000c64be8 0xc000c64c78] [0xc000c64bb8 0xc000c64c68] [0x932420 0x932420] 0xc001fbc4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:11:09.998: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:11:10.208: INFO: rc: 1
Feb 21 10:11:10.208: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000f4a4b0 exit status 1 <nil> <nil> true [0xc00198e8b0 0xc00198e8f0 0xc00198e940] [0xc00198e8b0 0xc00198e8f0 0xc00198e940] [0xc00198e8e0 0xc00198e918] [0x932420 0x932420] 0xc0017c49c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:11:20.209: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:11:20.345: INFO: rc: 1
Feb 21 10:11:20.345: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001933c50 exit status 1 <nil> <nil> true [0xc00000fb40 0xc00000fc80 0xc00000fcf8] [0xc00000fb40 0xc00000fc80 0xc00000fcf8] [0xc00000fc38 0xc00000fce8] [0x932420 0x932420] 0xc002133380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:11:30.346: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:11:30.476: INFO: rc: 1
Feb 21 10:11:30.476: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001933ef0 exit status 1 <nil> <nil> true [0xc00000fd18 0xc00000fda8 0xc00000fe88] [0xc00000fd18 0xc00000fda8 0xc00000fe88] [0xc00000fd68 0xc00000fe18] [0x932420 0x932420] 0xc002133680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:11:40.476: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:11:40.665: INFO: rc: 1
Feb 21 10:11:40.665: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019b04e0 exit status 1 <nil> <nil> true [0xc00025c008 0xc00025c558 0xc00025c780] [0xc00025c008 0xc00025c558 0xc00025c780] [0xc00025c540 0xc00025c778] [0x932420 0x932420] 0xc001590840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:11:50.665: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:11:50.803: INFO: rc: 1
Feb 21 10:11:50.803: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0026142d0 exit status 1 <nil> <nil> true [0xc000c64058 0xc000c640a8 0xc000c64188] [0xc000c64058 0xc000c640a8 0xc000c64188] [0xc000c640a0 0xc000c64108] [0x932420 0x932420] 0xc002476780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:12:00.804: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:12:00.948: INFO: rc: 1
Feb 21 10:12:00.948: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014882d0 exit status 1 <nil> <nil> true [0xc00000e098 0xc00000e448 0xc00000e8a0] [0xc00000e098 0xc00000e448 0xc00000e8a0] [0xc00000e120 0xc00000e6c8] [0x932420 0x932420] 0xc0027ac240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:12:10.949: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:12:11.309: INFO: rc: 1
Feb 21 10:12:11.309: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0014885a0 exit status 1 <nil> <nil> true [0xc00000e970 0xc00000ec18 0xc00000ee48] [0xc00000e970 0xc00000ec18 0xc00000ee48] [0xc00000ea88 0xc00000edb8] [0x932420 0x932420] 0xc0027ac540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:12:21.310: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:12:21.515: INFO: rc: 1
Feb 21 10:12:21.515: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001488810 exit status 1 <nil> <nil> true [0xc00000ef10 0xc00000f210 0xc00000f4a0] [0xc00000ef10 0xc00000f210 0xc00000f4a0] [0xc00000f098 0xc00000f3a8] [0x932420 0x932420] 0xc0027ac8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:12:31.515: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:12:31.678: INFO: rc: 1
Feb 21 10:12:31.678: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002614630 exit status 1 <nil> <nil> true [0xc000c64238 0xc000c64290 0xc000c642e8] [0xc000c64238 0xc000c64290 0xc000c642e8] [0xc000c64270 0xc000c642c0] [0x932420 0x932420] 0xc002476a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:12:41.678: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:12:41.786: INFO: rc: 1
Feb 21 10:12:41.787: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0026149c0 exit status 1 <nil> <nil> true [0xc000c64328 0xc000c643a8 0xc000c64420] [0xc000c64328 0xc000c643a8 0xc000c64420] [0xc000c64398 0xc000c643d8] [0x932420 0x932420] 0xc002476de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:12:51.787: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:12:51.920: INFO: rc: 1
Feb 21 10:12:51.920: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001488a80 exit status 1 <nil> <nil> true [0xc00000f5f0 0xc00000f688 0xc00000f760] [0xc00000f5f0 0xc00000f688 0xc00000f760] [0xc00000f668 0xc00000f6f0] [0x932420 0x932420] 0xc0027ad500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:13:01.920: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:13:02.075: INFO: rc: 1
Feb 21 10:13:02.075: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002614d20 exit status 1 <nil> <nil> true [0xc000c64428 0xc000c64498 0xc000c644f0] [0xc000c64428 0xc000c64498 0xc000c644f0] [0xc000c64468 0xc000c644e8] [0x932420 0x932420] 0xc002477140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:13:12.076: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:13:12.208: INFO: rc: 1
Feb 21 10:13:12.208: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019b08d0 exit status 1 <nil> <nil> true [0xc00025c790 0xc00025c7c0 0xc00025c7f8] [0xc00025c790 0xc00025c7c0 0xc00025c7f8] [0xc00025c7a8 0xc00025c7f0] [0x932420 0x932420] 0xc001590b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:13:22.208: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:13:22.364: INFO: rc: 1
Feb 21 10:13:22.365: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019b0cc0 exit status 1 <nil> <nil> true [0xc00025c800 0xc00025c838 0xc00025c870] [0xc00025c800 0xc00025c838 0xc00025c870] [0xc00025c810 0xc00025c868] [0x932420 0x932420] 0xc001590e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:13:32.365: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:13:32.563: INFO: rc: 1
Feb 21 10:13:32.563: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001488cf0 exit status 1 <nil> <nil> true [0xc00000f828 0xc00000f958 0xc00000fa68] [0xc00000f828 0xc00000f958 0xc00000fa68] [0xc00000f908 0xc00000fa28] [0x932420 0x932420] 0xc0027ad800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:13:42.563: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:13:42.687: INFO: rc: 1
Feb 21 10:13:42.687: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002614300 exit status 1 <nil> <nil> true [0xc000c64058 0xc000c640a8 0xc000c64188] [0xc000c64058 0xc000c640a8 0xc000c64188] [0xc000c640a0 0xc000c64108] [0x932420 0x932420] 0xc002476780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:13:52.687: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:13:52.791: INFO: rc: 1
Feb 21 10:13:52.791: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b82a0 exit status 1 <nil> <nil> true [0xc00025c000 0xc00025c540 0xc00025c768] [0xc00025c000 0xc00025c540 0xc00025c768] [0xc00025c050 0xc00025c568] [0x932420 0x932420] 0xc001590840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:14:02.791: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:14:02.938: INFO: rc: 1
Feb 21 10:14:02.939: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0026145d0 exit status 1 <nil> <nil> true [0xc000c64238 0xc000c64290 0xc000c642e8] [0xc000c64238 0xc000c64290 0xc000c642e8] [0xc000c64270 0xc000c642c0] [0x932420 0x932420] 0xc002476a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:14:12.940: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:14:13.049: INFO: rc: 1
Feb 21 10:14:13.050: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b8570 exit status 1 <nil> <nil> true [0xc00025c778 0xc00025c7a0 0xc00025c7e0] [0xc00025c778 0xc00025c7a0 0xc00025c7e0] [0xc00025c790 0xc00025c7c0] [0x932420 0x932420] 0xc001590b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:14:23.050: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:14:23.151: INFO: rc: 1
Feb 21 10:14:23.151: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b87e0 exit status 1 <nil> <nil> true [0xc00025c7f0 0xc00025c808 0xc00025c848] [0xc00025c7f0 0xc00025c808 0xc00025c848] [0xc00025c800 0xc00025c838] [0x932420 0x932420] 0xc001590e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:14:33.151: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:14:33.306: INFO: rc: 1
Feb 21 10:14:33.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002614a20 exit status 1 <nil> <nil> true [0xc000c64328 0xc000c643a8 0xc000c64420] [0xc000c64328 0xc000c643a8 0xc000c64420] [0xc000c64398 0xc000c643d8] [0x932420 0x932420] 0xc002476de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:14:43.307: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:14:43.405: INFO: rc: 1
Feb 21 10:14:43.406: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002614d80 exit status 1 <nil> <nil> true [0xc000c64428 0xc000c64498 0xc000c644f0] [0xc000c64428 0xc000c64498 0xc000c644f0] [0xc000c64468 0xc000c644e8] [0x932420 0x932420] 0xc002477140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:14:53.406: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:14:53.519: INFO: rc: 1
Feb 21 10:14:53.519: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b8a80 exit status 1 <nil> <nil> true [0xc00025c868 0xc00025c888 0xc00025c8c0] [0xc00025c868 0xc00025c888 0xc00025c8c0] [0xc00025c880 0xc00025c8b0] [0x932420 0x932420] 0xc001591140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:15:03.519: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:15:03.650: INFO: rc: 1
Feb 21 10:15:03.650: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b8d20 exit status 1 <nil> <nil> true [0xc00025c8f8 0xc00025c928 0xc00025c980] [0xc00025c8f8 0xc00025c928 0xc00025c980] [0xc00025c910 0xc00025c958] [0x932420 0x932420] 0xc001591440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:15:13.651: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:15:13.758: INFO: rc: 1
Feb 21 10:15:13.759: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002615050 exit status 1 <nil> <nil> true [0xc000c64500 0xc000c64528 0xc000c64560] [0xc000c64500 0xc000c64528 0xc000c64560] [0xc000c64518 0xc000c64558] [0x932420 0x932420] 0xc0024774a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:15:23.759: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:15:23.889: INFO: rc: 1
Feb 21 10:15:23.889: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002615410 exit status 1 <nil> <nil> true [0xc000c64578 0xc000c645b0 0xc000c645e8] [0xc000c64578 0xc000c645b0 0xc000c645e8] [0xc000c645a8 0xc000c645d0] [0x932420 0x932420] 0xc0024777a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 21 10:15:33.890: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-p22nt ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 10:15:34.083: INFO: rc: 1
Feb 21 10:15:34.083: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 21 10:15:34.083: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 21 10:15:34.144: INFO: Deleting all statefulset in ns e2e-tests-statefulset-p22nt
Feb 21 10:15:34.154: INFO: Scaling statefulset ss to 0
Feb 21 10:15:34.184: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 10:15:34.198: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:15:34.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-p22nt" for this suite.
Feb 21 10:15:40.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:15:40.400: INFO: namespace: e2e-tests-statefulset-p22nt, resource: bindings, ignored listing per whitelist
Feb 21 10:15:40.687: INFO: namespace e2e-tests-statefulset-p22nt deletion completed in 6.434029585s

• [SLOW TEST:379.635 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:15:40.687: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-xqtsw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-a4228340-35c1-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 10:15:41.032: INFO: Waiting up to 5m0s for pod "pod-configmaps-a423595f-35c1-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-configmap-xqtsw" to be "success or failure"
Feb 21 10:15:41.037: INFO: Pod "pod-configmaps-a423595f-35c1-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.435616ms
Feb 21 10:15:43.047: INFO: Pod "pod-configmaps-a423595f-35c1-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014266469s
STEP: Saw pod success
Feb 21 10:15:43.047: INFO: Pod "pod-configmaps-a423595f-35c1-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:15:43.054: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-configmaps-a423595f-35c1-11e9-b7c5-a2b84e263bfe container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 10:15:43.083: INFO: Waiting for pod pod-configmaps-a423595f-35c1-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:15:43.090: INFO: Pod pod-configmaps-a423595f-35c1-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:15:43.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-xqtsw" for this suite.
Feb 21 10:15:49.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:15:49.534: INFO: namespace: e2e-tests-configmap-xqtsw, resource: bindings, ignored listing per whitelist
Feb 21 10:15:49.579: INFO: namespace e2e-tests-configmap-xqtsw deletion completed in 6.47570596s

• [SLOW TEST:8.892 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:15:49.579: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2sfsl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 21 10:15:52.732: INFO: Successfully updated pod "labelsupdatea99224c2-35c1-11e9-b7c5-a2b84e263bfe"
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:15:56.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2sfsl" for this suite.
Feb 21 10:16:18.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:16:18.988: INFO: namespace: e2e-tests-projected-2sfsl, resource: bindings, ignored listing per whitelist
Feb 21 10:16:19.228: INFO: namespace e2e-tests-projected-2sfsl deletion completed in 22.438357659s

• [SLOW TEST:29.649 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:16:19.229: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-t4j5f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 21 10:16:24.230: INFO: Successfully updated pod "annotationupdatebb2957d5-35c1-11e9-b7c5-a2b84e263bfe"
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:16:26.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t4j5f" for this suite.
Feb 21 10:16:48.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:16:48.372: INFO: namespace: e2e-tests-projected-t4j5f, resource: bindings, ignored listing per whitelist
Feb 21 10:16:48.803: INFO: namespace e2e-tests-projected-t4j5f deletion completed in 22.532356207s

• [SLOW TEST:29.575 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:16:48.804: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-hddqb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 10:16:49.215: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 21 10:16:49.227: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 21 10:16:54.239: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 21 10:16:54.239: INFO: Creating deployment "test-rolling-update-deployment"
Feb 21 10:16:54.249: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 21 10:16:54.264: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 21 10:16:56.287: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 21 10:16:56.297: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 21 10:16:56.324: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-hddqb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hddqb/deployments/test-rolling-update-deployment,UID:cfc88d33-35c1-11e9-890e-3aac84b8a476,ResourceVersion:22657,Generation:1,CreationTimestamp:2019-02-21 10:16:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-21 10:16:54 +0000 UTC 2019-02-21 10:16:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-21 10:16:56 +0000 UTC 2019-02-21 10:16:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-65b7695dcf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 21 10:16:56.333: INFO: New ReplicaSet "test-rolling-update-deployment-65b7695dcf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf,GenerateName:,Namespace:e2e-tests-deployment-hddqb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hddqb/replicasets/test-rolling-update-deployment-65b7695dcf,UID:cfcb29e6-35c1-11e9-890e-3aac84b8a476,ResourceVersion:22650,Generation:1,CreationTimestamp:2019-02-21 10:16:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment cfc88d33-35c1-11e9-890e-3aac84b8a476 0xc001d054b7 0xc001d054b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 21 10:16:56.333: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 21 10:16:56.333: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-hddqb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hddqb/replicasets/test-rolling-update-controller,UID:ccc98705-35c1-11e9-890e-3aac84b8a476,ResourceVersion:22656,Generation:2,CreationTimestamp:2019-02-21 10:16:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment cfc88d33-35c1-11e9-890e-3aac84b8a476 0xc001d053f7 0xc001d053f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 10:16:56.340: INFO: Pod "test-rolling-update-deployment-65b7695dcf-gn2qg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf-gn2qg,GenerateName:test-rolling-update-deployment-65b7695dcf-,Namespace:e2e-tests-deployment-hddqb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hddqb/pods/test-rolling-update-deployment-65b7695dcf-gn2qg,UID:cfcbc1e2-35c1-11e9-890e-3aac84b8a476,ResourceVersion:22649,Generation:0,CreationTimestamp:2019-02-21 10:16:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.189/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-65b7695dcf cfcb29e6-35c1-11e9-890e-3aac84b8a476 0xc001d05f47 0xc001d05f48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tvvhl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tvvhl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-tvvhl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d05fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d05fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 10:16:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 10:16:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 10:16:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 10:16:54 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:100.96.1.189,StartTime:2019-02-21 10:16:54 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-21 10:16:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://43a3d5ff0383a02061ada7dff72c3f0a24dcad9557de31be2881b1c43a914e00}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:16:56.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-hddqb" for this suite.
Feb 21 10:17:02.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:17:02.842: INFO: namespace: e2e-tests-deployment-hddqb, resource: bindings, ignored listing per whitelist
Feb 21 10:17:02.842: INFO: namespace e2e-tests-deployment-hddqb deletion completed in 6.485142252s

• [SLOW TEST:14.038 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:17:02.843: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xj6bx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-d522b85d-35c1-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 10:17:03.253: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d5241c3f-35c1-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-xj6bx" to be "success or failure"
Feb 21 10:17:03.264: INFO: Pod "pod-projected-configmaps-d5241c3f-35c1-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.021156ms
Feb 21 10:17:05.271: INFO: Pod "pod-projected-configmaps-d5241c3f-35c1-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018468316s
STEP: Saw pod success
Feb 21 10:17:05.272: INFO: Pod "pod-projected-configmaps-d5241c3f-35c1-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:17:05.280: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-configmaps-d5241c3f-35c1-11e9-b7c5-a2b84e263bfe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 10:17:05.314: INFO: Waiting for pod pod-projected-configmaps-d5241c3f-35c1-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:17:05.318: INFO: Pod pod-projected-configmaps-d5241c3f-35c1-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:17:05.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xj6bx" for this suite.
Feb 21 10:17:11.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:17:11.451: INFO: namespace: e2e-tests-projected-xj6bx, resource: bindings, ignored listing per whitelist
Feb 21 10:17:11.953: INFO: namespace e2e-tests-projected-xj6bx deletion completed in 6.62414799s

• [SLOW TEST:9.110 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:17:11.953: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-dxvb6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 21 10:17:12.654: INFO: Waiting up to 5m0s for pod "pod-dabf4853-35c1-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-dxvb6" to be "success or failure"
Feb 21 10:17:12.659: INFO: Pod "pod-dabf4853-35c1-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.610607ms
Feb 21 10:17:14.665: INFO: Pod "pod-dabf4853-35c1-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01049623s
STEP: Saw pod success
Feb 21 10:17:14.665: INFO: Pod "pod-dabf4853-35c1-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:17:14.669: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-dabf4853-35c1-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 10:17:14.692: INFO: Waiting for pod pod-dabf4853-35c1-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:17:14.697: INFO: Pod pod-dabf4853-35c1-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:17:14.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dxvb6" for this suite.
Feb 21 10:17:20.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:17:20.777: INFO: namespace: e2e-tests-emptydir-dxvb6, resource: bindings, ignored listing per whitelist
Feb 21 10:17:20.950: INFO: namespace e2e-tests-emptydir-dxvb6 deletion completed in 6.243138332s

• [SLOW TEST:8.997 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:17:20.950: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-bpsk9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-bpsk9/secret-test-e0272394-35c1-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 10:17:21.726: INFO: Waiting up to 5m0s for pod "pod-configmaps-e027eeeb-35c1-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-secrets-bpsk9" to be "success or failure"
Feb 21 10:17:21.730: INFO: Pod "pod-configmaps-e027eeeb-35c1-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.690057ms
Feb 21 10:17:23.739: INFO: Pod "pod-configmaps-e027eeeb-35c1-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012055068s
STEP: Saw pod success
Feb 21 10:17:23.739: INFO: Pod "pod-configmaps-e027eeeb-35c1-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:17:23.743: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-configmaps-e027eeeb-35c1-11e9-b7c5-a2b84e263bfe container env-test: <nil>
STEP: delete the pod
Feb 21 10:17:23.769: INFO: Waiting for pod pod-configmaps-e027eeeb-35c1-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:17:23.774: INFO: Pod pod-configmaps-e027eeeb-35c1-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:17:23.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bpsk9" for this suite.
Feb 21 10:17:29.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:17:29.852: INFO: namespace: e2e-tests-secrets-bpsk9, resource: bindings, ignored listing per whitelist
Feb 21 10:17:30.030: INFO: namespace e2e-tests-secrets-bpsk9 deletion completed in 6.24926111s

• [SLOW TEST:9.081 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:17:30.031: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-6k2qm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 21 10:17:32.650: INFO: Pod pod-hostip-e575fef1-35c1-11e9-b7c5-a2b84e263bfe has hostIP: 10.250.0.19
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:17:32.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6k2qm" for this suite.
Feb 21 10:17:54.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:17:54.788: INFO: namespace: e2e-tests-pods-6k2qm, resource: bindings, ignored listing per whitelist
Feb 21 10:17:54.884: INFO: namespace e2e-tests-pods-6k2qm deletion completed in 22.2252301s

• [SLOW TEST:24.853 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:17:54.884: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-swq76
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 21 10:17:55.719: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix197099170/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:17:55.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-swq76" for this suite.
Feb 21 10:18:01.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:18:02.597: INFO: namespace: e2e-tests-kubectl-swq76, resource: bindings, ignored listing per whitelist
Feb 21 10:18:02.597: INFO: namespace e2e-tests-kubectl-swq76 deletion completed in 6.79166318s

• [SLOW TEST:7.713 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:18:02.598: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-kx495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 21 10:18:03.462: INFO: created pod pod-service-account-defaultsa
Feb 21 10:18:03.462: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 21 10:18:03.467: INFO: created pod pod-service-account-mountsa
Feb 21 10:18:03.467: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 21 10:18:03.474: INFO: created pod pod-service-account-nomountsa
Feb 21 10:18:03.474: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 21 10:18:03.480: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 21 10:18:03.480: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 21 10:18:03.488: INFO: created pod pod-service-account-mountsa-mountspec
Feb 21 10:18:03.488: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 21 10:18:03.496: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 21 10:18:03.496: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 21 10:18:03.503: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 21 10:18:03.503: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 21 10:18:03.517: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 21 10:18:03.517: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 21 10:18:03.528: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 21 10:18:03.528: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:18:03.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-kx495" for this suite.
Feb 21 10:18:25.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:18:26.378: INFO: namespace: e2e-tests-svcaccounts-kx495, resource: bindings, ignored listing per whitelist
Feb 21 10:18:26.390: INFO: namespace e2e-tests-svcaccounts-kx495 deletion completed in 22.854050887s

• [SLOW TEST:23.792 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:18:26.390: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-fmx77
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 21 10:18:26.729: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-fmx77" to be "success or failure"
Feb 21 10:18:26.737: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281909ms
Feb 21 10:18:28.744: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014263061s
STEP: Saw pod success
Feb 21 10:18:28.744: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 21 10:18:28.752: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 21 10:18:28.776: INFO: Waiting for pod pod-host-path-test to disappear
Feb 21 10:18:28.780: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:18:28.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-fmx77" for this suite.
Feb 21 10:18:34.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:18:35.214: INFO: namespace: e2e-tests-hostpath-fmx77, resource: bindings, ignored listing per whitelist
Feb 21 10:18:35.237: INFO: namespace e2e-tests-hostpath-fmx77 deletion completed in 6.44601843s

• [SLOW TEST:8.848 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:18:35.237: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wwlt8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 21 10:18:35.524: INFO: Waiting up to 5m0s for pod "pod-0c2421a2-35c2-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-wwlt8" to be "success or failure"
Feb 21 10:18:35.530: INFO: Pod "pod-0c2421a2-35c2-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.729552ms
Feb 21 10:18:37.537: INFO: Pod "pod-0c2421a2-35c2-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012882799s
STEP: Saw pod success
Feb 21 10:18:37.537: INFO: Pod "pod-0c2421a2-35c2-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:18:37.541: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-0c2421a2-35c2-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 10:18:37.564: INFO: Waiting for pod pod-0c2421a2-35c2-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:18:37.569: INFO: Pod pod-0c2421a2-35c2-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:18:37.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wwlt8" for this suite.
Feb 21 10:18:43.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:18:43.818: INFO: namespace: e2e-tests-emptydir-wwlt8, resource: bindings, ignored listing per whitelist
Feb 21 10:18:43.856: INFO: namespace e2e-tests-emptydir-wwlt8 deletion completed in 6.275502388s

• [SLOW TEST:8.619 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:18:43.857: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rtp6n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 21 10:18:44.924: INFO: Waiting up to 5m0s for pod "downward-api-11bec571-35c2-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-rtp6n" to be "success or failure"
Feb 21 10:18:44.928: INFO: Pod "downward-api-11bec571-35c2-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.752008ms
Feb 21 10:18:46.965: INFO: Pod "downward-api-11bec571-35c2-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040793891s
STEP: Saw pod success
Feb 21 10:18:46.965: INFO: Pod "downward-api-11bec571-35c2-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:18:46.971: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downward-api-11bec571-35c2-11e9-b7c5-a2b84e263bfe container dapi-container: <nil>
STEP: delete the pod
Feb 21 10:18:47.000: INFO: Waiting for pod downward-api-11bec571-35c2-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:18:47.007: INFO: Pod downward-api-11bec571-35c2-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:18:47.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rtp6n" for this suite.
Feb 21 10:18:53.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:18:53.110: INFO: namespace: e2e-tests-downward-api-rtp6n, resource: bindings, ignored listing per whitelist
Feb 21 10:18:53.227: INFO: namespace e2e-tests-downward-api-rtp6n deletion completed in 6.20857299s

• [SLOW TEST:9.371 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:18:53.228: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2v7mg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-16df548d-35c2-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume configMaps
Feb 21 10:18:53.535: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-16e042d5-35c2-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-2v7mg" to be "success or failure"
Feb 21 10:18:53.543: INFO: Pod "pod-projected-configmaps-16e042d5-35c2-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.748564ms
Feb 21 10:18:55.549: INFO: Pod "pod-projected-configmaps-16e042d5-35c2-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014322601s
STEP: Saw pod success
Feb 21 10:18:55.549: INFO: Pod "pod-projected-configmaps-16e042d5-35c2-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:18:55.553: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-configmaps-16e042d5-35c2-11e9-b7c5-a2b84e263bfe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 10:18:55.586: INFO: Waiting for pod pod-projected-configmaps-16e042d5-35c2-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:18:55.590: INFO: Pod pod-projected-configmaps-16e042d5-35c2-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:18:55.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2v7mg" for this suite.
Feb 21 10:19:01.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:19:01.694: INFO: namespace: e2e-tests-projected-2v7mg, resource: bindings, ignored listing per whitelist
Feb 21 10:19:01.838: INFO: namespace e2e-tests-projected-2v7mg deletion completed in 6.241829583s

• [SLOW TEST:8.610 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:19:01.838: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-5r5rf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-swgk7
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-vwb7v
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:19:08.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-5r5rf" for this suite.
Feb 21 10:19:14.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:19:14.954: INFO: namespace: e2e-tests-namespaces-5r5rf, resource: bindings, ignored listing per whitelist
Feb 21 10:19:15.001: INFO: namespace e2e-tests-namespaces-5r5rf deletion completed in 6.23277464s
STEP: Destroying namespace "e2e-tests-nsdeletetest-swgk7" for this suite.
Feb 21 10:19:15.007: INFO: Namespace e2e-tests-nsdeletetest-swgk7 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-vwb7v" for this suite.
Feb 21 10:19:21.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:19:21.099: INFO: namespace: e2e-tests-nsdeletetest-vwb7v, resource: bindings, ignored listing per whitelist
Feb 21 10:19:21.235: INFO: namespace e2e-tests-nsdeletetest-vwb7v deletion completed in 6.22855468s

• [SLOW TEST:19.398 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:19:21.236: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-n9dp8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-278ffa48-35c2-11e9-b7c5-a2b84e263bfe
Feb 21 10:19:21.538: INFO: Pod name my-hostname-basic-278ffa48-35c2-11e9-b7c5-a2b84e263bfe: Found 1 pods out of 1
Feb 21 10:19:21.538: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-278ffa48-35c2-11e9-b7c5-a2b84e263bfe" are running
Feb 21 10:19:23.549: INFO: Pod "my-hostname-basic-278ffa48-35c2-11e9-b7c5-a2b84e263bfe-jxflx" is running (conditions: [])
Feb 21 10:19:23.550: INFO: Trying to dial the pod
Feb 21 10:19:28.657: INFO: Controller my-hostname-basic-278ffa48-35c2-11e9-b7c5-a2b84e263bfe: Got expected result from replica 1 [my-hostname-basic-278ffa48-35c2-11e9-b7c5-a2b84e263bfe-jxflx]: "my-hostname-basic-278ffa48-35c2-11e9-b7c5-a2b84e263bfe-jxflx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:19:28.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-n9dp8" for this suite.
Feb 21 10:19:34.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:19:34.781: INFO: namespace: e2e-tests-replication-controller-n9dp8, resource: bindings, ignored listing per whitelist
Feb 21 10:19:34.904: INFO: namespace e2e-tests-replication-controller-n9dp8 deletion completed in 6.233433822s

• [SLOW TEST:13.669 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:19:34.905: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-hg7mg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 21 10:19:35.209: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 21 10:19:35.221: INFO: Waiting for terminating namespaces to be deleted...
Feb 21 10:19:35.226: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw before test
Feb 21 10:19:35.245: INFO: blackbox-exporter-64f6f7f998-frlvk from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 21 10:19:35.245: INFO: calico-node-bbf5b from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 10:19:35.245: INFO: addons-kubernetes-dashboard-5f64f76bd-gld9p from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 21 10:19:35.245: INFO: coredns-5f4748c5f-zzvr9 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container coredns ready: true, restart count 0
Feb 21 10:19:35.245: INFO: addons-nginx-ingress-controller-55d976867d-zqg87 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 10:19:35.245: INFO: node-exporter-7nlxq from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 10:19:35.245: INFO: addons-kube-lego-648f8c9f5c-88qt5 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container kube-lego ready: true, restart count 0
Feb 21 10:19:35.245: INFO: metrics-server-5c5588c85c-lwhcv from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container metrics-server ready: true, restart count 0
Feb 21 10:19:35.245: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-j8nvn from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 21 10:19:35.245: INFO: kube-proxy-pxd78 from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 10:19:35.245: INFO: vpn-shoot-597d7f599b-56cgp from kube-system started at 2019-02-21 08:00:28 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.245: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 21 10:19:35.245: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl before test
Feb 21 10:19:35.291: INFO: calico-node-gfth2 from kube-system started at 2019-02-21 08:00:47 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.291: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 10:19:35.291: INFO: node-exporter-j5s5c from kube-system started at 2019-02-21 08:00:48 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.291: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 10:19:35.291: INFO: kube-proxy-2jctn from kube-system started at 2019-02-21 08:00:47 +0000 UTC (1 container statuses recorded)
Feb 21 10:19:35.291: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.158559cb0cc15a17], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:19:36.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-hg7mg" for this suite.
Feb 21 10:19:42.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:19:42.559: INFO: namespace: e2e-tests-sched-pred-hg7mg, resource: bindings, ignored listing per whitelist
Feb 21 10:19:42.573: INFO: namespace e2e-tests-sched-pred-hg7mg deletion completed in 6.242651823s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:7.669 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:19:42.574: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-zm2fk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 10:19:42.937: INFO: (0) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 11.01682ms)
Feb 21 10:19:42.991: INFO: (1) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 53.651337ms)
Feb 21 10:19:43.002: INFO: (2) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 11.044717ms)
Feb 21 10:19:43.010: INFO: (3) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.212376ms)
Feb 21 10:19:43.018: INFO: (4) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.811695ms)
Feb 21 10:19:43.025: INFO: (5) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.500693ms)
Feb 21 10:19:43.035: INFO: (6) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.892416ms)
Feb 21 10:19:43.041: INFO: (7) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.594797ms)
Feb 21 10:19:43.047: INFO: (8) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.331607ms)
Feb 21 10:19:43.054: INFO: (9) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.867882ms)
Feb 21 10:19:43.060: INFO: (10) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.664377ms)
Feb 21 10:19:43.066: INFO: (11) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.541884ms)
Feb 21 10:19:43.084: INFO: (12) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.84098ms)
Feb 21 10:19:43.093: INFO: (13) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.799672ms)
Feb 21 10:19:43.101: INFO: (14) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.395792ms)
Feb 21 10:19:43.108: INFO: (15) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.51726ms)
Feb 21 10:19:43.116: INFO: (16) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.917711ms)
Feb 21 10:19:43.123: INFO: (17) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.456042ms)
Feb 21 10:19:43.131: INFO: (18) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.055493ms)
Feb 21 10:19:43.139: INFO: (19) /api/v1/nodes/shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.651406ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:19:43.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-zm2fk" for this suite.
Feb 21 10:19:49.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:19:49.227: INFO: namespace: e2e-tests-proxy-zm2fk, resource: bindings, ignored listing per whitelist
Feb 21 10:19:49.395: INFO: namespace e2e-tests-proxy-zm2fk deletion completed in 6.244462785s

• [SLOW TEST:6.822 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:19:49.396: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rvjcj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-388caa29-35c2-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 10:19:50.037: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-388ddaea-35c2-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-rvjcj" to be "success or failure"
Feb 21 10:19:50.043: INFO: Pod "pod-projected-secrets-388ddaea-35c2-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.870445ms
Feb 21 10:19:52.051: INFO: Pod "pod-projected-secrets-388ddaea-35c2-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013771826s
STEP: Saw pod success
Feb 21 10:19:52.051: INFO: Pod "pod-projected-secrets-388ddaea-35c2-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:19:52.056: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-secrets-388ddaea-35c2-11e9-b7c5-a2b84e263bfe container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 10:19:52.081: INFO: Waiting for pod pod-projected-secrets-388ddaea-35c2-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:19:52.085: INFO: Pod pod-projected-secrets-388ddaea-35c2-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:19:52.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rvjcj" for this suite.
Feb 21 10:19:58.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:19:58.279: INFO: namespace: e2e-tests-projected-rvjcj, resource: bindings, ignored listing per whitelist
Feb 21 10:19:58.357: INFO: namespace e2e-tests-projected-rvjcj deletion completed in 6.260335369s

• [SLOW TEST:8.962 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:19:58.358: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-l9n7p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 10:19:59.219: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-l9n7p'
Feb 21 10:20:00.056: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 21 10:20:00.057: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 21 10:20:00.067: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-4k4rv]
Feb 21 10:20:00.067: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-4k4rv" in namespace "e2e-tests-kubectl-l9n7p" to be "running and ready"
Feb 21 10:20:00.072: INFO: Pod "e2e-test-nginx-rc-4k4rv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.796714ms
Feb 21 10:20:02.082: INFO: Pod "e2e-test-nginx-rc-4k4rv": Phase="Running", Reason="", readiness=true. Elapsed: 2.014833927s
Feb 21 10:20:02.082: INFO: Pod "e2e-test-nginx-rc-4k4rv" satisfied condition "running and ready"
Feb 21 10:20:02.082: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-4k4rv]
Feb 21 10:20:02.083: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-l9n7p'
Feb 21 10:20:02.232: INFO: stderr: ""
Feb 21 10:20:02.232: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1251
Feb 21 10:20:02.232: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-l9n7p'
Feb 21 10:20:02.354: INFO: stderr: ""
Feb 21 10:20:02.354: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:20:02.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l9n7p" for this suite.
Feb 21 10:20:08.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:20:08.444: INFO: namespace: e2e-tests-kubectl-l9n7p, resource: bindings, ignored listing per whitelist
Feb 21 10:20:08.871: INFO: namespace e2e-tests-kubectl-l9n7p deletion completed in 6.505305996s

• [SLOW TEST:10.513 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:20:08.872: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-rwg8n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 21 10:20:13.419: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:13.425: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 10:20:15.425: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:15.437: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 10:20:17.425: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:17.433: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 10:20:19.425: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:19.432: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 10:20:21.425: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:21.431: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 10:20:23.425: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:23.433: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 10:20:25.425: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:25.465: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 10:20:27.425: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:27.435: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 10:20:29.425: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:29.435: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 10:20:31.425: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:31.434: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 10:20:33.425: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 10:20:33.432: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:20:33.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-rwg8n" for this suite.
Feb 21 10:20:55.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:20:55.630: INFO: namespace: e2e-tests-container-lifecycle-hook-rwg8n, resource: bindings, ignored listing per whitelist
Feb 21 10:20:55.663: INFO: namespace e2e-tests-container-lifecycle-hook-rwg8n deletion completed in 22.207841646s

• [SLOW TEST:46.792 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:20:55.664: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-jnm7m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 21 10:20:56.230: INFO: Waiting up to 5m0s for pod "var-expansion-60026ba1-35c2-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-var-expansion-jnm7m" to be "success or failure"
Feb 21 10:20:56.235: INFO: Pod "var-expansion-60026ba1-35c2-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.725446ms
Feb 21 10:20:58.243: INFO: Pod "var-expansion-60026ba1-35c2-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012740911s
STEP: Saw pod success
Feb 21 10:20:58.243: INFO: Pod "var-expansion-60026ba1-35c2-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:20:58.247: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod var-expansion-60026ba1-35c2-11e9-b7c5-a2b84e263bfe container dapi-container: <nil>
STEP: delete the pod
Feb 21 10:20:58.272: INFO: Waiting for pod var-expansion-60026ba1-35c2-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:20:58.284: INFO: Pod var-expansion-60026ba1-35c2-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:20:58.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-jnm7m" for this suite.
Feb 21 10:21:04.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:21:04.573: INFO: namespace: e2e-tests-var-expansion-jnm7m, resource: bindings, ignored listing per whitelist
Feb 21 10:21:04.748: INFO: namespace e2e-tests-var-expansion-jnm7m deletion completed in 6.456291833s

• [SLOW TEST:9.084 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:21:04.748: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-s5dvg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-x6zx4
STEP: Creating secret with name secret-test-656e2acb-35c2-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 10:21:05.549: INFO: Waiting up to 5m0s for pod "pod-secrets-659083a2-35c2-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-secrets-s5dvg" to be "success or failure"
Feb 21 10:21:05.555: INFO: Pod "pod-secrets-659083a2-35c2-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.136861ms
Feb 21 10:21:07.563: INFO: Pod "pod-secrets-659083a2-35c2-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013771938s
STEP: Saw pod success
Feb 21 10:21:07.563: INFO: Pod "pod-secrets-659083a2-35c2-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:21:07.570: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-secrets-659083a2-35c2-11e9-b7c5-a2b84e263bfe container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 10:21:07.604: INFO: Waiting for pod pod-secrets-659083a2-35c2-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:21:07.609: INFO: Pod pod-secrets-659083a2-35c2-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:21:07.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-s5dvg" for this suite.
Feb 21 10:21:13.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:21:13.722: INFO: namespace: e2e-tests-secrets-s5dvg, resource: bindings, ignored listing per whitelist
Feb 21 10:21:13.872: INFO: namespace e2e-tests-secrets-s5dvg deletion completed in 6.256006575s
STEP: Destroying namespace "e2e-tests-secret-namespace-x6zx4" for this suite.
Feb 21 10:21:19.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:21:20.232: INFO: namespace: e2e-tests-secret-namespace-x6zx4, resource: bindings, ignored listing per whitelist
Feb 21 10:21:20.444: INFO: namespace e2e-tests-secret-namespace-x6zx4 deletion completed in 6.571992894s

• [SLOW TEST:15.696 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:21:20.444: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ncwkn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 21 10:21:23.629: INFO: Successfully updated pod "labelsupdate6eaff2e7-35c2-11e9-b7c5-a2b84e263bfe"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:21:27.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ncwkn" for this suite.
Feb 21 10:21:49.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:21:50.309: INFO: namespace: e2e-tests-downward-api-ncwkn, resource: bindings, ignored listing per whitelist
Feb 21 10:21:50.417: INFO: namespace e2e-tests-downward-api-ncwkn deletion completed in 22.728084206s

• [SLOW TEST:29.972 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:21:50.417: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-jsznp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-bk27
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 10:21:50.835: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-bk27" in namespace "e2e-tests-subpath-jsznp" to be "success or failure"
Feb 21 10:21:50.843: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Pending", Reason="", readiness=false. Elapsed: 8.105627ms
Feb 21 10:21:52.853: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01837331s
Feb 21 10:21:54.861: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Running", Reason="", readiness=false. Elapsed: 4.025785851s
Feb 21 10:21:56.871: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Running", Reason="", readiness=false. Elapsed: 6.035548275s
Feb 21 10:21:58.879: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Running", Reason="", readiness=false. Elapsed: 8.043840101s
Feb 21 10:22:00.886: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Running", Reason="", readiness=false. Elapsed: 10.051117176s
Feb 21 10:22:02.914: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Running", Reason="", readiness=false. Elapsed: 12.078726097s
Feb 21 10:22:04.929: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Running", Reason="", readiness=false. Elapsed: 14.094190237s
Feb 21 10:22:06.945: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Running", Reason="", readiness=false. Elapsed: 16.110220972s
Feb 21 10:22:08.954: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Running", Reason="", readiness=false. Elapsed: 18.118627813s
Feb 21 10:22:10.968: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Running", Reason="", readiness=false. Elapsed: 20.13320064s
Feb 21 10:22:12.976: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Running", Reason="", readiness=false. Elapsed: 22.141210846s
Feb 21 10:22:14.996: INFO: Pod "pod-subpath-test-downwardapi-bk27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.160829647s
STEP: Saw pod success
Feb 21 10:22:14.996: INFO: Pod "pod-subpath-test-downwardapi-bk27" satisfied condition "success or failure"
Feb 21 10:22:15.219: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-subpath-test-downwardapi-bk27 container test-container-subpath-downwardapi-bk27: <nil>
STEP: delete the pod
Feb 21 10:22:15.470: INFO: Waiting for pod pod-subpath-test-downwardapi-bk27 to disappear
Feb 21 10:22:15.475: INFO: Pod pod-subpath-test-downwardapi-bk27 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-bk27
Feb 21 10:22:15.475: INFO: Deleting pod "pod-subpath-test-downwardapi-bk27" in namespace "e2e-tests-subpath-jsznp"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:22:15.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-jsznp" for this suite.
Feb 21 10:22:21.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:22:21.559: INFO: namespace: e2e-tests-subpath-jsznp, resource: bindings, ignored listing per whitelist
Feb 21 10:22:22.209: INFO: namespace e2e-tests-subpath-jsznp deletion completed in 6.707979117s

• [SLOW TEST:31.792 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:22:22.209: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-c7qjc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 10:22:22.618: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-c7qjc'
Feb 21 10:22:22.827: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 21 10:22:22.827: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
Feb 21 10:22:24.841: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-c7qjc'
Feb 21 10:22:24.966: INFO: stderr: ""
Feb 21 10:22:24.966: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:22:24.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-c7qjc" for this suite.
Feb 21 10:22:30.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:22:31.125: INFO: namespace: e2e-tests-kubectl-c7qjc, resource: bindings, ignored listing per whitelist
Feb 21 10:22:31.473: INFO: namespace e2e-tests-kubectl-c7qjc deletion completed in 6.498025895s

• [SLOW TEST:9.266 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:22:31.475: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-8zh8t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 10:22:31.986: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 21 10:22:32.215: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-8zh8t/daemonsets","resourceVersion":"23871"},"items":null}

Feb 21 10:22:32.219: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-8zh8t/pods","resourceVersion":"23871"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:22:32.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-8zh8t" for this suite.
Feb 21 10:22:38.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:22:38.809: INFO: namespace: e2e-tests-daemonsets-8zh8t, resource: bindings, ignored listing per whitelist
Feb 21 10:22:38.858: INFO: namespace e2e-tests-daemonsets-8zh8t deletion completed in 6.579085677s

S [SKIPPING] [7.382 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 21 10:22:31.986: Requires at least 2 nodes (not -1)

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:22:38.858: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-j59td
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 21 10:22:39.248: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 21 10:22:44.254: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 21 10:22:44.254: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 21 10:22:44.288: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-j59td,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j59td/deployments/test-cleanup-deployment,UID:a069c224-35c2-11e9-890e-3aac84b8a476,ResourceVersion:23915,Generation:1,CreationTimestamp:2019-02-21 10:22:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Feb 21 10:22:44.294: INFO: New ReplicaSet "test-cleanup-deployment-755f6b95cc" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc,GenerateName:,Namespace:e2e-tests-deployment-j59td,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j59td/replicasets/test-cleanup-deployment-755f6b95cc,UID:a06c3e7a-35c2-11e9-890e-3aac84b8a476,ResourceVersion:23917,Generation:1,CreationTimestamp:2019-02-21 10:22:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a069c224-35c2-11e9-890e-3aac84b8a476 0xc00131bb97 0xc00131bb98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 10:22:44.294: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 21 10:22:44.294: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-j59td,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j59td/replicasets/test-cleanup-controller,UID:9d696549-35c2-11e9-890e-3aac84b8a476,ResourceVersion:23916,Generation:1,CreationTimestamp:2019-02-21 10:22:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a069c224-35c2-11e9-890e-3aac84b8a476 0xc00131bad7 0xc00131bad8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 21 10:22:44.301: INFO: Pod "test-cleanup-controller-6zw7b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-6zw7b,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-j59td,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j59td/pods/test-cleanup-controller-6zw7b,UID:9d6bb440-35c2-11e9-890e-3aac84b8a476,ResourceVersion:23910,Generation:0,CreationTimestamp:2019-02-21 10:22:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.213/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 9d696549-35c2-11e9-890e-3aac84b8a476 0xc001cdc837 0xc001cdc838}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xkzfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xkzfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xkzfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001cdc8a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001cdc8c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 10:22:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 10:22:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 10:22:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 10:22:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.19,PodIP:100.96.1.213,StartTime:2019-02-21 10:22:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-21 10:22:40 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://b4f4f0008dec55c27187bc06baedff248b6401b4becdee695020d6cfe8c93c50}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 10:22:44.301: INFO: Pod "test-cleanup-deployment-755f6b95cc-2ntx9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc-2ntx9,GenerateName:test-cleanup-deployment-755f6b95cc-,Namespace:e2e-tests-deployment-j59td,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j59td/pods/test-cleanup-deployment-755f6b95cc-2ntx9,UID:a06ce33e-35c2-11e9-890e-3aac84b8a476,ResourceVersion:23921,Generation:0,CreationTimestamp:2019-02-21 10:22:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-755f6b95cc a06c3e7a-35c2-11e9-890e-3aac84b8a476 0xc001cdc9f7 0xc001cdc9f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-xkzfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xkzfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-xkzfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001cdcaf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001cdcb10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-21 10:22:44 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:22:44.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-j59td" for this suite.
Feb 21 10:22:50.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:22:50.388: INFO: namespace: e2e-tests-deployment-j59td, resource: bindings, ignored listing per whitelist
Feb 21 10:22:50.565: INFO: namespace e2e-tests-deployment-j59td deletion completed in 6.256302742s

• [SLOW TEST:11.706 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:22:50.565: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-fmbjd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-fmbjd
Feb 21 10:22:53.050: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-fmbjd
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 10:22:53.058: INFO: Initial restart count of pod liveness-exec is 0
Feb 21 10:23:39.283: INFO: Restart count of pod e2e-tests-container-probe-fmbjd/liveness-exec is now 1 (46.224669551s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:23:39.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fmbjd" for this suite.
Feb 21 10:23:45.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:23:45.367: INFO: namespace: e2e-tests-container-probe-fmbjd, resource: bindings, ignored listing per whitelist
Feb 21 10:23:45.742: INFO: namespace e2e-tests-container-probe-fmbjd deletion completed in 6.437915704s

• [SLOW TEST:55.178 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:23:45.743: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-2zn6n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 21 10:23:50.162: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 10:23:50.167: INFO: Pod pod-with-poststart-http-hook still exists
Feb 21 10:23:52.167: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 10:23:52.175: INFO: Pod pod-with-poststart-http-hook still exists
Feb 21 10:23:54.167: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 10:23:54.174: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:23:54.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-2zn6n" for this suite.
Feb 21 10:24:16.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:24:16.290: INFO: namespace: e2e-tests-container-lifecycle-hook-2zn6n, resource: bindings, ignored listing per whitelist
Feb 21 10:24:16.693: INFO: namespace e2e-tests-container-lifecycle-hook-2zn6n deletion completed in 22.510700748s

• [SLOW TEST:30.951 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:24:16.694: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-6nnf7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 21 10:24:17.741: INFO: Waiting up to 5m0s for pod "pod-d81e71b5-35c2-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-6nnf7" to be "success or failure"
Feb 21 10:24:17.750: INFO: Pod "pod-d81e71b5-35c2-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.789693ms
Feb 21 10:24:19.758: INFO: Pod "pod-d81e71b5-35c2-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016295235s
STEP: Saw pod success
Feb 21 10:24:19.758: INFO: Pod "pod-d81e71b5-35c2-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:24:19.765: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-d81e71b5-35c2-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 10:24:19.790: INFO: Waiting for pod pod-d81e71b5-35c2-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:24:19.794: INFO: Pod pod-d81e71b5-35c2-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:24:19.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6nnf7" for this suite.
Feb 21 10:24:25.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:24:25.917: INFO: namespace: e2e-tests-emptydir-6nnf7, resource: bindings, ignored listing per whitelist
Feb 21 10:24:26.332: INFO: namespace e2e-tests-emptydir-6nnf7 deletion completed in 6.498557332s

• [SLOW TEST:9.639 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:24:26.333: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-qprw2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-4x6r
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 10:24:26.636: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-4x6r" in namespace "e2e-tests-subpath-qprw2" to be "success or failure"
Feb 21 10:24:26.641: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Pending", Reason="", readiness=false. Elapsed: 5.618875ms
Feb 21 10:24:28.649: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013253579s
Feb 21 10:24:30.655: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Running", Reason="", readiness=false. Elapsed: 4.019147214s
Feb 21 10:24:32.730: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Running", Reason="", readiness=false. Elapsed: 6.094094663s
Feb 21 10:24:34.737: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Running", Reason="", readiness=false. Elapsed: 8.101522593s
Feb 21 10:24:36.748: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Running", Reason="", readiness=false. Elapsed: 10.112125363s
Feb 21 10:24:38.755: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Running", Reason="", readiness=false. Elapsed: 12.119556471s
Feb 21 10:24:40.774: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Running", Reason="", readiness=false. Elapsed: 14.138113891s
Feb 21 10:24:42.781: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Running", Reason="", readiness=false. Elapsed: 16.145627717s
Feb 21 10:24:44.792: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Running", Reason="", readiness=false. Elapsed: 18.156060096s
Feb 21 10:24:46.799: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Running", Reason="", readiness=false. Elapsed: 20.163667693s
Feb 21 10:24:48.811: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Running", Reason="", readiness=false. Elapsed: 22.175727713s
Feb 21 10:24:50.817: INFO: Pod "pod-subpath-test-projected-4x6r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.181407659s
STEP: Saw pod success
Feb 21 10:24:50.817: INFO: Pod "pod-subpath-test-projected-4x6r" satisfied condition "success or failure"
Feb 21 10:24:50.821: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-subpath-test-projected-4x6r container test-container-subpath-projected-4x6r: <nil>
STEP: delete the pod
Feb 21 10:24:50.847: INFO: Waiting for pod pod-subpath-test-projected-4x6r to disappear
Feb 21 10:24:50.853: INFO: Pod pod-subpath-test-projected-4x6r no longer exists
STEP: Deleting pod pod-subpath-test-projected-4x6r
Feb 21 10:24:50.854: INFO: Deleting pod "pod-subpath-test-projected-4x6r" in namespace "e2e-tests-subpath-qprw2"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:24:50.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-qprw2" for this suite.
Feb 21 10:24:56.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:24:57.069: INFO: namespace: e2e-tests-subpath-qprw2, resource: bindings, ignored listing per whitelist
Feb 21 10:24:57.452: INFO: namespace e2e-tests-subpath-qprw2 deletion completed in 6.585990448s

• [SLOW TEST:31.120 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:24:57.452: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-mdsl4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 21 10:24:57.823: INFO: Waiting up to 5m0s for pod "client-containers-f002851b-35c2-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-containers-mdsl4" to be "success or failure"
Feb 21 10:24:57.829: INFO: Pod "client-containers-f002851b-35c2-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.334225ms
Feb 21 10:24:59.841: INFO: Pod "client-containers-f002851b-35c2-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017849505s
STEP: Saw pod success
Feb 21 10:24:59.841: INFO: Pod "client-containers-f002851b-35c2-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:24:59.847: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod client-containers-f002851b-35c2-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 10:24:59.873: INFO: Waiting for pod client-containers-f002851b-35c2-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:24:59.879: INFO: Pod client-containers-f002851b-35c2-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:24:59.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-mdsl4" for this suite.
Feb 21 10:25:05.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:25:06.358: INFO: namespace: e2e-tests-containers-mdsl4, resource: bindings, ignored listing per whitelist
Feb 21 10:25:06.598: INFO: namespace e2e-tests-containers-mdsl4 deletion completed in 6.712034796s

• [SLOW TEST:9.146 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:25:06.598: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-nccfr
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 21 10:25:07.446: INFO: Waiting up to 5m0s for pod "pod-f5beefd9-35c2-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-nccfr" to be "success or failure"
Feb 21 10:25:07.456: INFO: Pod "pod-f5beefd9-35c2-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 9.293486ms
Feb 21 10:25:09.462: INFO: Pod "pod-f5beefd9-35c2-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015753974s
STEP: Saw pod success
Feb 21 10:25:09.462: INFO: Pod "pod-f5beefd9-35c2-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:25:09.467: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-f5beefd9-35c2-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 10:25:09.493: INFO: Waiting for pod pod-f5beefd9-35c2-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:25:09.720: INFO: Pod pod-f5beefd9-35c2-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:25:09.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nccfr" for this suite.
Feb 21 10:25:15.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:25:15.892: INFO: namespace: e2e-tests-emptydir-nccfr, resource: bindings, ignored listing per whitelist
Feb 21 10:25:16.258: INFO: namespace e2e-tests-emptydir-nccfr deletion completed in 6.521631598s

• [SLOW TEST:9.660 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:25:16.259: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-p54g9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-p54g9
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 10:25:16.632: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 21 10:25:30.800: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://100.96.1.222:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-p54g9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:25:30.801: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:25:31.408: INFO: Found all expected endpoints: [netserver-0]
Feb 21 10:25:31.416: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://100.96.0.68:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-p54g9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 10:25:31.416: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 21 10:25:31.869: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:25:31.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-p54g9" for this suite.
Feb 21 10:25:53.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:25:54.077: INFO: namespace: e2e-tests-pod-network-test-p54g9, resource: bindings, ignored listing per whitelist
Feb 21 10:25:54.121: INFO: namespace e2e-tests-pod-network-test-p54g9 deletion completed in 22.238804967s

• [SLOW TEST:37.862 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:25:54.121: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-qsplg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 21 10:25:54.622: INFO: Waiting up to 5m0s for pod "pod-11dd7841-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-qsplg" to be "success or failure"
Feb 21 10:25:54.628: INFO: Pod "pod-11dd7841-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.536088ms
Feb 21 10:25:56.639: INFO: Pod "pod-11dd7841-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016855826s
STEP: Saw pod success
Feb 21 10:25:56.639: INFO: Pod "pod-11dd7841-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:25:56.649: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-11dd7841-35c3-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 10:25:56.676: INFO: Waiting for pod pod-11dd7841-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:25:56.680: INFO: Pod pod-11dd7841-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:25:56.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qsplg" for this suite.
Feb 21 10:26:02.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:26:03.229: INFO: namespace: e2e-tests-emptydir-qsplg, resource: bindings, ignored listing per whitelist
Feb 21 10:26:03.334: INFO: namespace e2e-tests-emptydir-qsplg deletion completed in 6.64730878s

• [SLOW TEST:9.213 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:26:03.335: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-dsj49
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-dsj49
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-dsj49
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-dsj49
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-dsj49
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-dsj49
Feb 21 10:26:06.018: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-dsj49, name: ss-0, uid: 18a6b9ca-35c3-11e9-890e-3aac84b8a476, status phase: Failed. Waiting for statefulset controller to delete.
Feb 21 10:26:06.048: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-dsj49, name: ss-0, uid: 18a6b9ca-35c3-11e9-890e-3aac84b8a476, status phase: Failed. Waiting for statefulset controller to delete.
Feb 21 10:26:06.051: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-dsj49
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-dsj49
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-dsj49 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 21 10:26:08.317: INFO: Deleting all statefulset in ns e2e-tests-statefulset-dsj49
Feb 21 10:26:08.334: INFO: Scaling statefulset ss to 0
Feb 21 10:26:18.385: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 10:26:18.393: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:26:18.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-dsj49" for this suite.
Feb 21 10:26:24.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:26:24.794: INFO: namespace: e2e-tests-statefulset-dsj49, resource: bindings, ignored listing per whitelist
Feb 21 10:26:24.900: INFO: namespace e2e-tests-statefulset-dsj49 deletion completed in 6.270011585s

• [SLOW TEST:21.565 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:26:24.900: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-thpmq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 10:26:25.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24491f67-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-thpmq" to be "success or failure"
Feb 21 10:26:25.539: INFO: Pod "downwardapi-volume-24491f67-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.561039ms
Feb 21 10:26:27.548: INFO: Pod "downwardapi-volume-24491f67-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015392373s
STEP: Saw pod success
Feb 21 10:26:27.548: INFO: Pod "downwardapi-volume-24491f67-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:26:27.555: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-24491f67-35c3-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 10:26:27.582: INFO: Waiting for pod downwardapi-volume-24491f67-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:26:27.588: INFO: Pod downwardapi-volume-24491f67-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:26:27.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-thpmq" for this suite.
Feb 21 10:26:33.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:26:34.250: INFO: namespace: e2e-tests-projected-thpmq, resource: bindings, ignored listing per whitelist
Feb 21 10:26:34.314: INFO: namespace e2e-tests-projected-thpmq deletion completed in 6.711867227s

• [SLOW TEST:9.413 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:26:34.314: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-nk5lk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:26:34.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-nk5lk" for this suite.
Feb 21 10:26:40.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:26:41.031: INFO: namespace: e2e-tests-services-nk5lk, resource: bindings, ignored listing per whitelist
Feb 21 10:26:41.040: INFO: namespace e2e-tests-services-nk5lk deletion completed in 6.301486622s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:6.726 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:26:41.041: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4fdbj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-2dd1eb62-35c3-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 10:26:41.768: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2df5052e-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-4fdbj" to be "success or failure"
Feb 21 10:26:41.792: INFO: Pod "pod-projected-secrets-2df5052e-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 23.786718ms
Feb 21 10:26:43.814: INFO: Pod "pod-projected-secrets-2df5052e-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046261309s
STEP: Saw pod success
Feb 21 10:26:43.815: INFO: Pod "pod-projected-secrets-2df5052e-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:26:43.825: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-secrets-2df5052e-35c3-11e9-b7c5-a2b84e263bfe container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 10:26:43.879: INFO: Waiting for pod pod-projected-secrets-2df5052e-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:26:43.884: INFO: Pod pod-projected-secrets-2df5052e-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:26:43.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4fdbj" for this suite.
Feb 21 10:26:49.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:26:50.053: INFO: namespace: e2e-tests-projected-4fdbj, resource: bindings, ignored listing per whitelist
Feb 21 10:26:50.221: INFO: namespace e2e-tests-projected-4fdbj deletion completed in 6.328371013s

• [SLOW TEST:9.180 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:26:50.221: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-xb5f2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 21 10:26:51.135: INFO: Waiting up to 5m0s for pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-297m7" in namespace "e2e-tests-svcaccounts-xb5f2" to be "success or failure"
Feb 21 10:26:51.142: INFO: Pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-297m7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299603ms
Feb 21 10:26:53.148: INFO: Pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-297m7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013002262s
STEP: Saw pod success
Feb 21 10:26:53.149: INFO: Pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-297m7" satisfied condition "success or failure"
Feb 21 10:26:53.157: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-297m7 container token-test: <nil>
STEP: delete the pod
Feb 21 10:26:53.179: INFO: Waiting for pod pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-297m7 to disappear
Feb 21 10:26:53.184: INFO: Pod pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-297m7 no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 21 10:26:53.190: INFO: Waiting up to 5m0s for pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-5dqcb" in namespace "e2e-tests-svcaccounts-xb5f2" to be "success or failure"
Feb 21 10:26:53.196: INFO: Pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-5dqcb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.826679ms
Feb 21 10:26:55.202: INFO: Pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-5dqcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012450408s
STEP: Saw pod success
Feb 21 10:26:55.202: INFO: Pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-5dqcb" satisfied condition "success or failure"
Feb 21 10:26:55.208: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-5dqcb container root-ca-test: <nil>
STEP: delete the pod
Feb 21 10:26:55.232: INFO: Waiting for pod pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-5dqcb to disappear
Feb 21 10:26:55.242: INFO: Pod pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-5dqcb no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 21 10:26:55.256: INFO: Waiting up to 5m0s for pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-9q4jj" in namespace "e2e-tests-svcaccounts-xb5f2" to be "success or failure"
Feb 21 10:26:55.263: INFO: Pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-9q4jj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091022ms
Feb 21 10:26:57.323: INFO: Pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-9q4jj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.066824649s
STEP: Saw pod success
Feb 21 10:26:57.323: INFO: Pod "pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-9q4jj" satisfied condition "success or failure"
Feb 21 10:26:57.335: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-9q4jj container namespace-test: <nil>
STEP: delete the pod
Feb 21 10:26:57.445: INFO: Waiting for pod pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-9q4jj to disappear
Feb 21 10:26:57.452: INFO: Pod pod-service-account-338c3cbe-35c3-11e9-b7c5-a2b84e263bfe-9q4jj no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:26:57.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-xb5f2" for this suite.
Feb 21 10:27:03.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:27:03.679: INFO: namespace: e2e-tests-svcaccounts-xb5f2, resource: bindings, ignored listing per whitelist
Feb 21 10:27:03.759: INFO: namespace e2e-tests-svcaccounts-xb5f2 deletion completed in 6.231018351s

• [SLOW TEST:13.538 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:27:03.765: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-57f6w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 10:27:04.426: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b781668-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-57f6w" to be "success or failure"
Feb 21 10:27:04.433: INFO: Pod "downwardapi-volume-3b781668-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.03379ms
Feb 21 10:27:06.438: INFO: Pod "downwardapi-volume-3b781668-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011756933s
STEP: Saw pod success
Feb 21 10:27:06.438: INFO: Pod "downwardapi-volume-3b781668-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:27:06.442: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-3b781668-35c3-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 10:27:06.464: INFO: Waiting for pod downwardapi-volume-3b781668-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:27:06.468: INFO: Pod downwardapi-volume-3b781668-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:27:06.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-57f6w" for this suite.
Feb 21 10:27:12.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:27:12.899: INFO: namespace: e2e-tests-projected-57f6w, resource: bindings, ignored listing per whitelist
Feb 21 10:27:12.942: INFO: namespace e2e-tests-projected-57f6w deletion completed in 6.464542684s

• [SLOW TEST:9.177 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:27:12.948: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5cxx6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 21 10:27:13.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-410427b4-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-5cxx6" to be "success or failure"
Feb 21 10:27:13.739: INFO: Pod "downwardapi-volume-410427b4-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.145908ms
Feb 21 10:27:15.746: INFO: Pod "downwardapi-volume-410427b4-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014770485s
STEP: Saw pod success
Feb 21 10:27:15.749: INFO: Pod "downwardapi-volume-410427b4-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:27:15.755: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downwardapi-volume-410427b4-35c3-11e9-b7c5-a2b84e263bfe container client-container: <nil>
STEP: delete the pod
Feb 21 10:27:15.780: INFO: Waiting for pod downwardapi-volume-410427b4-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:27:15.784: INFO: Pod downwardapi-volume-410427b4-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:27:15.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5cxx6" for this suite.
Feb 21 10:27:21.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:27:22.465: INFO: namespace: e2e-tests-projected-5cxx6, resource: bindings, ignored listing per whitelist
Feb 21 10:27:22.552: INFO: namespace e2e-tests-projected-5cxx6 deletion completed in 6.753893476s

• [SLOW TEST:9.605 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:27:22.553: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-tbjhm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 10:27:23.487: INFO: Number of nodes with available pods: 0
Feb 21 10:27:23.487: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 10:27:24.500: INFO: Number of nodes with available pods: 0
Feb 21 10:27:24.500: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 10:27:25.504: INFO: Number of nodes with available pods: 2
Feb 21 10:27:25.504: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 21 10:27:25.546: INFO: Number of nodes with available pods: 1
Feb 21 10:27:25.546: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 10:27:26.562: INFO: Number of nodes with available pods: 1
Feb 21 10:27:26.563: INFO: Node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-c4cbw is running more than one daemon pod
Feb 21 10:27:27.561: INFO: Number of nodes with available pods: 2
Feb 21 10:27:27.561: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-tbjhm, will wait for the garbage collector to delete the pods
Feb 21 10:27:27.638: INFO: Deleting {extensions DaemonSet} daemon-set took: 11.757627ms
Feb 21 10:27:27.738: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.57134ms
Feb 21 10:28:01.743: INFO: Number of nodes with available pods: 0
Feb 21 10:28:01.744: INFO: Number of running nodes: 0, number of available pods: 0
Feb 21 10:28:01.752: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-tbjhm/daemonsets","resourceVersion":"25022"},"items":null}

Feb 21 10:28:01.756: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-tbjhm/pods","resourceVersion":"25022"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:28:01.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-tbjhm" for this suite.
Feb 21 10:28:07.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:28:07.900: INFO: namespace: e2e-tests-daemonsets-tbjhm, resource: bindings, ignored listing per whitelist
Feb 21 10:28:08.034: INFO: namespace e2e-tests-daemonsets-tbjhm deletion completed in 6.252786149s

• [SLOW TEST:45.482 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:28:08.035: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-t9dqr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 21 10:28:08.311: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:28:11.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-t9dqr" for this suite.
Feb 21 10:28:33.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:28:33.594: INFO: namespace: e2e-tests-init-container-t9dqr, resource: bindings, ignored listing per whitelist
Feb 21 10:28:33.967: INFO: namespace e2e-tests-init-container-t9dqr deletion completed in 22.405508397s

• [SLOW TEST:25.932 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:28:33.968: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-tcdk9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-713350cc-35c3-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 10:28:34.574: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-71344ab5-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-tcdk9" to be "success or failure"
Feb 21 10:28:34.791: INFO: Pod "pod-projected-secrets-71344ab5-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 216.375028ms
Feb 21 10:28:36.798: INFO: Pod "pod-projected-secrets-71344ab5-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.22348606s
STEP: Saw pod success
Feb 21 10:28:36.798: INFO: Pod "pod-projected-secrets-71344ab5-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:28:36.802: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-secrets-71344ab5-35c3-11e9-b7c5-a2b84e263bfe container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 10:28:36.828: INFO: Waiting for pod pod-projected-secrets-71344ab5-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:28:36.833: INFO: Pod pod-projected-secrets-71344ab5-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:28:36.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tcdk9" for this suite.
Feb 21 10:28:42.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:28:43.011: INFO: namespace: e2e-tests-projected-tcdk9, resource: bindings, ignored listing per whitelist
Feb 21 10:28:43.046: INFO: namespace e2e-tests-projected-tcdk9 deletion completed in 6.206369548s

• [SLOW TEST:9.079 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:28:43.047: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-7xmf8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0221 10:28:53.477743   32452 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 21 10:28:53.477: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:28:53.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-7xmf8" for this suite.
Feb 21 10:28:59.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:28:59.636: INFO: namespace: e2e-tests-gc-7xmf8, resource: bindings, ignored listing per whitelist
Feb 21 10:28:59.665: INFO: namespace e2e-tests-gc-7xmf8 deletion completed in 6.180013083s

• [SLOW TEST:16.619 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:28:59.666: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rm7hn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-807d4eb7-35c3-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 10:29:00.232: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-807f295e-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-rm7hn" to be "success or failure"
Feb 21 10:29:00.238: INFO: Pod "pod-projected-secrets-807f295e-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375513ms
Feb 21 10:29:02.246: INFO: Pod "pod-projected-secrets-807f295e-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013729886s
STEP: Saw pod success
Feb 21 10:29:02.246: INFO: Pod "pod-projected-secrets-807f295e-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:29:02.250: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-projected-secrets-807f295e-35c3-11e9-b7c5-a2b84e263bfe container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 10:29:02.276: INFO: Waiting for pod pod-projected-secrets-807f295e-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:29:02.280: INFO: Pod pod-projected-secrets-807f295e-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:29:02.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rm7hn" for this suite.
Feb 21 10:29:08.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:29:08.329: INFO: namespace: e2e-tests-projected-rm7hn, resource: bindings, ignored listing per whitelist
Feb 21 10:29:08.506: INFO: namespace e2e-tests-projected-rm7hn deletion completed in 6.219267087s

• [SLOW TEST:8.840 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:29:08.506: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-k2nq7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-859f3ad1-35c3-11e9-b7c5-a2b84e263bfe
STEP: Creating secret with name secret-projected-all-test-volume-859f3ab8-35c3-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 21 10:29:08.845: INFO: Waiting up to 5m0s for pod "projected-volume-859f3a78-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-projected-k2nq7" to be "success or failure"
Feb 21 10:29:08.848: INFO: Pod "projected-volume-859f3a78-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.606716ms
Feb 21 10:29:10.854: INFO: Pod "projected-volume-859f3a78-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009507697s
STEP: Saw pod success
Feb 21 10:29:10.854: INFO: Pod "projected-volume-859f3a78-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:29:10.859: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod projected-volume-859f3a78-35c3-11e9-b7c5-a2b84e263bfe container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 21 10:29:10.883: INFO: Waiting for pod projected-volume-859f3a78-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:29:10.887: INFO: Pod projected-volume-859f3a78-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:29:10.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k2nq7" for this suite.
Feb 21 10:29:16.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:29:17.232: INFO: namespace: e2e-tests-projected-k2nq7, resource: bindings, ignored listing per whitelist
Feb 21 10:29:17.382: INFO: namespace e2e-tests-projected-k2nq7 deletion completed in 6.486651188s

• [SLOW TEST:8.876 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:29:17.383: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-54cv9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 21 10:29:17.718: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:29:17.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-54cv9" for this suite.
Feb 21 10:29:23.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:29:24.047: INFO: namespace: e2e-tests-kubectl-54cv9, resource: bindings, ignored listing per whitelist
Feb 21 10:29:24.088: INFO: namespace e2e-tests-kubectl-54cv9 deletion completed in 6.166371991s

• [SLOW TEST:6.705 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:29:24.088: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-spc4w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8eec4244-35c3-11e9-b7c5-a2b84e263bfe
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-8eec4244-35c3-11e9-b7c5-a2b84e263bfe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:29:28.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-spc4w" for this suite.
Feb 21 10:29:50.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:29:50.744: INFO: namespace: e2e-tests-projected-spc4w, resource: bindings, ignored listing per whitelist
Feb 21 10:29:50.817: INFO: namespace e2e-tests-projected-spc4w deletion completed in 22.238925058s

• [SLOW TEST:26.729 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:29:50.818: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2r56p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 21 10:29:51.314: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-2r56p'
Feb 21 10:29:51.608: INFO: stderr: ""
Feb 21 10:29:51.608: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 21 10:29:52.615: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 10:29:52.615: INFO: Found 0 / 1
Feb 21 10:29:53.616: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 10:29:53.616: INFO: Found 1 / 1
Feb 21 10:29:53.616: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 21 10:29:53.623: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 10:29:53.623: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 21 10:29:53.623: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml patch pod redis-master-nkp6l --namespace=e2e-tests-kubectl-2r56p -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 21 10:29:53.731: INFO: stderr: ""
Feb 21 10:29:53.732: INFO: stdout: "pod/redis-master-nkp6l patched\n"
STEP: checking annotations
Feb 21 10:29:53.736: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 10:29:53.736: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:29:53.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2r56p" for this suite.
Feb 21 10:30:15.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:30:15.862: INFO: namespace: e2e-tests-kubectl-2r56p, resource: bindings, ignored listing per whitelist
Feb 21 10:30:15.968: INFO: namespace e2e-tests-kubectl-2r56p deletion completed in 22.224813376s

• [SLOW TEST:25.150 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:30:15.968: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-skk4z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 21 10:30:16.418: INFO: Waiting up to 5m0s for pod "downward-api-ade847fe-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-downward-api-skk4z" to be "success or failure"
Feb 21 10:30:16.423: INFO: Pod "downward-api-ade847fe-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.971404ms
Feb 21 10:30:18.431: INFO: Pod "downward-api-ade847fe-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0127155s
STEP: Saw pod success
Feb 21 10:30:18.431: INFO: Pod "downward-api-ade847fe-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:30:18.438: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod downward-api-ade847fe-35c3-11e9-b7c5-a2b84e263bfe container dapi-container: <nil>
STEP: delete the pod
Feb 21 10:30:18.462: INFO: Waiting for pod downward-api-ade847fe-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:30:18.466: INFO: Pod downward-api-ade847fe-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:30:18.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-skk4z" for this suite.
Feb 21 10:30:24.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:30:24.538: INFO: namespace: e2e-tests-downward-api-skk4z, resource: bindings, ignored listing per whitelist
Feb 21 10:30:24.639: INFO: namespace e2e-tests-downward-api-skk4z deletion completed in 6.167947111s

• [SLOW TEST:8.671 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:30:24.640: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-bgwft
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-67jhn
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb 21 10:30:34.187: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-hpt98
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:30:51.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-bgwft" for this suite.
Feb 21 10:30:57.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:30:57.413: INFO: namespace: e2e-tests-namespaces-bgwft, resource: bindings, ignored listing per whitelist
Feb 21 10:30:57.554: INFO: namespace e2e-tests-namespaces-bgwft deletion completed in 6.225800267s
STEP: Destroying namespace "e2e-tests-nsdeletetest-67jhn" for this suite.
Feb 21 10:30:57.558: INFO: Namespace e2e-tests-nsdeletetest-67jhn was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-hpt98" for this suite.
Feb 21 10:31:03.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:31:03.617: INFO: namespace: e2e-tests-nsdeletetest-hpt98, resource: bindings, ignored listing per whitelist
Feb 21 10:31:03.734: INFO: namespace e2e-tests-nsdeletetest-hpt98 deletion completed in 6.175817163s

• [SLOW TEST:39.094 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:31:03.734: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-6zclr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 21 10:31:04.022: INFO: Waiting up to 5m0s for pod "pod-ca481556-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-emptydir-6zclr" to be "success or failure"
Feb 21 10:31:04.026: INFO: Pod "pod-ca481556-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.34585ms
Feb 21 10:31:06.034: INFO: Pod "pod-ca481556-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011844508s
STEP: Saw pod success
Feb 21 10:31:06.034: INFO: Pod "pod-ca481556-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:31:06.039: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-ca481556-35c3-11e9-b7c5-a2b84e263bfe container test-container: <nil>
STEP: delete the pod
Feb 21 10:31:06.129: INFO: Waiting for pod pod-ca481556-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:31:06.137: INFO: Pod pod-ca481556-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:31:06.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6zclr" for this suite.
Feb 21 10:31:12.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:31:12.289: INFO: namespace: e2e-tests-emptydir-6zclr, resource: bindings, ignored listing per whitelist
Feb 21 10:31:12.477: INFO: namespace e2e-tests-emptydir-6zclr deletion completed in 6.333363632s

• [SLOW TEST:8.743 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:31:12.478: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-tj5mt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 10:31:13.017: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-tj5mt'
Feb 21 10:31:13.642: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 21 10:31:13.642: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 21 10:31:13.652: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 21 10:31:13.658: INFO: scanned /root for discovery docs: <nil>
Feb 21 10:31:13.658: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-tj5mt'
Feb 21 10:31:29.485: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 21 10:31:29.485: INFO: stdout: "Created e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7\nScaling up e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 21 10:31:29.485: INFO: stdout: "Created e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7\nScaling up e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 21 10:31:29.485: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-tj5mt'
Feb 21 10:31:29.657: INFO: stderr: ""
Feb 21 10:31:29.657: INFO: stdout: "e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7-tzkbn "
Feb 21 10:31:29.657: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7-tzkbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tj5mt'
Feb 21 10:31:29.760: INFO: stderr: ""
Feb 21 10:31:29.760: INFO: stdout: "true"
Feb 21 10:31:29.760: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7-tzkbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tj5mt'
Feb 21 10:31:29.866: INFO: stderr: ""
Feb 21 10:31:29.866: INFO: stdout: "nginx:1.14-alpine"
Feb 21 10:31:29.866: INFO: e2e-test-nginx-rc-57541ec7393ba350326244a92b66b8d7-tzkbn is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
Feb 21 10:31:29.867: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-qerj9.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-tj5mt'
Feb 21 10:31:29.978: INFO: stderr: ""
Feb 21 10:31:29.978: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:31:29.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tj5mt" for this suite.
Feb 21 10:31:52.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:31:52.053: INFO: namespace: e2e-tests-kubectl-tj5mt, resource: bindings, ignored listing per whitelist
Feb 21 10:31:52.257: INFO: namespace e2e-tests-kubectl-tj5mt deletion completed in 22.272988209s

• [SLOW TEST:39.779 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:31:52.257: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6xg54
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-e73fc5f3-35c3-11e9-b7c5-a2b84e263bfe
STEP: Creating a pod to test consume secrets
Feb 21 10:31:52.627: INFO: Waiting up to 5m0s for pod "pod-secrets-e740b5ac-35c3-11e9-b7c5-a2b84e263bfe" in namespace "e2e-tests-secrets-6xg54" to be "success or failure"
Feb 21 10:31:52.631: INFO: Pod "pod-secrets-e740b5ac-35c3-11e9-b7c5-a2b84e263bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154249ms
Feb 21 10:31:54.879: INFO: Pod "pod-secrets-e740b5ac-35c3-11e9-b7c5-a2b84e263bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.251755216s
STEP: Saw pod success
Feb 21 10:31:54.879: INFO: Pod "pod-secrets-e740b5ac-35c3-11e9-b7c5-a2b84e263bfe" satisfied condition "success or failure"
Feb 21 10:31:54.884: INFO: Trying to get logs from node shoot--it--pub-os-qerj9-cpu-worker-z1-7d6845dfc9-zxtrl pod pod-secrets-e740b5ac-35c3-11e9-b7c5-a2b84e263bfe container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 10:31:54.908: INFO: Waiting for pod pod-secrets-e740b5ac-35c3-11e9-b7c5-a2b84e263bfe to disappear
Feb 21 10:31:54.912: INFO: Pod pod-secrets-e740b5ac-35c3-11e9-b7c5-a2b84e263bfe no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:31:54.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6xg54" for this suite.
Feb 21 10:32:00.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:32:01.219: INFO: namespace: e2e-tests-secrets-6xg54, resource: bindings, ignored listing per whitelist
Feb 21 10:32:01.388: INFO: namespace e2e-tests-secrets-6xg54 deletion completed in 6.465985101s

• [SLOW TEST:9.131 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 21 10:32:01.388: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qpsvg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-ecaddf56-35c3-11e9-b7c5-a2b84e263bfe
STEP: Creating secret with name s-test-opt-upd-ecaddfb9-35c3-11e9-b7c5-a2b84e263bfe
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ecaddf56-35c3-11e9-b7c5-a2b84e263bfe
STEP: Updating secret s-test-opt-upd-ecaddfb9-35c3-11e9-b7c5-a2b84e263bfe
STEP: Creating secret with name s-test-opt-create-ecaddfe3-35c3-11e9-b7c5-a2b84e263bfe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 21 10:32:08.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qpsvg" for this suite.
Feb 21 10:32:30.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 10:32:30.450: INFO: namespace: e2e-tests-projected-qpsvg, resource: bindings, ignored listing per whitelist
Feb 21 10:32:30.459: INFO: namespace e2e-tests-projected-qpsvg deletion completed in 22.211565831s

• [SLOW TEST:29.072 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSFeb 21 10:32:30.460: INFO: Running AfterSuite actions on all node
Feb 21 10:32:30.460: INFO: Running AfterSuite actions on node 1
Feb 21 10:32:30.460: INFO: Skipping dumping logs from cluster

Ran 187 of 2011 Specs in 7612.898 seconds
SUCCESS! -- 187 Passed | 0 Failed | 0 Flaked | 0 Pending | 1824 Skipped PASS

Ginkgo ran 1 suite in 2h6m54.095610005s
Test Suite Passed
