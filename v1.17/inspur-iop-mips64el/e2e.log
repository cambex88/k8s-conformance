I0210 08:27:40.411848      23 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-754024871
I0210 08:27:40.411994      23 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0210 08:27:40.412946      23 e2e.go:109] Starting e2e run "18d479c9-9dfb-4697-a934-ba3fe3414b0a" on Ginkgo node 1
{"msg":"Test Suite starting","total":278,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1581323249 - Will randomize all specs
Will run 278 of 4814 specs

Feb 10 08:27:40.885: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:27:40.896: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 10 08:27:41.112: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 10 08:27:41.303: INFO: 32 / 32 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 10 08:27:41.303: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Feb 10 08:27:41.303: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 10 08:27:41.360: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 10 08:27:41.360: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'coredns' (0 seconds elapsed)
Feb 10 08:27:41.360: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'localpv-provisioner-ds' (0 seconds elapsed)
Feb 10 08:27:41.360: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ntp' (0 seconds elapsed)
Feb 10 08:27:41.361: INFO: e2e test version: v1.17.0
Feb 10 08:27:41.364: INFO: kube-apiserver version: v1.17.0
Feb 10 08:27:41.364: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:27:41.385: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:27:41.388: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
Feb 10 08:27:41.683: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 10 08:27:41.740: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 08:27:41.906: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c867e0df-2e8f-4c7d-837f-fd4d564fa175" in namespace "projected-1666" to be "success or failure"
Feb 10 08:27:41.918: INFO: Pod "downwardapi-volume-c867e0df-2e8f-4c7d-837f-fd4d564fa175": Phase="Pending", Reason="", readiness=false. Elapsed: 11.511544ms
Feb 10 08:27:43.932: INFO: Pod "downwardapi-volume-c867e0df-2e8f-4c7d-837f-fd4d564fa175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02536981s
Feb 10 08:27:45.946: INFO: Pod "downwardapi-volume-c867e0df-2e8f-4c7d-837f-fd4d564fa175": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039438353s
Feb 10 08:27:47.958: INFO: Pod "downwardapi-volume-c867e0df-2e8f-4c7d-837f-fd4d564fa175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051788674s
STEP: Saw pod success
Feb 10 08:27:47.958: INFO: Pod "downwardapi-volume-c867e0df-2e8f-4c7d-837f-fd4d564fa175" satisfied condition "success or failure"
Feb 10 08:27:47.968: INFO: Trying to get logs from node master3 pod downwardapi-volume-c867e0df-2e8f-4c7d-837f-fd4d564fa175 container client-container: <nil>
STEP: delete the pod
Feb 10 08:27:48.355: INFO: Waiting for pod downwardapi-volume-c867e0df-2e8f-4c7d-837f-fd4d564fa175 to disappear
Feb 10 08:27:48.366: INFO: Pod downwardapi-volume-c867e0df-2e8f-4c7d-837f-fd4d564fa175 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:27:48.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1666" for this suite.

• [SLOW TEST:7.017 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":278,"completed":1,"skipped":21,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:27:48.405: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:27:48.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9562" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":278,"completed":2,"skipped":25,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:27:48.883: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 08:28:36.601: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 08:28:38.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920116, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920116, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920116, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920116, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 08:28:40.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920116, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920116, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920116, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920116, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 08:28:43.684: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:28:43.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1401" for this suite.
STEP: Destroying namespace "webhook-1401-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:55.393 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":278,"completed":3,"skipped":60,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:28:44.282: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 10 08:28:44.742: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:28:58.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7210" for this suite.

• [SLOW TEST:14.418 seconds]
[k8s.io] Pods
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":278,"completed":4,"skipped":114,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:28:58.703: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:28:59.218: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 10 08:28:59.265: INFO: Number of nodes with available pods: 0
Feb 10 08:28:59.265: INFO: Node master1 is running more than one daemon pod
Feb 10 08:29:00.302: INFO: Number of nodes with available pods: 0
Feb 10 08:29:00.302: INFO: Node master1 is running more than one daemon pod
Feb 10 08:29:01.300: INFO: Number of nodes with available pods: 0
Feb 10 08:29:01.300: INFO: Node master1 is running more than one daemon pod
Feb 10 08:29:02.301: INFO: Number of nodes with available pods: 0
Feb 10 08:29:02.309: INFO: Node master1 is running more than one daemon pod
Feb 10 08:29:03.307: INFO: Number of nodes with available pods: 0
Feb 10 08:29:03.307: INFO: Node master1 is running more than one daemon pod
Feb 10 08:29:04.301: INFO: Number of nodes with available pods: 2
Feb 10 08:29:04.302: INFO: Node master2 is running more than one daemon pod
Feb 10 08:29:05.296: INFO: Number of nodes with available pods: 3
Feb 10 08:29:05.297: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 10 08:29:05.430: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:05.430: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:05.430: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:06.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:06.478: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:06.478: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:07.476: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:07.476: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:07.476: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:08.478: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:08.478: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:08.478: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:09.482: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:09.483: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:09.483: INFO: Pod daemon-set-4n4rj is not available
Feb 10 08:29:09.483: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:10.475: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:10.476: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:10.476: INFO: Pod daemon-set-4n4rj is not available
Feb 10 08:29:10.476: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:11.476: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:11.476: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:11.476: INFO: Pod daemon-set-4n4rj is not available
Feb 10 08:29:11.477: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:12.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:12.477: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:12.477: INFO: Pod daemon-set-4n4rj is not available
Feb 10 08:29:12.477: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:13.475: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:13.475: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:13.475: INFO: Pod daemon-set-4n4rj is not available
Feb 10 08:29:13.475: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:14.475: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:14.476: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:14.476: INFO: Pod daemon-set-4n4rj is not available
Feb 10 08:29:14.476: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:15.494: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:15.494: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:15.494: INFO: Pod daemon-set-4n4rj is not available
Feb 10 08:29:15.494: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:16.479: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:16.479: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:16.480: INFO: Pod daemon-set-4n4rj is not available
Feb 10 08:29:16.480: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:17.478: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:17.478: INFO: Wrong image for pod: daemon-set-4n4rj. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:17.478: INFO: Pod daemon-set-4n4rj is not available
Feb 10 08:29:17.478: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:18.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:18.477: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:18.477: INFO: Pod daemon-set-s4nmp is not available
Feb 10 08:29:19.481: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:19.481: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:19.481: INFO: Pod daemon-set-s4nmp is not available
Feb 10 08:29:20.474: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:20.474: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:20.474: INFO: Pod daemon-set-s4nmp is not available
Feb 10 08:29:21.474: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:21.475: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:21.475: INFO: Pod daemon-set-s4nmp is not available
Feb 10 08:29:22.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:22.477: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:23.475: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:23.475: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:24.482: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:24.482: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:25.476: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:25.476: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:26.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:26.477: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:26.477: INFO: Pod daemon-set-p9ggq is not available
Feb 10 08:29:27.475: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:27.475: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:27.475: INFO: Pod daemon-set-p9ggq is not available
Feb 10 08:29:28.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:28.477: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:28.477: INFO: Pod daemon-set-p9ggq is not available
Feb 10 08:29:29.475: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:29.475: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:29.476: INFO: Pod daemon-set-p9ggq is not available
Feb 10 08:29:30.479: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:30.480: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:30.480: INFO: Pod daemon-set-p9ggq is not available
Feb 10 08:29:31.478: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:31.479: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:31.479: INFO: Pod daemon-set-p9ggq is not available
Feb 10 08:29:32.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:32.477: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:32.478: INFO: Pod daemon-set-p9ggq is not available
Feb 10 08:29:33.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:33.478: INFO: Wrong image for pod: daemon-set-p9ggq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:33.478: INFO: Pod daemon-set-p9ggq is not available
Feb 10 08:29:34.476: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:34.476: INFO: Pod daemon-set-5nwsk is not available
Feb 10 08:29:35.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:35.477: INFO: Pod daemon-set-5nwsk is not available
Feb 10 08:29:36.479: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:36.479: INFO: Pod daemon-set-5nwsk is not available
Feb 10 08:29:37.479: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:37.479: INFO: Pod daemon-set-5nwsk is not available
Feb 10 08:29:38.478: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:38.478: INFO: Pod daemon-set-5nwsk is not available
Feb 10 08:29:39.476: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:39.476: INFO: Pod daemon-set-5nwsk is not available
Feb 10 08:29:40.482: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:41.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:42.479: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:43.476: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:43.477: INFO: Pod daemon-set-4jtxh is not available
Feb 10 08:29:44.478: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:44.479: INFO: Pod daemon-set-4jtxh is not available
Feb 10 08:29:45.477: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:45.477: INFO: Pod daemon-set-4jtxh is not available
Feb 10 08:29:46.476: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:46.476: INFO: Pod daemon-set-4jtxh is not available
Feb 10 08:29:47.475: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:47.475: INFO: Pod daemon-set-4jtxh is not available
Feb 10 08:29:48.475: INFO: Wrong image for pod: daemon-set-4jtxh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 10 08:29:48.475: INFO: Pod daemon-set-4jtxh is not available
Feb 10 08:29:49.476: INFO: Pod daemon-set-jjfdn is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 10 08:29:49.529: INFO: Number of nodes with available pods: 2
Feb 10 08:29:49.530: INFO: Node master3 is running more than one daemon pod
Feb 10 08:29:50.566: INFO: Number of nodes with available pods: 2
Feb 10 08:29:50.566: INFO: Node master3 is running more than one daemon pod
Feb 10 08:29:51.565: INFO: Number of nodes with available pods: 2
Feb 10 08:29:51.566: INFO: Node master3 is running more than one daemon pod
Feb 10 08:29:52.568: INFO: Number of nodes with available pods: 2
Feb 10 08:29:52.568: INFO: Node master3 is running more than one daemon pod
Feb 10 08:29:53.570: INFO: Number of nodes with available pods: 3
Feb 10 08:29:53.571: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6388, will wait for the garbage collector to delete the pods
Feb 10 08:29:53.743: INFO: Deleting DaemonSet.extensions daemon-set took: 31.833749ms
Feb 10 08:29:54.149: INFO: Terminating DaemonSet.extensions daemon-set pods took: 406.295629ms
Feb 10 08:30:08.670: INFO: Number of nodes with available pods: 0
Feb 10 08:30:08.670: INFO: Number of running nodes: 0, number of available pods: 0
Feb 10 08:30:08.686: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6388/daemonsets","resourceVersion":"4641721"},"items":null}

Feb 10 08:30:08.698: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6388/pods","resourceVersion":"4641721"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:30:08.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6388" for this suite.

• [SLOW TEST:70.097 seconds]
[sig-apps] Daemon set [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":278,"completed":5,"skipped":122,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:30:08.801: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4510
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 10 08:30:09.313: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4510 /api/v1/namespaces/watch-4510/configmaps/e2e-watch-test-label-changed 6c5c4dda-50e3-4a42-ae01-6b077c5ac1d0 4641730 0 2020-02-10 08:30:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 10 08:30:09.317: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4510 /api/v1/namespaces/watch-4510/configmaps/e2e-watch-test-label-changed 6c5c4dda-50e3-4a42-ae01-6b077c5ac1d0 4641731 0 2020-02-10 08:30:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 10 08:30:09.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4510 /api/v1/namespaces/watch-4510/configmaps/e2e-watch-test-label-changed 6c5c4dda-50e3-4a42-ae01-6b077c5ac1d0 4641732 0 2020-02-10 08:30:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 10 08:30:19.437: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4510 /api/v1/namespaces/watch-4510/configmaps/e2e-watch-test-label-changed 6c5c4dda-50e3-4a42-ae01-6b077c5ac1d0 4641807 0 2020-02-10 08:30:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 10 08:30:19.437: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4510 /api/v1/namespaces/watch-4510/configmaps/e2e-watch-test-label-changed 6c5c4dda-50e3-4a42-ae01-6b077c5ac1d0 4641808 0 2020-02-10 08:30:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 10 08:30:19.438: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4510 /api/v1/namespaces/watch-4510/configmaps/e2e-watch-test-label-changed 6c5c4dda-50e3-4a42-ae01-6b077c5ac1d0 4641809 0 2020-02-10 08:30:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:30:19.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4510" for this suite.

• [SLOW TEST:10.671 seconds]
[sig-api-machinery] Watchers
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":278,"completed":6,"skipped":131,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:30:19.475: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2197
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run rc
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create an rc from an image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 10 08:30:19.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2197'
Feb 10 08:30:22.739: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 10 08:30:22.739: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Feb 10 08:30:22.775: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-l6q4m]
Feb 10 08:30:22.776: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-l6q4m" in namespace "kubectl-2197" to be "running and ready"
Feb 10 08:30:22.788: INFO: Pod "e2e-test-httpd-rc-l6q4m": Phase="Pending", Reason="", readiness=false. Elapsed: 12.259223ms
Feb 10 08:30:24.805: INFO: Pod "e2e-test-httpd-rc-l6q4m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029484659s
Feb 10 08:30:26.817: INFO: Pod "e2e-test-httpd-rc-l6q4m": Phase="Running", Reason="", readiness=true. Elapsed: 4.041067371s
Feb 10 08:30:26.817: INFO: Pod "e2e-test-httpd-rc-l6q4m" satisfied condition "running and ready"
Feb 10 08:30:26.817: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-l6q4m]
Feb 10 08:30:26.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 logs rc/e2e-test-httpd-rc --namespace=kubectl-2197'
Feb 10 08:30:27.755: INFO: stderr: ""
Feb 10 08:30:27.755: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.156.32.74. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.156.32.74. Set the 'ServerName' directive globally to suppress this message\n[Mon Feb 10 08:30:26.529839 2020] [mpm_event:notice] [pid 1:tid 1099338783424] AH00489: Apache/2.4.41 (Unix) configured -- resuming normal operations\n[Mon Feb 10 08:30:26.530618 2020] [core:notice] [pid 1:tid 1099338783424] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Feb 10 08:30:27.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete rc e2e-test-httpd-rc --namespace=kubectl-2197'
Feb 10 08:30:28.296: INFO: stderr: ""
Feb 10 08:30:28.296: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:30:28.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2197" for this suite.

• [SLOW TEST:8.878 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1608
    should create an rc from an image  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":278,"completed":7,"skipped":210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:30:28.355: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:30:35.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1096" for this suite.

• [SLOW TEST:6.876 seconds]
[k8s.io] Kubelet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command in a pod
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":278,"completed":8,"skipped":233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:30:35.233: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:30:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3058" for this suite.

• [SLOW TEST:11.685 seconds]
[sig-api-machinery] ResourceQuota
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":278,"completed":9,"skipped":257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:30:46.920: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 08:31:21.901: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 08:31:23.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920281, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920281, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920282, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920281, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 08:31:25.959: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920281, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920281, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920282, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920281, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 08:31:29.009: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:31:29.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6687" for this suite.
STEP: Destroying namespace "webhook-6687-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:42.940 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":278,"completed":10,"skipped":297,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:31:29.869: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if v1 is in available api versions  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Feb 10 08:31:30.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 api-versions'
Feb 10 08:31:30.985: INFO: stderr: ""
Feb 10 08:31:30.986: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:31:30.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9406" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":278,"completed":11,"skipped":313,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:31:31.038: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:31:45.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3311" for this suite.

• [SLOW TEST:14.488 seconds]
[sig-apps] Job
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":278,"completed":12,"skipped":350,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:31:45.527: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Feb 10 08:31:46.009: INFO: Waiting up to 5m0s for pod "client-containers-3c4ee91a-1dd1-4562-acae-2055efbf05e3" in namespace "containers-7614" to be "success or failure"
Feb 10 08:31:46.022: INFO: Pod "client-containers-3c4ee91a-1dd1-4562-acae-2055efbf05e3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.313283ms
Feb 10 08:31:48.041: INFO: Pod "client-containers-3c4ee91a-1dd1-4562-acae-2055efbf05e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032400985s
Feb 10 08:31:50.056: INFO: Pod "client-containers-3c4ee91a-1dd1-4562-acae-2055efbf05e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046571124s
Feb 10 08:31:52.084: INFO: Pod "client-containers-3c4ee91a-1dd1-4562-acae-2055efbf05e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074542785s
STEP: Saw pod success
Feb 10 08:31:52.084: INFO: Pod "client-containers-3c4ee91a-1dd1-4562-acae-2055efbf05e3" satisfied condition "success or failure"
Feb 10 08:31:52.096: INFO: Trying to get logs from node master3 pod client-containers-3c4ee91a-1dd1-4562-acae-2055efbf05e3 container test-container: <nil>
STEP: delete the pod
Feb 10 08:31:52.516: INFO: Waiting for pod client-containers-3c4ee91a-1dd1-4562-acae-2055efbf05e3 to disappear
Feb 10 08:31:52.527: INFO: Pod client-containers-3c4ee91a-1dd1-4562-acae-2055efbf05e3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:31:52.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7614" for this suite.

• [SLOW TEST:7.037 seconds]
[k8s.io] Docker Containers
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":278,"completed":13,"skipped":354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:31:52.566: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a working application  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Feb 10 08:31:52.983: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Feb 10 08:31:52.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-288'
Feb 10 08:31:54.366: INFO: stderr: ""
Feb 10 08:31:54.366: INFO: stdout: "service/agnhost-slave created\n"
Feb 10 08:31:54.368: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Feb 10 08:31:54.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-288'
Feb 10 08:31:55.408: INFO: stderr: ""
Feb 10 08:31:55.409: INFO: stdout: "service/agnhost-master created\n"
Feb 10 08:31:55.411: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 10 08:31:55.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-288'
Feb 10 08:31:56.848: INFO: stderr: ""
Feb 10 08:31:56.849: INFO: stdout: "service/frontend created\n"
Feb 10 08:31:56.851: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 10 08:31:56.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-288'
Feb 10 08:31:58.261: INFO: stderr: ""
Feb 10 08:31:58.261: INFO: stdout: "deployment.apps/frontend created\n"
Feb 10 08:31:58.266: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 10 08:31:58.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-288'
Feb 10 08:31:59.390: INFO: stderr: ""
Feb 10 08:31:59.390: INFO: stdout: "deployment.apps/agnhost-master created\n"
Feb 10 08:31:59.393: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 10 08:31:59.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-288'
Feb 10 08:32:00.958: INFO: stderr: ""
Feb 10 08:32:00.958: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Feb 10 08:32:00.958: INFO: Waiting for all frontend pods to be Running.
Feb 10 08:32:06.012: INFO: Waiting for frontend to serve content.
Feb 10 08:32:11.045: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Feb 10 08:32:16.081: INFO: Trying to add a new entry to the guestbook.
Feb 10 08:32:16.115: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 10 08:32:16.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete --grace-period=0 --force -f - --namespace=kubectl-288'
Feb 10 08:32:16.714: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 10 08:32:16.715: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 10 08:32:16.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete --grace-period=0 --force -f - --namespace=kubectl-288'
Feb 10 08:32:17.357: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 10 08:32:17.357: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 10 08:32:17.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete --grace-period=0 --force -f - --namespace=kubectl-288'
Feb 10 08:32:17.976: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 10 08:32:17.977: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 10 08:32:17.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete --grace-period=0 --force -f - --namespace=kubectl-288'
Feb 10 08:32:18.556: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 10 08:32:18.557: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 10 08:32:18.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete --grace-period=0 --force -f - --namespace=kubectl-288'
Feb 10 08:32:19.080: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 10 08:32:19.080: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 10 08:32:19.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete --grace-period=0 --force -f - --namespace=kubectl-288'
Feb 10 08:32:19.718: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 10 08:32:19.718: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:32:19.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-288" for this suite.

• [SLOW TEST:27.192 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:385
    should create and stop a working application  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":278,"completed":14,"skipped":397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:32:19.765: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 08:32:20.251: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45a1ab0c-5e69-434e-961f-c950bbc47618" in namespace "projected-186" to be "success or failure"
Feb 10 08:32:20.268: INFO: Pod "downwardapi-volume-45a1ab0c-5e69-434e-961f-c950bbc47618": Phase="Pending", Reason="", readiness=false. Elapsed: 16.327023ms
Feb 10 08:32:22.291: INFO: Pod "downwardapi-volume-45a1ab0c-5e69-434e-961f-c950bbc47618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039713037s
Feb 10 08:32:24.318: INFO: Pod "downwardapi-volume-45a1ab0c-5e69-434e-961f-c950bbc47618": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066342029s
Feb 10 08:32:26.344: INFO: Pod "downwardapi-volume-45a1ab0c-5e69-434e-961f-c950bbc47618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.09193846s
STEP: Saw pod success
Feb 10 08:32:26.344: INFO: Pod "downwardapi-volume-45a1ab0c-5e69-434e-961f-c950bbc47618" satisfied condition "success or failure"
Feb 10 08:32:26.363: INFO: Trying to get logs from node master1 pod downwardapi-volume-45a1ab0c-5e69-434e-961f-c950bbc47618 container client-container: <nil>
STEP: delete the pod
Feb 10 08:32:26.832: INFO: Waiting for pod downwardapi-volume-45a1ab0c-5e69-434e-961f-c950bbc47618 to disappear
Feb 10 08:32:26.849: INFO: Pod downwardapi-volume-45a1ab0c-5e69-434e-961f-c950bbc47618 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:32:26.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-186" for this suite.

• [SLOW TEST:7.122 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":15,"skipped":433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:32:26.889: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-27f189ce-7f62-4d36-8ae4-e6682214c589
STEP: Creating a pod to test consume configMaps
Feb 10 08:32:27.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-2f65f218-8076-4afe-9b3a-fd7ffea9de9a" in namespace "configmap-4411" to be "success or failure"
Feb 10 08:32:27.393: INFO: Pod "pod-configmaps-2f65f218-8076-4afe-9b3a-fd7ffea9de9a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.930043ms
Feb 10 08:32:29.405: INFO: Pod "pod-configmaps-2f65f218-8076-4afe-9b3a-fd7ffea9de9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023760508s
Feb 10 08:32:31.418: INFO: Pod "pod-configmaps-2f65f218-8076-4afe-9b3a-fd7ffea9de9a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036903791s
Feb 10 08:32:33.430: INFO: Pod "pod-configmaps-2f65f218-8076-4afe-9b3a-fd7ffea9de9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049027433s
STEP: Saw pod success
Feb 10 08:32:33.430: INFO: Pod "pod-configmaps-2f65f218-8076-4afe-9b3a-fd7ffea9de9a" satisfied condition "success or failure"
Feb 10 08:32:33.442: INFO: Trying to get logs from node master3 pod pod-configmaps-2f65f218-8076-4afe-9b3a-fd7ffea9de9a container configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 08:32:33.533: INFO: Waiting for pod pod-configmaps-2f65f218-8076-4afe-9b3a-fd7ffea9de9a to disappear
Feb 10 08:32:33.543: INFO: Pod pod-configmaps-2f65f218-8076-4afe-9b3a-fd7ffea9de9a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:32:33.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4411" for this suite.

• [SLOW TEST:6.690 seconds]
[sig-storage] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":16,"skipped":457,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:32:33.581: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3888
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 10 08:32:34.040: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-a 4d35b7a4-3536-4b80-ba71-9b1dc5b29f7d 4642855 0 2020-02-10 08:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 10 08:32:34.041: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-a 4d35b7a4-3536-4b80-ba71-9b1dc5b29f7d 4642855 0 2020-02-10 08:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 10 08:32:44.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-a 4d35b7a4-3536-4b80-ba71-9b1dc5b29f7d 4642893 0 2020-02-10 08:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 10 08:32:44.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-a 4d35b7a4-3536-4b80-ba71-9b1dc5b29f7d 4642893 0 2020-02-10 08:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 10 08:32:54.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-a 4d35b7a4-3536-4b80-ba71-9b1dc5b29f7d 4642920 0 2020-02-10 08:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 10 08:32:54.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-a 4d35b7a4-3536-4b80-ba71-9b1dc5b29f7d 4642920 0 2020-02-10 08:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 10 08:33:04.159: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-a 4d35b7a4-3536-4b80-ba71-9b1dc5b29f7d 4642948 0 2020-02-10 08:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 10 08:33:04.159: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-a 4d35b7a4-3536-4b80-ba71-9b1dc5b29f7d 4642948 0 2020-02-10 08:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 10 08:33:14.194: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-b 38356f6a-d87c-40ee-a3f8-d3a2285e1acd 4642976 0 2020-02-10 08:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 10 08:33:14.194: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-b 38356f6a-d87c-40ee-a3f8-d3a2285e1acd 4642976 0 2020-02-10 08:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 10 08:33:24.235: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-b 38356f6a-d87c-40ee-a3f8-d3a2285e1acd 4643002 0 2020-02-10 08:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 10 08:33:24.235: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3888 /api/v1/namespaces/watch-3888/configmaps/e2e-watch-test-configmap-b 38356f6a-d87c-40ee-a3f8-d3a2285e1acd 4643002 0 2020-02-10 08:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:33:34.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3888" for this suite.

• [SLOW TEST:60.701 seconds]
[sig-api-machinery] Watchers
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":278,"completed":17,"skipped":467,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:33:34.284: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 10 08:33:34.702: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 10 08:33:34.750: INFO: Waiting for terminating namespaces to be deleted...
Feb 10 08:33:34.761: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Feb 10 08:33:34.799: INFO: kube-apiserver-master1 from kube-system started at 2020-02-05 16:29:54 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.800: INFO: 	Container kube-apiserver ready: true, restart count 3021
Feb 10 08:33:34.800: INFO: ss2-1 from statefulset-6054 started at 2020-02-10 05:24:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.800: INFO: 	Container webserver ready: true, restart count 0
Feb 10 08:33:34.800: INFO: kube-controller-manager-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.800: INFO: 	Container kube-controller-manager ready: true, restart count 565
Feb 10 08:33:34.800: INFO: ntp-zj7fc from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.800: INFO: 	Container ntp ready: true, restart count 1
Feb 10 08:33:34.801: INFO: calico-node-p64wz from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.801: INFO: 	Container calico-node ready: true, restart count 1
Feb 10 08:33:34.801: INFO: simpletest-rc-to-be-deleted-f48d7 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.801: INFO: 	Container nginx ready: true, restart count 0
Feb 10 08:33:34.801: INFO: kube-scheduler-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.801: INFO: 	Container kube-scheduler ready: true, restart count 572
Feb 10 08:33:34.801: INFO: nginx-proxy-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.801: INFO: 	Container nginx-proxy ready: true, restart count 845
Feb 10 08:33:34.801: INFO: coredns-hfdns from kube-system started at 2020-01-15 01:54:01 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.801: INFO: 	Container coredns ready: true, restart count 1
Feb 10 08:33:34.801: INFO: agnhost-deployment-54964f567d-nvhtm from kube-system started at 2020-02-10 07:28:38 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.801: INFO: 	Container agnhost ready: true, restart count 0
Feb 10 08:33:34.801: INFO: sonobuoy-e2e-job-26d59f1c3cea4685 from sonobuoy started at 2020-02-10 08:27:25 +0000 UTC (2 container statuses recorded)
Feb 10 08:33:34.801: INFO: 	Container e2e ready: true, restart count 0
Feb 10 08:33:34.801: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 10 08:33:34.801: INFO: kube-proxy-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.801: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 08:33:34.801: INFO: localpv-provisioner-ds-nt4r5 from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:34.801: INFO: 	Container provisioner ready: true, restart count 1
Feb 10 08:33:34.801: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Feb 10 08:33:35.145: INFO: simpletest-rc-to-be-deleted-62s49 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.145: INFO: 	Container nginx ready: true, restart count 0
Feb 10 08:33:35.145: INFO: sonobuoy from sonobuoy started at 2020-02-10 08:27:21 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.145: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 10 08:33:35.145: INFO: kube-proxy-master2 from kube-system started at 2020-02-05 16:28:09 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.145: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 08:33:35.145: INFO: coredns-sv7qc from kube-system started at 2020-01-15 02:50:54 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.145: INFO: 	Container coredns ready: true, restart count 1
Feb 10 08:33:35.145: INFO: calico-node-q6h5s from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.145: INFO: 	Container calico-node ready: true, restart count 2
Feb 10 08:33:35.145: INFO: ss2-2 from statefulset-6054 started at 2020-02-10 05:25:22 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.145: INFO: 	Container webserver ready: true, restart count 0
Feb 10 08:33:35.145: INFO: metrics-server-6c6fd884-md7t8 from kube-system started at 2020-01-15 01:54:18 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.145: INFO: 	Container metrics-server ready: true, restart count 1
Feb 10 08:33:35.145: INFO: simpletest-rc-to-be-deleted-hvgr7 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.145: INFO: 	Container nginx ready: true, restart count 0
Feb 10 08:33:35.145: INFO: nginx-proxy-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.145: INFO: 	Container nginx-proxy ready: true, restart count 846
Feb 10 08:33:35.146: INFO: kube-apiserver-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.146: INFO: 	Container kube-apiserver ready: true, restart count 563
Feb 10 08:33:35.146: INFO: localpv-provisioner-ds-sd5nt from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.146: INFO: 	Container provisioner ready: true, restart count 3
Feb 10 08:33:35.146: INFO: tiller-deploy-75cd95b6b4-tkpds from kube-system started at 2020-02-07 13:38:14 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.146: INFO: 	Container tiller ready: true, restart count 0
Feb 10 08:33:35.146: INFO: kube-scheduler-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.146: INFO: 	Container kube-scheduler ready: true, restart count 561
Feb 10 08:33:35.146: INFO: kube-controller-manager-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.146: INFO: 	Container kube-controller-manager ready: true, restart count 581
Feb 10 08:33:35.146: INFO: ntp-g2jhr from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.146: INFO: 	Container ntp ready: true, restart count 1
Feb 10 08:33:35.146: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Feb 10 08:33:35.189: INFO: simpletest-rc-to-be-deleted-clkth from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container nginx ready: true, restart count 0
Feb 10 08:33:35.189: INFO: calico-node-6fr4n from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container calico-node ready: true, restart count 2
Feb 10 08:33:35.189: INFO: localpv-provisioner-ds-vhkpq from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container provisioner ready: true, restart count 3
Feb 10 08:33:35.189: INFO: simpletest-rc-to-be-deleted-g226k from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container nginx ready: true, restart count 0
Feb 10 08:33:35.189: INFO: ss2-0 from statefulset-6054 started at 2020-02-10 05:25:17 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container webserver ready: true, restart count 0
Feb 10 08:33:35.189: INFO: ntp-c9lwf from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container ntp ready: true, restart count 1
Feb 10 08:33:35.189: INFO: calico-kube-controllers-6fcbd694f7-zs7dw from kube-system started at 2020-02-07 13:38:14 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 10 08:33:35.189: INFO: kube-proxy-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 08:33:35.189: INFO: kube-scheduler-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container kube-scheduler ready: true, restart count 574
Feb 10 08:33:35.189: INFO: nginx-proxy-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container nginx-proxy ready: true, restart count 851
Feb 10 08:33:35.189: INFO: coredns-bsmzw from kube-system started at 2020-01-15 01:54:55 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container coredns ready: true, restart count 1
Feb 10 08:33:35.189: INFO: ntp-server-8cbc779-hj2rj from kube-system started at 2020-01-15 03:48:56 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container ntp-server ready: true, restart count 1
Feb 10 08:33:35.189: INFO: kube-apiserver-master3 from kube-system started at 2020-02-05 16:28:26 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container kube-apiserver ready: true, restart count 563
Feb 10 08:33:35.189: INFO: kube-controller-manager-master3 from kube-system started at 2020-02-05 16:28:26 +0000 UTC (1 container statuses recorded)
Feb 10 08:33:35.189: INFO: 	Container kube-controller-manager ready: true, restart count 553
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node master1
STEP: verifying the node has the label node master2
STEP: verifying the node has the label node master3
Feb 10 08:33:35.455: INFO: Pod simpletest-rc-to-be-deleted-62s49 requesting resource cpu=0m on Node master2
Feb 10 08:33:35.455: INFO: Pod simpletest-rc-to-be-deleted-clkth requesting resource cpu=0m on Node master3
Feb 10 08:33:35.455: INFO: Pod simpletest-rc-to-be-deleted-f48d7 requesting resource cpu=0m on Node master1
Feb 10 08:33:35.455: INFO: Pod simpletest-rc-to-be-deleted-g226k requesting resource cpu=0m on Node master3
Feb 10 08:33:35.455: INFO: Pod simpletest-rc-to-be-deleted-hvgr7 requesting resource cpu=0m on Node master2
Feb 10 08:33:35.455: INFO: Pod agnhost-deployment-54964f567d-nvhtm requesting resource cpu=0m on Node master1
Feb 10 08:33:35.455: INFO: Pod calico-kube-controllers-6fcbd694f7-zs7dw requesting resource cpu=30m on Node master3
Feb 10 08:33:35.455: INFO: Pod calico-node-6fr4n requesting resource cpu=150m on Node master3
Feb 10 08:33:35.456: INFO: Pod calico-node-p64wz requesting resource cpu=150m on Node master1
Feb 10 08:33:35.456: INFO: Pod calico-node-q6h5s requesting resource cpu=150m on Node master2
Feb 10 08:33:35.456: INFO: Pod coredns-bsmzw requesting resource cpu=100m on Node master3
Feb 10 08:33:35.456: INFO: Pod coredns-hfdns requesting resource cpu=100m on Node master1
Feb 10 08:33:35.456: INFO: Pod coredns-sv7qc requesting resource cpu=100m on Node master2
Feb 10 08:33:35.456: INFO: Pod kube-apiserver-master1 requesting resource cpu=100m on Node master1
Feb 10 08:33:35.456: INFO: Pod kube-apiserver-master2 requesting resource cpu=100m on Node master2
Feb 10 08:33:35.456: INFO: Pod kube-apiserver-master3 requesting resource cpu=100m on Node master3
Feb 10 08:33:35.456: INFO: Pod kube-controller-manager-master1 requesting resource cpu=100m on Node master1
Feb 10 08:33:35.456: INFO: Pod kube-controller-manager-master2 requesting resource cpu=100m on Node master2
Feb 10 08:33:35.456: INFO: Pod kube-controller-manager-master3 requesting resource cpu=100m on Node master3
Feb 10 08:33:35.456: INFO: Pod kube-proxy-master1 requesting resource cpu=150m on Node master1
Feb 10 08:33:35.456: INFO: Pod kube-proxy-master2 requesting resource cpu=150m on Node master2
Feb 10 08:33:35.456: INFO: Pod kube-proxy-master3 requesting resource cpu=150m on Node master3
Feb 10 08:33:35.456: INFO: Pod kube-scheduler-master1 requesting resource cpu=80m on Node master1
Feb 10 08:33:35.456: INFO: Pod kube-scheduler-master2 requesting resource cpu=80m on Node master2
Feb 10 08:33:35.456: INFO: Pod kube-scheduler-master3 requesting resource cpu=80m on Node master3
Feb 10 08:33:35.456: INFO: Pod localpv-provisioner-ds-nt4r5 requesting resource cpu=0m on Node master1
Feb 10 08:33:35.456: INFO: Pod localpv-provisioner-ds-sd5nt requesting resource cpu=0m on Node master2
Feb 10 08:33:35.456: INFO: Pod localpv-provisioner-ds-vhkpq requesting resource cpu=0m on Node master3
Feb 10 08:33:35.456: INFO: Pod metrics-server-6c6fd884-md7t8 requesting resource cpu=0m on Node master2
Feb 10 08:33:35.456: INFO: Pod nginx-proxy-master1 requesting resource cpu=25m on Node master1
Feb 10 08:33:35.456: INFO: Pod nginx-proxy-master2 requesting resource cpu=25m on Node master2
Feb 10 08:33:35.456: INFO: Pod nginx-proxy-master3 requesting resource cpu=25m on Node master3
Feb 10 08:33:35.456: INFO: Pod ntp-c9lwf requesting resource cpu=0m on Node master3
Feb 10 08:33:35.456: INFO: Pod ntp-g2jhr requesting resource cpu=0m on Node master2
Feb 10 08:33:35.456: INFO: Pod ntp-server-8cbc779-hj2rj requesting resource cpu=0m on Node master3
Feb 10 08:33:35.456: INFO: Pod ntp-zj7fc requesting resource cpu=0m on Node master1
Feb 10 08:33:35.456: INFO: Pod tiller-deploy-75cd95b6b4-tkpds requesting resource cpu=0m on Node master2
Feb 10 08:33:35.456: INFO: Pod sonobuoy requesting resource cpu=0m on Node master2
Feb 10 08:33:35.456: INFO: Pod sonobuoy-e2e-job-26d59f1c3cea4685 requesting resource cpu=0m on Node master1
Feb 10 08:33:35.456: INFO: Pod ss2-0 requesting resource cpu=0m on Node master3
Feb 10 08:33:35.456: INFO: Pod ss2-1 requesting resource cpu=0m on Node master1
Feb 10 08:33:35.456: INFO: Pod ss2-2 requesting resource cpu=0m on Node master2
STEP: Starting Pods to consume most of the cluster CPU.
Feb 10 08:33:35.456: INFO: Creating a pod which consumes cpu=4406m on Node master1
Feb 10 08:33:35.485: INFO: Creating a pod which consumes cpu=4406m on Node master2
Feb 10 08:33:35.507: INFO: Creating a pod which consumes cpu=4385m on Node master3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3cfbe63a-6ac0-4acb-87f1-7466a4f23fbd.15f1fd733bbf7391], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2717/filler-pod-3cfbe63a-6ac0-4acb-87f1-7466a4f23fbd to master2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3cfbe63a-6ac0-4acb-87f1-7466a4f23fbd.15f1fd73f9330e3d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3cfbe63a-6ac0-4acb-87f1-7466a4f23fbd.15f1fd740c757122], Reason = [Created], Message = [Created container filler-pod-3cfbe63a-6ac0-4acb-87f1-7466a4f23fbd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3cfbe63a-6ac0-4acb-87f1-7466a4f23fbd.15f1fd7431a59176], Reason = [Started], Message = [Started container filler-pod-3cfbe63a-6ac0-4acb-87f1-7466a4f23fbd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e6ef029-5466-4594-b165-88a40d71d732.15f1fd733954a962], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2717/filler-pod-5e6ef029-5466-4594-b165-88a40d71d732 to master1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e6ef029-5466-4594-b165-88a40d71d732.15f1fd73f9719757], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e6ef029-5466-4594-b165-88a40d71d732.15f1fd740bc4b374], Reason = [Created], Message = [Created container filler-pod-5e6ef029-5466-4594-b165-88a40d71d732]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e6ef029-5466-4594-b165-88a40d71d732.15f1fd742f8c601b], Reason = [Started], Message = [Started container filler-pod-5e6ef029-5466-4594-b165-88a40d71d732]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-625607bb-e801-4af3-ad5d-e17c8984e57e.15f1fd733d5a2079], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2717/filler-pod-625607bb-e801-4af3-ad5d-e17c8984e57e to master3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-625607bb-e801-4af3-ad5d-e17c8984e57e.15f1fd73f17e5ab4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-625607bb-e801-4af3-ad5d-e17c8984e57e.15f1fd740483e56a], Reason = [Created], Message = [Created container filler-pod-625607bb-e801-4af3-ad5d-e17c8984e57e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-625607bb-e801-4af3-ad5d-e17c8984e57e.15f1fd7427923f75], Reason = [Started], Message = [Started container filler-pod-625607bb-e801-4af3-ad5d-e17c8984e57e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f1fd74a7688e20], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node master3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:33:42.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2717" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:8.613 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":278,"completed":18,"skipped":498,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:33:42.898: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 10 08:33:57.851: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 10 08:33:59.887: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920437, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920437, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920437, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920437, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 08:34:01.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920437, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920437, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920437, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920437, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 08:34:04.954: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:34:04.978: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:34:11.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3669" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:29.132 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":278,"completed":19,"skipped":505,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:34:12.031: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 10 08:34:12.505: INFO: Waiting up to 5m0s for pod "downward-api-cb228c10-5765-4ccd-aae4-292b67ccba4a" in namespace "downward-api-4864" to be "success or failure"
Feb 10 08:34:12.545: INFO: Pod "downward-api-cb228c10-5765-4ccd-aae4-292b67ccba4a": Phase="Pending", Reason="", readiness=false. Elapsed: 39.812466ms
Feb 10 08:34:14.561: INFO: Pod "downward-api-cb228c10-5765-4ccd-aae4-292b67ccba4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055575465s
Feb 10 08:34:16.576: INFO: Pod "downward-api-cb228c10-5765-4ccd-aae4-292b67ccba4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070744482s
Feb 10 08:34:18.594: INFO: Pod "downward-api-cb228c10-5765-4ccd-aae4-292b67ccba4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088706398s
STEP: Saw pod success
Feb 10 08:34:18.594: INFO: Pod "downward-api-cb228c10-5765-4ccd-aae4-292b67ccba4a" satisfied condition "success or failure"
Feb 10 08:34:18.605: INFO: Trying to get logs from node master1 pod downward-api-cb228c10-5765-4ccd-aae4-292b67ccba4a container dapi-container: <nil>
STEP: delete the pod
Feb 10 08:34:18.687: INFO: Waiting for pod downward-api-cb228c10-5765-4ccd-aae4-292b67ccba4a to disappear
Feb 10 08:34:18.701: INFO: Pod downward-api-cb228c10-5765-4ccd-aae4-292b67ccba4a no longer exists
[AfterEach] [sig-node] Downward API
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:34:18.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4864" for this suite.

• [SLOW TEST:6.705 seconds]
[sig-node] Downward API
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide host IP as an env var [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":278,"completed":20,"skipped":509,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:34:18.739: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-8e6a4692-c776-4f3d-83c1-3565cc1f0924
STEP: Creating a pod to test consume secrets
Feb 10 08:34:19.217: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b1cea64a-d83a-4ea4-86e4-1318a0e6d5ea" in namespace "projected-2693" to be "success or failure"
Feb 10 08:34:19.228: INFO: Pod "pod-projected-secrets-b1cea64a-d83a-4ea4-86e4-1318a0e6d5ea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.355582ms
Feb 10 08:34:21.244: INFO: Pod "pod-projected-secrets-b1cea64a-d83a-4ea4-86e4-1318a0e6d5ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026785655s
Feb 10 08:34:23.258: INFO: Pod "pod-projected-secrets-b1cea64a-d83a-4ea4-86e4-1318a0e6d5ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041446307s
Feb 10 08:34:25.271: INFO: Pod "pod-projected-secrets-b1cea64a-d83a-4ea4-86e4-1318a0e6d5ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054723997s
STEP: Saw pod success
Feb 10 08:34:25.272: INFO: Pod "pod-projected-secrets-b1cea64a-d83a-4ea4-86e4-1318a0e6d5ea" satisfied condition "success or failure"
Feb 10 08:34:25.287: INFO: Trying to get logs from node master3 pod pod-projected-secrets-b1cea64a-d83a-4ea4-86e4-1318a0e6d5ea container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 10 08:34:25.379: INFO: Waiting for pod pod-projected-secrets-b1cea64a-d83a-4ea4-86e4-1318a0e6d5ea to disappear
Feb 10 08:34:25.391: INFO: Pod pod-projected-secrets-b1cea64a-d83a-4ea4-86e4-1318a0e6d5ea no longer exists
[AfterEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:34:25.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2693" for this suite.

• [SLOW TEST:6.691 seconds]
[sig-storage] Projected secret
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":21,"skipped":514,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:34:25.431: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl rolling-update
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1672
[It] should support rolling-update to same image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 10 08:34:25.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-5849'
Feb 10 08:34:26.477: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 10 08:34:26.478: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Feb 10 08:34:26.507: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 10 08:34:26.522: INFO: scanned /root for discovery docs: <nil>
Feb 10 08:34:26.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5849'
Feb 10 08:34:43.962: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 10 08:34:43.962: INFO: stdout: "Created e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4\nScaling up e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Feb 10 08:34:43.963: INFO: stdout: "Created e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4\nScaling up e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Feb 10 08:34:43.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-5849'
Feb 10 08:34:44.510: INFO: stderr: ""
Feb 10 08:34:44.510: INFO: stdout: "e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4-xc675 "
Feb 10 08:34:44.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4-xc675 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5849'
Feb 10 08:34:45.032: INFO: stderr: ""
Feb 10 08:34:45.032: INFO: stdout: "true"
Feb 10 08:34:45.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4-xc675 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5849'
Feb 10 08:34:45.516: INFO: stderr: ""
Feb 10 08:34:45.517: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Feb 10 08:34:45.517: INFO: e2e-test-httpd-rc-067b7fb99e7c9b0ac7cc11e72854c6a4-xc675 is verified up and running
[AfterEach] Kubectl rolling-update
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1678
Feb 10 08:34:45.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete rc e2e-test-httpd-rc --namespace=kubectl-5849'
Feb 10 08:34:46.032: INFO: stderr: ""
Feb 10 08:34:46.032: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:34:46.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5849" for this suite.

• [SLOW TEST:20.647 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1667
    should support rolling-update to same image  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":278,"completed":22,"skipped":518,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:34:46.088: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 10 08:34:46.548: INFO: Waiting up to 5m0s for pod "pod-d0422ccd-ffad-4c66-bdd0-556b98ad3881" in namespace "emptydir-5186" to be "success or failure"
Feb 10 08:34:46.560: INFO: Pod "pod-d0422ccd-ffad-4c66-bdd0-556b98ad3881": Phase="Pending", Reason="", readiness=false. Elapsed: 11.152501ms
Feb 10 08:34:48.576: INFO: Pod "pod-d0422ccd-ffad-4c66-bdd0-556b98ad3881": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027456073s
Feb 10 08:34:50.589: INFO: Pod "pod-d0422ccd-ffad-4c66-bdd0-556b98ad3881": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040805803s
Feb 10 08:34:52.608: INFO: Pod "pod-d0422ccd-ffad-4c66-bdd0-556b98ad3881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059656171s
STEP: Saw pod success
Feb 10 08:34:52.608: INFO: Pod "pod-d0422ccd-ffad-4c66-bdd0-556b98ad3881" satisfied condition "success or failure"
Feb 10 08:34:52.618: INFO: Trying to get logs from node master3 pod pod-d0422ccd-ffad-4c66-bdd0-556b98ad3881 container test-container: <nil>
STEP: delete the pod
Feb 10 08:34:52.697: INFO: Waiting for pod pod-d0422ccd-ffad-4c66-bdd0-556b98ad3881 to disappear
Feb 10 08:34:52.707: INFO: Pod pod-d0422ccd-ffad-4c66-bdd0-556b98ad3881 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:34:52.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5186" for this suite.

• [SLOW TEST:6.657 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":23,"skipped":538,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:34:52.750: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:35:10.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6056" for this suite.

• [SLOW TEST:17.621 seconds]
[sig-api-machinery] ResourceQuota
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":278,"completed":24,"skipped":543,"failed":0}
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:35:10.371: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:35:10.817: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c8101286-eca9-4250-be09-3de3d59bddaa" in namespace "security-context-test-8043" to be "success or failure"
Feb 10 08:35:10.829: INFO: Pod "alpine-nnp-false-c8101286-eca9-4250-be09-3de3d59bddaa": Phase="Pending", Reason="", readiness=false. Elapsed: 11.391001ms
Feb 10 08:35:12.841: INFO: Pod "alpine-nnp-false-c8101286-eca9-4250-be09-3de3d59bddaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023300473s
Feb 10 08:35:14.854: INFO: Pod "alpine-nnp-false-c8101286-eca9-4250-be09-3de3d59bddaa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036814364s
Feb 10 08:35:16.877: INFO: Pod "alpine-nnp-false-c8101286-eca9-4250-be09-3de3d59bddaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059309374s
Feb 10 08:35:16.877: INFO: Pod "alpine-nnp-false-c8101286-eca9-4250-be09-3de3d59bddaa" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:35:16.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8043" for this suite.

• [SLOW TEST:6.623 seconds]
[k8s.io] Security Context
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":25,"skipped":543,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:35:16.995: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8465
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:35:23.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8465" for this suite.

• [SLOW TEST:6.575 seconds]
[k8s.io] Kubelet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a read only busybox container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":26,"skipped":548,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:35:23.571: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-3694/configmap-test-7e224e67-2a52-4e9e-a419-5426c60af2cd
STEP: Creating a pod to test consume configMaps
Feb 10 08:35:24.061: INFO: Waiting up to 5m0s for pod "pod-configmaps-ecee455f-9e07-49a4-ac4a-6516765f8efe" in namespace "configmap-3694" to be "success or failure"
Feb 10 08:35:24.080: INFO: Pod "pod-configmaps-ecee455f-9e07-49a4-ac4a-6516765f8efe": Phase="Pending", Reason="", readiness=false. Elapsed: 18.451242ms
Feb 10 08:35:26.098: INFO: Pod "pod-configmaps-ecee455f-9e07-49a4-ac4a-6516765f8efe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036734705s
Feb 10 08:35:28.115: INFO: Pod "pod-configmaps-ecee455f-9e07-49a4-ac4a-6516765f8efe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054200365s
Feb 10 08:35:30.130: INFO: Pod "pod-configmaps-ecee455f-9e07-49a4-ac4a-6516765f8efe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068489244s
STEP: Saw pod success
Feb 10 08:35:30.130: INFO: Pod "pod-configmaps-ecee455f-9e07-49a4-ac4a-6516765f8efe" satisfied condition "success or failure"
Feb 10 08:35:30.141: INFO: Trying to get logs from node master1 pod pod-configmaps-ecee455f-9e07-49a4-ac4a-6516765f8efe container env-test: <nil>
STEP: delete the pod
Feb 10 08:35:30.247: INFO: Waiting for pod pod-configmaps-ecee455f-9e07-49a4-ac4a-6516765f8efe to disappear
Feb 10 08:35:30.256: INFO: Pod pod-configmaps-ecee455f-9e07-49a4-ac4a-6516765f8efe no longer exists
[AfterEach] [sig-node] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:35:30.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3694" for this suite.

• [SLOW TEST:6.719 seconds]
[sig-node] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":278,"completed":27,"skipped":550,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:35:30.290: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2375
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-713f1704-02ce-4623-b224-ed36b9fc8031
STEP: Creating a pod to test consume configMaps
Feb 10 08:35:30.770: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e269b1cf-3d40-4b8b-b636-6876a14183d2" in namespace "projected-2375" to be "success or failure"
Feb 10 08:35:30.790: INFO: Pod "pod-projected-configmaps-e269b1cf-3d40-4b8b-b636-6876a14183d2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.764743ms
Feb 10 08:35:32.806: INFO: Pod "pod-projected-configmaps-e269b1cf-3d40-4b8b-b636-6876a14183d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03579108s
Feb 10 08:35:34.820: INFO: Pod "pod-projected-configmaps-e269b1cf-3d40-4b8b-b636-6876a14183d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049337475s
Feb 10 08:35:36.834: INFO: Pod "pod-projected-configmaps-e269b1cf-3d40-4b8b-b636-6876a14183d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062913808s
STEP: Saw pod success
Feb 10 08:35:36.834: INFO: Pod "pod-projected-configmaps-e269b1cf-3d40-4b8b-b636-6876a14183d2" satisfied condition "success or failure"
Feb 10 08:35:36.845: INFO: Trying to get logs from node master1 pod pod-projected-configmaps-e269b1cf-3d40-4b8b-b636-6876a14183d2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 08:35:36.945: INFO: Waiting for pod pod-projected-configmaps-e269b1cf-3d40-4b8b-b636-6876a14183d2 to disappear
Feb 10 08:35:36.957: INFO: Pod pod-projected-configmaps-e269b1cf-3d40-4b8b-b636-6876a14183d2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:35:36.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2375" for this suite.

• [SLOW TEST:6.699 seconds]
[sig-storage] Projected configMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":28,"skipped":556,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:35:36.990: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6923
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-5af9c0b5-914a-440f-84aa-4082c5d8c3a5
STEP: Creating a pod to test consume configMaps
Feb 10 08:35:37.474: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a61f18e-3799-41ec-87a8-695922d6c9d7" in namespace "configmap-6923" to be "success or failure"
Feb 10 08:35:37.488: INFO: Pod "pod-configmaps-7a61f18e-3799-41ec-87a8-695922d6c9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.426261ms
Feb 10 08:35:39.505: INFO: Pod "pod-configmaps-7a61f18e-3799-41ec-87a8-695922d6c9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031435833s
Feb 10 08:35:41.520: INFO: Pod "pod-configmaps-7a61f18e-3799-41ec-87a8-695922d6c9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046487103s
Feb 10 08:35:43.532: INFO: Pod "pod-configmaps-7a61f18e-3799-41ec-87a8-695922d6c9d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058303331s
STEP: Saw pod success
Feb 10 08:35:43.532: INFO: Pod "pod-configmaps-7a61f18e-3799-41ec-87a8-695922d6c9d7" satisfied condition "success or failure"
Feb 10 08:35:43.543: INFO: Trying to get logs from node master1 pod pod-configmaps-7a61f18e-3799-41ec-87a8-695922d6c9d7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 08:35:43.623: INFO: Waiting for pod pod-configmaps-7a61f18e-3799-41ec-87a8-695922d6c9d7 to disappear
Feb 10 08:35:43.646: INFO: Pod pod-configmaps-7a61f18e-3799-41ec-87a8-695922d6c9d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:35:43.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6923" for this suite.

• [SLOW TEST:6.689 seconds]
[sig-storage] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":29,"skipped":556,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:35:43.680: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:36:44.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6840" for this suite.

• [SLOW TEST:60.554 seconds]
[k8s.io] Probing container
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":278,"completed":30,"skipped":561,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:36:44.236: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 08:36:44.680: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be04a4e5-a663-4363-afab-b04ede0e8b69" in namespace "projected-8244" to be "success or failure"
Feb 10 08:36:44.695: INFO: Pod "downwardapi-volume-be04a4e5-a663-4363-afab-b04ede0e8b69": Phase="Pending", Reason="", readiness=false. Elapsed: 14.620362ms
Feb 10 08:36:46.707: INFO: Pod "downwardapi-volume-be04a4e5-a663-4363-afab-b04ede0e8b69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027052342s
Feb 10 08:36:48.719: INFO: Pod "downwardapi-volume-be04a4e5-a663-4363-afab-b04ede0e8b69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038733461s
Feb 10 08:36:50.737: INFO: Pod "downwardapi-volume-be04a4e5-a663-4363-afab-b04ede0e8b69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056922418s
STEP: Saw pod success
Feb 10 08:36:50.737: INFO: Pod "downwardapi-volume-be04a4e5-a663-4363-afab-b04ede0e8b69" satisfied condition "success or failure"
Feb 10 08:36:50.747: INFO: Trying to get logs from node master3 pod downwardapi-volume-be04a4e5-a663-4363-afab-b04ede0e8b69 container client-container: <nil>
STEP: delete the pod
Feb 10 08:36:50.835: INFO: Waiting for pod downwardapi-volume-be04a4e5-a663-4363-afab-b04ede0e8b69 to disappear
Feb 10 08:36:50.850: INFO: Pod downwardapi-volume-be04a4e5-a663-4363-afab-b04ede0e8b69 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:36:50.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8244" for this suite.

• [SLOW TEST:6.653 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":278,"completed":31,"skipped":575,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:36:50.890: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8558
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:37:07.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8558" for this suite.

• [SLOW TEST:16.802 seconds]
[sig-api-machinery] ResourceQuota
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":278,"completed":32,"skipped":579,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:37:07.693: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 08:37:08.126: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cce1ca8d-0269-407e-8879-ab4ad22c940a" in namespace "downward-api-3906" to be "success or failure"
Feb 10 08:37:08.137: INFO: Pod "downwardapi-volume-cce1ca8d-0269-407e-8879-ab4ad22c940a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.762461ms
Feb 10 08:37:10.155: INFO: Pod "downwardapi-volume-cce1ca8d-0269-407e-8879-ab4ad22c940a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028456184s
Feb 10 08:37:12.171: INFO: Pod "downwardapi-volume-cce1ca8d-0269-407e-8879-ab4ad22c940a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044536306s
Feb 10 08:37:14.189: INFO: Pod "downwardapi-volume-cce1ca8d-0269-407e-8879-ab4ad22c940a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062574166s
STEP: Saw pod success
Feb 10 08:37:14.189: INFO: Pod "downwardapi-volume-cce1ca8d-0269-407e-8879-ab4ad22c940a" satisfied condition "success or failure"
Feb 10 08:37:14.199: INFO: Trying to get logs from node master3 pod downwardapi-volume-cce1ca8d-0269-407e-8879-ab4ad22c940a container client-container: <nil>
STEP: delete the pod
Feb 10 08:37:14.291: INFO: Waiting for pod downwardapi-volume-cce1ca8d-0269-407e-8879-ab4ad22c940a to disappear
Feb 10 08:37:14.306: INFO: Pod downwardapi-volume-cce1ca8d-0269-407e-8879-ab4ad22c940a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:37:14.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3906" for this suite.

• [SLOW TEST:6.645 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":278,"completed":33,"skipped":584,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:37:14.340: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-15d63568-3623-4f88-815e-b85a850ad0dd
STEP: Creating a pod to test consume secrets
Feb 10 08:37:14.790: INFO: Waiting up to 5m0s for pod "pod-secrets-8690bb0f-0101-4fbb-83d9-d687c93024de" in namespace "secrets-1534" to be "success or failure"
Feb 10 08:37:14.801: INFO: Pod "pod-secrets-8690bb0f-0101-4fbb-83d9-d687c93024de": Phase="Pending", Reason="", readiness=false. Elapsed: 11.093441ms
Feb 10 08:37:16.813: INFO: Pod "pod-secrets-8690bb0f-0101-4fbb-83d9-d687c93024de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022700559s
Feb 10 08:37:18.827: INFO: Pod "pod-secrets-8690bb0f-0101-4fbb-83d9-d687c93024de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036654716s
Feb 10 08:37:20.842: INFO: Pod "pod-secrets-8690bb0f-0101-4fbb-83d9-d687c93024de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051977171s
STEP: Saw pod success
Feb 10 08:37:20.843: INFO: Pod "pod-secrets-8690bb0f-0101-4fbb-83d9-d687c93024de" satisfied condition "success or failure"
Feb 10 08:37:20.853: INFO: Trying to get logs from node master3 pod pod-secrets-8690bb0f-0101-4fbb-83d9-d687c93024de container secret-volume-test: <nil>
STEP: delete the pod
Feb 10 08:37:20.955: INFO: Waiting for pod pod-secrets-8690bb0f-0101-4fbb-83d9-d687c93024de to disappear
Feb 10 08:37:20.975: INFO: Pod pod-secrets-8690bb0f-0101-4fbb-83d9-d687c93024de no longer exists
[AfterEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:37:20.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1534" for this suite.

• [SLOW TEST:6.671 seconds]
[sig-storage] Secrets
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":34,"skipped":590,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:37:21.013: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9840
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 10 08:37:21.435: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 10 08:38:16.905: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:38:35.054: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:39:30.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9840" for this suite.

• [SLOW TEST:129.433 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":278,"completed":35,"skipped":605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:39:30.449: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-291
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-72713043-e4f8-4b2c-a78f-0eb216a0527b
STEP: Creating a pod to test consume secrets
Feb 10 08:39:30.897: INFO: Waiting up to 5m0s for pod "pod-secrets-95b6cd43-f642-49e4-a205-6acee790f7e3" in namespace "secrets-291" to be "success or failure"
Feb 10 08:39:30.908: INFO: Pod "pod-secrets-95b6cd43-f642-49e4-a205-6acee790f7e3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.043313ms
Feb 10 08:39:32.926: INFO: Pod "pod-secrets-95b6cd43-f642-49e4-a205-6acee790f7e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028481533s
Feb 10 08:39:34.947: INFO: Pod "pod-secrets-95b6cd43-f642-49e4-a205-6acee790f7e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049716974s
Feb 10 08:39:36.960: INFO: Pod "pod-secrets-95b6cd43-f642-49e4-a205-6acee790f7e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063089102s
STEP: Saw pod success
Feb 10 08:39:36.960: INFO: Pod "pod-secrets-95b6cd43-f642-49e4-a205-6acee790f7e3" satisfied condition "success or failure"
Feb 10 08:39:36.971: INFO: Trying to get logs from node master3 pod pod-secrets-95b6cd43-f642-49e4-a205-6acee790f7e3 container secret-volume-test: <nil>
STEP: delete the pod
Feb 10 08:39:37.357: INFO: Waiting for pod pod-secrets-95b6cd43-f642-49e4-a205-6acee790f7e3 to disappear
Feb 10 08:39:37.367: INFO: Pod pod-secrets-95b6cd43-f642-49e4-a205-6acee790f7e3 no longer exists
[AfterEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:39:37.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-291" for this suite.

• [SLOW TEST:6.956 seconds]
[sig-storage] Secrets
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":36,"skipped":656,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:39:37.405: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6370
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0210 08:40:08.466877      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 10 08:40:08.467: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:40:08.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6370" for this suite.

• [SLOW TEST:31.093 seconds]
[sig-api-machinery] Garbage collector
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":278,"completed":37,"skipped":670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:40:08.500: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 08:40:51.761: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 08:40:53.797: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920851, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920851, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920851, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920851, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 08:40:55.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920851, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920851, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920851, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920851, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 08:40:58.851: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:40:58.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-592" for this suite.
STEP: Destroying namespace "webhook-592-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:50.649 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":278,"completed":38,"skipped":698,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:40:59.150: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 08:41:34.515: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 08:41:36.550: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920894, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920894, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920894, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920894, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 08:41:38.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920894, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920894, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920894, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716920894, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 08:41:41.603: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:41:42.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2891" for this suite.
STEP: Destroying namespace "webhook-2891-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:43.555 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":278,"completed":39,"skipped":705,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:41:42.707: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-4601c722-19ed-4f8f-b8eb-896883187d44
STEP: Creating a pod to test consume secrets
Feb 10 08:41:43.259: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29dacbf0-54d2-4ff4-a11e-01be9c0df0ce" in namespace "projected-5062" to be "success or failure"
Feb 10 08:41:43.271: INFO: Pod "pod-projected-secrets-29dacbf0-54d2-4ff4-a11e-01be9c0df0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 11.363092ms
Feb 10 08:41:45.298: INFO: Pod "pod-projected-secrets-29dacbf0-54d2-4ff4-a11e-01be9c0df0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038229627s
Feb 10 08:41:47.312: INFO: Pod "pod-projected-secrets-29dacbf0-54d2-4ff4-a11e-01be9c0df0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051993423s
Feb 10 08:41:49.329: INFO: Pod "pod-projected-secrets-29dacbf0-54d2-4ff4-a11e-01be9c0df0ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06966296s
STEP: Saw pod success
Feb 10 08:41:49.329: INFO: Pod "pod-projected-secrets-29dacbf0-54d2-4ff4-a11e-01be9c0df0ce" satisfied condition "success or failure"
Feb 10 08:41:49.340: INFO: Trying to get logs from node master3 pod pod-projected-secrets-29dacbf0-54d2-4ff4-a11e-01be9c0df0ce container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 10 08:41:49.768: INFO: Waiting for pod pod-projected-secrets-29dacbf0-54d2-4ff4-a11e-01be9c0df0ce to disappear
Feb 10 08:41:49.778: INFO: Pod pod-projected-secrets-29dacbf0-54d2-4ff4-a11e-01be9c0df0ce no longer exists
[AfterEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:41:49.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5062" for this suite.

• [SLOW TEST:7.109 seconds]
[sig-storage] Projected secret
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":40,"skipped":734,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:41:49.819: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7880
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should support proxy with --port 0  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Feb 10 08:41:50.240: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-754024871 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:41:50.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7880" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":278,"completed":41,"skipped":736,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:41:50.771: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:41:56.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3835" for this suite.

• [SLOW TEST:5.313 seconds]
[sig-api-machinery] Watchers
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":278,"completed":42,"skipped":738,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:41:56.087: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6640
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:41:56.543: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:42:58.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6640" for this suite.

• [SLOW TEST:62.262 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":278,"completed":43,"skipped":747,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:42:58.352: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9400
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb 10 08:43:09.001: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0210 08:43:09.001465      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:43:09.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9400" for this suite.

• [SLOW TEST:10.713 seconds]
[sig-api-machinery] Garbage collector
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":278,"completed":44,"skipped":750,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:43:09.069: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1713
[It] should create a deployment from an image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 10 08:43:09.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-8927'
Feb 10 08:43:12.608: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 10 08:43:12.608: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1718
Feb 10 08:43:14.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8927'
Feb 10 08:43:15.215: INFO: stderr: ""
Feb 10 08:43:15.216: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:43:15.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8927" for this suite.

• [SLOW TEST:6.198 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
    should create a deployment from an image  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":278,"completed":45,"skipped":752,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:43:15.268: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-66
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 10 08:43:22.348: INFO: Successfully updated pod "pod-update-6408ab35-76bb-4d48-a29a-8faa606038ca"
STEP: verifying the updated pod is in kubernetes
Feb 10 08:43:22.375: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:43:22.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-66" for this suite.

• [SLOW TEST:7.147 seconds]
[k8s.io] Pods
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be updated [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":278,"completed":46,"skipped":755,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:43:22.416: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-616748c9-c371-4428-8474-c6468630b34f
STEP: Creating a pod to test consume configMaps
Feb 10 08:43:22.855: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-75e15683-4d8f-4ac3-b552-1a95219ea402" in namespace "projected-7539" to be "success or failure"
Feb 10 08:43:22.868: INFO: Pod "pod-projected-configmaps-75e15683-4d8f-4ac3-b552-1a95219ea402": Phase="Pending", Reason="", readiness=false. Elapsed: 12.235391ms
Feb 10 08:43:24.882: INFO: Pod "pod-projected-configmaps-75e15683-4d8f-4ac3-b552-1a95219ea402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025956321s
Feb 10 08:43:26.898: INFO: Pod "pod-projected-configmaps-75e15683-4d8f-4ac3-b552-1a95219ea402": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04207237s
Feb 10 08:43:28.920: INFO: Pod "pod-projected-configmaps-75e15683-4d8f-4ac3-b552-1a95219ea402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06402372s
STEP: Saw pod success
Feb 10 08:43:28.920: INFO: Pod "pod-projected-configmaps-75e15683-4d8f-4ac3-b552-1a95219ea402" satisfied condition "success or failure"
Feb 10 08:43:28.935: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-75e15683-4d8f-4ac3-b552-1a95219ea402 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 08:43:29.336: INFO: Waiting for pod pod-projected-configmaps-75e15683-4d8f-4ac3-b552-1a95219ea402 to disappear
Feb 10 08:43:29.347: INFO: Pod pod-projected-configmaps-75e15683-4d8f-4ac3-b552-1a95219ea402 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:43:29.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7539" for this suite.

• [SLOW TEST:6.980 seconds]
[sig-storage] Projected configMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":47,"skipped":758,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:43:29.397: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0210 08:43:40.247708      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 10 08:43:40.248: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:43:40.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-934" for this suite.

• [SLOW TEST:10.899 seconds]
[sig-api-machinery] Garbage collector
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":278,"completed":48,"skipped":765,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:43:40.321: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5014
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 10 08:43:45.900: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:43:45.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5014" for this suite.

• [SLOW TEST:5.729 seconds]
[k8s.io] Container Runtime
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":49,"skipped":782,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:43:46.051: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1622.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1622.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1622.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1622.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1622.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1622.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1622.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 59.171.155.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.155.171.59_udp@PTR;check="$$(dig +tcp +noall +answer +search 59.171.155.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.155.171.59_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1622.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1622.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1622.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1622.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1622.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1622.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1622.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1622.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1622.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 59.171.155.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.155.171.59_udp@PTR;check="$$(dig +tcp +noall +answer +search 59.171.155.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.155.171.59_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 10 08:43:56.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-1622.svc.cluster.local from pod dns-1622/dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453: the server could not find the requested resource (get pods dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453)
Feb 10 08:43:56.811: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1622.svc.cluster.local from pod dns-1622/dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453: the server could not find the requested resource (get pods dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453)
Feb 10 08:43:56.823: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local from pod dns-1622/dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453: the server could not find the requested resource (get pods dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453)
Feb 10 08:43:56.839: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local from pod dns-1622/dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453: the server could not find the requested resource (get pods dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453)
Feb 10 08:43:56.946: INFO: Unable to read jessie_udp@dns-test-service.dns-1622.svc.cluster.local from pod dns-1622/dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453: the server could not find the requested resource (get pods dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453)
Feb 10 08:43:56.958: INFO: Unable to read jessie_tcp@dns-test-service.dns-1622.svc.cluster.local from pod dns-1622/dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453: the server could not find the requested resource (get pods dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453)
Feb 10 08:43:56.969: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local from pod dns-1622/dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453: the server could not find the requested resource (get pods dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453)
Feb 10 08:43:56.985: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local from pod dns-1622/dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453: the server could not find the requested resource (get pods dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453)
Feb 10 08:43:57.073: INFO: Lookups using dns-1622/dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453 failed for: [wheezy_udp@dns-test-service.dns-1622.svc.cluster.local wheezy_tcp@dns-test-service.dns-1622.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local jessie_udp@dns-test-service.dns-1622.svc.cluster.local jessie_tcp@dns-test-service.dns-1622.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1622.svc.cluster.local]

Feb 10 08:44:02.367: INFO: DNS probes using dns-1622/dns-test-f41163c5-20d6-4bbe-b393-45bf870ff453 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:44:02.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1622" for this suite.

• [SLOW TEST:16.629 seconds]
[sig-network] DNS
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":278,"completed":50,"skipped":787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:44:02.683: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:44:03.146: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6df687e3-2b3e-46ba-9c7e-4ea710433b83" in namespace "security-context-test-9798" to be "success or failure"
Feb 10 08:44:03.155: INFO: Pod "busybox-user-65534-6df687e3-2b3e-46ba-9c7e-4ea710433b83": Phase="Pending", Reason="", readiness=false. Elapsed: 9.334089ms
Feb 10 08:44:05.169: INFO: Pod "busybox-user-65534-6df687e3-2b3e-46ba-9c7e-4ea710433b83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023031273s
Feb 10 08:44:07.182: INFO: Pod "busybox-user-65534-6df687e3-2b3e-46ba-9c7e-4ea710433b83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035642553s
Feb 10 08:44:09.194: INFO: Pod "busybox-user-65534-6df687e3-2b3e-46ba-9c7e-4ea710433b83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048326889s
Feb 10 08:44:09.194: INFO: Pod "busybox-user-65534-6df687e3-2b3e-46ba-9c7e-4ea710433b83" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:44:09.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9798" for this suite.

• [SLOW TEST:6.548 seconds]
[k8s.io] Security Context
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  When creating a container with runAsUser
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:43
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":51,"skipped":818,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:44:09.233: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3817
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 10 08:44:09.676: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 10 08:44:09.728: INFO: Waiting for terminating namespaces to be deleted...
Feb 10 08:44:09.739: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Feb 10 08:44:10.080: INFO: kube-apiserver-master1 from kube-system started at 2020-02-05 16:29:54 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.080: INFO: 	Container kube-apiserver ready: true, restart count 3021
Feb 10 08:44:10.080: INFO: ss2-1 from statefulset-6054 started at 2020-02-10 05:24:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.080: INFO: 	Container webserver ready: true, restart count 0
Feb 10 08:44:10.080: INFO: kube-controller-manager-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.080: INFO: 	Container kube-controller-manager ready: true, restart count 565
Feb 10 08:44:10.080: INFO: ntp-zj7fc from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.080: INFO: 	Container ntp ready: true, restart count 1
Feb 10 08:44:10.080: INFO: calico-node-p64wz from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.080: INFO: 	Container calico-node ready: true, restart count 1
Feb 10 08:44:10.080: INFO: simpletest-rc-to-be-deleted-f48d7 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.080: INFO: 	Container nginx ready: true, restart count 0
Feb 10 08:44:10.080: INFO: kube-scheduler-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.080: INFO: 	Container kube-scheduler ready: true, restart count 572
Feb 10 08:44:10.081: INFO: nginx-proxy-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.081: INFO: 	Container nginx-proxy ready: true, restart count 845
Feb 10 08:44:10.081: INFO: coredns-hfdns from kube-system started at 2020-01-15 01:54:01 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.081: INFO: 	Container coredns ready: true, restart count 1
Feb 10 08:44:10.081: INFO: agnhost-deployment-54964f567d-nvhtm from kube-system started at 2020-02-10 07:28:38 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.081: INFO: 	Container agnhost ready: true, restart count 0
Feb 10 08:44:10.081: INFO: sonobuoy-e2e-job-26d59f1c3cea4685 from sonobuoy started at 2020-02-10 08:27:25 +0000 UTC (2 container statuses recorded)
Feb 10 08:44:10.081: INFO: 	Container e2e ready: true, restart count 0
Feb 10 08:44:10.081: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 10 08:44:10.081: INFO: kube-proxy-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.081: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 08:44:10.081: INFO: localpv-provisioner-ds-nt4r5 from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.081: INFO: 	Container provisioner ready: true, restart count 1
Feb 10 08:44:10.081: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Feb 10 08:44:10.424: INFO: metrics-server-6c6fd884-md7t8 from kube-system started at 2020-01-15 01:54:18 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container metrics-server ready: true, restart count 1
Feb 10 08:44:10.425: INFO: simpletest-rc-to-be-deleted-hvgr7 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container nginx ready: true, restart count 0
Feb 10 08:44:10.425: INFO: nginx-proxy-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container nginx-proxy ready: true, restart count 846
Feb 10 08:44:10.425: INFO: kube-apiserver-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container kube-apiserver ready: true, restart count 563
Feb 10 08:44:10.425: INFO: localpv-provisioner-ds-sd5nt from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container provisioner ready: true, restart count 3
Feb 10 08:44:10.425: INFO: tiller-deploy-75cd95b6b4-tkpds from kube-system started at 2020-02-07 13:38:14 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container tiller ready: true, restart count 0
Feb 10 08:44:10.425: INFO: kube-scheduler-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container kube-scheduler ready: true, restart count 561
Feb 10 08:44:10.425: INFO: kube-controller-manager-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container kube-controller-manager ready: true, restart count 581
Feb 10 08:44:10.425: INFO: ntp-g2jhr from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container ntp ready: true, restart count 1
Feb 10 08:44:10.425: INFO: sonobuoy from sonobuoy started at 2020-02-10 08:27:21 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 10 08:44:10.425: INFO: kube-proxy-master2 from kube-system started at 2020-02-05 16:28:09 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 08:44:10.425: INFO: coredns-sv7qc from kube-system started at 2020-01-15 02:50:54 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container coredns ready: true, restart count 1
Feb 10 08:44:10.425: INFO: calico-node-q6h5s from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container calico-node ready: true, restart count 2
Feb 10 08:44:10.425: INFO: ss2-2 from statefulset-6054 started at 2020-02-10 05:25:22 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container webserver ready: true, restart count 0
Feb 10 08:44:10.425: INFO: simpletest-rc-to-be-deleted-62s49 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.425: INFO: 	Container nginx ready: true, restart count 0
Feb 10 08:44:10.425: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Feb 10 08:44:10.466: INFO: ntp-c9lwf from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container ntp ready: true, restart count 1
Feb 10 08:44:10.466: INFO: calico-kube-controllers-6fcbd694f7-zs7dw from kube-system started at 2020-02-07 13:38:14 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 10 08:44:10.466: INFO: ss2-0 from statefulset-6054 started at 2020-02-10 05:25:17 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container webserver ready: true, restart count 0
Feb 10 08:44:10.466: INFO: kube-apiserver-master3 from kube-system started at 2020-02-05 16:28:26 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container kube-apiserver ready: true, restart count 563
Feb 10 08:44:10.466: INFO: kube-controller-manager-master3 from kube-system started at 2020-02-05 16:28:26 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container kube-controller-manager ready: true, restart count 553
Feb 10 08:44:10.466: INFO: kube-proxy-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 08:44:10.466: INFO: kube-scheduler-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container kube-scheduler ready: true, restart count 574
Feb 10 08:44:10.466: INFO: nginx-proxy-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container nginx-proxy ready: true, restart count 851
Feb 10 08:44:10.466: INFO: coredns-bsmzw from kube-system started at 2020-01-15 01:54:55 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container coredns ready: true, restart count 1
Feb 10 08:44:10.466: INFO: ntp-server-8cbc779-hj2rj from kube-system started at 2020-01-15 03:48:56 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container ntp-server ready: true, restart count 1
Feb 10 08:44:10.466: INFO: calico-node-6fr4n from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container calico-node ready: true, restart count 2
Feb 10 08:44:10.466: INFO: simpletest-rc-to-be-deleted-clkth from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container nginx ready: true, restart count 0
Feb 10 08:44:10.466: INFO: localpv-provisioner-ds-vhkpq from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container provisioner ready: true, restart count 3
Feb 10 08:44:10.466: INFO: simpletest-rc-to-be-deleted-g226k from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.466: INFO: 	Container nginx ready: true, restart count 0
Feb 10 08:44:10.467: INFO: busybox-user-65534-6df687e3-2b3e-46ba-9c7e-4ea710433b83 from security-context-test-9798 started at 2020-02-10 08:44:03 +0000 UTC (1 container statuses recorded)
Feb 10 08:44:10.467: INFO: 	Container busybox-user-65534-6df687e3-2b3e-46ba-9c7e-4ea710433b83 ready: false, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c3953c1a-337d-4e49-9ec8-a7803100f337 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c3953c1a-337d-4e49-9ec8-a7803100f337 off the node master3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c3953c1a-337d-4e49-9ec8-a7803100f337
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:44:22.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3817" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:13.538 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":278,"completed":52,"skipped":841,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:44:22.775: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6709
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:44:23.198: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 10 08:44:40.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-6709 create -f -'
Feb 10 08:44:44.765: INFO: stderr: ""
Feb 10 08:44:44.765: INFO: stdout: "e2e-test-crd-publish-openapi-2463-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 10 08:44:44.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-6709 delete e2e-test-crd-publish-openapi-2463-crds test-cr'
Feb 10 08:44:45.788: INFO: stderr: ""
Feb 10 08:44:45.789: INFO: stdout: "e2e-test-crd-publish-openapi-2463-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 10 08:44:45.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-6709 apply -f -'
Feb 10 08:44:47.216: INFO: stderr: ""
Feb 10 08:44:47.216: INFO: stdout: "e2e-test-crd-publish-openapi-2463-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 10 08:44:47.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-6709 delete e2e-test-crd-publish-openapi-2463-crds test-cr'
Feb 10 08:44:47.770: INFO: stderr: ""
Feb 10 08:44:47.771: INFO: stdout: "e2e-test-crd-publish-openapi-2463-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 10 08:44:47.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 explain e2e-test-crd-publish-openapi-2463-crds'
Feb 10 08:44:49.057: INFO: stderr: ""
Feb 10 08:44:49.058: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2463-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:45:02.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6709" for this suite.

• [SLOW TEST:39.542 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":278,"completed":53,"skipped":855,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:45:02.320: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-28ea071e-f6da-470a-b6d6-17e5419384de in namespace container-probe-1721
Feb 10 08:45:08.848: INFO: Started pod liveness-28ea071e-f6da-470a-b6d6-17e5419384de in namespace container-probe-1721
STEP: checking the pod's current state and verifying that restartCount is present
Feb 10 08:45:08.858: INFO: Initial restart count of pod liveness-28ea071e-f6da-470a-b6d6-17e5419384de is 0
Feb 10 08:45:18.954: INFO: Restart count of pod container-probe-1721/liveness-28ea071e-f6da-470a-b6d6-17e5419384de is now 1 (10.09531553s elapsed)
Feb 10 08:45:39.101: INFO: Restart count of pod container-probe-1721/liveness-28ea071e-f6da-470a-b6d6-17e5419384de is now 2 (30.241917841s elapsed)
Feb 10 08:46:01.258: INFO: Restart count of pod container-probe-1721/liveness-28ea071e-f6da-470a-b6d6-17e5419384de is now 3 (52.399045361s elapsed)
Feb 10 08:46:19.393: INFO: Restart count of pod container-probe-1721/liveness-28ea071e-f6da-470a-b6d6-17e5419384de is now 4 (1m10.534091854s elapsed)
Feb 10 08:47:29.895: INFO: Restart count of pod container-probe-1721/liveness-28ea071e-f6da-470a-b6d6-17e5419384de is now 5 (2m21.036846619s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:47:29.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1721" for this suite.

• [SLOW TEST:147.658 seconds]
[k8s.io] Probing container
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":278,"completed":54,"skipped":869,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:47:29.978: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 08:48:08.099: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 08:48:10.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921288, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921288, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921288, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921288, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 08:48:12.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921288, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921288, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921288, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921288, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 08:48:15.186: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:48:28.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6300" for this suite.
STEP: Destroying namespace "webhook-6300-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:58.374 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":278,"completed":55,"skipped":871,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:48:28.360: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:48:28.804: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 10 08:48:28.849: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 10 08:48:33.868: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 10 08:48:33.869: INFO: Creating deployment "test-rolling-update-deployment"
Feb 10 08:48:33.893: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 10 08:48:33.921: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 10 08:48:35.944: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 10 08:48:35.954: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921314, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921314, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921314, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921313, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 08:48:37.966: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921314, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921314, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921314, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921313, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 08:48:39.973: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 10 08:48:40.004: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3427 /apis/apps/v1/namespaces/deployment-3427/deployments/test-rolling-update-deployment 79010096-d9cf-48f8-aaf1-096974834b85 4647648 1 2020-02-10 08:48:33 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000f89f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-10 08:48:34 +0000 UTC,LastTransitionTime:2020-02-10 08:48:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-02-10 08:48:39 +0000 UTC,LastTransitionTime:2020-02-10 08:48:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 10 08:48:40.017: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-3427 /apis/apps/v1/namespaces/deployment-3427/replicasets/test-rolling-update-deployment-67cf4f6444 3eb1ce8d-968c-4d3c-b1e6-bc47b429b5af 4647637 1 2020-02-10 08:48:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 79010096-d9cf-48f8-aaf1-096974834b85 0xc003c3a437 0xc003c3a438}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003c3a4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 10 08:48:40.017: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 10 08:48:40.018: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3427 /apis/apps/v1/namespaces/deployment-3427/replicasets/test-rolling-update-controller 6b8607ba-61e9-415a-8b57-8ff93badd4fa 4647646 2 2020-02-10 08:48:28 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 79010096-d9cf-48f8-aaf1-096974834b85 0xc003c3a367 0xc003c3a368}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c3a3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 10 08:48:40.030: INFO: Pod "test-rolling-update-deployment-67cf4f6444-z2tz4" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-z2tz4 test-rolling-update-deployment-67cf4f6444- deployment-3427 /api/v1/namespaces/deployment-3427/pods/test-rolling-update-deployment-67cf4f6444-z2tz4 21518693-b0bf-4c76-a873-5aa5b1600c10 4647636 0 2020-02-10 08:48:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 3eb1ce8d-968c-4d3c-b1e6-bc47b429b5af 0xc003c3a967 0xc003c3a968}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sh77b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sh77b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sh77b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 08:48:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 08:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 08:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 08:48:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:10.156.161.142,StartTime:2020-02-10 08:48:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 08:48:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker://sha256:01966e4b12a05550df97ce433c8309c377e2de6d6e52b3454fb80c5f9b6f8212,ContainerID:docker://77eddd074f608af48f33f5c7946c89217c6f9090d366c43ab589fa996d4c62e5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.161.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:48:40.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3427" for this suite.

• [SLOW TEST:11.711 seconds]
[sig-apps] Deployment
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":278,"completed":56,"skipped":887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:48:40.073: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-k9ll
STEP: Creating a pod to test atomic-volume-subpath
Feb 10 08:48:40.546: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-k9ll" in namespace "subpath-3679" to be "success or failure"
Feb 10 08:48:40.560: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Pending", Reason="", readiness=false. Elapsed: 13.67293ms
Feb 10 08:48:42.579: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03300262s
Feb 10 08:48:44.597: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050487686s
Feb 10 08:48:46.610: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Running", Reason="", readiness=true. Elapsed: 6.063643806s
Feb 10 08:48:48.627: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Running", Reason="", readiness=true. Elapsed: 8.080435346s
Feb 10 08:48:50.641: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Running", Reason="", readiness=true. Elapsed: 10.094423741s
Feb 10 08:48:52.662: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Running", Reason="", readiness=true. Elapsed: 12.115363398s
Feb 10 08:48:54.678: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Running", Reason="", readiness=true. Elapsed: 14.131976329s
Feb 10 08:48:56.691: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Running", Reason="", readiness=true. Elapsed: 16.144449434s
Feb 10 08:48:58.701: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Running", Reason="", readiness=true. Elapsed: 18.155067176s
Feb 10 08:49:00.716: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Running", Reason="", readiness=true. Elapsed: 20.169939238s
Feb 10 08:49:02.740: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Running", Reason="", readiness=true. Elapsed: 22.193789482s
Feb 10 08:49:04.757: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Running", Reason="", readiness=true. Elapsed: 24.21111288s
Feb 10 08:49:06.770: INFO: Pod "pod-subpath-test-secret-k9ll": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.223966232s
STEP: Saw pod success
Feb 10 08:49:06.771: INFO: Pod "pod-subpath-test-secret-k9ll" satisfied condition "success or failure"
Feb 10 08:49:06.783: INFO: Trying to get logs from node master3 pod pod-subpath-test-secret-k9ll container test-container-subpath-secret-k9ll: <nil>
STEP: delete the pod
Feb 10 08:49:07.200: INFO: Waiting for pod pod-subpath-test-secret-k9ll to disappear
Feb 10 08:49:07.211: INFO: Pod pod-subpath-test-secret-k9ll no longer exists
STEP: Deleting pod pod-subpath-test-secret-k9ll
Feb 10 08:49:07.211: INFO: Deleting pod "pod-subpath-test-secret-k9ll" in namespace "subpath-3679"
[AfterEach] [sig-storage] Subpath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:49:07.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3679" for this suite.

• [SLOW TEST:27.182 seconds]
[sig-storage] Subpath
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":278,"completed":57,"skipped":909,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:49:07.256: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 08:49:07.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e1c78e0-7ff7-429f-b53e-9e4960b252b7" in namespace "downward-api-2614" to be "success or failure"
Feb 10 08:49:07.728: INFO: Pod "downwardapi-volume-4e1c78e0-7ff7-429f-b53e-9e4960b252b7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.92169ms
Feb 10 08:49:09.740: INFO: Pod "downwardapi-volume-4e1c78e0-7ff7-429f-b53e-9e4960b252b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026547856s
Feb 10 08:49:11.754: INFO: Pod "downwardapi-volume-4e1c78e0-7ff7-429f-b53e-9e4960b252b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040572502s
Feb 10 08:49:13.768: INFO: Pod "downwardapi-volume-4e1c78e0-7ff7-429f-b53e-9e4960b252b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054591185s
STEP: Saw pod success
Feb 10 08:49:13.768: INFO: Pod "downwardapi-volume-4e1c78e0-7ff7-429f-b53e-9e4960b252b7" satisfied condition "success or failure"
Feb 10 08:49:13.779: INFO: Trying to get logs from node master3 pod downwardapi-volume-4e1c78e0-7ff7-429f-b53e-9e4960b252b7 container client-container: <nil>
STEP: delete the pod
Feb 10 08:49:13.870: INFO: Waiting for pod downwardapi-volume-4e1c78e0-7ff7-429f-b53e-9e4960b252b7 to disappear
Feb 10 08:49:13.883: INFO: Pod downwardapi-volume-4e1c78e0-7ff7-429f-b53e-9e4960b252b7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:49:13.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2614" for this suite.

• [SLOW TEST:6.662 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":58,"skipped":910,"failed":0}
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:49:13.918: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2844
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-69869a9a-9b75-490e-98bc-8266650bdcfc
STEP: Creating a pod to test consume secrets
Feb 10 08:49:14.377: INFO: Waiting up to 5m0s for pod "pod-secrets-455b5670-d12c-4107-870b-768a0639fc9e" in namespace "secrets-2844" to be "success or failure"
Feb 10 08:49:14.388: INFO: Pod "pod-secrets-455b5670-d12c-4107-870b-768a0639fc9e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.884627ms
Feb 10 08:49:16.406: INFO: Pod "pod-secrets-455b5670-d12c-4107-870b-768a0639fc9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028436448s
Feb 10 08:49:18.420: INFO: Pod "pod-secrets-455b5670-d12c-4107-870b-768a0639fc9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042802845s
Feb 10 08:49:20.435: INFO: Pod "pod-secrets-455b5670-d12c-4107-870b-768a0639fc9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057565739s
STEP: Saw pod success
Feb 10 08:49:20.435: INFO: Pod "pod-secrets-455b5670-d12c-4107-870b-768a0639fc9e" satisfied condition "success or failure"
Feb 10 08:49:20.445: INFO: Trying to get logs from node master3 pod pod-secrets-455b5670-d12c-4107-870b-768a0639fc9e container secret-env-test: <nil>
STEP: delete the pod
Feb 10 08:49:20.521: INFO: Waiting for pod pod-secrets-455b5670-d12c-4107-870b-768a0639fc9e to disappear
Feb 10 08:49:20.537: INFO: Pod pod-secrets-455b5670-d12c-4107-870b-768a0639fc9e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:49:20.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2844" for this suite.

• [SLOW TEST:6.664 seconds]
[sig-api-machinery] Secrets
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":278,"completed":59,"skipped":910,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:49:20.585: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-0b89189c-23a8-4aa3-8003-e0ef7ff8d612
[AfterEach] [sig-api-machinery] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:49:21.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7200" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":278,"completed":60,"skipped":919,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:49:21.078: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:49:21.615: INFO: Create a RollingUpdate DaemonSet
Feb 10 08:49:21.635: INFO: Check that daemon pods launch on every node of the cluster
Feb 10 08:49:21.668: INFO: Number of nodes with available pods: 0
Feb 10 08:49:21.668: INFO: Node master1 is running more than one daemon pod
Feb 10 08:49:22.699: INFO: Number of nodes with available pods: 0
Feb 10 08:49:22.700: INFO: Node master1 is running more than one daemon pod
Feb 10 08:49:23.704: INFO: Number of nodes with available pods: 0
Feb 10 08:49:23.704: INFO: Node master1 is running more than one daemon pod
Feb 10 08:49:24.708: INFO: Number of nodes with available pods: 0
Feb 10 08:49:24.708: INFO: Node master1 is running more than one daemon pod
Feb 10 08:49:25.706: INFO: Number of nodes with available pods: 0
Feb 10 08:49:25.706: INFO: Node master1 is running more than one daemon pod
Feb 10 08:49:26.709: INFO: Number of nodes with available pods: 1
Feb 10 08:49:26.709: INFO: Node master1 is running more than one daemon pod
Feb 10 08:49:27.701: INFO: Number of nodes with available pods: 3
Feb 10 08:49:27.702: INFO: Number of running nodes: 3, number of available pods: 3
Feb 10 08:49:27.702: INFO: Update the DaemonSet to trigger a rollout
Feb 10 08:49:27.729: INFO: Updating DaemonSet daemon-set
Feb 10 08:49:44.788: INFO: Roll back the DaemonSet before rollout is complete
Feb 10 08:49:44.820: INFO: Updating DaemonSet daemon-set
Feb 10 08:49:44.820: INFO: Make sure DaemonSet rollback is complete
Feb 10 08:49:44.836: INFO: Wrong image for pod: daemon-set-kskwc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 10 08:49:44.836: INFO: Pod daemon-set-kskwc is not available
Feb 10 08:49:45.871: INFO: Wrong image for pod: daemon-set-kskwc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 10 08:49:45.871: INFO: Pod daemon-set-kskwc is not available
Feb 10 08:49:46.873: INFO: Pod daemon-set-xd7bs is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3670, will wait for the garbage collector to delete the pods
Feb 10 08:49:47.009: INFO: Deleting DaemonSet.extensions daemon-set took: 26.211957ms
Feb 10 08:49:47.510: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.706757ms
Feb 10 08:49:58.724: INFO: Number of nodes with available pods: 0
Feb 10 08:49:58.724: INFO: Number of running nodes: 0, number of available pods: 0
Feb 10 08:49:58.734: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3670/daemonsets","resourceVersion":"4648125"},"items":null}

Feb 10 08:49:58.744: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3670/pods","resourceVersion":"4648125"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:49:58.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3670" for this suite.

• [SLOW TEST:37.749 seconds]
[sig-apps] Daemon set [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":278,"completed":61,"skipped":926,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:49:58.832: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-3049
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:49:59.233: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Creating first CR 
Feb 10 08:50:04.915: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-10T08:50:04Z generation:1 name:name1 resourceVersion:4648185 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:8f221027-599e-4042-a29d-7e71e5a2fecf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 10 08:50:14.945: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-10T08:50:14Z generation:1 name:name2 resourceVersion:4648217 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5475f5f1-3d6f-46e4-8544-cb877bccd10c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 10 08:50:24.975: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-10T08:50:04Z generation:2 name:name1 resourceVersion:4648244 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:8f221027-599e-4042-a29d-7e71e5a2fecf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 10 08:50:35.005: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-10T08:50:14Z generation:2 name:name2 resourceVersion:4648271 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5475f5f1-3d6f-46e4-8544-cb877bccd10c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 10 08:50:45.037: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-10T08:50:04Z generation:2 name:name1 resourceVersion:4648294 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:8f221027-599e-4042-a29d-7e71e5a2fecf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 10 08:50:55.074: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-10T08:50:14Z generation:2 name:name2 resourceVersion:4648321 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:5475f5f1-3d6f-46e4-8544-cb877bccd10c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:51:05.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3049" for this suite.

• [SLOW TEST:66.835 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":278,"completed":62,"skipped":935,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:51:05.668: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Feb 10 08:51:06.116: INFO: Waiting up to 5m0s for pod "client-containers-d36fdebb-8675-4555-a77f-5b1b1159fa01" in namespace "containers-5362" to be "success or failure"
Feb 10 08:51:06.127: INFO: Pod "client-containers-d36fdebb-8675-4555-a77f-5b1b1159fa01": Phase="Pending", Reason="", readiness=false. Elapsed: 10.329306ms
Feb 10 08:51:08.144: INFO: Pod "client-containers-d36fdebb-8675-4555-a77f-5b1b1159fa01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028108417s
Feb 10 08:51:10.158: INFO: Pod "client-containers-d36fdebb-8675-4555-a77f-5b1b1159fa01": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041618783s
Feb 10 08:51:12.171: INFO: Pod "client-containers-d36fdebb-8675-4555-a77f-5b1b1159fa01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055018706s
STEP: Saw pod success
Feb 10 08:51:12.171: INFO: Pod "client-containers-d36fdebb-8675-4555-a77f-5b1b1159fa01" satisfied condition "success or failure"
Feb 10 08:51:12.185: INFO: Trying to get logs from node master3 pod client-containers-d36fdebb-8675-4555-a77f-5b1b1159fa01 container test-container: <nil>
STEP: delete the pod
Feb 10 08:51:12.619: INFO: Waiting for pod client-containers-d36fdebb-8675-4555-a77f-5b1b1159fa01 to disappear
Feb 10 08:51:12.632: INFO: Pod client-containers-d36fdebb-8675-4555-a77f-5b1b1159fa01 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:51:12.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5362" for this suite.

• [SLOW TEST:7.008 seconds]
[k8s.io] Docker Containers
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":278,"completed":63,"skipped":952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:51:12.681: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-276
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-89bd043a-49d7-48a1-9246-ad1c04e209c7
STEP: Creating a pod to test consume configMaps
Feb 10 08:51:13.137: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4bcd1b35-5c20-40ad-813d-01dbfc806064" in namespace "projected-276" to be "success or failure"
Feb 10 08:51:13.149: INFO: Pod "pod-projected-configmaps-4bcd1b35-5c20-40ad-813d-01dbfc806064": Phase="Pending", Reason="", readiness=false. Elapsed: 11.454267ms
Feb 10 08:51:15.168: INFO: Pod "pod-projected-configmaps-4bcd1b35-5c20-40ad-813d-01dbfc806064": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031085809s
Feb 10 08:51:17.184: INFO: Pod "pod-projected-configmaps-4bcd1b35-5c20-40ad-813d-01dbfc806064": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046319347s
Feb 10 08:51:19.198: INFO: Pod "pod-projected-configmaps-4bcd1b35-5c20-40ad-813d-01dbfc806064": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060942322s
STEP: Saw pod success
Feb 10 08:51:19.199: INFO: Pod "pod-projected-configmaps-4bcd1b35-5c20-40ad-813d-01dbfc806064" satisfied condition "success or failure"
Feb 10 08:51:19.209: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-4bcd1b35-5c20-40ad-813d-01dbfc806064 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 08:51:19.287: INFO: Waiting for pod pod-projected-configmaps-4bcd1b35-5c20-40ad-813d-01dbfc806064 to disappear
Feb 10 08:51:19.298: INFO: Pod pod-projected-configmaps-4bcd1b35-5c20-40ad-813d-01dbfc806064 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:51:19.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-276" for this suite.

• [SLOW TEST:6.658 seconds]
[sig-storage] Projected configMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":278,"completed":64,"skipped":1021,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:51:19.342: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Feb 10 08:51:25.879: INFO: Pod pod-hostip-aba52d73-99c2-4fb0-8404-fd86d010ad90 has hostIP: 172.16.3.137
[AfterEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:51:25.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8517" for this suite.

• [SLOW TEST:6.573 seconds]
[k8s.io] Pods
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should get a host IP [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":278,"completed":65,"skipped":1024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:51:25.921: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-a3174c08-436b-4d46-b966-608581980d4f
STEP: Creating a pod to test consume secrets
Feb 10 08:51:26.375: INFO: Waiting up to 5m0s for pod "pod-secrets-9b3c7747-6fd7-4f35-b554-dea8225bdc97" in namespace "secrets-4578" to be "success or failure"
Feb 10 08:51:26.390: INFO: Pod "pod-secrets-9b3c7747-6fd7-4f35-b554-dea8225bdc97": Phase="Pending", Reason="", readiness=false. Elapsed: 11.534147ms
Feb 10 08:51:28.406: INFO: Pod "pod-secrets-9b3c7747-6fd7-4f35-b554-dea8225bdc97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02777079s
Feb 10 08:51:30.420: INFO: Pod "pod-secrets-9b3c7747-6fd7-4f35-b554-dea8225bdc97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04235563s
Feb 10 08:51:32.434: INFO: Pod "pod-secrets-9b3c7747-6fd7-4f35-b554-dea8225bdc97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055936047s
STEP: Saw pod success
Feb 10 08:51:32.434: INFO: Pod "pod-secrets-9b3c7747-6fd7-4f35-b554-dea8225bdc97" satisfied condition "success or failure"
Feb 10 08:51:32.444: INFO: Trying to get logs from node master1 pod pod-secrets-9b3c7747-6fd7-4f35-b554-dea8225bdc97 container secret-volume-test: <nil>
STEP: delete the pod
Feb 10 08:51:32.853: INFO: Waiting for pod pod-secrets-9b3c7747-6fd7-4f35-b554-dea8225bdc97 to disappear
Feb 10 08:51:32.870: INFO: Pod pod-secrets-9b3c7747-6fd7-4f35-b554-dea8225bdc97 no longer exists
[AfterEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:51:32.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4578" for this suite.

• [SLOW TEST:6.983 seconds]
[sig-storage] Secrets
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":66,"skipped":1047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:51:32.909: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should support --unix-socket=/path  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Feb 10 08:51:33.335: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-754024871 proxy --unix-socket=/tmp/kubectl-proxy-unix784769170/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:51:33.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8693" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":278,"completed":67,"skipped":1075,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:51:33.696: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run job
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1768
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 10 08:51:34.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1112'
Feb 10 08:51:34.733: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 10 08:51:34.733: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1773
Feb 10 08:51:34.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete jobs e2e-test-httpd-job --namespace=kubectl-1112'
Feb 10 08:51:35.319: INFO: stderr: ""
Feb 10 08:51:35.319: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:51:35.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1112" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":278,"completed":68,"skipped":1096,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:51:35.371: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8514
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-nsdl
STEP: Creating a pod to test atomic-volume-subpath
Feb 10 08:51:35.844: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-nsdl" in namespace "subpath-8514" to be "success or failure"
Feb 10 08:51:35.861: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Pending", Reason="", readiness=false. Elapsed: 16.676909ms
Feb 10 08:51:37.875: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030355399s
Feb 10 08:51:39.889: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044401406s
Feb 10 08:51:41.901: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Running", Reason="", readiness=true. Elapsed: 6.05688051s
Feb 10 08:51:43.914: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Running", Reason="", readiness=true. Elapsed: 8.069534632s
Feb 10 08:51:45.931: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Running", Reason="", readiness=true. Elapsed: 10.086437113s
Feb 10 08:51:47.943: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Running", Reason="", readiness=true. Elapsed: 12.098644989s
Feb 10 08:51:49.958: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Running", Reason="", readiness=true. Elapsed: 14.113393984s
Feb 10 08:51:51.973: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Running", Reason="", readiness=true. Elapsed: 16.128753297s
Feb 10 08:51:53.986: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Running", Reason="", readiness=true. Elapsed: 18.141520085s
Feb 10 08:51:56.007: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Running", Reason="", readiness=true. Elapsed: 20.162518796s
Feb 10 08:51:58.021: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Running", Reason="", readiness=true. Elapsed: 22.17658072s
Feb 10 08:52:00.036: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Running", Reason="", readiness=true. Elapsed: 24.191441342s
Feb 10 08:52:02.057: INFO: Pod "pod-subpath-test-downwardapi-nsdl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.212203625s
STEP: Saw pod success
Feb 10 08:52:02.057: INFO: Pod "pod-subpath-test-downwardapi-nsdl" satisfied condition "success or failure"
Feb 10 08:52:02.070: INFO: Trying to get logs from node master3 pod pod-subpath-test-downwardapi-nsdl container test-container-subpath-downwardapi-nsdl: <nil>
STEP: delete the pod
Feb 10 08:52:02.152: INFO: Waiting for pod pod-subpath-test-downwardapi-nsdl to disappear
Feb 10 08:52:02.163: INFO: Pod pod-subpath-test-downwardapi-nsdl no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-nsdl
Feb 10 08:52:02.163: INFO: Deleting pod "pod-subpath-test-downwardapi-nsdl" in namespace "subpath-8514"
[AfterEach] [sig-storage] Subpath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:52:02.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8514" for this suite.

• [SLOW TEST:26.839 seconds]
[sig-storage] Subpath
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":278,"completed":69,"skipped":1096,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:52:02.216: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6095
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:52:02.664: INFO: (0) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 22.967313ms)
Feb 10 08:52:02.686: INFO: (1) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 21.061331ms)
Feb 10 08:52:02.714: INFO: (2) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 28.563456ms)
Feb 10 08:52:02.736: INFO: (3) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 21.390992ms)
Feb 10 08:52:02.754: INFO: (4) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.64995ms)
Feb 10 08:52:02.770: INFO: (5) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 16.055189ms)
Feb 10 08:52:02.788: INFO: (6) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.599269ms)
Feb 10 08:52:02.806: INFO: (7) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.36033ms)
Feb 10 08:52:02.823: INFO: (8) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.103089ms)
Feb 10 08:52:02.841: INFO: (9) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.828369ms)
Feb 10 08:52:02.859: INFO: (10) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.213549ms)
Feb 10 08:52:02.877: INFO: (11) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.42455ms)
Feb 10 08:52:02.901: INFO: (12) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 23.231193ms)
Feb 10 08:52:02.919: INFO: (13) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.77815ms)
Feb 10 08:52:02.937: INFO: (14) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.367149ms)
Feb 10 08:52:02.955: INFO: (15) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 18.010309ms)
Feb 10 08:52:02.973: INFO: (16) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.485129ms)
Feb 10 08:52:02.995: INFO: (17) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 21.077692ms)
Feb 10 08:52:03.013: INFO: (18) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.34993ms)
Feb 10 08:52:03.036: INFO: (19) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 22.999932ms)
[AfterEach] version v1
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:52:03.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6095" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":278,"completed":70,"skipped":1105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:52:03.082: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-560
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-8d453644-707d-4109-ac41-7aa19c687804
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:52:09.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-560" for this suite.

• [SLOW TEST:6.656 seconds]
[sig-storage] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":71,"skipped":1173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:52:09.747: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1198
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:52:10.240: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fac83dce-c424-44b8-94fc-332a871982d2", Controller:(*bool)(0xc003c3b83a), BlockOwnerDeletion:(*bool)(0xc003c3b83b)}}
Feb 10 08:52:10.267: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1560234e-e59f-4b08-9fa7-5f83cbbd562b", Controller:(*bool)(0xc003c3b9fa), BlockOwnerDeletion:(*bool)(0xc003c3b9fb)}}
Feb 10 08:52:10.292: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8ae14ea8-a101-4532-924f-e6f5b9aeda34", Controller:(*bool)(0xc000f899ca), BlockOwnerDeletion:(*bool)(0xc000f899cb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:52:15.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1198" for this suite.

• [SLOW TEST:5.631 seconds]
[sig-api-machinery] Garbage collector
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":278,"completed":72,"skipped":1216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:52:15.379: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1272
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 10 08:52:21.915: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1272 PodName:pod-sharedvolume-8b0c52ef-c9c7-4bca-9bd2-299e5fef496b ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:21.915: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:22.741: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:52:22.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1272" for this suite.

• [SLOW TEST:7.400 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  pod should support shared volumes between containers [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":278,"completed":73,"skipped":1258,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:52:22.782: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 10 08:52:37.305: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1287 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:37.305: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:38.190: INFO: Exec stderr: ""
Feb 10 08:52:38.190: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1287 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:38.191: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:38.962: INFO: Exec stderr: ""
Feb 10 08:52:38.962: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1287 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:38.962: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:39.782: INFO: Exec stderr: ""
Feb 10 08:52:39.782: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1287 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:40.577: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 10 08:52:40.577: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1287 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:40.577: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:41.435: INFO: Exec stderr: ""
Feb 10 08:52:41.435: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1287 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:41.436: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:42.200: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 10 08:52:42.200: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1287 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:42.200: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:43.009: INFO: Exec stderr: ""
Feb 10 08:52:43.010: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1287 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:43.010: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:43.824: INFO: Exec stderr: ""
Feb 10 08:52:43.824: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1287 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:43.824: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:44.620: INFO: Exec stderr: ""
Feb 10 08:52:44.621: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1287 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 08:52:44.621: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 08:52:45.405: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:52:45.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1287" for this suite.

• [SLOW TEST:22.687 seconds]
[k8s.io] KubeletManagedEtcHosts
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":74,"skipped":1270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:52:45.483: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7256
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0210 08:52:52.125568      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 10 08:52:52.130: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:52:52.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7256" for this suite.

• [SLOW TEST:6.702 seconds]
[sig-api-machinery] Garbage collector
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":278,"completed":75,"skipped":1295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:52:52.189: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 10 08:53:05.322: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 10 08:53:05.343: INFO: Pod pod-with-poststart-http-hook still exists
Feb 10 08:53:07.343: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 10 08:53:07.358: INFO: Pod pod-with-poststart-http-hook still exists
Feb 10 08:53:09.343: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 10 08:53:09.359: INFO: Pod pod-with-poststart-http-hook still exists
Feb 10 08:53:11.343: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 10 08:53:11.357: INFO: Pod pod-with-poststart-http-hook still exists
Feb 10 08:53:13.343: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 10 08:53:13.362: INFO: Pod pod-with-poststart-http-hook still exists
Feb 10 08:53:15.343: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 10 08:53:15.357: INFO: Pod pod-with-poststart-http-hook still exists
Feb 10 08:53:17.343: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 10 08:53:17.357: INFO: Pod pod-with-poststart-http-hook still exists
Feb 10 08:53:19.345: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 10 08:53:19.358: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:53:19.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6002" for this suite.

• [SLOW TEST:27.207 seconds]
[k8s.io] Container Lifecycle Hook
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":278,"completed":76,"skipped":1327,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:53:19.397: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-2a8d1c94-1514-4510-aca7-c410dc9359bb
STEP: Creating a pod to test consume secrets
Feb 10 08:53:19.917: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62565736-5550-43fd-a66e-a171a21832d1" in namespace "projected-7633" to be "success or failure"
Feb 10 08:53:19.928: INFO: Pod "pod-projected-secrets-62565736-5550-43fd-a66e-a171a21832d1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.938906ms
Feb 10 08:53:21.940: INFO: Pod "pod-projected-secrets-62565736-5550-43fd-a66e-a171a21832d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022702664s
Feb 10 08:53:23.959: INFO: Pod "pod-projected-secrets-62565736-5550-43fd-a66e-a171a21832d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040989902s
Feb 10 08:53:25.971: INFO: Pod "pod-projected-secrets-62565736-5550-43fd-a66e-a171a21832d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053532696s
STEP: Saw pod success
Feb 10 08:53:25.972: INFO: Pod "pod-projected-secrets-62565736-5550-43fd-a66e-a171a21832d1" satisfied condition "success or failure"
Feb 10 08:53:25.982: INFO: Trying to get logs from node master2 pod pod-projected-secrets-62565736-5550-43fd-a66e-a171a21832d1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 10 08:53:26.114: INFO: Waiting for pod pod-projected-secrets-62565736-5550-43fd-a66e-a171a21832d1 to disappear
Feb 10 08:53:26.126: INFO: Pod pod-projected-secrets-62565736-5550-43fd-a66e-a171a21832d1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:53:26.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7633" for this suite.

• [SLOW TEST:6.764 seconds]
[sig-storage] Projected secret
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":77,"skipped":1336,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:53:26.167: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should add annotations for pods in rc  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Feb 10 08:53:26.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-2803'
Feb 10 08:53:28.052: INFO: stderr: ""
Feb 10 08:53:28.053: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 10 08:53:29.068: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 08:53:29.068: INFO: Found 0 / 1
Feb 10 08:53:30.068: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 08:53:30.068: INFO: Found 0 / 1
Feb 10 08:53:31.072: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 08:53:31.072: INFO: Found 0 / 1
Feb 10 08:53:32.075: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 08:53:32.075: INFO: Found 0 / 1
Feb 10 08:53:33.067: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 08:53:33.067: INFO: Found 1 / 1
Feb 10 08:53:33.067: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 10 08:53:33.079: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 08:53:33.079: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 10 08:53:33.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 patch pod agnhost-master-xjgb6 --namespace=kubectl-2803 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 10 08:53:33.610: INFO: stderr: ""
Feb 10 08:53:33.611: INFO: stdout: "pod/agnhost-master-xjgb6 patched\n"
STEP: checking annotations
Feb 10 08:53:33.626: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 08:53:33.627: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:53:33.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2803" for this suite.

• [SLOW TEST:7.498 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1519
    should add annotations for pods in rc  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":278,"completed":78,"skipped":1344,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:53:33.669: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-p7ct
STEP: Creating a pod to test atomic-volume-subpath
Feb 10 08:53:34.137: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p7ct" in namespace "subpath-3303" to be "success or failure"
Feb 10 08:53:34.152: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Pending", Reason="", readiness=false. Elapsed: 15.342407ms
Feb 10 08:53:36.167: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030612529s
Feb 10 08:53:38.180: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043649128s
Feb 10 08:53:40.195: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Running", Reason="", readiness=true. Elapsed: 6.058408985s
Feb 10 08:53:42.208: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Running", Reason="", readiness=true. Elapsed: 8.071357659s
Feb 10 08:53:44.225: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Running", Reason="", readiness=true. Elapsed: 10.088474252s
Feb 10 08:53:46.239: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Running", Reason="", readiness=true. Elapsed: 12.102432601s
Feb 10 08:53:48.252: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Running", Reason="", readiness=true. Elapsed: 14.114821287s
Feb 10 08:53:50.265: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Running", Reason="", readiness=true. Elapsed: 16.128665772s
Feb 10 08:53:52.279: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Running", Reason="", readiness=true. Elapsed: 18.142316433s
Feb 10 08:53:54.295: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Running", Reason="", readiness=true. Elapsed: 20.158320614s
Feb 10 08:53:56.308: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Running", Reason="", readiness=true. Elapsed: 22.171725211s
Feb 10 08:53:58.321: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Running", Reason="", readiness=true. Elapsed: 24.184595925s
Feb 10 08:54:00.335: INFO: Pod "pod-subpath-test-configmap-p7ct": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.197787197s
STEP: Saw pod success
Feb 10 08:54:00.335: INFO: Pod "pod-subpath-test-configmap-p7ct" satisfied condition "success or failure"
Feb 10 08:54:00.344: INFO: Trying to get logs from node master3 pod pod-subpath-test-configmap-p7ct container test-container-subpath-configmap-p7ct: <nil>
STEP: delete the pod
Feb 10 08:54:00.739: INFO: Waiting for pod pod-subpath-test-configmap-p7ct to disappear
Feb 10 08:54:00.759: INFO: Pod pod-subpath-test-configmap-p7ct no longer exists
STEP: Deleting pod pod-subpath-test-configmap-p7ct
Feb 10 08:54:00.759: INFO: Deleting pod "pod-subpath-test-configmap-p7ct" in namespace "subpath-3303"
[AfterEach] [sig-storage] Subpath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:54:00.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3303" for this suite.

• [SLOW TEST:27.135 seconds]
[sig-storage] Subpath
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":278,"completed":79,"skipped":1363,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:54:00.805: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4221
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4221
[It] Should recreate evicted statefulset [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4221
STEP: Creating statefulset with conflicting port in namespace statefulset-4221
STEP: Waiting until pod test-pod will start running in namespace statefulset-4221
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4221
Feb 10 08:54:07.330: INFO: Observed stateful pod in namespace: statefulset-4221, name: ss-0, uid: f767f91a-8168-4799-a283-da0c9347c8de, status phase: Pending. Waiting for statefulset controller to delete.
Feb 10 08:54:07.907: INFO: Observed stateful pod in namespace: statefulset-4221, name: ss-0, uid: f767f91a-8168-4799-a283-da0c9347c8de, status phase: Failed. Waiting for statefulset controller to delete.
Feb 10 08:54:07.933: INFO: Observed stateful pod in namespace: statefulset-4221, name: ss-0, uid: f767f91a-8168-4799-a283-da0c9347c8de, status phase: Failed. Waiting for statefulset controller to delete.
Feb 10 08:54:07.947: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4221
STEP: Removing pod with conflicting port in namespace statefulset-4221
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4221 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 10 08:54:14.058: INFO: Deleting all statefulset in ns statefulset-4221
Feb 10 08:54:14.068: INFO: Scaling statefulset ss to 0
Feb 10 08:54:34.122: INFO: Waiting for statefulset status.replicas updated to 0
Feb 10 08:54:34.138: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:54:34.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4221" for this suite.

• [SLOW TEST:33.418 seconds]
[sig-apps] StatefulSet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":278,"completed":80,"skipped":1365,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:54:34.230: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 10 08:54:41.300: INFO: Successfully updated pod "annotationupdatedd9855fc-b0b6-4a35-8916-f3ac8bc432da"
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:54:43.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4165" for this suite.

• [SLOW TEST:9.181 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":278,"completed":81,"skipped":1382,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:54:43.415: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-2774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-2774
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2774
STEP: Deleting pre-stop pod
Feb 10 08:55:00.998: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:55:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2774" for this suite.

• [SLOW TEST:17.651 seconds]
[k8s.io] [sig-node] PreStop
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":278,"completed":82,"skipped":1386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:55:01.067: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-677
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should do a rolling update of a replication controller  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Feb 10 08:55:01.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-677'
Feb 10 08:55:05.910: INFO: stderr: ""
Feb 10 08:55:05.910: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 10 08:55:05.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-677'
Feb 10 08:55:06.477: INFO: stderr: ""
Feb 10 08:55:06.477: INFO: stdout: "update-demo-nautilus-s995z update-demo-nautilus-sr9h2 "
Feb 10 08:55:06.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-s995z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-677'
Feb 10 08:55:07.098: INFO: stderr: ""
Feb 10 08:55:07.099: INFO: stdout: ""
Feb 10 08:55:07.099: INFO: update-demo-nautilus-s995z is created but not running
Feb 10 08:55:12.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-677'
Feb 10 08:55:12.630: INFO: stderr: ""
Feb 10 08:55:12.630: INFO: stdout: "update-demo-nautilus-s995z update-demo-nautilus-sr9h2 "
Feb 10 08:55:12.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-s995z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-677'
Feb 10 08:55:13.142: INFO: stderr: ""
Feb 10 08:55:13.142: INFO: stdout: "true"
Feb 10 08:55:13.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-s995z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-677'
Feb 10 08:55:13.679: INFO: stderr: ""
Feb 10 08:55:13.679: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 10 08:55:13.679: INFO: validating pod update-demo-nautilus-s995z
Feb 10 08:55:13.704: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 10 08:55:13.705: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 10 08:55:13.705: INFO: update-demo-nautilus-s995z is verified up and running
Feb 10 08:55:13.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-sr9h2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-677'
Feb 10 08:55:14.247: INFO: stderr: ""
Feb 10 08:55:14.247: INFO: stdout: "true"
Feb 10 08:55:14.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-sr9h2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-677'
Feb 10 08:55:14.764: INFO: stderr: ""
Feb 10 08:55:14.764: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 10 08:55:14.764: INFO: validating pod update-demo-nautilus-sr9h2
Feb 10 08:55:14.789: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 10 08:55:14.790: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 10 08:55:14.790: INFO: update-demo-nautilus-sr9h2 is verified up and running
STEP: rolling-update to new replication controller
Feb 10 08:55:14.802: INFO: scanned /root for discovery docs: <nil>
Feb 10 08:55:14.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-677'
Feb 10 08:55:42.024: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 10 08:55:42.024: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 10 08:55:42.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-677'
Feb 10 08:55:42.547: INFO: stderr: ""
Feb 10 08:55:42.547: INFO: stdout: "update-demo-kitten-dkk9n update-demo-kitten-xpthj "
Feb 10 08:55:42.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-kitten-dkk9n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-677'
Feb 10 08:55:43.120: INFO: stderr: ""
Feb 10 08:55:43.120: INFO: stdout: "true"
Feb 10 08:55:43.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-kitten-dkk9n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-677'
Feb 10 08:55:43.667: INFO: stderr: ""
Feb 10 08:55:43.667: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 10 08:55:43.668: INFO: validating pod update-demo-kitten-dkk9n
Feb 10 08:55:43.689: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 10 08:55:43.690: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 10 08:55:43.690: INFO: update-demo-kitten-dkk9n is verified up and running
Feb 10 08:55:43.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-kitten-xpthj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-677'
Feb 10 08:55:44.171: INFO: stderr: ""
Feb 10 08:55:44.172: INFO: stdout: "true"
Feb 10 08:55:44.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-kitten-xpthj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-677'
Feb 10 08:55:44.680: INFO: stderr: ""
Feb 10 08:55:44.680: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 10 08:55:44.680: INFO: validating pod update-demo-kitten-xpthj
Feb 10 08:55:44.700: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 10 08:55:44.701: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 10 08:55:44.701: INFO: update-demo-kitten-xpthj is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:55:44.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-677" for this suite.

• [SLOW TEST:43.672 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should do a rolling update of a replication controller  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":278,"completed":83,"skipped":1416,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:55:44.741: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5458
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5458.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5458.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5458.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5458.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5458.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5458.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 10 08:55:53.298: INFO: DNS probes using dns-5458/dns-test-fbc51d2f-ccf2-4340-a6ae-33e7f74c72e5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:55:53.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5458" for this suite.

• [SLOW TEST:8.655 seconds]
[sig-network] DNS
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":278,"completed":84,"skipped":1446,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:55:53.398: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6078
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6078 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6078;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6078 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6078;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6078.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6078.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6078.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6078.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6078.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6078.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6078.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6078.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6078.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6078.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6078.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6078.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6078.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 43.224.155.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.155.224.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.224.155.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.155.224.43_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6078 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6078;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6078 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6078;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6078.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6078.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6078.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6078.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6078.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6078.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6078.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6078.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6078.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6078.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6078.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6078.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6078.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 43.224.155.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.155.224.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.224.155.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.155.224.43_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 10 08:56:01.968: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:01.980: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:01.992: INFO: Unable to read wheezy_udp@dns-test-service.dns-6078 from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.003: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6078 from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.014: INFO: Unable to read wheezy_udp@dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.029: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.045: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.057: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.149: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.160: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.172: INFO: Unable to read jessie_udp@dns-test-service.dns-6078 from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.185: INFO: Unable to read jessie_tcp@dns-test-service.dns-6078 from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.195: INFO: Unable to read jessie_udp@dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.208: INFO: Unable to read jessie_tcp@dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.222: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.234: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:02.312: INFO: Lookups using dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6078 wheezy_tcp@dns-test-service.dns-6078 wheezy_udp@dns-test-service.dns-6078.svc wheezy_tcp@dns-test-service.dns-6078.svc wheezy_udp@_http._tcp.dns-test-service.dns-6078.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6078.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6078 jessie_tcp@dns-test-service.dns-6078 jessie_udp@dns-test-service.dns-6078.svc jessie_tcp@dns-test-service.dns-6078.svc jessie_udp@_http._tcp.dns-test-service.dns-6078.svc jessie_tcp@_http._tcp.dns-test-service.dns-6078.svc]

Feb 10 08:56:07.330: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:07.344: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:07.357: INFO: Unable to read wheezy_udp@dns-test-service.dns-6078 from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:07.368: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6078 from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:07.380: INFO: Unable to read wheezy_udp@dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:07.393: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:07.405: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:07.418: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6078.svc from pod dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e: the server could not find the requested resource (get pods dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e)
Feb 10 08:56:07.677: INFO: Lookups using dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6078 wheezy_tcp@dns-test-service.dns-6078 wheezy_udp@dns-test-service.dns-6078.svc wheezy_tcp@dns-test-service.dns-6078.svc wheezy_udp@_http._tcp.dns-test-service.dns-6078.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6078.svc]

Feb 10 08:56:12.774: INFO: DNS probes using dns-6078/dns-test-cd7ce042-588a-4b15-a7ed-1a5e55fb0a8e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:56:13.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6078" for this suite.

• [SLOW TEST:19.731 seconds]
[sig-network] DNS
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":278,"completed":85,"skipped":1469,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:56:13.131: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4084
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-4e40650d-4f0d-40f7-ae5b-5a0e1b191131
STEP: Creating a pod to test consume secrets
Feb 10 08:56:13.645: INFO: Waiting up to 5m0s for pod "pod-secrets-d6a44eb6-1bd1-4744-9063-daad7bb7d588" in namespace "secrets-4084" to be "success or failure"
Feb 10 08:56:13.662: INFO: Pod "pod-secrets-d6a44eb6-1bd1-4744-9063-daad7bb7d588": Phase="Pending", Reason="", readiness=false. Elapsed: 16.33183ms
Feb 10 08:56:15.675: INFO: Pod "pod-secrets-d6a44eb6-1bd1-4744-9063-daad7bb7d588": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02948646s
Feb 10 08:56:17.697: INFO: Pod "pod-secrets-d6a44eb6-1bd1-4744-9063-daad7bb7d588": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050807479s
Feb 10 08:56:19.710: INFO: Pod "pod-secrets-d6a44eb6-1bd1-4744-9063-daad7bb7d588": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064011539s
STEP: Saw pod success
Feb 10 08:56:19.710: INFO: Pod "pod-secrets-d6a44eb6-1bd1-4744-9063-daad7bb7d588" satisfied condition "success or failure"
Feb 10 08:56:19.723: INFO: Trying to get logs from node master3 pod pod-secrets-d6a44eb6-1bd1-4744-9063-daad7bb7d588 container secret-volume-test: <nil>
STEP: delete the pod
Feb 10 08:56:20.107: INFO: Waiting for pod pod-secrets-d6a44eb6-1bd1-4744-9063-daad7bb7d588 to disappear
Feb 10 08:56:20.117: INFO: Pod pod-secrets-d6a44eb6-1bd1-4744-9063-daad7bb7d588 no longer exists
[AfterEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:56:20.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4084" for this suite.

• [SLOW TEST:7.025 seconds]
[sig-storage] Secrets
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":86,"skipped":1479,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:56:20.161: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 08:56:45.029: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 10 08:56:47.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 08:56:49.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 08:56:51.090: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716921805, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 08:56:54.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:56:54.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9474" for this suite.
STEP: Destroying namespace "webhook-9474-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:34.582 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":278,"completed":87,"skipped":1492,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:56:54.750: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 08:56:55.574: INFO: (0) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 349.606529ms)
Feb 10 08:56:55.606: INFO: (1) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 31.761614ms)
Feb 10 08:56:55.624: INFO: (2) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.859193ms)
Feb 10 08:56:55.643: INFO: (3) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.573772ms)
Feb 10 08:56:55.661: INFO: (4) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.802873ms)
Feb 10 08:56:55.678: INFO: (5) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.232831ms)
Feb 10 08:56:55.696: INFO: (6) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.406771ms)
Feb 10 08:56:55.712: INFO: (7) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 15.854407ms)
Feb 10 08:56:55.729: INFO: (8) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 16.719929ms)
Feb 10 08:56:55.747: INFO: (9) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.300931ms)
Feb 10 08:56:55.764: INFO: (10) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.426591ms)
Feb 10 08:56:55.782: INFO: (11) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.02265ms)
Feb 10 08:56:55.800: INFO: (12) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.480451ms)
Feb 10 08:56:55.818: INFO: (13) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.430211ms)
Feb 10 08:56:55.841: INFO: (14) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 22.939707ms)
Feb 10 08:56:55.858: INFO: (15) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.169871ms)
Feb 10 08:56:55.876: INFO: (16) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.740832ms)
Feb 10 08:56:55.894: INFO: (17) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.381711ms)
Feb 10 08:56:55.912: INFO: (18) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 18.398454ms)
Feb 10 08:56:55.929: INFO: (19) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/"... (200; 17.03075ms)
[AfterEach] version v1
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:56:55.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6694" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":278,"completed":88,"skipped":1524,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:56:55.966: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6949
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-0c5df1ce-1183-4a61-b2fe-36b9bd6d4499
STEP: Creating configMap with name cm-test-opt-upd-d0f7370a-6f91-402b-80ba-2b905abdd1ed
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0c5df1ce-1183-4a61-b2fe-36b9bd6d4499
STEP: Updating configmap cm-test-opt-upd-d0f7370a-6f91-402b-80ba-2b905abdd1ed
STEP: Creating configMap with name cm-test-opt-create-902dd8ab-a374-445c-95e5-8ee2889e2690
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:57:06.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6949" for this suite.

• [SLOW TEST:10.911 seconds]
[sig-storage] Projected configMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":89,"skipped":1524,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:57:06.877: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-c955761d-893e-4693-a917-5a313a098d44
Feb 10 08:57:07.334: INFO: Pod name my-hostname-basic-c955761d-893e-4693-a917-5a313a098d44: Found 0 pods out of 1
Feb 10 08:57:12.348: INFO: Pod name my-hostname-basic-c955761d-893e-4693-a917-5a313a098d44: Found 1 pods out of 1
Feb 10 08:57:12.349: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c955761d-893e-4693-a917-5a313a098d44" are running
Feb 10 08:57:14.373: INFO: Pod "my-hostname-basic-c955761d-893e-4693-a917-5a313a098d44-mczvm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-10 08:57:07 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-10 08:57:07 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-c955761d-893e-4693-a917-5a313a098d44]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-10 08:57:07 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-c955761d-893e-4693-a917-5a313a098d44]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-10 08:57:07 +0000 UTC Reason: Message:}])
Feb 10 08:57:14.373: INFO: Trying to dial the pod
Feb 10 08:57:19.416: INFO: Controller my-hostname-basic-c955761d-893e-4693-a917-5a313a098d44: Got expected result from replica 1 [my-hostname-basic-c955761d-893e-4693-a917-5a313a098d44-mczvm]: "my-hostname-basic-c955761d-893e-4693-a917-5a313a098d44-mczvm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:57:19.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2964" for this suite.

• [SLOW TEST:12.575 seconds]
[sig-apps] ReplicationController
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":278,"completed":90,"skipped":1533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:57:19.459: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2449
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-330
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4782
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:57:55.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2449" for this suite.
STEP: Destroying namespace "nsdeletetest-330" for this suite.
Feb 10 08:57:55.835: INFO: Namespace nsdeletetest-330 was already deleted
STEP: Destroying namespace "nsdeletetest-4782" for this suite.

• [SLOW TEST:36.396 seconds]
[sig-api-machinery] Namespaces [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":278,"completed":91,"skipped":1559,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:57:55.855: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 10 08:57:57.126: INFO: Pod name wrapped-volume-race-496461fe-6927-4e6f-9daf-82d97493c3cd: Found 0 pods out of 5
Feb 10 08:58:02.155: INFO: Pod name wrapped-volume-race-496461fe-6927-4e6f-9daf-82d97493c3cd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-496461fe-6927-4e6f-9daf-82d97493c3cd in namespace emptydir-wrapper-9472, will wait for the garbage collector to delete the pods
Feb 10 08:58:18.372: INFO: Deleting ReplicationController wrapped-volume-race-496461fe-6927-4e6f-9daf-82d97493c3cd took: 35.275114ms
Feb 10 08:58:18.872: INFO: Terminating ReplicationController wrapped-volume-race-496461fe-6927-4e6f-9daf-82d97493c3cd pods took: 500.650669ms
STEP: Creating RC which spawns configmap-volume pods
Feb 10 08:58:28.444: INFO: Pod name wrapped-volume-race-b6a12185-e210-4691-a280-293df4469cef: Found 0 pods out of 5
Feb 10 08:58:33.474: INFO: Pod name wrapped-volume-race-b6a12185-e210-4691-a280-293df4469cef: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b6a12185-e210-4691-a280-293df4469cef in namespace emptydir-wrapper-9472, will wait for the garbage collector to delete the pods
Feb 10 08:58:49.699: INFO: Deleting ReplicationController wrapped-volume-race-b6a12185-e210-4691-a280-293df4469cef took: 37.614556ms
Feb 10 08:58:50.202: INFO: Terminating ReplicationController wrapped-volume-race-b6a12185-e210-4691-a280-293df4469cef pods took: 502.171882ms
STEP: Creating RC which spawns configmap-volume pods
Feb 10 08:59:08.780: INFO: Pod name wrapped-volume-race-5664d52f-520d-485e-84c6-6e6364489946: Found 0 pods out of 5
Feb 10 08:59:13.808: INFO: Pod name wrapped-volume-race-5664d52f-520d-485e-84c6-6e6364489946: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5664d52f-520d-485e-84c6-6e6364489946 in namespace emptydir-wrapper-9472, will wait for the garbage collector to delete the pods
Feb 10 08:59:31.993: INFO: Deleting ReplicationController wrapped-volume-race-5664d52f-520d-485e-84c6-6e6364489946 took: 29.859012ms
Feb 10 08:59:32.494: INFO: Terminating ReplicationController wrapped-volume-race-5664d52f-520d-485e-84c6-6e6364489946 pods took: 500.758671ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 08:59:50.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9472" for this suite.

• [SLOW TEST:114.650 seconds]
[sig-storage] EmptyDir wrapper volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":278,"completed":92,"skipped":1562,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 08:59:50.508: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1184
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-bea14d01-dcbf-4671-a98b-e229e4c08338
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-bea14d01-dcbf-4671-a98b-e229e4c08338
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:01:10.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1184" for this suite.

• [SLOW TEST:80.470 seconds]
[sig-storage] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":93,"skipped":1563,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:01:10.978: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-199
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Feb 10 09:01:11.385: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:02:26.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-199" for this suite.

• [SLOW TEST:75.152 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":278,"completed":94,"skipped":1581,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:02:26.131: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6708
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 10 09:02:26.586: INFO: Waiting up to 5m0s for pod "pod-7b0c6984-8f7f-4110-a486-ae61ebf9124d" in namespace "emptydir-6708" to be "success or failure"
Feb 10 09:02:26.597: INFO: Pod "pod-7b0c6984-8f7f-4110-a486-ae61ebf9124d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.311439ms
Feb 10 09:02:28.618: INFO: Pod "pod-7b0c6984-8f7f-4110-a486-ae61ebf9124d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030840806s
Feb 10 09:02:30.629: INFO: Pod "pod-7b0c6984-8f7f-4110-a486-ae61ebf9124d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042381346s
Feb 10 09:02:32.643: INFO: Pod "pod-7b0c6984-8f7f-4110-a486-ae61ebf9124d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0565969s
STEP: Saw pod success
Feb 10 09:02:32.643: INFO: Pod "pod-7b0c6984-8f7f-4110-a486-ae61ebf9124d" satisfied condition "success or failure"
Feb 10 09:02:32.654: INFO: Trying to get logs from node master3 pod pod-7b0c6984-8f7f-4110-a486-ae61ebf9124d container test-container: <nil>
STEP: delete the pod
Feb 10 09:02:32.727: INFO: Waiting for pod pod-7b0c6984-8f7f-4110-a486-ae61ebf9124d to disappear
Feb 10 09:02:32.741: INFO: Pod pod-7b0c6984-8f7f-4110-a486-ae61ebf9124d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:02:32.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6708" for this suite.

• [SLOW TEST:6.645 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":95,"skipped":1583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:02:32.778: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should scale a replication controller  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Feb 10 09:02:33.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-3670'
Feb 10 09:02:34.542: INFO: stderr: ""
Feb 10 09:02:34.542: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 10 09:02:34.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3670'
Feb 10 09:02:35.145: INFO: stderr: ""
Feb 10 09:02:35.145: INFO: stdout: "update-demo-nautilus-fxj6q update-demo-nautilus-w94c2 "
Feb 10 09:02:35.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-fxj6q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:02:35.721: INFO: stderr: ""
Feb 10 09:02:35.722: INFO: stdout: ""
Feb 10 09:02:35.722: INFO: update-demo-nautilus-fxj6q is created but not running
Feb 10 09:02:40.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3670'
Feb 10 09:02:41.229: INFO: stderr: ""
Feb 10 09:02:41.229: INFO: stdout: "update-demo-nautilus-fxj6q update-demo-nautilus-w94c2 "
Feb 10 09:02:41.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-fxj6q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:02:41.761: INFO: stderr: ""
Feb 10 09:02:41.762: INFO: stdout: "true"
Feb 10 09:02:41.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-fxj6q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:02:42.234: INFO: stderr: ""
Feb 10 09:02:42.235: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 10 09:02:42.235: INFO: validating pod update-demo-nautilus-fxj6q
Feb 10 09:02:42.258: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 10 09:02:42.259: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 10 09:02:42.259: INFO: update-demo-nautilus-fxj6q is verified up and running
Feb 10 09:02:42.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-w94c2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:02:42.758: INFO: stderr: ""
Feb 10 09:02:42.759: INFO: stdout: "true"
Feb 10 09:02:42.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-w94c2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:02:43.267: INFO: stderr: ""
Feb 10 09:02:43.267: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 10 09:02:43.267: INFO: validating pod update-demo-nautilus-w94c2
Feb 10 09:02:43.289: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 10 09:02:43.290: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 10 09:02:43.290: INFO: update-demo-nautilus-w94c2 is verified up and running
STEP: scaling down the replication controller
Feb 10 09:02:43.297: INFO: scanned /root for discovery docs: <nil>
Feb 10 09:02:43.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3670'
Feb 10 09:02:44.942: INFO: stderr: ""
Feb 10 09:02:44.942: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 10 09:02:44.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3670'
Feb 10 09:02:45.474: INFO: stderr: ""
Feb 10 09:02:45.474: INFO: stdout: "update-demo-nautilus-fxj6q update-demo-nautilus-w94c2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 10 09:02:50.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3670'
Feb 10 09:02:50.997: INFO: stderr: ""
Feb 10 09:02:50.997: INFO: stdout: "update-demo-nautilus-fxj6q update-demo-nautilus-w94c2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 10 09:02:55.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3670'
Feb 10 09:02:56.519: INFO: stderr: ""
Feb 10 09:02:56.519: INFO: stdout: "update-demo-nautilus-fxj6q "
Feb 10 09:02:56.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-fxj6q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:02:57.060: INFO: stderr: ""
Feb 10 09:02:57.060: INFO: stdout: "true"
Feb 10 09:02:57.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-fxj6q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:02:57.568: INFO: stderr: ""
Feb 10 09:02:57.569: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 10 09:02:57.569: INFO: validating pod update-demo-nautilus-fxj6q
Feb 10 09:02:57.585: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 10 09:02:57.585: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 10 09:02:57.585: INFO: update-demo-nautilus-fxj6q is verified up and running
STEP: scaling up the replication controller
Feb 10 09:02:57.595: INFO: scanned /root for discovery docs: <nil>
Feb 10 09:02:57.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3670'
Feb 10 09:02:59.170: INFO: stderr: ""
Feb 10 09:02:59.170: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 10 09:02:59.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3670'
Feb 10 09:02:59.672: INFO: stderr: ""
Feb 10 09:02:59.672: INFO: stdout: "update-demo-nautilus-fxj6q update-demo-nautilus-rztng "
Feb 10 09:02:59.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-fxj6q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:03:00.165: INFO: stderr: ""
Feb 10 09:03:00.166: INFO: stdout: "true"
Feb 10 09:03:00.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-fxj6q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:03:00.680: INFO: stderr: ""
Feb 10 09:03:00.681: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 10 09:03:00.681: INFO: validating pod update-demo-nautilus-fxj6q
Feb 10 09:03:00.696: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 10 09:03:00.697: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 10 09:03:00.697: INFO: update-demo-nautilus-fxj6q is verified up and running
Feb 10 09:03:00.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-rztng -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:03:01.207: INFO: stderr: ""
Feb 10 09:03:01.208: INFO: stdout: ""
Feb 10 09:03:01.208: INFO: update-demo-nautilus-rztng is created but not running
Feb 10 09:03:06.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3670'
Feb 10 09:03:06.711: INFO: stderr: ""
Feb 10 09:03:06.712: INFO: stdout: "update-demo-nautilus-fxj6q update-demo-nautilus-rztng "
Feb 10 09:03:06.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-fxj6q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:03:07.205: INFO: stderr: ""
Feb 10 09:03:07.206: INFO: stdout: "true"
Feb 10 09:03:07.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-fxj6q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:03:07.741: INFO: stderr: ""
Feb 10 09:03:07.741: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 10 09:03:07.742: INFO: validating pod update-demo-nautilus-fxj6q
Feb 10 09:03:07.758: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 10 09:03:07.758: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 10 09:03:07.758: INFO: update-demo-nautilus-fxj6q is verified up and running
Feb 10 09:03:07.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-rztng -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:03:08.233: INFO: stderr: ""
Feb 10 09:03:08.234: INFO: stdout: "true"
Feb 10 09:03:08.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-rztng -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Feb 10 09:03:08.720: INFO: stderr: ""
Feb 10 09:03:08.720: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 10 09:03:08.721: INFO: validating pod update-demo-nautilus-rztng
Feb 10 09:03:08.738: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 10 09:03:08.738: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 10 09:03:08.739: INFO: update-demo-nautilus-rztng is verified up and running
STEP: using delete to clean up resources
Feb 10 09:03:08.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete --grace-period=0 --force -f - --namespace=kubectl-3670'
Feb 10 09:03:09.281: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 10 09:03:09.282: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 10 09:03:09.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3670'
Feb 10 09:03:09.826: INFO: stderr: "No resources found in kubectl-3670 namespace.\n"
Feb 10 09:03:09.826: INFO: stdout: ""
Feb 10 09:03:09.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -l name=update-demo --namespace=kubectl-3670 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 10 09:03:10.356: INFO: stderr: ""
Feb 10 09:03:10.357: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:03:10.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3670" for this suite.

• [SLOW TEST:37.622 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should scale a replication controller  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":278,"completed":96,"skipped":1611,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:03:10.402: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-76
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 10 09:03:15.983: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:03:16.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-76" for this suite.

• [SLOW TEST:5.738 seconds]
[k8s.io] Container Runtime
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":97,"skipped":1618,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:03:16.141: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-9371
STEP: creating replication controller nodeport-test in namespace services-9371
I0210 09:03:16.751315      23 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-9371, replica count: 2
I0210 09:03:19.803442      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 09:03:22.803796      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 10 09:03:22.803: INFO: Creating new exec pod
Feb 10 09:03:29.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-9371 execpodkxj97 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb 10 09:03:31.307: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/*] succeeded!\n"
Feb 10 09:03:31.307: INFO: stdout: ""
Feb 10 09:03:31.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-9371 execpodkxj97 -- /bin/sh -x -c nc -zv -t -w 2 10.155.43.234 80'
Feb 10 09:03:32.658: INFO: stderr: "+ nc -zv -t -w 2 10.155.43.234 80\nConnection to 10.155.43.234 80 port [tcp/*] succeeded!\n"
Feb 10 09:03:32.658: INFO: stdout: ""
Feb 10 09:03:32.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-9371 execpodkxj97 -- /bin/sh -x -c nc -zv -t -w 2 172.16.3.136 31032'
Feb 10 09:03:33.962: INFO: stderr: "+ nc -zv -t -w 2 172.16.3.136 31032\nConnection to 172.16.3.136 31032 port [tcp/*] succeeded!\n"
Feb 10 09:03:33.963: INFO: stdout: ""
Feb 10 09:03:33.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-9371 execpodkxj97 -- /bin/sh -x -c nc -zv -t -w 2 172.16.3.135 31032'
Feb 10 09:03:35.280: INFO: stderr: "+ nc -zv -t -w 2 172.16.3.135 31032\nConnection to 172.16.3.135 31032 port [tcp/*] succeeded!\n"
Feb 10 09:03:35.280: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:03:35.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9371" for this suite.
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:19.182 seconds]
[sig-network] Services
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":278,"completed":98,"skipped":1623,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:03:35.327: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 10 09:03:35.797: INFO: Waiting up to 5m0s for pod "downward-api-3655ce47-324e-48b8-a5de-9a6fd8350c2b" in namespace "downward-api-9108" to be "success or failure"
Feb 10 09:03:35.811: INFO: Pod "downward-api-3655ce47-324e-48b8-a5de-9a6fd8350c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.096242ms
Feb 10 09:03:37.827: INFO: Pod "downward-api-3655ce47-324e-48b8-a5de-9a6fd8350c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028835688s
Feb 10 09:03:39.848: INFO: Pod "downward-api-3655ce47-324e-48b8-a5de-9a6fd8350c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050085253s
Feb 10 09:03:41.869: INFO: Pod "downward-api-3655ce47-324e-48b8-a5de-9a6fd8350c2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071247029s
STEP: Saw pod success
Feb 10 09:03:41.869: INFO: Pod "downward-api-3655ce47-324e-48b8-a5de-9a6fd8350c2b" satisfied condition "success or failure"
Feb 10 09:03:41.885: INFO: Trying to get logs from node master3 pod downward-api-3655ce47-324e-48b8-a5de-9a6fd8350c2b container dapi-container: <nil>
STEP: delete the pod
Feb 10 09:03:42.017: INFO: Waiting for pod downward-api-3655ce47-324e-48b8-a5de-9a6fd8350c2b to disappear
Feb 10 09:03:42.027: INFO: Pod downward-api-3655ce47-324e-48b8-a5de-9a6fd8350c2b no longer exists
[AfterEach] [sig-node] Downward API
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:03:42.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9108" for this suite.

• [SLOW TEST:6.741 seconds]
[sig-node] Downward API
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":278,"completed":99,"skipped":1631,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:03:42.068: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9105
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:03:42.512: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 10 09:04:00.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9105 create -f -'
Feb 10 09:04:04.146: INFO: stderr: ""
Feb 10 09:04:04.147: INFO: stdout: "e2e-test-crd-publish-openapi-3067-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 10 09:04:04.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9105 delete e2e-test-crd-publish-openapi-3067-crds test-cr'
Feb 10 09:04:04.716: INFO: stderr: ""
Feb 10 09:04:04.717: INFO: stdout: "e2e-test-crd-publish-openapi-3067-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 10 09:04:04.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9105 apply -f -'
Feb 10 09:04:05.697: INFO: stderr: ""
Feb 10 09:04:05.698: INFO: stdout: "e2e-test-crd-publish-openapi-3067-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 10 09:04:05.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9105 delete e2e-test-crd-publish-openapi-3067-crds test-cr'
Feb 10 09:04:06.211: INFO: stderr: ""
Feb 10 09:04:06.211: INFO: stdout: "e2e-test-crd-publish-openapi-3067-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 10 09:04:06.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 explain e2e-test-crd-publish-openapi-3067-crds'
Feb 10 09:04:07.546: INFO: stderr: ""
Feb 10 09:04:07.547: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3067-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:04:20.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9105" for this suite.

• [SLOW TEST:38.165 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":278,"completed":100,"skipped":1631,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:04:20.236: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:04:20.691: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 10 09:04:25.708: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 10 09:04:25.708: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 10 09:04:27.719: INFO: Creating deployment "test-rollover-deployment"
Feb 10 09:04:27.751: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 10 09:04:29.773: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 10 09:04:29.793: INFO: Ensure that both replica sets have 1 created replica
Feb 10 09:04:29.809: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 10 09:04:29.840: INFO: Updating deployment test-rollover-deployment
Feb 10 09:04:29.841: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 10 09:04:31.863: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 10 09:04:31.889: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 10 09:04:31.911: INFO: all replica sets need to contain the pod-template-hash label
Feb 10 09:04:31.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922270, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:04:33.940: INFO: all replica sets need to contain the pod-template-hash label
Feb 10 09:04:33.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922270, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:04:35.944: INFO: all replica sets need to contain the pod-template-hash label
Feb 10 09:04:35.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922274, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:04:37.937: INFO: all replica sets need to contain the pod-template-hash label
Feb 10 09:04:37.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922274, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:04:39.935: INFO: all replica sets need to contain the pod-template-hash label
Feb 10 09:04:39.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922274, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:04:41.934: INFO: all replica sets need to contain the pod-template-hash label
Feb 10 09:04:41.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922274, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:04:43.936: INFO: all replica sets need to contain the pod-template-hash label
Feb 10 09:04:43.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922274, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922267, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:04:45.939: INFO: 
Feb 10 09:04:45.939: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 10 09:04:45.981: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8519 /apis/apps/v1/namespaces/deployment-8519/deployments/test-rollover-deployment b56d192a-a9ef-4e37-a73c-68159867ec29 4653660 2 2020-02-10 09:04:27 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004476868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-10 09:04:27 +0000 UTC,LastTransitionTime:2020-02-10 09:04:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-02-10 09:04:44 +0000 UTC,LastTransitionTime:2020-02-10 09:04:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 10 09:04:45.995: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-8519 /apis/apps/v1/namespaces/deployment-8519/replicasets/test-rollover-deployment-574d6dfbff 343921a4-e149-43f2-8da3-e332f5328e92 4653649 2 2020-02-10 09:04:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b56d192a-a9ef-4e37-a73c-68159867ec29 0xc004476ef7 0xc004476ef8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004476f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 10 09:04:45.995: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 10 09:04:45.995: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8519 /apis/apps/v1/namespaces/deployment-8519/replicasets/test-rollover-controller 2f01a608-08d8-4d33-9f27-68b906e754c4 4653658 2 2020-02-10 09:04:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b56d192a-a9ef-4e37-a73c-68159867ec29 0xc004476da7 0xc004476da8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004476e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 10 09:04:45.996: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-8519 /apis/apps/v1/namespaces/deployment-8519/replicasets/test-rollover-deployment-f6c94f66c 5ea3b102-958d-4318-8509-12868c31d2a0 4653597 2 2020-02-10 09:04:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b56d192a-a9ef-4e37-a73c-68159867ec29 0xc004477020 0xc004477021}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0044770a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 10 09:04:46.016: INFO: Pod "test-rollover-deployment-574d6dfbff-jss8g" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-jss8g test-rollover-deployment-574d6dfbff- deployment-8519 /api/v1/namespaces/deployment-8519/pods/test-rollover-deployment-574d6dfbff-jss8g 1ea0a610-27e7-448f-9b36-beb896c22ecf 4653620 0 2020-02-10 09:04:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 343921a4-e149-43f2-8da3-e332f5328e92 0xc004477837 0xc004477838}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5jrnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5jrnz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5jrnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:04:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:04:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:04:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:04:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:10.156.161.175,StartTime:2020-02-10 09:04:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 09:04:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker://sha256:01966e4b12a05550df97ce433c8309c377e2de6d6e52b3454fb80c5f9b6f8212,ContainerID:docker://4b83adf317bd39102f016e30829a2749319668e4b465f590c6488444db120d64,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.161.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:04:46.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8519" for this suite.

• [SLOW TEST:25.813 seconds]
[sig-apps] Deployment
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":278,"completed":101,"skipped":1718,"failed":0}
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:04:46.051: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 09:04:46.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b12b2912-62b1-49a3-8cc5-b5c86a57f93b" in namespace "downward-api-5405" to be "success or failure"
Feb 10 09:04:46.509: INFO: Pod "downwardapi-volume-b12b2912-62b1-49a3-8cc5-b5c86a57f93b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.429264ms
Feb 10 09:04:48.520: INFO: Pod "downwardapi-volume-b12b2912-62b1-49a3-8cc5-b5c86a57f93b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027767772s
Feb 10 09:04:50.544: INFO: Pod "downwardapi-volume-b12b2912-62b1-49a3-8cc5-b5c86a57f93b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051041818s
Feb 10 09:04:52.556: INFO: Pod "downwardapi-volume-b12b2912-62b1-49a3-8cc5-b5c86a57f93b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063505841s
STEP: Saw pod success
Feb 10 09:04:52.557: INFO: Pod "downwardapi-volume-b12b2912-62b1-49a3-8cc5-b5c86a57f93b" satisfied condition "success or failure"
Feb 10 09:04:52.569: INFO: Trying to get logs from node master3 pod downwardapi-volume-b12b2912-62b1-49a3-8cc5-b5c86a57f93b container client-container: <nil>
STEP: delete the pod
Feb 10 09:04:52.689: INFO: Waiting for pod downwardapi-volume-b12b2912-62b1-49a3-8cc5-b5c86a57f93b to disappear
Feb 10 09:04:52.701: INFO: Pod downwardapi-volume-b12b2912-62b1-49a3-8cc5-b5c86a57f93b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:04:52.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5405" for this suite.

• [SLOW TEST:6.684 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":102,"skipped":1718,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:04:52.736: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:05:29.786: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 09:05:31.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922329, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922329, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922329, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922329, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:05:33.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922329, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922329, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922329, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922329, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:05:36.879: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
Feb 10 09:05:46.995: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:05:58.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3479" for this suite.
STEP: Destroying namespace "webhook-3479-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:65.613 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":278,"completed":103,"skipped":1727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:05:58.354: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 10 09:05:59.035: INFO: Number of nodes with available pods: 0
Feb 10 09:05:59.036: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:00.073: INFO: Number of nodes with available pods: 0
Feb 10 09:06:00.073: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:01.075: INFO: Number of nodes with available pods: 0
Feb 10 09:06:01.075: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:02.072: INFO: Number of nodes with available pods: 0
Feb 10 09:06:02.072: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:03.070: INFO: Number of nodes with available pods: 0
Feb 10 09:06:03.070: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:04.083: INFO: Number of nodes with available pods: 1
Feb 10 09:06:04.083: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:05.072: INFO: Number of nodes with available pods: 3
Feb 10 09:06:05.072: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 10 09:06:05.160: INFO: Number of nodes with available pods: 2
Feb 10 09:06:05.160: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:06.192: INFO: Number of nodes with available pods: 2
Feb 10 09:06:06.192: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:07.192: INFO: Number of nodes with available pods: 2
Feb 10 09:06:07.193: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:08.200: INFO: Number of nodes with available pods: 2
Feb 10 09:06:08.200: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:09.196: INFO: Number of nodes with available pods: 2
Feb 10 09:06:09.197: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:10.192: INFO: Number of nodes with available pods: 2
Feb 10 09:06:10.192: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:11.198: INFO: Number of nodes with available pods: 2
Feb 10 09:06:11.198: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:12.197: INFO: Number of nodes with available pods: 2
Feb 10 09:06:12.197: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:13.196: INFO: Number of nodes with available pods: 2
Feb 10 09:06:13.196: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:14.195: INFO: Number of nodes with available pods: 2
Feb 10 09:06:14.195: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:15.195: INFO: Number of nodes with available pods: 2
Feb 10 09:06:15.195: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:16.201: INFO: Number of nodes with available pods: 2
Feb 10 09:06:16.201: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:17.199: INFO: Number of nodes with available pods: 2
Feb 10 09:06:17.199: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:18.210: INFO: Number of nodes with available pods: 2
Feb 10 09:06:18.210: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:19.193: INFO: Number of nodes with available pods: 2
Feb 10 09:06:19.193: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:20.198: INFO: Number of nodes with available pods: 2
Feb 10 09:06:20.198: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:21.198: INFO: Number of nodes with available pods: 2
Feb 10 09:06:21.198: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:22.194: INFO: Number of nodes with available pods: 2
Feb 10 09:06:22.194: INFO: Node master1 is running more than one daemon pod
Feb 10 09:06:23.193: INFO: Number of nodes with available pods: 3
Feb 10 09:06:23.193: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-779, will wait for the garbage collector to delete the pods
Feb 10 09:06:23.293: INFO: Deleting DaemonSet.extensions daemon-set took: 25.533361ms
Feb 10 09:06:23.693: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.636179ms
Feb 10 09:06:38.710: INFO: Number of nodes with available pods: 0
Feb 10 09:06:38.710: INFO: Number of running nodes: 0, number of available pods: 0
Feb 10 09:06:38.721: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-779/daemonsets","resourceVersion":"4654222"},"items":null}

Feb 10 09:06:38.730: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-779/pods","resourceVersion":"4654222"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:06:38.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-779" for this suite.

• [SLOW TEST:40.459 seconds]
[sig-apps] Daemon set [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":278,"completed":104,"skipped":1778,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:06:38.816: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7918
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 09:06:39.263: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1e0fa48-415e-40e2-be86-9aeacf34c23d" in namespace "downward-api-7918" to be "success or failure"
Feb 10 09:06:39.282: INFO: Pod "downwardapi-volume-c1e0fa48-415e-40e2-be86-9aeacf34c23d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.664224ms
Feb 10 09:06:41.295: INFO: Pod "downwardapi-volume-c1e0fa48-415e-40e2-be86-9aeacf34c23d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030836058s
Feb 10 09:06:43.311: INFO: Pod "downwardapi-volume-c1e0fa48-415e-40e2-be86-9aeacf34c23d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047226469s
Feb 10 09:06:45.323: INFO: Pod "downwardapi-volume-c1e0fa48-415e-40e2-be86-9aeacf34c23d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059565578s
STEP: Saw pod success
Feb 10 09:06:45.324: INFO: Pod "downwardapi-volume-c1e0fa48-415e-40e2-be86-9aeacf34c23d" satisfied condition "success or failure"
Feb 10 09:06:45.336: INFO: Trying to get logs from node master3 pod downwardapi-volume-c1e0fa48-415e-40e2-be86-9aeacf34c23d container client-container: <nil>
STEP: delete the pod
Feb 10 09:06:45.735: INFO: Waiting for pod downwardapi-volume-c1e0fa48-415e-40e2-be86-9aeacf34c23d to disappear
Feb 10 09:06:45.747: INFO: Pod downwardapi-volume-c1e0fa48-415e-40e2-be86-9aeacf34c23d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:06:45.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7918" for this suite.

• [SLOW TEST:6.974 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":105,"skipped":1791,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:06:45.791: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:06:46.226: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 10 09:06:51.249: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 10 09:06:51.250: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 10 09:06:51.325: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5672 /apis/apps/v1/namespaces/deployment-5672/deployments/test-cleanup-deployment d19940d9-4f38-47dd-857e-3543e22bd353 4654339 1 2020-02-10 09:06:51 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00210fe08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 10 09:06:51.344: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Feb 10 09:06:51.344: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 10 09:06:51.345: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5672 /apis/apps/v1/namespaces/deployment-5672/replicasets/test-cleanup-controller b7cef38f-3603-4557-9a12-c0c269b99d68 4654341 1 2020-02-10 09:06:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d19940d9-4f38-47dd-857e-3543e22bd353 0xc0042fe187 0xc0042fe188}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042fe1e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 10 09:06:51.360: INFO: Pod "test-cleanup-controller-2r5s4" is available:
&Pod{ObjectMeta:{test-cleanup-controller-2r5s4 test-cleanup-controller- deployment-5672 /api/v1/namespaces/deployment-5672/pods/test-cleanup-controller-2r5s4 47cb4aeb-f9a4-4c66-aa08-496ab48d093b 4654332 0 2020-02-10 09:06:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller b7cef38f-3603-4557-9a12-c0c269b99d68 0xc0042fe4f7 0xc0042fe4f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-66z4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-66z4f,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-66z4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:06:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:06:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:06:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:06:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:10.156.32.143,StartTime:2020-02-10 09:06:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 09:06:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://21bbe13f0b662c98384bd9480c8a2132b12f9f2a84542fbde387c946d82791f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.32.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:06:51.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5672" for this suite.

• [SLOW TEST:5.626 seconds]
[sig-apps] Deployment
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":278,"completed":106,"skipped":1796,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:06:51.418: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-acf86aa5-3fe4-4981-b047-44060ce2cb6a
STEP: Creating a pod to test consume configMaps
Feb 10 09:06:51.954: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d43a9978-9003-45c8-8ec9-4d1ac045423a" in namespace "projected-4188" to be "success or failure"
Feb 10 09:06:51.965: INFO: Pod "pod-projected-configmaps-d43a9978-9003-45c8-8ec9-4d1ac045423a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.530424ms
Feb 10 09:06:53.986: INFO: Pod "pod-projected-configmaps-d43a9978-9003-45c8-8ec9-4d1ac045423a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031787441s
Feb 10 09:06:56.009: INFO: Pod "pod-projected-configmaps-d43a9978-9003-45c8-8ec9-4d1ac045423a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054410908s
Feb 10 09:06:58.021: INFO: Pod "pod-projected-configmaps-d43a9978-9003-45c8-8ec9-4d1ac045423a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067381541s
STEP: Saw pod success
Feb 10 09:06:58.022: INFO: Pod "pod-projected-configmaps-d43a9978-9003-45c8-8ec9-4d1ac045423a" satisfied condition "success or failure"
Feb 10 09:06:58.032: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-d43a9978-9003-45c8-8ec9-4d1ac045423a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 09:06:58.142: INFO: Waiting for pod pod-projected-configmaps-d43a9978-9003-45c8-8ec9-4d1ac045423a to disappear
Feb 10 09:06:58.153: INFO: Pod pod-projected-configmaps-d43a9978-9003-45c8-8ec9-4d1ac045423a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:06:58.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4188" for this suite.

• [SLOW TEST:6.783 seconds]
[sig-storage] Projected configMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":107,"skipped":1806,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:06:58.203: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0210 09:06:59.842883      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 10 09:06:59.843: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:06:59.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8411" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":278,"completed":108,"skipped":1821,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:06:59.889: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1326
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:07:00.324: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:07:08.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1326" for this suite.

• [SLOW TEST:8.628 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":278,"completed":109,"skipped":1822,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:07:08.519: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 09:07:08.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-adf7cee2-6d3b-4595-a32b-0e557d7a4ad3" in namespace "downward-api-548" to be "success or failure"
Feb 10 09:07:08.985: INFO: Pod "downwardapi-volume-adf7cee2-6d3b-4595-a32b-0e557d7a4ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.471875ms
Feb 10 09:07:11.010: INFO: Pod "downwardapi-volume-adf7cee2-6d3b-4595-a32b-0e557d7a4ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040151537s
Feb 10 09:07:13.036: INFO: Pod "downwardapi-volume-adf7cee2-6d3b-4595-a32b-0e557d7a4ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066586969s
Feb 10 09:07:15.056: INFO: Pod "downwardapi-volume-adf7cee2-6d3b-4595-a32b-0e557d7a4ad3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.086512655s
STEP: Saw pod success
Feb 10 09:07:15.056: INFO: Pod "downwardapi-volume-adf7cee2-6d3b-4595-a32b-0e557d7a4ad3" satisfied condition "success or failure"
Feb 10 09:07:15.069: INFO: Trying to get logs from node master3 pod downwardapi-volume-adf7cee2-6d3b-4595-a32b-0e557d7a4ad3 container client-container: <nil>
STEP: delete the pod
Feb 10 09:07:15.161: INFO: Waiting for pod downwardapi-volume-adf7cee2-6d3b-4595-a32b-0e557d7a4ad3 to disappear
Feb 10 09:07:15.172: INFO: Pod downwardapi-volume-adf7cee2-6d3b-4595-a32b-0e557d7a4ad3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:07:15.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-548" for this suite.

• [SLOW TEST:6.695 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":110,"skipped":1848,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:07:15.216: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-2df65f9a-8882-4786-b0d8-d95e7ef1b00a
STEP: Creating a pod to test consume configMaps
Feb 10 09:07:15.667: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d1645f81-67cd-4ca1-a542-743340339b66" in namespace "projected-9404" to be "success or failure"
Feb 10 09:07:15.679: INFO: Pod "pod-projected-configmaps-d1645f81-67cd-4ca1-a542-743340339b66": Phase="Pending", Reason="", readiness=false. Elapsed: 11.772267ms
Feb 10 09:07:17.693: INFO: Pod "pod-projected-configmaps-d1645f81-67cd-4ca1-a542-743340339b66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025961224s
Feb 10 09:07:19.705: INFO: Pod "pod-projected-configmaps-d1645f81-67cd-4ca1-a542-743340339b66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038517286s
Feb 10 09:07:21.723: INFO: Pod "pod-projected-configmaps-d1645f81-67cd-4ca1-a542-743340339b66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055590826s
STEP: Saw pod success
Feb 10 09:07:21.723: INFO: Pod "pod-projected-configmaps-d1645f81-67cd-4ca1-a542-743340339b66" satisfied condition "success or failure"
Feb 10 09:07:21.734: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-d1645f81-67cd-4ca1-a542-743340339b66 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 09:07:21.821: INFO: Waiting for pod pod-projected-configmaps-d1645f81-67cd-4ca1-a542-743340339b66 to disappear
Feb 10 09:07:21.833: INFO: Pod pod-projected-configmaps-d1645f81-67cd-4ca1-a542-743340339b66 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:07:21.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9404" for this suite.

• [SLOW TEST:6.651 seconds]
[sig-storage] Projected configMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":111,"skipped":1855,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:07:21.868: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 10 09:07:22.297: INFO: Waiting up to 5m0s for pod "pod-5ef279f4-e8ee-4e92-a4c3-1de8d55e76fc" in namespace "emptydir-3324" to be "success or failure"
Feb 10 09:07:22.308: INFO: Pod "pod-5ef279f4-e8ee-4e92-a4c3-1de8d55e76fc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.696464ms
Feb 10 09:07:24.327: INFO: Pod "pod-5ef279f4-e8ee-4e92-a4c3-1de8d55e76fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029667412s
Feb 10 09:07:26.342: INFO: Pod "pod-5ef279f4-e8ee-4e92-a4c3-1de8d55e76fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04458698s
Feb 10 09:07:28.358: INFO: Pod "pod-5ef279f4-e8ee-4e92-a4c3-1de8d55e76fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060885339s
STEP: Saw pod success
Feb 10 09:07:28.358: INFO: Pod "pod-5ef279f4-e8ee-4e92-a4c3-1de8d55e76fc" satisfied condition "success or failure"
Feb 10 09:07:28.376: INFO: Trying to get logs from node master3 pod pod-5ef279f4-e8ee-4e92-a4c3-1de8d55e76fc container test-container: <nil>
STEP: delete the pod
Feb 10 09:07:28.474: INFO: Waiting for pod pod-5ef279f4-e8ee-4e92-a4c3-1de8d55e76fc to disappear
Feb 10 09:07:28.487: INFO: Pod pod-5ef279f4-e8ee-4e92-a4c3-1de8d55e76fc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:07:28.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3324" for this suite.

• [SLOW TEST:6.651 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":112,"skipped":1861,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:07:28.520: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4460
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Feb 10 09:07:28.964: INFO: Waiting up to 5m0s for pod "var-expansion-3a83830b-2bc7-4eba-a860-dab34eaa9224" in namespace "var-expansion-4460" to be "success or failure"
Feb 10 09:07:28.975: INFO: Pod "var-expansion-3a83830b-2bc7-4eba-a860-dab34eaa9224": Phase="Pending", Reason="", readiness=false. Elapsed: 11.512865ms
Feb 10 09:07:31.001: INFO: Pod "var-expansion-3a83830b-2bc7-4eba-a860-dab34eaa9224": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036996168s
Feb 10 09:07:33.013: INFO: Pod "var-expansion-3a83830b-2bc7-4eba-a860-dab34eaa9224": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04890985s
Feb 10 09:07:35.036: INFO: Pod "var-expansion-3a83830b-2bc7-4eba-a860-dab34eaa9224": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071923424s
STEP: Saw pod success
Feb 10 09:07:35.036: INFO: Pod "var-expansion-3a83830b-2bc7-4eba-a860-dab34eaa9224" satisfied condition "success or failure"
Feb 10 09:07:35.045: INFO: Trying to get logs from node master3 pod var-expansion-3a83830b-2bc7-4eba-a860-dab34eaa9224 container dapi-container: <nil>
STEP: delete the pod
Feb 10 09:07:35.151: INFO: Waiting for pod var-expansion-3a83830b-2bc7-4eba-a860-dab34eaa9224 to disappear
Feb 10 09:07:35.163: INFO: Pod var-expansion-3a83830b-2bc7-4eba-a860-dab34eaa9224 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:07:35.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4460" for this suite.

• [SLOW TEST:6.678 seconds]
[k8s.io] Variable Expansion
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":278,"completed":113,"skipped":1863,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:07:35.200: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 10 09:07:35.685: INFO: Waiting up to 5m0s for pod "pod-02b749a8-b84b-4138-9fc1-c53e252076f2" in namespace "emptydir-6272" to be "success or failure"
Feb 10 09:07:35.696: INFO: Pod "pod-02b749a8-b84b-4138-9fc1-c53e252076f2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.576603ms
Feb 10 09:07:37.707: INFO: Pod "pod-02b749a8-b84b-4138-9fc1-c53e252076f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022331977s
Feb 10 09:07:39.725: INFO: Pod "pod-02b749a8-b84b-4138-9fc1-c53e252076f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040177773s
Feb 10 09:07:41.736: INFO: Pod "pod-02b749a8-b84b-4138-9fc1-c53e252076f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050957181s
STEP: Saw pod success
Feb 10 09:07:41.736: INFO: Pod "pod-02b749a8-b84b-4138-9fc1-c53e252076f2" satisfied condition "success or failure"
Feb 10 09:07:41.745: INFO: Trying to get logs from node master3 pod pod-02b749a8-b84b-4138-9fc1-c53e252076f2 container test-container: <nil>
STEP: delete the pod
Feb 10 09:07:41.836: INFO: Waiting for pod pod-02b749a8-b84b-4138-9fc1-c53e252076f2 to disappear
Feb 10 09:07:41.847: INFO: Pod pod-02b749a8-b84b-4138-9fc1-c53e252076f2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:07:41.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6272" for this suite.

• [SLOW TEST:6.679 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":114,"skipped":1871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:07:41.882: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6268
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:07:42.284: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:07:47.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6268" for this suite.

• [SLOW TEST:6.102 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":278,"completed":115,"skipped":1900,"failed":0}
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:07:47.984: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Feb 10 09:07:49.018: INFO: created pod pod-service-account-defaultsa
Feb 10 09:07:49.019: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 10 09:07:49.036: INFO: created pod pod-service-account-mountsa
Feb 10 09:07:49.037: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 10 09:07:49.060: INFO: created pod pod-service-account-nomountsa
Feb 10 09:07:49.060: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 10 09:07:49.091: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 10 09:07:49.091: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 10 09:07:49.113: INFO: created pod pod-service-account-mountsa-mountspec
Feb 10 09:07:49.113: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 10 09:07:49.131: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 10 09:07:49.131: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 10 09:07:49.165: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 10 09:07:49.165: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 10 09:07:49.205: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 10 09:07:49.205: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 10 09:07:49.271: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 10 09:07:49.271: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:07:49.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-793" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":278,"completed":116,"skipped":1900,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:07:49.326: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7605
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:07:49.843: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:08:00.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7605" for this suite.

• [SLOW TEST:11.125 seconds]
[k8s.io] Pods
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":278,"completed":117,"skipped":1901,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:08:00.452: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:08:41.116: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 09:08:43.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922521, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922521, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922521, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922521, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:08:45.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922521, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922521, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922521, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922521, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:08:48.200: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
Feb 10 09:08:58.294: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:08:58.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7419" for this suite.
STEP: Destroying namespace "webhook-7419-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:58.443 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":278,"completed":118,"skipped":1948,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:08:58.899: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:09:10.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5430" for this suite.

• [SLOW TEST:11.666 seconds]
[sig-api-machinery] ResourceQuota
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":278,"completed":119,"skipped":1965,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:09:10.567: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 10 09:09:11.095: INFO: Number of nodes with available pods: 0
Feb 10 09:09:11.095: INFO: Node master1 is running more than one daemon pod
Feb 10 09:09:12.139: INFO: Number of nodes with available pods: 0
Feb 10 09:09:12.139: INFO: Node master1 is running more than one daemon pod
Feb 10 09:09:13.129: INFO: Number of nodes with available pods: 0
Feb 10 09:09:13.130: INFO: Node master1 is running more than one daemon pod
Feb 10 09:09:14.132: INFO: Number of nodes with available pods: 0
Feb 10 09:09:14.132: INFO: Node master1 is running more than one daemon pod
Feb 10 09:09:15.130: INFO: Number of nodes with available pods: 0
Feb 10 09:09:15.130: INFO: Node master1 is running more than one daemon pod
Feb 10 09:09:16.148: INFO: Number of nodes with available pods: 1
Feb 10 09:09:16.148: INFO: Node master1 is running more than one daemon pod
Feb 10 09:09:17.136: INFO: Number of nodes with available pods: 3
Feb 10 09:09:17.136: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 10 09:09:17.230: INFO: Number of nodes with available pods: 2
Feb 10 09:09:17.230: INFO: Node master3 is running more than one daemon pod
Feb 10 09:09:18.268: INFO: Number of nodes with available pods: 2
Feb 10 09:09:18.268: INFO: Node master3 is running more than one daemon pod
Feb 10 09:09:19.293: INFO: Number of nodes with available pods: 2
Feb 10 09:09:19.293: INFO: Node master3 is running more than one daemon pod
Feb 10 09:09:20.260: INFO: Number of nodes with available pods: 2
Feb 10 09:09:20.260: INFO: Node master3 is running more than one daemon pod
Feb 10 09:09:21.263: INFO: Number of nodes with available pods: 2
Feb 10 09:09:21.263: INFO: Node master3 is running more than one daemon pod
Feb 10 09:09:22.272: INFO: Number of nodes with available pods: 3
Feb 10 09:09:22.272: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5235, will wait for the garbage collector to delete the pods
Feb 10 09:09:22.394: INFO: Deleting DaemonSet.extensions daemon-set took: 34.641106ms
Feb 10 09:09:22.797: INFO: Terminating DaemonSet.extensions daemon-set pods took: 402.974718ms
Feb 10 09:09:38.215: INFO: Number of nodes with available pods: 0
Feb 10 09:09:38.216: INFO: Number of running nodes: 0, number of available pods: 0
Feb 10 09:09:38.226: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5235/daemonsets","resourceVersion":"4655567"},"items":null}

Feb 10 09:09:38.238: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5235/pods","resourceVersion":"4655567"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:09:38.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5235" for this suite.

• [SLOW TEST:27.768 seconds]
[sig-apps] Daemon set [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":278,"completed":120,"skipped":1975,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:09:38.335: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 10 09:09:45.702: INFO: Successfully updated pod "annotationupdate84764711-6147-41b1-914a-761cabcf6003"
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:09:47.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5647" for this suite.

• [SLOW TEST:9.482 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":278,"completed":121,"skipped":1979,"failed":0}
SS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:09:47.819: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:09:48.258: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6ff400f9-c807-40e0-9956-9cde0e57baac" in namespace "security-context-test-5488" to be "success or failure"
Feb 10 09:09:48.272: INFO: Pod "busybox-privileged-false-6ff400f9-c807-40e0-9956-9cde0e57baac": Phase="Pending", Reason="", readiness=false. Elapsed: 14.337186ms
Feb 10 09:09:50.287: INFO: Pod "busybox-privileged-false-6ff400f9-c807-40e0-9956-9cde0e57baac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02950126s
Feb 10 09:09:52.300: INFO: Pod "busybox-privileged-false-6ff400f9-c807-40e0-9956-9cde0e57baac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04267838s
Feb 10 09:09:54.318: INFO: Pod "busybox-privileged-false-6ff400f9-c807-40e0-9956-9cde0e57baac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059879737s
Feb 10 09:09:54.318: INFO: Pod "busybox-privileged-false-6ff400f9-c807-40e0-9956-9cde0e57baac" satisfied condition "success or failure"
Feb 10 09:09:54.681: INFO: Got logs for pod "busybox-privileged-false-6ff400f9-c807-40e0-9956-9cde0e57baac": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:09:54.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5488" for this suite.

• [SLOW TEST:6.903 seconds]
[k8s.io] Security Context
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  When creating a pod with privileged
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:225
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":122,"skipped":1981,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:09:54.723: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-965
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-965
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-965 to expose endpoints map[]
Feb 10 09:09:55.203: INFO: Get endpoints failed (14.064965ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 10 09:09:56.216: INFO: successfully validated that service endpoint-test2 in namespace services-965 exposes endpoints map[] (1.027146045s elapsed)
STEP: Creating pod pod1 in namespace services-965
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-965 to expose endpoints map[pod1:[80]]
Feb 10 09:10:00.372: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.124879265s elapsed, will retry)
Feb 10 09:10:01.405: INFO: successfully validated that service endpoint-test2 in namespace services-965 exposes endpoints map[pod1:[80]] (5.157504686s elapsed)
STEP: Creating pod pod2 in namespace services-965
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-965 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 10 09:10:05.648: INFO: Unexpected endpoints: found map[b857d766-6390-48dd-aaed-4a604540cf12:[80]], expected map[pod1:[80] pod2:[80]] (4.216393273s elapsed, will retry)
Feb 10 09:10:07.719: INFO: successfully validated that service endpoint-test2 in namespace services-965 exposes endpoints map[pod1:[80] pod2:[80]] (6.286910136s elapsed)
STEP: Deleting pod pod1 in namespace services-965
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-965 to expose endpoints map[pod2:[80]]
Feb 10 09:10:08.809: INFO: successfully validated that service endpoint-test2 in namespace services-965 exposes endpoints map[pod2:[80]] (1.057586707s elapsed)
STEP: Deleting pod pod2 in namespace services-965
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-965 to expose endpoints map[]
Feb 10 09:10:09.857: INFO: successfully validated that service endpoint-test2 in namespace services-965 exposes endpoints map[] (1.024264885s elapsed)
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:10:09.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-965" for this suite.
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:15.250 seconds]
[sig-network] Services
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":278,"completed":123,"skipped":1984,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:10:09.975: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:10:54.351: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 09:10:56.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922654, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922654, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922654, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922654, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:10:58.408: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922654, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922654, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922654, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716922654, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:11:01.447: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 10 09:11:08.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 attach --namespace=webhook-2417 to-be-attached-pod -i -c=container1'
Feb 10 09:11:09.647: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:11:09.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2417" for this suite.
STEP: Destroying namespace "webhook-2417-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:59.927 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":278,"completed":124,"skipped":1988,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:11:09.904: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-67802d7b-f8c6-4ef4-9bdf-5daac6aa4ac2
STEP: Creating a pod to test consume configMaps
Feb 10 09:11:10.410: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c4967a8f-8416-4bfb-a82a-a38719c5b3b5" in namespace "projected-2374" to be "success or failure"
Feb 10 09:11:10.420: INFO: Pod "pod-projected-configmaps-c4967a8f-8416-4bfb-a82a-a38719c5b3b5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.653895ms
Feb 10 09:11:12.440: INFO: Pod "pod-projected-configmaps-c4967a8f-8416-4bfb-a82a-a38719c5b3b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030104415s
Feb 10 09:11:14.459: INFO: Pod "pod-projected-configmaps-c4967a8f-8416-4bfb-a82a-a38719c5b3b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049049948s
Feb 10 09:11:16.476: INFO: Pod "pod-projected-configmaps-c4967a8f-8416-4bfb-a82a-a38719c5b3b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066486967s
STEP: Saw pod success
Feb 10 09:11:16.476: INFO: Pod "pod-projected-configmaps-c4967a8f-8416-4bfb-a82a-a38719c5b3b5" satisfied condition "success or failure"
Feb 10 09:11:16.487: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-c4967a8f-8416-4bfb-a82a-a38719c5b3b5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 09:11:16.591: INFO: Waiting for pod pod-projected-configmaps-c4967a8f-8416-4bfb-a82a-a38719c5b3b5 to disappear
Feb 10 09:11:16.601: INFO: Pod pod-projected-configmaps-c4967a8f-8416-4bfb-a82a-a38719c5b3b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:11:16.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2374" for this suite.

• [SLOW TEST:6.734 seconds]
[sig-storage] Projected configMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":278,"completed":125,"skipped":1999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:11:16.643: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4155, will wait for the garbage collector to delete the pods
Feb 10 09:11:25.196: INFO: Deleting Job.batch foo took: 21.054046ms
Feb 10 09:11:25.597: INFO: Terminating Job.batch foo pods took: 400.579505ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:12:08.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4155" for this suite.

• [SLOW TEST:52.106 seconds]
[sig-apps] Job
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":278,"completed":126,"skipped":2049,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:12:08.753: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:12:15.305: INFO: Waiting up to 5m0s for pod "client-envvars-1378bca0-bca9-47a4-9edb-db02a9adbe6c" in namespace "pods-3642" to be "success or failure"
Feb 10 09:12:15.319: INFO: Pod "client-envvars-1378bca0-bca9-47a4-9edb-db02a9adbe6c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.799923ms
Feb 10 09:12:17.331: INFO: Pod "client-envvars-1378bca0-bca9-47a4-9edb-db02a9adbe6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024941131s
Feb 10 09:12:19.353: INFO: Pod "client-envvars-1378bca0-bca9-47a4-9edb-db02a9adbe6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047635692s
Feb 10 09:12:21.377: INFO: Pod "client-envvars-1378bca0-bca9-47a4-9edb-db02a9adbe6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071565558s
STEP: Saw pod success
Feb 10 09:12:21.377: INFO: Pod "client-envvars-1378bca0-bca9-47a4-9edb-db02a9adbe6c" satisfied condition "success or failure"
Feb 10 09:12:21.389: INFO: Trying to get logs from node master1 pod client-envvars-1378bca0-bca9-47a4-9edb-db02a9adbe6c container env3cont: <nil>
STEP: delete the pod
Feb 10 09:12:21.798: INFO: Waiting for pod client-envvars-1378bca0-bca9-47a4-9edb-db02a9adbe6c to disappear
Feb 10 09:12:21.811: INFO: Pod client-envvars-1378bca0-bca9-47a4-9edb-db02a9adbe6c no longer exists
[AfterEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:12:21.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3642" for this suite.

• [SLOW TEST:13.099 seconds]
[k8s.io] Pods
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":278,"completed":127,"skipped":2067,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:12:21.853: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-486eb325-1f4f-46b3-8ccd-75a9bda8e4d9
STEP: Creating a pod to test consume secrets
Feb 10 09:12:22.304: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9abe6580-de10-47eb-9a85-e89abba9e9b6" in namespace "projected-2702" to be "success or failure"
Feb 10 09:12:22.315: INFO: Pod "pod-projected-secrets-9abe6580-de10-47eb-9a85-e89abba9e9b6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.09205ms
Feb 10 09:12:24.332: INFO: Pod "pod-projected-secrets-9abe6580-de10-47eb-9a85-e89abba9e9b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027497344s
Feb 10 09:12:26.353: INFO: Pod "pod-projected-secrets-9abe6580-de10-47eb-9a85-e89abba9e9b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048740381s
Feb 10 09:12:28.402: INFO: Pod "pod-projected-secrets-9abe6580-de10-47eb-9a85-e89abba9e9b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.097526579s
STEP: Saw pod success
Feb 10 09:12:28.402: INFO: Pod "pod-projected-secrets-9abe6580-de10-47eb-9a85-e89abba9e9b6" satisfied condition "success or failure"
Feb 10 09:12:28.415: INFO: Trying to get logs from node master1 pod pod-projected-secrets-9abe6580-de10-47eb-9a85-e89abba9e9b6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 10 09:12:28.494: INFO: Waiting for pod pod-projected-secrets-9abe6580-de10-47eb-9a85-e89abba9e9b6 to disappear
Feb 10 09:12:28.507: INFO: Pod pod-projected-secrets-9abe6580-de10-47eb-9a85-e89abba9e9b6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:12:28.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2702" for this suite.

• [SLOW TEST:6.687 seconds]
[sig-storage] Projected secret
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":128,"skipped":2079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:12:28.543: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:12:29.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8151" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":278,"completed":129,"skipped":2126,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:12:29.101: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create services for rc  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Feb 10 09:12:29.604: INFO: namespace kubectl-8293
Feb 10 09:12:29.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-8293'
Feb 10 09:12:31.004: INFO: stderr: ""
Feb 10 09:12:31.005: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 10 09:12:32.023: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 09:12:32.023: INFO: Found 0 / 1
Feb 10 09:12:33.020: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 09:12:33.020: INFO: Found 0 / 1
Feb 10 09:12:34.019: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 09:12:34.019: INFO: Found 0 / 1
Feb 10 09:12:35.023: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 09:12:35.023: INFO: Found 0 / 1
Feb 10 09:12:36.019: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 09:12:36.019: INFO: Found 0 / 1
Feb 10 09:12:37.019: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 09:12:37.019: INFO: Found 1 / 1
Feb 10 09:12:37.019: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 10 09:12:37.031: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 09:12:37.031: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 10 09:12:37.031: INFO: wait on agnhost-master startup in kubectl-8293 
Feb 10 09:12:37.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 logs agnhost-master-t6hfw agnhost-master --namespace=kubectl-8293'
Feb 10 09:12:37.880: INFO: stderr: ""
Feb 10 09:12:37.881: INFO: stdout: "Paused\n"
STEP: exposing RC
Feb 10 09:12:37.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8293'
Feb 10 09:12:38.485: INFO: stderr: ""
Feb 10 09:12:38.486: INFO: stdout: "service/rm2 exposed\n"
Feb 10 09:12:38.506: INFO: Service rm2 in namespace kubectl-8293 found.
STEP: exposing service
Feb 10 09:12:40.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8293'
Feb 10 09:12:41.086: INFO: stderr: ""
Feb 10 09:12:41.086: INFO: stdout: "service/rm3 exposed\n"
Feb 10 09:12:41.102: INFO: Service rm3 in namespace kubectl-8293 found.
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:12:43.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8293" for this suite.

• [SLOW TEST:14.062 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
    should create services for rc  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":278,"completed":130,"skipped":2174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:12:43.167: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 09:12:43.597: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d62e792f-6788-4021-902a-fc01facb5ae6" in namespace "downward-api-1883" to be "success or failure"
Feb 10 09:12:43.607: INFO: Pod "downwardapi-volume-d62e792f-6788-4021-902a-fc01facb5ae6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.156744ms
Feb 10 09:12:45.619: INFO: Pod "downwardapi-volume-d62e792f-6788-4021-902a-fc01facb5ae6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022216932s
Feb 10 09:12:47.633: INFO: Pod "downwardapi-volume-d62e792f-6788-4021-902a-fc01facb5ae6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035798049s
Feb 10 09:12:49.652: INFO: Pod "downwardapi-volume-d62e792f-6788-4021-902a-fc01facb5ae6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055420813s
STEP: Saw pod success
Feb 10 09:12:49.653: INFO: Pod "downwardapi-volume-d62e792f-6788-4021-902a-fc01facb5ae6" satisfied condition "success or failure"
Feb 10 09:12:49.666: INFO: Trying to get logs from node master3 pod downwardapi-volume-d62e792f-6788-4021-902a-fc01facb5ae6 container client-container: <nil>
STEP: delete the pod
Feb 10 09:12:50.173: INFO: Waiting for pod downwardapi-volume-d62e792f-6788-4021-902a-fc01facb5ae6 to disappear
Feb 10 09:12:50.201: INFO: Pod downwardapi-volume-d62e792f-6788-4021-902a-fc01facb5ae6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:12:50.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1883" for this suite.

• [SLOW TEST:7.070 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":278,"completed":131,"skipped":2205,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:12:50.238: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 10 09:12:50.675: INFO: Waiting up to 5m0s for pod "pod-b25b03d7-f944-4bd7-a034-2a2853986d44" in namespace "emptydir-7488" to be "success or failure"
Feb 10 09:12:50.686: INFO: Pod "pod-b25b03d7-f944-4bd7-a034-2a2853986d44": Phase="Pending", Reason="", readiness=false. Elapsed: 10.975226ms
Feb 10 09:12:52.698: INFO: Pod "pod-b25b03d7-f944-4bd7-a034-2a2853986d44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022821602s
Feb 10 09:12:54.711: INFO: Pod "pod-b25b03d7-f944-4bd7-a034-2a2853986d44": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036023568s
Feb 10 09:12:56.725: INFO: Pod "pod-b25b03d7-f944-4bd7-a034-2a2853986d44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049604771s
STEP: Saw pod success
Feb 10 09:12:56.725: INFO: Pod "pod-b25b03d7-f944-4bd7-a034-2a2853986d44" satisfied condition "success or failure"
Feb 10 09:12:56.737: INFO: Trying to get logs from node master3 pod pod-b25b03d7-f944-4bd7-a034-2a2853986d44 container test-container: <nil>
STEP: delete the pod
Feb 10 09:12:56.821: INFO: Waiting for pod pod-b25b03d7-f944-4bd7-a034-2a2853986d44 to disappear
Feb 10 09:12:56.833: INFO: Pod pod-b25b03d7-f944-4bd7-a034-2a2853986d44 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:12:56.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7488" for this suite.

• [SLOW TEST:6.630 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":132,"skipped":2206,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:12:56.868: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1464
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-71bbeb63-361b-4474-8500-601502122387
STEP: Creating secret with name s-test-opt-upd-a167f96b-97ab-49e7-927a-6ad0471f2a9c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-71bbeb63-361b-4474-8500-601502122387
STEP: Updating secret s-test-opt-upd-a167f96b-97ab-49e7-927a-6ad0471f2a9c
STEP: Creating secret with name s-test-opt-create-80125e88-8556-461f-8b41-894d41da7bd1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:13:07.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1464" for this suite.

• [SLOW TEST:10.876 seconds]
[sig-storage] Secrets
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":133,"skipped":2213,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:13:07.746: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7541
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Feb 10 09:13:08.217: INFO: Created pod &Pod{ObjectMeta:{dns-7541  dns-7541 /api/v1/namespaces/dns-7541/pods/dns-7541 f4acbc46-1e11-4522-b719-90cb20493fc0 4656802 0 2020-02-10 09:13:08 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2lt9t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2lt9t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2lt9t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Feb 10 09:13:14.247: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7541 PodName:dns-7541 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 09:13:14.247: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Verifying customized DNS server is configured on pod...
Feb 10 09:13:15.092: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7541 PodName:dns-7541 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 09:13:15.092: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 09:13:15.905: INFO: Deleting pod dns-7541...
[AfterEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:13:15.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7541" for this suite.

• [SLOW TEST:8.244 seconds]
[sig-network] DNS
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":278,"completed":134,"skipped":2225,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:13:15.992: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 09:13:16.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a72426bd-361d-4afe-9406-73b57cbd5ab0" in namespace "projected-9441" to be "success or failure"
Feb 10 09:13:16.445: INFO: Pod "downwardapi-volume-a72426bd-361d-4afe-9406-73b57cbd5ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.783646ms
Feb 10 09:13:18.461: INFO: Pod "downwardapi-volume-a72426bd-361d-4afe-9406-73b57cbd5ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026506909s
Feb 10 09:13:20.479: INFO: Pod "downwardapi-volume-a72426bd-361d-4afe-9406-73b57cbd5ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044949446s
Feb 10 09:13:22.498: INFO: Pod "downwardapi-volume-a72426bd-361d-4afe-9406-73b57cbd5ab0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063792888s
STEP: Saw pod success
Feb 10 09:13:22.498: INFO: Pod "downwardapi-volume-a72426bd-361d-4afe-9406-73b57cbd5ab0" satisfied condition "success or failure"
Feb 10 09:13:22.509: INFO: Trying to get logs from node master1 pod downwardapi-volume-a72426bd-361d-4afe-9406-73b57cbd5ab0 container client-container: <nil>
STEP: delete the pod
Feb 10 09:13:22.604: INFO: Waiting for pod downwardapi-volume-a72426bd-361d-4afe-9406-73b57cbd5ab0 to disappear
Feb 10 09:13:22.616: INFO: Pod downwardapi-volume-a72426bd-361d-4afe-9406-73b57cbd5ab0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:13:22.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9441" for this suite.

• [SLOW TEST:6.663 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":135,"skipped":2244,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:13:22.655: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-d9ff3691-0875-436b-a0be-90de30307c3e
STEP: Creating secret with name secret-projected-all-test-volume-cfc4daa2-306c-4b68-a953-036b0902d20b
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 10 09:13:23.139: INFO: Waiting up to 5m0s for pod "projected-volume-2675f582-b7b5-4adc-9112-7a14bad25f0f" in namespace "projected-1753" to be "success or failure"
Feb 10 09:13:23.149: INFO: Pod "projected-volume-2675f582-b7b5-4adc-9112-7a14bad25f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.221563ms
Feb 10 09:13:25.163: INFO: Pod "projected-volume-2675f582-b7b5-4adc-9112-7a14bad25f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023901153s
Feb 10 09:13:27.177: INFO: Pod "projected-volume-2675f582-b7b5-4adc-9112-7a14bad25f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038697952s
Feb 10 09:13:29.191: INFO: Pod "projected-volume-2675f582-b7b5-4adc-9112-7a14bad25f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052283809s
STEP: Saw pod success
Feb 10 09:13:29.191: INFO: Pod "projected-volume-2675f582-b7b5-4adc-9112-7a14bad25f0f" satisfied condition "success or failure"
Feb 10 09:13:29.202: INFO: Trying to get logs from node master3 pod projected-volume-2675f582-b7b5-4adc-9112-7a14bad25f0f container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 10 09:13:29.281: INFO: Waiting for pod projected-volume-2675f582-b7b5-4adc-9112-7a14bad25f0f to disappear
Feb 10 09:13:29.294: INFO: Pod projected-volume-2675f582-b7b5-4adc-9112-7a14bad25f0f no longer exists
[AfterEach] [sig-storage] Projected combined
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:13:29.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1753" for this suite.

• [SLOW TEST:6.673 seconds]
[sig-storage] Projected combined
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":278,"completed":136,"skipped":2247,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:13:29.329: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:13:40.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7856" for this suite.

• [SLOW TEST:11.596 seconds]
[sig-api-machinery] ResourceQuota
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":278,"completed":137,"skipped":2247,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:13:40.926: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3038
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1295
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1614
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:13:48.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3038" for this suite.
STEP: Destroying namespace "nsdeletetest-1295" for this suite.
Feb 10 09:13:48.253: INFO: Namespace nsdeletetest-1295 was already deleted
STEP: Destroying namespace "nsdeletetest-1614" for this suite.

• [SLOW TEST:7.342 seconds]
[sig-api-machinery] Namespaces [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":278,"completed":138,"skipped":2255,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:13:48.269: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-12
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:13:48.745: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 10 09:13:48.775: INFO: Number of nodes with available pods: 0
Feb 10 09:13:48.775: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 10 09:13:48.853: INFO: Number of nodes with available pods: 0
Feb 10 09:13:48.853: INFO: Node master3 is running more than one daemon pod
Feb 10 09:13:49.870: INFO: Number of nodes with available pods: 0
Feb 10 09:13:49.871: INFO: Node master3 is running more than one daemon pod
Feb 10 09:13:50.866: INFO: Number of nodes with available pods: 0
Feb 10 09:13:50.866: INFO: Node master3 is running more than one daemon pod
Feb 10 09:13:51.876: INFO: Number of nodes with available pods: 0
Feb 10 09:13:51.876: INFO: Node master3 is running more than one daemon pod
Feb 10 09:13:52.871: INFO: Number of nodes with available pods: 0
Feb 10 09:13:52.872: INFO: Node master3 is running more than one daemon pod
Feb 10 09:13:53.870: INFO: Number of nodes with available pods: 1
Feb 10 09:13:53.870: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 10 09:13:53.927: INFO: Number of nodes with available pods: 1
Feb 10 09:13:53.927: INFO: Number of running nodes: 0, number of available pods: 1
Feb 10 09:13:54.940: INFO: Number of nodes with available pods: 0
Feb 10 09:13:54.940: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 10 09:13:54.974: INFO: Number of nodes with available pods: 0
Feb 10 09:13:54.975: INFO: Node master3 is running more than one daemon pod
Feb 10 09:13:55.987: INFO: Number of nodes with available pods: 0
Feb 10 09:13:55.987: INFO: Node master3 is running more than one daemon pod
Feb 10 09:13:56.989: INFO: Number of nodes with available pods: 0
Feb 10 09:13:56.990: INFO: Node master3 is running more than one daemon pod
Feb 10 09:13:57.987: INFO: Number of nodes with available pods: 0
Feb 10 09:13:57.987: INFO: Node master3 is running more than one daemon pod
Feb 10 09:13:58.995: INFO: Number of nodes with available pods: 0
Feb 10 09:13:58.996: INFO: Node master3 is running more than one daemon pod
Feb 10 09:13:59.989: INFO: Number of nodes with available pods: 0
Feb 10 09:13:59.990: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:00.990: INFO: Number of nodes with available pods: 0
Feb 10 09:14:00.991: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:01.996: INFO: Number of nodes with available pods: 0
Feb 10 09:14:01.996: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:02.995: INFO: Number of nodes with available pods: 0
Feb 10 09:14:02.995: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:03.994: INFO: Number of nodes with available pods: 0
Feb 10 09:14:03.994: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:04.990: INFO: Number of nodes with available pods: 0
Feb 10 09:14:04.990: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:05.987: INFO: Number of nodes with available pods: 0
Feb 10 09:14:05.987: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:06.988: INFO: Number of nodes with available pods: 0
Feb 10 09:14:06.988: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:07.987: INFO: Number of nodes with available pods: 0
Feb 10 09:14:07.988: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:08.987: INFO: Number of nodes with available pods: 0
Feb 10 09:14:08.988: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:09.995: INFO: Number of nodes with available pods: 0
Feb 10 09:14:09.996: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:10.991: INFO: Number of nodes with available pods: 0
Feb 10 09:14:10.992: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:11.986: INFO: Number of nodes with available pods: 0
Feb 10 09:14:11.987: INFO: Node master3 is running more than one daemon pod
Feb 10 09:14:12.990: INFO: Number of nodes with available pods: 1
Feb 10 09:14:12.991: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-12, will wait for the garbage collector to delete the pods
Feb 10 09:14:13.101: INFO: Deleting DaemonSet.extensions daemon-set took: 25.974259ms
Feb 10 09:14:13.501: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.836259ms
Feb 10 09:14:28.717: INFO: Number of nodes with available pods: 0
Feb 10 09:14:28.717: INFO: Number of running nodes: 0, number of available pods: 0
Feb 10 09:14:28.727: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-12/daemonsets","resourceVersion":"4657287"},"items":null}

Feb 10 09:14:28.738: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-12/pods","resourceVersion":"4657287"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:14:28.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-12" for this suite.

• [SLOW TEST:40.580 seconds]
[sig-apps] Daemon set [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":278,"completed":139,"skipped":2275,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:14:28.854: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8302
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:14:29.265: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 10 09:14:31.375: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:14:32.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8302" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":278,"completed":140,"skipped":2292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:14:32.437: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 09:14:32.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a46c13a3-2479-4fb4-b7dc-7845edc36fc5" in namespace "projected-408" to be "success or failure"
Feb 10 09:14:32.908: INFO: Pod "downwardapi-volume-a46c13a3-2479-4fb4-b7dc-7845edc36fc5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.304618ms
Feb 10 09:14:34.920: INFO: Pod "downwardapi-volume-a46c13a3-2479-4fb4-b7dc-7845edc36fc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021623149s
Feb 10 09:14:36.933: INFO: Pod "downwardapi-volume-a46c13a3-2479-4fb4-b7dc-7845edc36fc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034780335s
Feb 10 09:14:38.955: INFO: Pod "downwardapi-volume-a46c13a3-2479-4fb4-b7dc-7845edc36fc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056758581s
STEP: Saw pod success
Feb 10 09:14:38.955: INFO: Pod "downwardapi-volume-a46c13a3-2479-4fb4-b7dc-7845edc36fc5" satisfied condition "success or failure"
Feb 10 09:14:38.968: INFO: Trying to get logs from node master3 pod downwardapi-volume-a46c13a3-2479-4fb4-b7dc-7845edc36fc5 container client-container: <nil>
STEP: delete the pod
Feb 10 09:14:39.072: INFO: Waiting for pod downwardapi-volume-a46c13a3-2479-4fb4-b7dc-7845edc36fc5 to disappear
Feb 10 09:14:39.080: INFO: Pod downwardapi-volume-a46c13a3-2479-4fb4-b7dc-7845edc36fc5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:14:39.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-408" for this suite.

• [SLOW TEST:6.676 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":141,"skipped":2319,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:14:39.115: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-7684/secret-test-abcda6f3-e4a0-448f-8360-92bcf5bc3f4a
STEP: Creating a pod to test consume secrets
Feb 10 09:14:39.619: INFO: Waiting up to 5m0s for pod "pod-configmaps-838ececd-6a34-4c94-bb59-ee318911fc24" in namespace "secrets-7684" to be "success or failure"
Feb 10 09:14:39.641: INFO: Pod "pod-configmaps-838ececd-6a34-4c94-bb59-ee318911fc24": Phase="Pending", Reason="", readiness=false. Elapsed: 22.417241ms
Feb 10 09:14:41.652: INFO: Pod "pod-configmaps-838ececd-6a34-4c94-bb59-ee318911fc24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033565705s
Feb 10 09:14:43.665: INFO: Pod "pod-configmaps-838ececd-6a34-4c94-bb59-ee318911fc24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046449324s
Feb 10 09:14:45.681: INFO: Pod "pod-configmaps-838ececd-6a34-4c94-bb59-ee318911fc24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062221002s
STEP: Saw pod success
Feb 10 09:14:45.681: INFO: Pod "pod-configmaps-838ececd-6a34-4c94-bb59-ee318911fc24" satisfied condition "success or failure"
Feb 10 09:14:45.693: INFO: Trying to get logs from node master3 pod pod-configmaps-838ececd-6a34-4c94-bb59-ee318911fc24 container env-test: <nil>
STEP: delete the pod
Feb 10 09:14:45.786: INFO: Waiting for pod pod-configmaps-838ececd-6a34-4c94-bb59-ee318911fc24 to disappear
Feb 10 09:14:45.798: INFO: Pod pod-configmaps-838ececd-6a34-4c94-bb59-ee318911fc24 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:14:45.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7684" for this suite.

• [SLOW TEST:6.719 seconds]
[sig-api-machinery] Secrets
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":278,"completed":142,"skipped":2336,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:14:45.835: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5704
STEP: Creating secret with name secret-test-b04e364a-77b2-4392-b643-f1cc73a98e14
STEP: Creating a pod to test consume secrets
Feb 10 09:14:46.683: INFO: Waiting up to 5m0s for pod "pod-secrets-4034460b-d305-4219-9169-a97afc2b75d6" in namespace "secrets-909" to be "success or failure"
Feb 10 09:14:46.694: INFO: Pod "pod-secrets-4034460b-d305-4219-9169-a97afc2b75d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.877539ms
Feb 10 09:14:48.709: INFO: Pod "pod-secrets-4034460b-d305-4219-9169-a97afc2b75d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025166904s
Feb 10 09:14:50.727: INFO: Pod "pod-secrets-4034460b-d305-4219-9169-a97afc2b75d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043413412s
Feb 10 09:14:52.744: INFO: Pod "pod-secrets-4034460b-d305-4219-9169-a97afc2b75d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060223884s
STEP: Saw pod success
Feb 10 09:14:52.744: INFO: Pod "pod-secrets-4034460b-d305-4219-9169-a97afc2b75d6" satisfied condition "success or failure"
Feb 10 09:14:52.755: INFO: Trying to get logs from node master3 pod pod-secrets-4034460b-d305-4219-9169-a97afc2b75d6 container secret-volume-test: <nil>
STEP: delete the pod
Feb 10 09:14:52.829: INFO: Waiting for pod pod-secrets-4034460b-d305-4219-9169-a97afc2b75d6 to disappear
Feb 10 09:14:52.838: INFO: Pod pod-secrets-4034460b-d305-4219-9169-a97afc2b75d6 no longer exists
[AfterEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:14:52.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-909" for this suite.
STEP: Destroying namespace "secret-namespace-5704" for this suite.

• [SLOW TEST:7.060 seconds]
[sig-storage] Secrets
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":278,"completed":143,"skipped":2343,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:14:52.897: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-630
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:14:59.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-630" for this suite.

• [SLOW TEST:6.543 seconds]
[k8s.io] Kubelet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox Pod with hostAliases
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":144,"skipped":2355,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:14:59.440: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:15:06.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7433" for this suite.

• [SLOW TEST:6.611 seconds]
[sig-storage] EmptyDir wrapper volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":278,"completed":145,"skipped":2359,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:15:06.060: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9985
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:15:14.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9985" for this suite.

• [SLOW TEST:8.511 seconds]
[k8s.io] Kubelet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command that always fails in a pod
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":278,"completed":146,"skipped":2407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:15:14.574: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 10 09:15:22.152: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:15:23.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7333" for this suite.

• [SLOW TEST:8.679 seconds]
[sig-apps] ReplicaSet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":278,"completed":147,"skipped":2433,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:15:23.257: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 10 09:15:23.670: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:15:32.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6990" for this suite.

• [SLOW TEST:9.622 seconds]
[k8s.io] InitContainer [NodeConformance]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":278,"completed":148,"skipped":2444,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:15:32.880: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-24e88847-6749-4c3b-a87a-7abded7ff339
STEP: Creating a pod to test consume configMaps
Feb 10 09:15:33.330: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b35d3b96-9b1a-4411-a3af-8ab8cf2ef8dd" in namespace "projected-6196" to be "success or failure"
Feb 10 09:15:33.344: INFO: Pod "pod-projected-configmaps-b35d3b96-9b1a-4411-a3af-8ab8cf2ef8dd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.611803ms
Feb 10 09:15:35.357: INFO: Pod "pod-projected-configmaps-b35d3b96-9b1a-4411-a3af-8ab8cf2ef8dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026365632s
Feb 10 09:15:37.370: INFO: Pod "pod-projected-configmaps-b35d3b96-9b1a-4411-a3af-8ab8cf2ef8dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039053573s
Feb 10 09:15:39.381: INFO: Pod "pod-projected-configmaps-b35d3b96-9b1a-4411-a3af-8ab8cf2ef8dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050918682s
STEP: Saw pod success
Feb 10 09:15:39.382: INFO: Pod "pod-projected-configmaps-b35d3b96-9b1a-4411-a3af-8ab8cf2ef8dd" satisfied condition "success or failure"
Feb 10 09:15:39.392: INFO: Trying to get logs from node master2 pod pod-projected-configmaps-b35d3b96-9b1a-4411-a3af-8ab8cf2ef8dd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 09:15:39.789: INFO: Waiting for pod pod-projected-configmaps-b35d3b96-9b1a-4411-a3af-8ab8cf2ef8dd to disappear
Feb 10 09:15:39.799: INFO: Pod pod-projected-configmaps-b35d3b96-9b1a-4411-a3af-8ab8cf2ef8dd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:15:39.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6196" for this suite.

• [SLOW TEST:6.958 seconds]
[sig-storage] Projected configMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":278,"completed":149,"skipped":2452,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:15:39.838: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:15:47.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3225" for this suite.

• [SLOW TEST:7.491 seconds]
[sig-api-machinery] ResourceQuota
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":278,"completed":150,"skipped":2455,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:15:47.330: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 09:15:47.761: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e420d2d-6ec5-4c11-b924-2328d7755fd7" in namespace "downward-api-8234" to be "success or failure"
Feb 10 09:15:47.772: INFO: Pod "downwardapi-volume-4e420d2d-6ec5-4c11-b924-2328d7755fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.993814ms
Feb 10 09:15:49.789: INFO: Pod "downwardapi-volume-4e420d2d-6ec5-4c11-b924-2328d7755fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027610533s
Feb 10 09:15:51.806: INFO: Pod "downwardapi-volume-4e420d2d-6ec5-4c11-b924-2328d7755fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044599226s
Feb 10 09:15:53.827: INFO: Pod "downwardapi-volume-4e420d2d-6ec5-4c11-b924-2328d7755fd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066027445s
STEP: Saw pod success
Feb 10 09:15:53.827: INFO: Pod "downwardapi-volume-4e420d2d-6ec5-4c11-b924-2328d7755fd7" satisfied condition "success or failure"
Feb 10 09:15:53.838: INFO: Trying to get logs from node master3 pod downwardapi-volume-4e420d2d-6ec5-4c11-b924-2328d7755fd7 container client-container: <nil>
STEP: delete the pod
Feb 10 09:15:53.926: INFO: Waiting for pod downwardapi-volume-4e420d2d-6ec5-4c11-b924-2328d7755fd7 to disappear
Feb 10 09:15:53.941: INFO: Pod downwardapi-volume-4e420d2d-6ec5-4c11-b924-2328d7755fd7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:15:53.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8234" for this suite.

• [SLOW TEST:6.645 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":278,"completed":151,"skipped":2459,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:15:53.976: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:15:54.411: INFO: Creating ReplicaSet my-hostname-basic-93f0fbe7-ce42-4353-9001-25d7ae6481d0
Feb 10 09:15:54.438: INFO: Pod name my-hostname-basic-93f0fbe7-ce42-4353-9001-25d7ae6481d0: Found 0 pods out of 1
Feb 10 09:15:59.451: INFO: Pod name my-hostname-basic-93f0fbe7-ce42-4353-9001-25d7ae6481d0: Found 1 pods out of 1
Feb 10 09:15:59.451: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-93f0fbe7-ce42-4353-9001-25d7ae6481d0" is running
Feb 10 09:15:59.467: INFO: Pod "my-hostname-basic-93f0fbe7-ce42-4353-9001-25d7ae6481d0-klwqb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-10 09:15:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-10 09:15:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-10 09:15:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-10 09:15:54 +0000 UTC Reason: Message:}])
Feb 10 09:15:59.468: INFO: Trying to dial the pod
Feb 10 09:16:04.511: INFO: Controller my-hostname-basic-93f0fbe7-ce42-4353-9001-25d7ae6481d0: Got expected result from replica 1 [my-hostname-basic-93f0fbe7-ce42-4353-9001-25d7ae6481d0-klwqb]: "my-hostname-basic-93f0fbe7-ce42-4353-9001-25d7ae6481d0-klwqb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:16:04.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9465" for this suite.

• [SLOW TEST:10.571 seconds]
[sig-apps] ReplicaSet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":278,"completed":152,"skipped":2468,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:16:04.549: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:16:05.005: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3d6e6ecb-25f8-4366-8642-6da0b71f01af" in namespace "security-context-test-994" to be "success or failure"
Feb 10 09:16:05.015: INFO: Pod "busybox-readonly-false-3d6e6ecb-25f8-4366-8642-6da0b71f01af": Phase="Pending", Reason="", readiness=false. Elapsed: 9.809609ms
Feb 10 09:16:07.037: INFO: Pod "busybox-readonly-false-3d6e6ecb-25f8-4366-8642-6da0b71f01af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031806291s
Feb 10 09:16:09.051: INFO: Pod "busybox-readonly-false-3d6e6ecb-25f8-4366-8642-6da0b71f01af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046615004s
Feb 10 09:16:11.064: INFO: Pod "busybox-readonly-false-3d6e6ecb-25f8-4366-8642-6da0b71f01af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059697806s
Feb 10 09:16:11.065: INFO: Pod "busybox-readonly-false-3d6e6ecb-25f8-4366-8642-6da0b71f01af" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:16:11.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-994" for this suite.

• [SLOW TEST:6.551 seconds]
[k8s.io] Security Context
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  When creating a pod with readOnlyRootFilesystem
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:164
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":278,"completed":153,"skipped":2488,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:16:11.101: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 10 09:16:18.131: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9d65b887-1bb9-4743-a749-9be176e9b536"
Feb 10 09:16:18.131: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9d65b887-1bb9-4743-a749-9be176e9b536" in namespace "pods-581" to be "terminated due to deadline exceeded"
Feb 10 09:16:18.143: INFO: Pod "pod-update-activedeadlineseconds-9d65b887-1bb9-4743-a749-9be176e9b536": Phase="Running", Reason="", readiness=true. Elapsed: 11.586794ms
Feb 10 09:16:20.156: INFO: Pod "pod-update-activedeadlineseconds-9d65b887-1bb9-4743-a749-9be176e9b536": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.025162718s
Feb 10 09:16:20.156: INFO: Pod "pod-update-activedeadlineseconds-9d65b887-1bb9-4743-a749-9be176e9b536" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:16:20.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-581" for this suite.

• [SLOW TEST:9.102 seconds]
[k8s.io] Pods
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":278,"completed":154,"skipped":2491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:16:20.205: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-3759
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 10 09:16:20.615: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 10 09:16:40.916: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.156.161.198 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3759 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 09:16:40.916: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 09:16:42.781: INFO: Found all expected endpoints: [netserver-0]
Feb 10 09:16:42.795: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.156.208.82 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3759 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 09:16:42.796: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 09:16:44.584: INFO: Found all expected endpoints: [netserver-1]
Feb 10 09:16:44.596: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.156.32.180 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3759 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 09:16:44.596: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 09:16:46.395: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:16:46.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3759" for this suite.

• [SLOW TEST:26.229 seconds]
[sig-network] Networking
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":155,"skipped":2523,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:16:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-3351
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:16:47.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3351" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":278,"completed":156,"skipped":2534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:16:47.075: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 10 09:16:53.578: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-754024871 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 10 09:17:09.218: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:17:09.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-638" for this suite.

• [SLOW TEST:22.197 seconds]
[k8s.io] [sig-node] Pods Extended
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":278,"completed":157,"skipped":2589,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:17:09.276: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-2194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb 10 09:17:09.734: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Feb 10 09:17:51.808: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Feb 10 09:17:53.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:17:55.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:17:57.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:17:59.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:01.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:03.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:05.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:07.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:09.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:11.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:13.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:15.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:17.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:19.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:21.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:23.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:25.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:27.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:29.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:31.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:33.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:35.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:37.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:39.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:41.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:43.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:45.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:47.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:49.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:51.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:53.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:55.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:57.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:18:59.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:01.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:03.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:05.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:07.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:09.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:11.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:13.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:15.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:17.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:19.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:21.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:23.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:25.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:27.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:29.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:31.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:33.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:35.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:37.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:39.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:41.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:43.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:45.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:47.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:49.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:51.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:53.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:55.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:57.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:19:59.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:01.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:03.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:05.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:07.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:09.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:11.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:13.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:15.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:17.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:19.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:21.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:24.007: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:25.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:27.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:29.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:31.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:33.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:35.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:37.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:39.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:41.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:43.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:45.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:47.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:49.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:51.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:53.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:55.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:57.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:20:59.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:01.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:03.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:05.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:07.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:09.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:11.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:13.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:15.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:17.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:19.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:21.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:23.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:25.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:27.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:29.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:31.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:33.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:35.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:37.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:39.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:41.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:43.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:45.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:47.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:49.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:51.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:53.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:55.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:57.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:21:59.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:22:01.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:22:03.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:22:05.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:22:07.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:22:09.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:22:11.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923071, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:22:42.608: INFO: Waited 28.587263867s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:22:43.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2194" for this suite.

• [SLOW TEST:334.078 seconds]
[sig-api-machinery] Aggregator
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":278,"completed":158,"skipped":2607,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:22:43.366: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 10 09:22:43.938: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9438 /api/v1/namespaces/watch-9438/configmaps/e2e-watch-test-resource-version 892fc76b-6b26-4879-aade-f91486e9349f 4659514 0 2020-02-10 09:22:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 10 09:22:43.939: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9438 /api/v1/namespaces/watch-9438/configmaps/e2e-watch-test-resource-version 892fc76b-6b26-4879-aade-f91486e9349f 4659515 0 2020-02-10 09:22:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:22:43.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9438" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":278,"completed":159,"skipped":2629,"failed":0}
SS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:22:43.981: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:22:44.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5125" for this suite.
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":278,"completed":160,"skipped":2631,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:22:44.482: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4309
[It] should have a working scale subresource [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-4309
Feb 10 09:22:44.955: INFO: Found 0 stateful pods, waiting for 1
Feb 10 09:22:54.972: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 10 09:22:55.032: INFO: Deleting all statefulset in ns statefulset-4309
Feb 10 09:22:55.040: INFO: Scaling statefulset ss to 0
Feb 10 09:23:05.138: INFO: Waiting for statefulset status.replicas updated to 0
Feb 10 09:23:05.148: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:23:05.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4309" for this suite.

• [SLOW TEST:20.760 seconds]
[sig-apps] StatefulSet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":278,"completed":161,"skipped":2666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:23:05.245: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3449
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 10 09:23:12.613: INFO: Successfully updated pod "labelsupdatee97da0df-f770-4a23-aa36-e1cc5cce8a49"
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:23:14.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3449" for this suite.

• [SLOW TEST:9.494 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":278,"completed":162,"skipped":2715,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:23:14.740: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create a job from an image, then delete the job  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Feb 10 09:23:15.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=kubectl-3606 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 10 09:23:23.724: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 10 09:23:23.725: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:23:25.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3606" for this suite.

• [SLOW TEST:11.060 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1924
    should create a job from an image, then delete the job  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":278,"completed":163,"skipped":2716,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:23:25.801: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 10 09:23:38.359: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 10 09:23:38.369: INFO: Pod pod-with-prestop-http-hook still exists
Feb 10 09:23:40.370: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 10 09:23:40.382: INFO: Pod pod-with-prestop-http-hook still exists
Feb 10 09:23:42.370: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 10 09:23:42.383: INFO: Pod pod-with-prestop-http-hook still exists
Feb 10 09:23:44.370: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 10 09:23:44.383: INFO: Pod pod-with-prestop-http-hook still exists
Feb 10 09:23:46.370: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 10 09:23:46.395: INFO: Pod pod-with-prestop-http-hook still exists
Feb 10 09:23:48.370: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 10 09:23:48.387: INFO: Pod pod-with-prestop-http-hook still exists
Feb 10 09:23:50.370: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 10 09:23:50.400: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:23:50.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9640" for this suite.

• [SLOW TEST:25.010 seconds]
[k8s.io] Container Lifecycle Hook
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":278,"completed":164,"skipped":2717,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:23:50.813: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4288
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-37cf72b2-63bc-4249-8707-67fe0dfd2a9a
STEP: Creating configMap with name cm-test-opt-upd-cd067ccd-c0f1-46a8-872c-2b4bea43a621
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-37cf72b2-63bc-4249-8707-67fe0dfd2a9a
STEP: Updating configmap cm-test-opt-upd-cd067ccd-c0f1-46a8-872c-2b4bea43a621
STEP: Creating configMap with name cm-test-opt-create-de6a7f4e-dbe9-451b-8218-6fec7f825167
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:25:31.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4288" for this suite.

• [SLOW TEST:100.691 seconds]
[sig-storage] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":165,"skipped":2732,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:25:31.506: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:25:31.902: INFO: Creating deployment "test-recreate-deployment"
Feb 10 09:25:31.921: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 10 09:25:31.937: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Feb 10 09:25:33.958: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 10 09:25:33.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923531, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923531, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923532, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923531, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:25:35.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923531, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923531, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923532, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923531, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:25:37.981: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 10 09:25:38.012: INFO: Updating deployment test-recreate-deployment
Feb 10 09:25:38.012: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 10 09:25:38.272: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7049 /apis/apps/v1/namespaces/deployment-7049/deployments/test-recreate-deployment 99d85e41-9858-40be-867d-0da3cff31fc4 4660352 2 2020-02-10 09:25:31 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00464cbf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-10 09:25:38 +0000 UTC,LastTransitionTime:2020-02-10 09:25:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-02-10 09:25:38 +0000 UTC,LastTransitionTime:2020-02-10 09:25:31 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 10 09:25:38.284: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-7049 /apis/apps/v1/namespaces/deployment-7049/replicasets/test-recreate-deployment-5f94c574ff 3135fd9e-7d12-4e36-9154-b2275fc3d801 4660351 1 2020-02-10 09:25:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 99d85e41-9858-40be-867d-0da3cff31fc4 0xc00464cfb7 0xc00464cfb8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00464d018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 10 09:25:38.284: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 10 09:25:38.284: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-7049 /apis/apps/v1/namespaces/deployment-7049/replicasets/test-recreate-deployment-799c574856 39131e57-9f1f-4346-8931-8182c204fc7b 4660341 2 2020-02-10 09:25:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 99d85e41-9858-40be-867d-0da3cff31fc4 0xc00464d087 0xc00464d088}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00464d0f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 10 09:25:38.297: INFO: Pod "test-recreate-deployment-5f94c574ff-zkkdf" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-zkkdf test-recreate-deployment-5f94c574ff- deployment-7049 /api/v1/namespaces/deployment-7049/pods/test-recreate-deployment-5f94c574ff-zkkdf 0215caa9-4c9f-4eba-b3d5-461c7cd9c1ab 4660353 0 2020-02-10 09:25:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 3135fd9e-7d12-4e36-9154-b2275fc3d801 0xc000b46747 0xc000b46748}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vmq7v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vmq7v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vmq7v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:25:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:,StartTime:2020-02-10 09:25:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:25:38.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7049" for this suite.

• [SLOW TEST:6.827 seconds]
[sig-apps] Deployment
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":278,"completed":166,"skipped":2753,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:25:38.339: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6741
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 10 09:25:38.744: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 09:25:57.011: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:26:54.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6741" for this suite.

• [SLOW TEST:75.921 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":278,"completed":167,"skipped":2762,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:26:54.262: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 09:26:54.687: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8146853-4ef0-43c9-a4aa-7359fec5f60c" in namespace "downward-api-585" to be "success or failure"
Feb 10 09:26:54.699: INFO: Pod "downwardapi-volume-d8146853-4ef0-43c9-a4aa-7359fec5f60c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.806844ms
Feb 10 09:26:56.711: INFO: Pod "downwardapi-volume-d8146853-4ef0-43c9-a4aa-7359fec5f60c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0232565s
Feb 10 09:26:58.722: INFO: Pod "downwardapi-volume-d8146853-4ef0-43c9-a4aa-7359fec5f60c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034199026s
Feb 10 09:27:00.734: INFO: Pod "downwardapi-volume-d8146853-4ef0-43c9-a4aa-7359fec5f60c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046489524s
STEP: Saw pod success
Feb 10 09:27:00.734: INFO: Pod "downwardapi-volume-d8146853-4ef0-43c9-a4aa-7359fec5f60c" satisfied condition "success or failure"
Feb 10 09:27:00.745: INFO: Trying to get logs from node master3 pod downwardapi-volume-d8146853-4ef0-43c9-a4aa-7359fec5f60c container client-container: <nil>
STEP: delete the pod
Feb 10 09:27:00.837: INFO: Waiting for pod downwardapi-volume-d8146853-4ef0-43c9-a4aa-7359fec5f60c to disappear
Feb 10 09:27:00.846: INFO: Pod downwardapi-volume-d8146853-4ef0-43c9-a4aa-7359fec5f60c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:27:00.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-585" for this suite.

• [SLOW TEST:6.616 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":278,"completed":168,"skipped":2791,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:27:00.878: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 10 09:27:01.408: INFO: Waiting up to 5m0s for pod "downward-api-7207690a-94f8-4cee-8367-587170fc0207" in namespace "downward-api-8028" to be "success or failure"
Feb 10 09:27:01.419: INFO: Pod "downward-api-7207690a-94f8-4cee-8367-587170fc0207": Phase="Pending", Reason="", readiness=false. Elapsed: 10.503983ms
Feb 10 09:27:03.430: INFO: Pod "downward-api-7207690a-94f8-4cee-8367-587170fc0207": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021783029s
Feb 10 09:27:05.446: INFO: Pod "downward-api-7207690a-94f8-4cee-8367-587170fc0207": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038114647s
Feb 10 09:27:07.458: INFO: Pod "downward-api-7207690a-94f8-4cee-8367-587170fc0207": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049897756s
STEP: Saw pod success
Feb 10 09:27:07.458: INFO: Pod "downward-api-7207690a-94f8-4cee-8367-587170fc0207" satisfied condition "success or failure"
Feb 10 09:27:07.469: INFO: Trying to get logs from node master3 pod downward-api-7207690a-94f8-4cee-8367-587170fc0207 container dapi-container: <nil>
STEP: delete the pod
Feb 10 09:27:07.548: INFO: Waiting for pod downward-api-7207690a-94f8-4cee-8367-587170fc0207 to disappear
Feb 10 09:27:07.557: INFO: Pod downward-api-7207690a-94f8-4cee-8367-587170fc0207 no longer exists
[AfterEach] [sig-node] Downward API
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:27:07.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8028" for this suite.

• [SLOW TEST:6.713 seconds]
[sig-node] Downward API
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":278,"completed":169,"skipped":2793,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:27:07.592: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8470
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:27:38.159: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 09:27:40.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923658, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923658, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923658, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923658, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:27:42.211: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923658, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923658, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923658, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923658, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:27:45.251: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:27:45.263: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1365-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:27:53.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8470" for this suite.
STEP: Destroying namespace "webhook-8470-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:46.465 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":278,"completed":170,"skipped":2799,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:27:54.057: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5348
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 10 09:27:54.527: INFO: Waiting up to 5m0s for pod "downward-api-0f736951-f662-4b83-8bdc-48d02fc7c025" in namespace "downward-api-5348" to be "success or failure"
Feb 10 09:27:54.537: INFO: Pod "downward-api-0f736951-f662-4b83-8bdc-48d02fc7c025": Phase="Pending", Reason="", readiness=false. Elapsed: 10.071342ms
Feb 10 09:27:56.556: INFO: Pod "downward-api-0f736951-f662-4b83-8bdc-48d02fc7c025": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029368246s
Feb 10 09:27:58.584: INFO: Pod "downward-api-0f736951-f662-4b83-8bdc-48d02fc7c025": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056622663s
Feb 10 09:28:00.597: INFO: Pod "downward-api-0f736951-f662-4b83-8bdc-48d02fc7c025": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070045109s
STEP: Saw pod success
Feb 10 09:28:00.597: INFO: Pod "downward-api-0f736951-f662-4b83-8bdc-48d02fc7c025" satisfied condition "success or failure"
Feb 10 09:28:00.607: INFO: Trying to get logs from node master3 pod downward-api-0f736951-f662-4b83-8bdc-48d02fc7c025 container dapi-container: <nil>
STEP: delete the pod
Feb 10 09:28:00.679: INFO: Waiting for pod downward-api-0f736951-f662-4b83-8bdc-48d02fc7c025 to disappear
Feb 10 09:28:00.703: INFO: Pod downward-api-0f736951-f662-4b83-8bdc-48d02fc7c025 no longer exists
[AfterEach] [sig-node] Downward API
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:28:00.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5348" for this suite.

• [SLOW TEST:6.684 seconds]
[sig-node] Downward API
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":278,"completed":171,"skipped":2830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:28:00.744: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5682
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:28:31.575: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Feb 10 09:28:33.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923711, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923711, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923711, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923711, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:28:35.630: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923711, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923711, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923711, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923711, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:28:38.668: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:28:38.681: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:28:47.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5682" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:47.191 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":278,"completed":172,"skipped":2863,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:28:47.940: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3791
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 10 09:28:48.384: INFO: Waiting up to 5m0s for pod "downward-api-5bbe1fba-321e-405d-8100-f12d9c7ebcba" in namespace "downward-api-3791" to be "success or failure"
Feb 10 09:28:48.395: INFO: Pod "downward-api-5bbe1fba-321e-405d-8100-f12d9c7ebcba": Phase="Pending", Reason="", readiness=false. Elapsed: 11.113001ms
Feb 10 09:28:50.407: INFO: Pod "downward-api-5bbe1fba-321e-405d-8100-f12d9c7ebcba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022986821s
Feb 10 09:28:52.427: INFO: Pod "downward-api-5bbe1fba-321e-405d-8100-f12d9c7ebcba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042969574s
Feb 10 09:28:54.445: INFO: Pod "downward-api-5bbe1fba-321e-405d-8100-f12d9c7ebcba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061144643s
STEP: Saw pod success
Feb 10 09:28:54.446: INFO: Pod "downward-api-5bbe1fba-321e-405d-8100-f12d9c7ebcba" satisfied condition "success or failure"
Feb 10 09:28:54.458: INFO: Trying to get logs from node master3 pod downward-api-5bbe1fba-321e-405d-8100-f12d9c7ebcba container dapi-container: <nil>
STEP: delete the pod
Feb 10 09:28:54.559: INFO: Waiting for pod downward-api-5bbe1fba-321e-405d-8100-f12d9c7ebcba to disappear
Feb 10 09:28:54.571: INFO: Pod downward-api-5bbe1fba-321e-405d-8100-f12d9c7ebcba no longer exists
[AfterEach] [sig-node] Downward API
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:28:54.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3791" for this suite.

• [SLOW TEST:6.669 seconds]
[sig-node] Downward API
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":278,"completed":173,"skipped":2913,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:28:54.612: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 10 09:29:01.645: INFO: Successfully updated pod "labelsupdateaf49cf2d-d5ff-4049-9788-6c0d304fde8c"
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:29:03.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6327" for this suite.

• [SLOW TEST:9.150 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":278,"completed":174,"skipped":2959,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:29:03.766: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9277
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:29:20.978: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 10 09:29:23.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923761, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923761, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923761, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923760, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:29:25.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923761, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923761, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923761, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923760, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:29:28.080: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:29:28.092: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6153-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:29:35.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9277" for this suite.
STEP: Destroying namespace "webhook-9277-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:31.850 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":278,"completed":175,"skipped":2979,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:29:35.617: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3745
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 10 09:29:36.049: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:29:43.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3745" for this suite.

• [SLOW TEST:8.240 seconds]
[k8s.io] InitContainer [NodeConformance]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartNever pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":278,"completed":176,"skipped":2983,"failed":0}
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:29:43.857: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Feb 10 09:29:44.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 cluster-info'
Feb 10 09:29:44.751: INFO: stderr: ""
Feb 10 09:29:44.751: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.155.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.155.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.155.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:29:44.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7642" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":278,"completed":177,"skipped":2983,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:29:44.792: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 10 09:29:45.245: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 10 09:29:50.256: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:29:51.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3382" for this suite.

• [SLOW TEST:6.569 seconds]
[sig-apps] ReplicationController
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":278,"completed":178,"skipped":2985,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:29:51.365: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-613
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl logs
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
STEP: creating an pod
Feb 10 09:29:51.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-613 -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 10 09:29:52.442: INFO: stderr: ""
Feb 10 09:29:52.443: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Feb 10 09:29:52.444: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 10 09:29:52.444: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-613" to be "running and ready, or succeeded"
Feb 10 09:29:52.461: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.213007ms
Feb 10 09:29:54.483: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039312151s
Feb 10 09:29:56.495: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050676259s
Feb 10 09:29:58.511: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 6.066923857s
Feb 10 09:29:58.511: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 10 09:29:58.511: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 10 09:29:58.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 logs logs-generator logs-generator --namespace=kubectl-613'
Feb 10 09:29:59.580: INFO: stderr: ""
Feb 10 09:29:59.581: INFO: stdout: "I0210 09:29:56.516489       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/hd9 444\nI0210 09:29:56.716627       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/b8jf 477\nI0210 09:29:56.916605       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/zmhq 304\nI0210 09:29:57.116592       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/l4f 200\nI0210 09:29:57.316837       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/r9qz 467\nI0210 09:29:57.516613       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/66b 250\nI0210 09:29:57.722644       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/v4g 224\nI0210 09:29:57.916592       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/m68k 377\nI0210 09:29:58.116615       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/dth 511\nI0210 09:29:58.316584       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/l2p 230\nI0210 09:29:58.516546       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/8qh 456\nI0210 09:29:58.716631       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/g55 215\nI0210 09:29:58.916609       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/4xr 494\nI0210 09:29:59.117207       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/mrk4 447\nI0210 09:29:59.320464       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/jpbg 201\nI0210 09:29:59.516642       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/smn 245\n"
STEP: limiting log lines
Feb 10 09:29:59.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 logs logs-generator logs-generator --namespace=kubectl-613 --tail=1'
Feb 10 09:30:00.173: INFO: stderr: ""
Feb 10 09:30:00.173: INFO: stdout: "I0210 09:30:00.116533       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/w57 443\n"
Feb 10 09:30:00.173: INFO: got output "I0210 09:30:00.116533       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/w57 443\n"
STEP: limiting log bytes
Feb 10 09:30:00.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 logs logs-generator logs-generator --namespace=kubectl-613 --limit-bytes=1'
Feb 10 09:30:01.064: INFO: stderr: ""
Feb 10 09:30:01.064: INFO: stdout: "I"
Feb 10 09:30:01.064: INFO: got output "I"
STEP: exposing timestamps
Feb 10 09:30:01.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 logs logs-generator logs-generator --namespace=kubectl-613 --tail=1 --timestamps'
Feb 10 09:30:01.640: INFO: stderr: ""
Feb 10 09:30:01.641: INFO: stdout: "2020-02-10T09:30:01.527401608Z I0210 09:30:01.519590       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/6z8 473\n"
Feb 10 09:30:01.641: INFO: got output "2020-02-10T09:30:01.527401608Z I0210 09:30:01.519590       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/6z8 473\n"
STEP: restricting to a time range
Feb 10 09:30:04.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 logs logs-generator logs-generator --namespace=kubectl-613 --since=1s'
Feb 10 09:30:04.716: INFO: stderr: ""
Feb 10 09:30:04.716: INFO: stdout: "I0210 09:30:03.716601       1 logs_generator.go:76] 36 PUT /api/v1/namespaces/ns/pods/84fm 391\nI0210 09:30:03.916622       1 logs_generator.go:76] 37 POST /api/v1/namespaces/kube-system/pods/vlgc 556\nI0210 09:30:04.117458       1 logs_generator.go:76] 38 POST /api/v1/namespaces/kube-system/pods/wdg 482\nI0210 09:30:04.316622       1 logs_generator.go:76] 39 PUT /api/v1/namespaces/default/pods/2wp 375\nI0210 09:30:04.516498       1 logs_generator.go:76] 40 POST /api/v1/namespaces/kube-system/pods/87qm 545\n"
Feb 10 09:30:04.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 logs logs-generator logs-generator --namespace=kubectl-613 --since=24h'
Feb 10 09:30:05.282: INFO: stderr: ""
Feb 10 09:30:05.283: INFO: stdout: "I0210 09:29:56.516489       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/hd9 444\nI0210 09:29:56.716627       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/b8jf 477\nI0210 09:29:56.916605       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/zmhq 304\nI0210 09:29:57.116592       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/l4f 200\nI0210 09:29:57.316837       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/r9qz 467\nI0210 09:29:57.516613       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/66b 250\nI0210 09:29:57.722644       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/v4g 224\nI0210 09:29:57.916592       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/m68k 377\nI0210 09:29:58.116615       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/dth 511\nI0210 09:29:58.316584       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/l2p 230\nI0210 09:29:58.516546       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/8qh 456\nI0210 09:29:58.716631       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/g55 215\nI0210 09:29:58.916609       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/4xr 494\nI0210 09:29:59.117207       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/mrk4 447\nI0210 09:29:59.320464       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/jpbg 201\nI0210 09:29:59.516642       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/smn 245\nI0210 09:29:59.716592       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/67s 358\nI0210 09:29:59.916543       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/7kzd 206\nI0210 09:30:00.116533       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/w57 443\nI0210 09:30:00.316536       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/nlv8 388\nI0210 09:30:00.516568       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/dmfz 522\nI0210 09:30:00.718848       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/xnx 423\nI0210 09:30:00.916555       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/hgdx 577\nI0210 09:30:01.116529       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/8np 520\nI0210 09:30:01.316523       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/dslw 394\nI0210 09:30:01.519590       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/6z8 473\nI0210 09:30:01.716555       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/f7jn 576\nI0210 09:30:01.918136       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/p9st 407\nI0210 09:30:02.119906       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/765h 375\nI0210 09:30:02.316996       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/pnnw 295\nI0210 09:30:02.516546       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/5nfx 353\nI0210 09:30:02.716602       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/kube-system/pods/sbt 537\nI0210 09:30:02.916569       1 logs_generator.go:76] 32 GET /api/v1/namespaces/kube-system/pods/v25 254\nI0210 09:30:03.116533       1 logs_generator.go:76] 33 POST /api/v1/namespaces/default/pods/hn9v 303\nI0210 09:30:03.316656       1 logs_generator.go:76] 34 POST /api/v1/namespaces/kube-system/pods/9mk 505\nI0210 09:30:03.516610       1 logs_generator.go:76] 35 GET /api/v1/namespaces/ns/pods/lzc6 365\nI0210 09:30:03.716601       1 logs_generator.go:76] 36 PUT /api/v1/namespaces/ns/pods/84fm 391\nI0210 09:30:03.916622       1 logs_generator.go:76] 37 POST /api/v1/namespaces/kube-system/pods/vlgc 556\nI0210 09:30:04.117458       1 logs_generator.go:76] 38 POST /api/v1/namespaces/kube-system/pods/wdg 482\nI0210 09:30:04.316622       1 logs_generator.go:76] 39 PUT /api/v1/namespaces/default/pods/2wp 375\nI0210 09:30:04.516498       1 logs_generator.go:76] 40 POST /api/v1/namespaces/kube-system/pods/87qm 545\nI0210 09:30:04.716576       1 logs_generator.go:76] 41 POST /api/v1/namespaces/kube-system/pods/fdx 472\nI0210 09:30:04.916559       1 logs_generator.go:76] 42 GET /api/v1/namespaces/kube-system/pods/6c4 365\nI0210 09:30:05.116635       1 logs_generator.go:76] 43 GET /api/v1/namespaces/kube-system/pods/256c 571\n"
[AfterEach] Kubectl logs
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
Feb 10 09:30:05.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete pod logs-generator --namespace=kubectl-613'
Feb 10 09:30:09.602: INFO: stderr: ""
Feb 10 09:30:09.603: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:30:09.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-613" for this suite.

• [SLOW TEST:18.278 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1440
    should be able to retrieve and filter logs  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":278,"completed":179,"skipped":2992,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:30:09.645: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Feb 10 09:30:10.063: INFO: Waiting up to 5m0s for pod "client-containers-b6e8fb60-7a3e-4492-8de2-b4c2b2fcaa43" in namespace "containers-5967" to be "success or failure"
Feb 10 09:30:10.074: INFO: Pod "client-containers-b6e8fb60-7a3e-4492-8de2-b4c2b2fcaa43": Phase="Pending", Reason="", readiness=false. Elapsed: 10.566855ms
Feb 10 09:30:12.086: INFO: Pod "client-containers-b6e8fb60-7a3e-4492-8de2-b4c2b2fcaa43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023138364s
Feb 10 09:30:14.103: INFO: Pod "client-containers-b6e8fb60-7a3e-4492-8de2-b4c2b2fcaa43": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040002002s
Feb 10 09:30:16.122: INFO: Pod "client-containers-b6e8fb60-7a3e-4492-8de2-b4c2b2fcaa43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058809164s
STEP: Saw pod success
Feb 10 09:30:16.122: INFO: Pod "client-containers-b6e8fb60-7a3e-4492-8de2-b4c2b2fcaa43" satisfied condition "success or failure"
Feb 10 09:30:16.135: INFO: Trying to get logs from node master3 pod client-containers-b6e8fb60-7a3e-4492-8de2-b4c2b2fcaa43 container test-container: <nil>
STEP: delete the pod
Feb 10 09:30:16.253: INFO: Waiting for pod client-containers-b6e8fb60-7a3e-4492-8de2-b4c2b2fcaa43 to disappear
Feb 10 09:30:16.264: INFO: Pod client-containers-b6e8fb60-7a3e-4492-8de2-b4c2b2fcaa43 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:30:16.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5967" for this suite.

• [SLOW TEST:6.661 seconds]
[k8s.io] Docker Containers
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":278,"completed":180,"skipped":2996,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:30:16.307: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check is all data is printed  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:30:16.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 version'
Feb 10 09:30:17.236: INFO: stderr: ""
Feb 10 09:30:17.237: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\", GitCommit:\"70132b0f130acc0bed193d9ba59dd186f0e634cf\", GitTreeState:\"archive\", BuildDate:\"2020-02-08T03:11:07Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/mips64le\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\", GitCommit:\"70132b0f130acc0bed193d9ba59dd186f0e634cf\", GitTreeState:\"archive\", BuildDate:\"2020-02-05T15:05:59Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/mips64le\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:30:17.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2821" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":278,"completed":181,"skipped":2998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:30:17.283: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7288
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 10 09:30:17.716: INFO: Waiting up to 5m0s for pod "pod-0b7bf692-cb63-40f4-8889-5c34cf5e9021" in namespace "emptydir-7288" to be "success or failure"
Feb 10 09:30:17.726: INFO: Pod "pod-0b7bf692-cb63-40f4-8889-5c34cf5e9021": Phase="Pending", Reason="", readiness=false. Elapsed: 10.495875ms
Feb 10 09:30:19.741: INFO: Pod "pod-0b7bf692-cb63-40f4-8889-5c34cf5e9021": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025196659s
Feb 10 09:30:21.762: INFO: Pod "pod-0b7bf692-cb63-40f4-8889-5c34cf5e9021": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045951814s
Feb 10 09:30:23.776: INFO: Pod "pod-0b7bf692-cb63-40f4-8889-5c34cf5e9021": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060530743s
STEP: Saw pod success
Feb 10 09:30:23.776: INFO: Pod "pod-0b7bf692-cb63-40f4-8889-5c34cf5e9021" satisfied condition "success or failure"
Feb 10 09:30:23.787: INFO: Trying to get logs from node master3 pod pod-0b7bf692-cb63-40f4-8889-5c34cf5e9021 container test-container: <nil>
STEP: delete the pod
Feb 10 09:30:23.879: INFO: Waiting for pod pod-0b7bf692-cb63-40f4-8889-5c34cf5e9021 to disappear
Feb 10 09:30:23.890: INFO: Pod pod-0b7bf692-cb63-40f4-8889-5c34cf5e9021 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:30:23.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7288" for this suite.

• [SLOW TEST:6.641 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":182,"skipped":3022,"failed":0}
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:30:23.925: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6593
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run default
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create an rc or deployment from an image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 10 09:30:24.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6593'
Feb 10 09:30:24.923: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 10 09:30:24.923: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1582
Feb 10 09:30:26.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6593'
Feb 10 09:30:27.523: INFO: stderr: ""
Feb 10 09:30:27.523: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:30:27.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6593" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":278,"completed":183,"skipped":3022,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:30:27.566: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-31
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:31:02.300: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 09:31:04.348: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923862, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923862, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923862, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923862, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:31:06.370: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923862, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923862, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923862, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716923862, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:31:09.396: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:31:10.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-31" for this suite.
STEP: Destroying namespace "webhook-31-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:42.794 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":278,"completed":184,"skipped":3025,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:31:10.382: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-5406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:31:10.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5406" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":278,"completed":185,"skipped":3072,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:31:10.862: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 10 09:31:11.343: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8775 /api/v1/namespaces/watch-8775/configmaps/e2e-watch-test-watch-closed 63203c73-d91f-4071-a223-16e84132fc84 4662130 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 10 09:31:11.343: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8775 /api/v1/namespaces/watch-8775/configmaps/e2e-watch-test-watch-closed 63203c73-d91f-4071-a223-16e84132fc84 4662132 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 10 09:31:11.399: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8775 /api/v1/namespaces/watch-8775/configmaps/e2e-watch-test-watch-closed 63203c73-d91f-4071-a223-16e84132fc84 4662133 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 10 09:31:11.400: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8775 /api/v1/namespaces/watch-8775/configmaps/e2e-watch-test-watch-closed 63203c73-d91f-4071-a223-16e84132fc84 4662134 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:31:11.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8775" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":278,"completed":186,"skipped":3076,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:31:11.437: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:31:11.833: INFO: Creating deployment "webserver-deployment"
Feb 10 09:31:11.866: INFO: Waiting for observed generation 1
Feb 10 09:31:13.911: INFO: Waiting for all required pods to come up
Feb 10 09:31:13.931: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 10 09:31:21.972: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 10 09:31:21.996: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 10 09:31:22.023: INFO: Updating deployment webserver-deployment
Feb 10 09:31:22.023: INFO: Waiting for observed generation 2
Feb 10 09:31:24.062: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 10 09:31:24.074: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 10 09:31:24.090: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 10 09:31:24.121: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 10 09:31:24.122: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 10 09:31:24.135: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 10 09:31:24.152: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 10 09:31:24.152: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 10 09:31:24.187: INFO: Updating deployment webserver-deployment
Feb 10 09:31:24.187: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 10 09:31:24.207: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 10 09:31:24.217: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 10 09:31:26.269: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6762 /apis/apps/v1/namespaces/deployment-6762/deployments/webserver-deployment 78ed034c-02f8-4564-9334-eec6e0fa06bb 4662504 3 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003c8b748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-10 09:31:24 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-02-10 09:31:24 +0000 UTC,LastTransitionTime:2020-02-10 09:31:11 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 10 09:31:26.281: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-6762 /apis/apps/v1/namespaces/deployment-6762/replicasets/webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 4662496 3 2020-02-10 09:31:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 78ed034c-02f8-4564-9334-eec6e0fa06bb 0xc003c8bc37 0xc003c8bc38}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003c8bca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 10 09:31:26.282: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 10 09:31:26.282: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-6762 /apis/apps/v1/namespaces/deployment-6762/replicasets/webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 4662490 3 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 78ed034c-02f8-4564-9334-eec6e0fa06bb 0xc003c8bb77 0xc003c8bb78}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003c8bbd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 10 09:31:26.316: INFO: Pod "webserver-deployment-595b5b9587-4ht8q" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4ht8q webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-4ht8q 4d37ffe3-f6a5-4059-b680-19a17850d9d2 4662338 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461c177 0xc00461c178}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:10.156.161.206,StartTime:2020-02-10 09:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 09:31:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://75c5513a92fd72a49c6d2f110736ad6553f2dbba4f1e7f50c50c7c6067703b1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.161.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.318: INFO: Pod "webserver-deployment-595b5b9587-4ngrc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4ngrc webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-4ngrc e66c93ff-2e01-44e4-a7c7-e6b632cc8864 4662356 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461c2f7 0xc00461c2f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:10.156.32.205,StartTime:2020-02-10 09:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 09:31:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://c8ef87fa8e0fbdaac967b29124f41c55cde7ae33dc7581734477b7960dda02e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.32.205,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.320: INFO: Pod "webserver-deployment-595b5b9587-5gwqm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5gwqm webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-5gwqm 2b7fb1fa-f0e6-44fd-9166-4fc65d875ce6 4662511 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461c487 0xc00461c488}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.321: INFO: Pod "webserver-deployment-595b5b9587-67qgc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-67qgc webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-67qgc fdbe5af1-005c-4b64-80b8-875f224372d6 4662534 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461c5e7 0xc00461c5e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.324: INFO: Pod "webserver-deployment-595b5b9587-7jq6b" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7jq6b webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-7jq6b 1ead79df-7b59-4205-b01a-7323e8feb61b 4662516 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461c747 0xc00461c748}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.136,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.326: INFO: Pod "webserver-deployment-595b5b9587-bmzbt" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bmzbt webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-bmzbt 26d72208-ea57-46e8-88aa-338159af26af 4662513 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461c8a7 0xc00461c8a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.136,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.328: INFO: Pod "webserver-deployment-595b5b9587-bwqcp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bwqcp webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-bwqcp c2c2af7c-9cb8-4230-8d32-2fcddf25d008 4662493 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461ca27 0xc00461ca28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.330: INFO: Pod "webserver-deployment-595b5b9587-ctv6s" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ctv6s webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-ctv6s 335667b3-5d39-41ea-98a5-b9ba575d749c 4662509 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461cb87 0xc00461cb88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.332: INFO: Pod "webserver-deployment-595b5b9587-dnm65" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dnm65 webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-dnm65 354e5769-7ac4-4284-a248-e22e9187554a 4662502 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461cce7 0xc00461cce8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.337: INFO: Pod "webserver-deployment-595b5b9587-fwhgb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fwhgb webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-fwhgb 43d67c5f-ce9d-441a-bb6b-7843897e9900 4662347 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461ce57 0xc00461ce58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:10.156.32.204,StartTime:2020-02-10 09:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 09:31:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://2e28161dcf8879b28906343998c4492dd8cb72f28d6ff4f0c93d1eaa096e4475,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.32.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.338: INFO: Pod "webserver-deployment-595b5b9587-n8n6h" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n8n6h webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-n8n6h 7ef44b92-0e3a-471c-84a0-56d8c72d7ba9 4662332 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461cfd7 0xc00461cfd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:10.156.161.207,StartTime:2020-02-10 09:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 09:31:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://ca8908ddc75172263245bc4b60e71e37c7ebbb3dda4c62db96987ce70b8e83a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.161.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.340: INFO: Pod "webserver-deployment-595b5b9587-nxhhz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nxhhz webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-nxhhz 18b0dfb0-4697-4102-b67e-60e2aab12f39 4662529 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461d157 0xc00461d158}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.342: INFO: Pod "webserver-deployment-595b5b9587-qgc4m" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qgc4m webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-qgc4m 5c4d874e-f0ac-4675-bb8d-ae3c647dc145 4662543 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461d2b7 0xc00461d2b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:,StartTime:2020-02-10 09:31:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.344: INFO: Pod "webserver-deployment-595b5b9587-rjtzg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rjtzg webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-rjtzg a6926b1e-c3d1-439a-bccd-f716bf79ec0b 4662538 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461d427 0xc00461d428}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:,StartTime:2020-02-10 09:31:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.352: INFO: Pod "webserver-deployment-595b5b9587-tgvft" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tgvft webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-tgvft 7139fdad-137f-4da9-ba42-58e1dbf1873c 4662527 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461d587 0xc00461d588}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.136,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.354: INFO: Pod "webserver-deployment-595b5b9587-trg4v" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-trg4v webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-trg4v cd16d4df-b7c4-49d9-9c43-e029b723d05b 4662320 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461d6e7 0xc00461d6e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.136,PodIP:10.156.208.83,StartTime:2020-02-10 09:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 09:31:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://18c26784798f31881322409df44b121a969201563f7ce08372009af047a4fad0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.208.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.356: INFO: Pod "webserver-deployment-595b5b9587-tx4kw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tx4kw webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-tx4kw 888d29bf-485f-4c73-912a-1e3db99d73a7 4662318 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461d867 0xc00461d868}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.136,PodIP:10.156.208.84,StartTime:2020-02-10 09:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 09:31:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://ec0b2acc50df3e6cd566632ea118cc2e4dfd0d489dac7f10b2eef097b02f470b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.208.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.357: INFO: Pod "webserver-deployment-595b5b9587-vc2m4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vc2m4 webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-vc2m4 f2b1afaf-f544-41c7-bdc7-d3dd9b751461 4662546 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461d9e7 0xc00461d9e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:,StartTime:2020-02-10 09:31:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.359: INFO: Pod "webserver-deployment-595b5b9587-vvmln" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vvmln webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-vvmln 68fa590d-81aa-4c42-842e-451a7bfd718a 4662341 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461db47 0xc00461db48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.136,PodIP:10.156.208.85,StartTime:2020-02-10 09:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 09:31:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://51a694d5277928db44ac49b5184f2c107bbff66adccc84a203722bed33a2d344,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.208.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.361: INFO: Pod "webserver-deployment-595b5b9587-w4z68" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-w4z68 webserver-deployment-595b5b9587- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-595b5b9587-w4z68 5ebdd9e2-39c1-4ee7-a366-957d14783d80 4662335 0 2020-02-10 09:31:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fefba1a6-5b45-4749-9f12-9a12b39c3e7e 0xc00461dcc7 0xc00461dcc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:10.156.161.208,StartTime:2020-02-10 09:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 09:31:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da,ContainerID:docker://d2b2abf4f4f065862240f0125d880aa44dc8076549ffc06633974485e151b68a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.161.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.363: INFO: Pod "webserver-deployment-c7997dcc8-44qhl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-44qhl webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-44qhl dd821490-07dd-493e-a894-17130ebeb974 4662507 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc00461de47 0xc00461de48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.136,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.365: INFO: Pod "webserver-deployment-c7997dcc8-82zz5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-82zz5 webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-82zz5 306c1640-ff1d-4354-a56c-21cc487a1f92 4662397 0 2020-02-10 09:31:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc00461dfc7 0xc00461dfc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.136,PodIP:,StartTime:2020-02-10 09:31:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.367: INFO: Pod "webserver-deployment-c7997dcc8-8bq4w" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8bq4w webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-8bq4w 2867f673-ce5c-47a2-9f7e-6a316265a8bd 4662414 0 2020-02-10 09:31:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc003070147 0xc003070148}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:,StartTime:2020-02-10 09:31:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.368: INFO: Pod "webserver-deployment-c7997dcc8-bxpn8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bxpn8 webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-bxpn8 85717bbc-b130-4863-b10b-dd0881b64516 4662540 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc0030702c7 0xc0030702c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:,StartTime:2020-02-10 09:31:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.370: INFO: Pod "webserver-deployment-c7997dcc8-csp6m" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-csp6m webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-csp6m d2654708-11e9-4344-8464-0e985c39568c 4662532 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc003070447 0xc003070448}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.372: INFO: Pod "webserver-deployment-c7997dcc8-flkkv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-flkkv webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-flkkv b01a853c-050e-4f0a-87ec-55f20704c917 4662533 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc0030705d7 0xc0030705d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.136,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.374: INFO: Pod "webserver-deployment-c7997dcc8-mcpwk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-mcpwk webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-mcpwk 7e24d4f7-e7e3-47c5-ade0-87da36d0727e 4662531 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc003070757 0xc003070758}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.136,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.375: INFO: Pod "webserver-deployment-c7997dcc8-pqz8r" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pqz8r webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-pqz8r 2f3db9a1-4216-4065-bfac-8e3de352dc5d 4662407 0 2020-02-10 09:31:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc0030708f7 0xc0030708f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:,StartTime:2020-02-10 09:31:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.378: INFO: Pod "webserver-deployment-c7997dcc8-rkpg5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rkpg5 webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-rkpg5 3153efbd-e3dd-4295-aaf6-53e097895639 4662548 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc003070a77 0xc003070a78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:,StartTime:2020-02-10 09:31:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.380: INFO: Pod "webserver-deployment-c7997dcc8-spz2q" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-spz2q webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-spz2q 2843bb2f-b46d-4fab-90e9-b7d7869a04ce 4662549 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc003070c07 0xc003070c08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:,StartTime:2020-02-10 09:31:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.381: INFO: Pod "webserver-deployment-c7997dcc8-vrqfn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vrqfn webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-vrqfn f1b48ea6-b3af-48ae-9bc7-6738f5251d52 4662519 0 2020-02-10 09:31:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc003070d87 0xc003070d88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:,StartTime:2020-02-10 09:31:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.384: INFO: Pod "webserver-deployment-c7997dcc8-xrkp4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xrkp4 webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-xrkp4 81419d13-f1b1-4f96-a86f-20de2048635c 4662394 0 2020-02-10 09:31:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc003070f07 0xc003070f08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.137,PodIP:,StartTime:2020-02-10 09:31:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 10 09:31:26.385: INFO: Pod "webserver-deployment-c7997dcc8-z5866" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z5866 webserver-deployment-c7997dcc8- deployment-6762 /api/v1/namespaces/deployment-6762/pods/webserver-deployment-c7997dcc8-z5866 3e2badc9-9a8e-44cc-93cf-7dd6f0e646c4 4662415 0 2020-02-10 09:31:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 6eeb9704-c889-4fe0-aa45-a957f0f7803d 0xc003071097 0xc003071098}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cjsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cjsh8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cjsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 09:31:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:,StartTime:2020-02-10 09:31:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:31:26.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6762" for this suite.

• [SLOW TEST:14.994 seconds]
[sig-apps] Deployment
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":278,"completed":187,"skipped":3091,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:31:26.434: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6772
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 09:31:27.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af1e8b01-e1ea-455d-83fc-2e3d8182930a" in namespace "downward-api-6772" to be "success or failure"
Feb 10 09:31:27.126: INFO: Pod "downwardapi-volume-af1e8b01-e1ea-455d-83fc-2e3d8182930a": Phase="Pending", Reason="", readiness=false. Elapsed: 29.428595ms
Feb 10 09:31:29.146: INFO: Pod "downwardapi-volume-af1e8b01-e1ea-455d-83fc-2e3d8182930a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049365693s
Feb 10 09:31:31.175: INFO: Pod "downwardapi-volume-af1e8b01-e1ea-455d-83fc-2e3d8182930a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077805165s
Feb 10 09:31:33.200: INFO: Pod "downwardapi-volume-af1e8b01-e1ea-455d-83fc-2e3d8182930a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.103608339s
Feb 10 09:31:35.229: INFO: Pod "downwardapi-volume-af1e8b01-e1ea-455d-83fc-2e3d8182930a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.13250916s
STEP: Saw pod success
Feb 10 09:31:35.230: INFO: Pod "downwardapi-volume-af1e8b01-e1ea-455d-83fc-2e3d8182930a" satisfied condition "success or failure"
Feb 10 09:31:35.253: INFO: Trying to get logs from node master1 pod downwardapi-volume-af1e8b01-e1ea-455d-83fc-2e3d8182930a container client-container: <nil>
STEP: delete the pod
Feb 10 09:31:35.817: INFO: Waiting for pod downwardapi-volume-af1e8b01-e1ea-455d-83fc-2e3d8182930a to disappear
Feb 10 09:31:35.846: INFO: Pod downwardapi-volume-af1e8b01-e1ea-455d-83fc-2e3d8182930a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:31:35.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6772" for this suite.

• [SLOW TEST:9.485 seconds]
[sig-storage] Downward API volume
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":278,"completed":188,"skipped":3095,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:31:35.920: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5639
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 10 09:31:36.498: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 10 09:31:36.631: INFO: Waiting for terminating namespaces to be deleted...
Feb 10 09:31:36.647: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Feb 10 09:31:36.868: INFO: webserver-deployment-c7997dcc8-pqz8r from deployment-6762 started at 2020-02-10 09:31:22 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.868: INFO: 	Container httpd ready: false, restart count 0
Feb 10 09:31:36.868: INFO: kube-apiserver-master1 from kube-system started at 2020-02-05 16:29:54 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.868: INFO: 	Container kube-apiserver ready: true, restart count 3021
Feb 10 09:31:36.868: INFO: ss2-1 from statefulset-6054 started at 2020-02-10 05:24:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.868: INFO: 	Container webserver ready: true, restart count 0
Feb 10 09:31:36.869: INFO: calico-node-p64wz from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.869: INFO: 	Container calico-node ready: true, restart count 1
Feb 10 09:31:36.869: INFO: simpletest-rc-to-be-deleted-f48d7 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.869: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:31:36.869: INFO: kube-controller-manager-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.869: INFO: 	Container kube-controller-manager ready: true, restart count 565
Feb 10 09:31:36.869: INFO: ntp-zj7fc from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.869: INFO: 	Container ntp ready: true, restart count 1
Feb 10 09:31:36.869: INFO: coredns-hfdns from kube-system started at 2020-01-15 01:54:01 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.869: INFO: 	Container coredns ready: true, restart count 1
Feb 10 09:31:36.870: INFO: agnhost-deployment-54964f567d-nvhtm from kube-system started at 2020-02-10 07:28:38 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.870: INFO: 	Container agnhost ready: true, restart count 0
Feb 10 09:31:36.870: INFO: sonobuoy-e2e-job-26d59f1c3cea4685 from sonobuoy started at 2020-02-10 08:27:25 +0000 UTC (2 container statuses recorded)
Feb 10 09:31:36.870: INFO: 	Container e2e ready: true, restart count 0
Feb 10 09:31:36.870: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 10 09:31:36.870: INFO: webserver-deployment-c7997dcc8-z5866 from deployment-6762 started at 2020-02-10 09:31:22 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.870: INFO: 	Container httpd ready: false, restart count 0
Feb 10 09:31:36.870: INFO: webserver-deployment-c7997dcc8-vrqfn from deployment-6762 started at 2020-02-10 09:31:24 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.871: INFO: 	Container httpd ready: false, restart count 0
Feb 10 09:31:36.871: INFO: kube-scheduler-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.871: INFO: 	Container kube-scheduler ready: true, restart count 572
Feb 10 09:31:36.871: INFO: nginx-proxy-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.871: INFO: 	Container nginx-proxy ready: true, restart count 845
Feb 10 09:31:36.871: INFO: kube-proxy-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.871: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 09:31:36.871: INFO: localpv-provisioner-ds-nt4r5 from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:36.872: INFO: 	Container provisioner ready: true, restart count 1
Feb 10 09:31:36.872: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Feb 10 09:31:37.747: INFO: simpletest-rc-to-be-deleted-62s49 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.748: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:31:37.748: INFO: sonobuoy from sonobuoy started at 2020-02-10 08:27:21 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.748: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 10 09:31:37.748: INFO: kube-proxy-master2 from kube-system started at 2020-02-05 16:28:09 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.748: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 09:31:37.748: INFO: coredns-sv7qc from kube-system started at 2020-01-15 02:50:54 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.749: INFO: 	Container coredns ready: true, restart count 1
Feb 10 09:31:37.749: INFO: calico-node-q6h5s from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.749: INFO: 	Container calico-node ready: true, restart count 2
Feb 10 09:31:37.749: INFO: ss2-2 from statefulset-6054 started at 2020-02-10 05:25:22 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.749: INFO: 	Container webserver ready: true, restart count 0
Feb 10 09:31:37.749: INFO: metrics-server-6c6fd884-md7t8 from kube-system started at 2020-01-15 01:54:18 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.749: INFO: 	Container metrics-server ready: true, restart count 1
Feb 10 09:31:37.749: INFO: simpletest-rc-to-be-deleted-hvgr7 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.749: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:31:37.749: INFO: nginx-proxy-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.749: INFO: 	Container nginx-proxy ready: true, restart count 846
Feb 10 09:31:37.750: INFO: kube-apiserver-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.750: INFO: 	Container kube-apiserver ready: true, restart count 563
Feb 10 09:31:37.750: INFO: localpv-provisioner-ds-sd5nt from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.750: INFO: 	Container provisioner ready: true, restart count 3
Feb 10 09:31:37.750: INFO: tiller-deploy-75cd95b6b4-tkpds from kube-system started at 2020-02-07 13:38:14 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.750: INFO: 	Container tiller ready: true, restart count 0
Feb 10 09:31:37.750: INFO: kube-scheduler-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.751: INFO: 	Container kube-scheduler ready: true, restart count 561
Feb 10 09:31:37.751: INFO: kube-controller-manager-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.751: INFO: 	Container kube-controller-manager ready: true, restart count 581
Feb 10 09:31:37.751: INFO: ntp-g2jhr from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:37.751: INFO: 	Container ntp ready: true, restart count 1
Feb 10 09:31:37.751: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Feb 10 09:31:38.001: INFO: kube-proxy-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.001: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 09:31:38.001: INFO: kube-scheduler-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.001: INFO: 	Container kube-scheduler ready: true, restart count 574
Feb 10 09:31:38.001: INFO: nginx-proxy-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.001: INFO: 	Container nginx-proxy ready: true, restart count 851
Feb 10 09:31:38.001: INFO: coredns-bsmzw from kube-system started at 2020-01-15 01:54:55 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container coredns ready: true, restart count 1
Feb 10 09:31:38.002: INFO: ntp-server-8cbc779-hj2rj from kube-system started at 2020-01-15 03:48:56 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container ntp-server ready: true, restart count 1
Feb 10 09:31:38.002: INFO: kube-apiserver-master3 from kube-system started at 2020-02-05 16:28:26 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container kube-apiserver ready: true, restart count 563
Feb 10 09:31:38.002: INFO: kube-controller-manager-master3 from kube-system started at 2020-02-05 16:28:26 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container kube-controller-manager ready: true, restart count 553
Feb 10 09:31:38.002: INFO: simpletest-rc-to-be-deleted-clkth from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:31:38.002: INFO: calico-node-6fr4n from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container calico-node ready: true, restart count 2
Feb 10 09:31:38.002: INFO: localpv-provisioner-ds-vhkpq from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container provisioner ready: true, restart count 3
Feb 10 09:31:38.002: INFO: simpletest-rc-to-be-deleted-g226k from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:31:38.002: INFO: ss2-0 from statefulset-6054 started at 2020-02-10 05:25:17 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container webserver ready: true, restart count 0
Feb 10 09:31:38.002: INFO: ntp-c9lwf from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container ntp ready: true, restart count 1
Feb 10 09:31:38.002: INFO: calico-kube-controllers-6fcbd694f7-zs7dw from kube-system started at 2020-02-07 13:38:14 +0000 UTC (1 container statuses recorded)
Feb 10 09:31:38.002: INFO: 	Container calico-kube-controllers ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f2009e1b2e484a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:31:39.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5639" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":278,"completed":189,"skipped":3099,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:31:39.398: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9704
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-9704
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Feb 10 09:31:40.111: INFO: Found 0 stateful pods, waiting for 3
Feb 10 09:31:50.130: INFO: Found 2 stateful pods, waiting for 3
Feb 10 09:32:00.126: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:32:00.126: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:32:00.126: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 10 09:32:00.204: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 10 09:32:10.297: INFO: Updating stateful set ss2
Feb 10 09:32:10.323: INFO: Waiting for Pod statefulset-9704/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Feb 10 09:32:20.496: INFO: Found 2 stateful pods, waiting for 3
Feb 10 09:32:30.517: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:32:30.517: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:32:30.517: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 10 09:32:40.512: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:32:40.512: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:32:40.513: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 10 09:32:40.578: INFO: Updating stateful set ss2
Feb 10 09:32:40.605: INFO: Waiting for Pod statefulset-9704/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 10 09:32:50.666: INFO: Updating stateful set ss2
Feb 10 09:32:50.691: INFO: Waiting for StatefulSet statefulset-9704/ss2 to complete update
Feb 10 09:32:50.691: INFO: Waiting for Pod statefulset-9704/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 10 09:33:00.728: INFO: Waiting for StatefulSet statefulset-9704/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 10 09:33:10.717: INFO: Deleting all statefulset in ns statefulset-9704
Feb 10 09:33:10.726: INFO: Scaling statefulset ss2 to 0
Feb 10 09:33:50.781: INFO: Waiting for statefulset status.replicas updated to 0
Feb 10 09:33:50.794: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:33:50.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9704" for this suite.

• [SLOW TEST:131.502 seconds]
[sig-apps] StatefulSet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":278,"completed":190,"skipped":3131,"failed":0}
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:33:50.903: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Feb 10 09:33:51.351: INFO: Waiting up to 5m0s for pod "var-expansion-1e576aba-8c26-49a6-b7c7-02811cfd0c87" in namespace "var-expansion-2745" to be "success or failure"
Feb 10 09:33:51.365: INFO: Pod "var-expansion-1e576aba-8c26-49a6-b7c7-02811cfd0c87": Phase="Pending", Reason="", readiness=false. Elapsed: 13.88079ms
Feb 10 09:33:53.378: INFO: Pod "var-expansion-1e576aba-8c26-49a6-b7c7-02811cfd0c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026609493s
Feb 10 09:33:55.392: INFO: Pod "var-expansion-1e576aba-8c26-49a6-b7c7-02811cfd0c87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040872185s
Feb 10 09:33:57.406: INFO: Pod "var-expansion-1e576aba-8c26-49a6-b7c7-02811cfd0c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054740226s
STEP: Saw pod success
Feb 10 09:33:57.407: INFO: Pod "var-expansion-1e576aba-8c26-49a6-b7c7-02811cfd0c87" satisfied condition "success or failure"
Feb 10 09:33:57.420: INFO: Trying to get logs from node master3 pod var-expansion-1e576aba-8c26-49a6-b7c7-02811cfd0c87 container dapi-container: <nil>
STEP: delete the pod
Feb 10 09:33:57.853: INFO: Waiting for pod var-expansion-1e576aba-8c26-49a6-b7c7-02811cfd0c87 to disappear
Feb 10 09:33:57.862: INFO: Pod var-expansion-1e576aba-8c26-49a6-b7c7-02811cfd0c87 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:33:57.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2745" for this suite.

• [SLOW TEST:6.997 seconds]
[k8s.io] Variable Expansion
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":278,"completed":191,"skipped":3131,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:33:57.905: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:34:33.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7264" for this suite.

• [SLOW TEST:35.325 seconds]
[k8s.io] Container Runtime
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":278,"completed":192,"skipped":3134,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:34:33.237: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3948.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3948.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3948.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3948.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3948.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3948.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 10 09:34:41.865: INFO: DNS probes using dns-3948/dns-test-6c402dea-fef1-4e56-b1ea-70f5b25152c6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:34:41.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3948" for this suite.

• [SLOW TEST:8.780 seconds]
[sig-network] DNS
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":278,"completed":193,"skipped":3135,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:34:42.019: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9041
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:34:55.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9041" for this suite.

• [SLOW TEST:13.722 seconds]
[sig-api-machinery] ResourceQuota
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":278,"completed":194,"skipped":3154,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:34:55.742: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 10 09:34:56.139: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:35:07.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9633" for this suite.

• [SLOW TEST:11.650 seconds]
[k8s.io] InitContainer [NodeConformance]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartAlways pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":278,"completed":195,"skipped":3169,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:35:07.395: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 10 09:35:19.957: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 10 09:35:19.974: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 10 09:35:21.975: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 10 09:35:21.994: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 10 09:35:23.975: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 10 09:35:23.988: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 10 09:35:25.975: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 10 09:35:25.990: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 10 09:35:27.981: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 10 09:35:27.999: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 10 09:35:29.975: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 10 09:35:29.986: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:35:30.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3588" for this suite.

• [SLOW TEST:22.961 seconds]
[k8s.io] Container Lifecycle Hook
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":278,"completed":196,"skipped":3175,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:35:30.358: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-2856
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Feb 10 09:35:30.794: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2856" to be "success or failure"
Feb 10 09:35:30.805: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.284805ms
Feb 10 09:35:32.825: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030558214s
Feb 10 09:35:34.838: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042984789s
Feb 10 09:35:36.851: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056512395s
STEP: Saw pod success
Feb 10 09:35:36.851: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 10 09:35:36.862: INFO: Trying to get logs from node master3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 10 09:35:37.292: INFO: Waiting for pod pod-host-path-test to disappear
Feb 10 09:35:37.303: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:35:37.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2856" for this suite.

• [SLOW TEST:6.992 seconds]
[sig-storage] HostPath
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":197,"skipped":3209,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:35:37.351: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:36:07.819: INFO: Container started at 2020-02-10 09:35:41 +0000 UTC, pod became ready at 2020-02-10 09:36:05 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:36:07.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5359" for this suite.

• [SLOW TEST:30.502 seconds]
[k8s.io] Probing container
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":278,"completed":198,"skipped":3219,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:36:07.854: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:36:27.748: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 10 09:36:29.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924187, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924187, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924187, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924187, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:36:31.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924187, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924187, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924187, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924187, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:36:34.830: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:36:35.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9570" for this suite.
STEP: Destroying namespace "webhook-9570-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:27.671 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":278,"completed":199,"skipped":3236,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:36:35.529: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl replace
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1877
[It] should update a single-container pod's image  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 10 09:36:35.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5064'
Feb 10 09:36:39.175: INFO: stderr: ""
Feb 10 09:36:39.175: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 10 09:36:44.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pod e2e-test-httpd-pod --namespace=kubectl-5064 -o json'
Feb 10 09:36:44.746: INFO: stderr: ""
Feb 10 09:36:44.746: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-02-10T09:36:39Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5064\",\n        \"resourceVersion\": \"4664660\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5064/pods/e2e-test-httpd-pod\",\n        \"uid\": \"0080095e-28b3-4475-9cd0-562cb37734c4\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-gnd48\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"master3\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-gnd48\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-gnd48\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-10T09:36:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-10T09:36:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-10T09:36:43Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-10T09:36:39Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://8d4a7e3d8a48f7e80ac8a5badcda2adb9202b940e87a26923156ddbe7eeb0a84\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker://sha256:0c2b0d33a386966885dd604f38d06db8c78942dbee8e1f5a8db17ad0d8a368da\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-10T09:36:43Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.3.137\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.156.32.227\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.156.32.227\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-10T09:36:39Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 10 09:36:44.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 replace -f - --namespace=kubectl-5064'
Feb 10 09:36:46.063: INFO: stderr: ""
Feb 10 09:36:46.064: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1882
Feb 10 09:36:46.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete pods e2e-test-httpd-pod --namespace=kubectl-5064'
Feb 10 09:36:51.703: INFO: stderr: ""
Feb 10 09:36:51.703: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:36:51.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5064" for this suite.

• [SLOW TEST:16.231 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1873
    should update a single-container pod's image  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":278,"completed":200,"skipped":3282,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:36:51.763: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-5472c838-f4ee-4b5a-844d-28fdae4a0841
STEP: Creating a pod to test consume configMaps
Feb 10 09:36:52.235: INFO: Waiting up to 5m0s for pod "pod-configmaps-65bc9f02-f62f-4ab5-8517-e9e2333680dd" in namespace "configmap-8586" to be "success or failure"
Feb 10 09:36:52.246: INFO: Pod "pod-configmaps-65bc9f02-f62f-4ab5-8517-e9e2333680dd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.388083ms
Feb 10 09:36:54.258: INFO: Pod "pod-configmaps-65bc9f02-f62f-4ab5-8517-e9e2333680dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023022889s
Feb 10 09:36:56.273: INFO: Pod "pod-configmaps-65bc9f02-f62f-4ab5-8517-e9e2333680dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038024746s
Feb 10 09:36:58.286: INFO: Pod "pod-configmaps-65bc9f02-f62f-4ab5-8517-e9e2333680dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050157715s
STEP: Saw pod success
Feb 10 09:36:58.286: INFO: Pod "pod-configmaps-65bc9f02-f62f-4ab5-8517-e9e2333680dd" satisfied condition "success or failure"
Feb 10 09:36:58.295: INFO: Trying to get logs from node master3 pod pod-configmaps-65bc9f02-f62f-4ab5-8517-e9e2333680dd container configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 09:36:58.405: INFO: Waiting for pod pod-configmaps-65bc9f02-f62f-4ab5-8517-e9e2333680dd to disappear
Feb 10 09:36:58.416: INFO: Pod pod-configmaps-65bc9f02-f62f-4ab5-8517-e9e2333680dd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:36:58.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8586" for this suite.

• [SLOW TEST:6.687 seconds]
[sig-storage] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":278,"completed":201,"skipped":3290,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:36:58.451: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-786
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should create and stop a replication controller  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Feb 10 09:36:58.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-786'
Feb 10 09:36:59.790: INFO: stderr: ""
Feb 10 09:36:59.790: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 10 09:36:59.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-786'
Feb 10 09:37:00.464: INFO: stderr: ""
Feb 10 09:37:00.464: INFO: stdout: "update-demo-nautilus-6fgtz update-demo-nautilus-ft8t9 "
Feb 10 09:37:00.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-6fgtz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-786'
Feb 10 09:37:00.992: INFO: stderr: ""
Feb 10 09:37:00.992: INFO: stdout: ""
Feb 10 09:37:00.992: INFO: update-demo-nautilus-6fgtz is created but not running
Feb 10 09:37:05.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-786'
Feb 10 09:37:06.517: INFO: stderr: ""
Feb 10 09:37:06.517: INFO: stdout: "update-demo-nautilus-6fgtz update-demo-nautilus-ft8t9 "
Feb 10 09:37:06.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-6fgtz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-786'
Feb 10 09:37:07.014: INFO: stderr: ""
Feb 10 09:37:07.015: INFO: stdout: "true"
Feb 10 09:37:07.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-6fgtz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-786'
Feb 10 09:37:07.533: INFO: stderr: ""
Feb 10 09:37:07.533: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 10 09:37:07.533: INFO: validating pod update-demo-nautilus-6fgtz
Feb 10 09:37:07.554: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 10 09:37:07.554: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 10 09:37:07.554: INFO: update-demo-nautilus-6fgtz is verified up and running
Feb 10 09:37:07.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-ft8t9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-786'
Feb 10 09:37:08.049: INFO: stderr: ""
Feb 10 09:37:08.050: INFO: stdout: "true"
Feb 10 09:37:08.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods update-demo-nautilus-ft8t9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-786'
Feb 10 09:37:08.538: INFO: stderr: ""
Feb 10 09:37:08.538: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 10 09:37:08.538: INFO: validating pod update-demo-nautilus-ft8t9
Feb 10 09:37:08.557: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 10 09:37:08.557: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 10 09:37:08.557: INFO: update-demo-nautilus-ft8t9 is verified up and running
STEP: using delete to clean up resources
Feb 10 09:37:08.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete --grace-period=0 --force -f - --namespace=kubectl-786'
Feb 10 09:37:09.048: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 10 09:37:09.049: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 10 09:37:09.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-786'
Feb 10 09:37:09.575: INFO: stderr: "No resources found in kubectl-786 namespace.\n"
Feb 10 09:37:09.575: INFO: stdout: ""
Feb 10 09:37:09.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -l name=update-demo --namespace=kubectl-786 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 10 09:37:10.110: INFO: stderr: ""
Feb 10 09:37:10.110: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:37:10.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-786" for this suite.

• [SLOW TEST:11.714 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should create and stop a replication controller  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":278,"completed":202,"skipped":3292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:37:10.171: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3565
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Feb 10 09:37:10.590: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:38:32.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3565" for this suite.

• [SLOW TEST:81.924 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":278,"completed":203,"skipped":3318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:38:32.096: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 10 09:38:32.501: INFO: PodSpec: initContainers in spec.initContainers
Feb 10 09:39:30.804: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a6c200e7-2ef7-4d0c-8bc7-fedefbdfa635", GenerateName:"", Namespace:"init-container-3831", SelfLink:"/api/v1/namespaces/init-container-3831/pods/pod-init-a6c200e7-2ef7-4d0c-8bc7-fedefbdfa635", UID:"4660a228-e11c-4c8f-97f0-24e0dc3e5c82", ResourceVersion:"4665294", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63716924312, loc:(*time.Location)(0x127bc8cc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"501360386"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-kf2zs", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc004a85340), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kf2zs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kf2zs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kf2zs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002f60e58), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"master3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0029f85a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002f60ef0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002f60f20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002f60f28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002f60f2c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924312, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924312, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924312, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924312, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.3.137", PodIP:"10.156.32.229", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.156.32.229"}}, StartTime:(*v1.Time)(0xc0038b2ba0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0020d43f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0020d4460)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker://sha256:9024aa0782a40c0c54ab944b62c94b033971d984fdd05b531ac7ac99a58d1503", ContainerID:"docker://07688baaf15656c70c6eb1fbb4c2ffea75210aadc463871838c0737aa5cee9e4", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038b2be0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038b2bc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002f60fcf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:39:30.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3831" for this suite.

• [SLOW TEST:58.756 seconds]
[k8s.io] InitContainer [NodeConformance]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":278,"completed":204,"skipped":3340,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:39:30.857: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-7bc44a5e-8de2-4ba9-8f6e-07ba3eb6bed9
STEP: Creating a pod to test consume configMaps
Feb 10 09:39:31.306: INFO: Waiting up to 5m0s for pod "pod-configmaps-f94555fc-8349-4952-aaf5-a698fe59de50" in namespace "configmap-4145" to be "success or failure"
Feb 10 09:39:31.319: INFO: Pod "pod-configmaps-f94555fc-8349-4952-aaf5-a698fe59de50": Phase="Pending", Reason="", readiness=false. Elapsed: 13.349261ms
Feb 10 09:39:33.333: INFO: Pod "pod-configmaps-f94555fc-8349-4952-aaf5-a698fe59de50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027209089s
Feb 10 09:39:35.347: INFO: Pod "pod-configmaps-f94555fc-8349-4952-aaf5-a698fe59de50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041343672s
Feb 10 09:39:37.363: INFO: Pod "pod-configmaps-f94555fc-8349-4952-aaf5-a698fe59de50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057174569s
STEP: Saw pod success
Feb 10 09:39:37.364: INFO: Pod "pod-configmaps-f94555fc-8349-4952-aaf5-a698fe59de50" satisfied condition "success or failure"
Feb 10 09:39:37.373: INFO: Trying to get logs from node master1 pod pod-configmaps-f94555fc-8349-4952-aaf5-a698fe59de50 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 09:39:37.743: INFO: Waiting for pod pod-configmaps-f94555fc-8349-4952-aaf5-a698fe59de50 to disappear
Feb 10 09:39:37.753: INFO: Pod pod-configmaps-f94555fc-8349-4952-aaf5-a698fe59de50 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:39:37.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4145" for this suite.

• [SLOW TEST:6.934 seconds]
[sig-storage] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":205,"skipped":3345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:39:37.796: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 10 09:39:44.392: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:39:44.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9563" for this suite.

• [SLOW TEST:6.714 seconds]
[k8s.io] Container Runtime
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":278,"completed":206,"skipped":3376,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:39:44.511: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7023
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 10 09:39:44.986: INFO: Waiting up to 5m0s for pod "pod-a150e6c7-c78c-4bd2-8c1e-005b41633597" in namespace "emptydir-7023" to be "success or failure"
Feb 10 09:39:44.997: INFO: Pod "pod-a150e6c7-c78c-4bd2-8c1e-005b41633597": Phase="Pending", Reason="", readiness=false. Elapsed: 11.07394ms
Feb 10 09:39:47.011: INFO: Pod "pod-a150e6c7-c78c-4bd2-8c1e-005b41633597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024561589s
Feb 10 09:39:49.023: INFO: Pod "pod-a150e6c7-c78c-4bd2-8c1e-005b41633597": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036631871s
Feb 10 09:39:51.033: INFO: Pod "pod-a150e6c7-c78c-4bd2-8c1e-005b41633597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047265028s
STEP: Saw pod success
Feb 10 09:39:51.033: INFO: Pod "pod-a150e6c7-c78c-4bd2-8c1e-005b41633597" satisfied condition "success or failure"
Feb 10 09:39:51.043: INFO: Trying to get logs from node master3 pod pod-a150e6c7-c78c-4bd2-8c1e-005b41633597 container test-container: <nil>
STEP: delete the pod
Feb 10 09:39:51.471: INFO: Waiting for pod pod-a150e6c7-c78c-4bd2-8c1e-005b41633597 to disappear
Feb 10 09:39:51.480: INFO: Pod pod-a150e6c7-c78c-4bd2-8c1e-005b41633597 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:39:51.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7023" for this suite.

• [SLOW TEST:7.001 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":207,"skipped":3382,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:39:51.513: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-tcd67 in namespace proxy-6259
I0210 09:39:51.995692      23 runners.go:189] Created replication controller with name: proxy-service-tcd67, namespace: proxy-6259, replica count: 1
I0210 09:39:53.048136      23 runners.go:189] proxy-service-tcd67 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 09:39:54.048574      23 runners.go:189] proxy-service-tcd67 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 09:39:55.051763      23 runners.go:189] proxy-service-tcd67 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 09:39:56.052137      23 runners.go:189] proxy-service-tcd67 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 09:39:57.052592      23 runners.go:189] proxy-service-tcd67 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0210 09:39:58.053024      23 runners.go:189] proxy-service-tcd67 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0210 09:39:59.053501      23 runners.go:189] proxy-service-tcd67 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0210 09:40:00.053906      23 runners.go:189] proxy-service-tcd67 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0210 09:40:01.054413      23 runners.go:189] proxy-service-tcd67 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 10 09:40:01.066: INFO: setup took 9.139031253s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 10 09:40:01.083: INFO: (0) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 17.38968ms)
Feb 10 09:40:01.087: INFO: (0) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 18.27892ms)
Feb 10 09:40:01.099: INFO: (0) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 33.024781ms)
Feb 10 09:40:01.103: INFO: (0) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 34.281981ms)
Feb 10 09:40:01.103: INFO: (0) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 35.251401ms)
Feb 10 09:40:01.107: INFO: (0) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 39.546961ms)
Feb 10 09:40:01.114: INFO: (0) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 46.605141ms)
Feb 10 09:40:01.115: INFO: (0) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 47.489941ms)
Feb 10 09:40:01.115: INFO: (0) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 47.722441ms)
Feb 10 09:40:01.115: INFO: (0) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 48.057981ms)
Feb 10 09:40:01.122: INFO: (0) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 55.112181ms)
Feb 10 09:40:01.283: INFO: (0) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 215.232343ms)
Feb 10 09:40:01.287: INFO: (0) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 218.604223ms)
Feb 10 09:40:01.296: INFO: (0) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 229.228763ms)
Feb 10 09:40:01.303: INFO: (0) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 236.534903ms)
Feb 10 09:40:01.303: INFO: (0) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 236.423143ms)
Feb 10 09:40:01.358: INFO: (1) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 52.912401ms)
Feb 10 09:40:01.358: INFO: (1) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 52.944801ms)
Feb 10 09:40:01.360: INFO: (1) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 55.149821ms)
Feb 10 09:40:01.360: INFO: (1) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 54.949701ms)
Feb 10 09:40:01.360: INFO: (1) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 55.159101ms)
Feb 10 09:40:01.360: INFO: (1) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 56.230321ms)
Feb 10 09:40:01.360: INFO: (1) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 54.709641ms)
Feb 10 09:40:01.361: INFO: (1) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 56.857081ms)
Feb 10 09:40:01.361: INFO: (1) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 56.307001ms)
Feb 10 09:40:01.361: INFO: (1) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 57.077461ms)
Feb 10 09:40:01.361: INFO: (1) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 56.929121ms)
Feb 10 09:40:01.362: INFO: (1) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 57.393541ms)
Feb 10 09:40:01.362: INFO: (1) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 56.913101ms)
Feb 10 09:40:01.369: INFO: (1) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 63.851001ms)
Feb 10 09:40:01.378: INFO: (1) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 73.530701ms)
Feb 10 09:40:01.380: INFO: (1) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 74.755861ms)
Feb 10 09:40:01.439: INFO: (2) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 57.583181ms)
Feb 10 09:40:01.440: INFO: (2) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 58.731101ms)
Feb 10 09:40:01.440: INFO: (2) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 58.154161ms)
Feb 10 09:40:01.440: INFO: (2) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 57.875081ms)
Feb 10 09:40:01.440: INFO: (2) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 58.286161ms)
Feb 10 09:40:01.443: INFO: (2) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 62.101601ms)
Feb 10 09:40:01.443: INFO: (2) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 62.439361ms)
Feb 10 09:40:01.444: INFO: (2) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 63.000301ms)
Feb 10 09:40:01.444: INFO: (2) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 63.740481ms)
Feb 10 09:40:01.444: INFO: (2) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 62.418441ms)
Feb 10 09:40:01.444: INFO: (2) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 62.975521ms)
Feb 10 09:40:01.444: INFO: (2) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 63.561441ms)
Feb 10 09:40:01.444: INFO: (2) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 62.586141ms)
Feb 10 09:40:01.444: INFO: (2) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 63.954541ms)
Feb 10 09:40:01.445: INFO: (2) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 64.855681ms)
Feb 10 09:40:01.450: INFO: (2) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 69.113841ms)
Feb 10 09:40:01.478: INFO: (3) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 26.33884ms)
Feb 10 09:40:01.485: INFO: (3) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 34.25336ms)
Feb 10 09:40:01.485: INFO: (3) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 34.36432ms)
Feb 10 09:40:01.497: INFO: (3) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 46.42658ms)
Feb 10 09:40:01.499: INFO: (3) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 47.99302ms)
Feb 10 09:40:01.500: INFO: (3) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 49.5604ms)
Feb 10 09:40:01.502: INFO: (3) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 50.71928ms)
Feb 10 09:40:01.502: INFO: (3) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 51.80322ms)
Feb 10 09:40:01.503: INFO: (3) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 51.53566ms)
Feb 10 09:40:01.511: INFO: (3) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 60.42444ms)
Feb 10 09:40:01.513: INFO: (3) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 62.04958ms)
Feb 10 09:40:01.513: INFO: (3) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 62.79326ms)
Feb 10 09:40:01.514: INFO: (3) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 62.7148ms)
Feb 10 09:40:01.514: INFO: (3) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 62.67154ms)
Feb 10 09:40:01.514: INFO: (3) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 62.96714ms)
Feb 10 09:40:01.516: INFO: (3) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 64.612921ms)
Feb 10 09:40:01.550: INFO: (4) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 33.5217ms)
Feb 10 09:40:01.550: INFO: (4) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 33.59836ms)
Feb 10 09:40:01.553: INFO: (4) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 36.39526ms)
Feb 10 09:40:01.554: INFO: (4) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 37.12014ms)
Feb 10 09:40:01.554: INFO: (4) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 37.3885ms)
Feb 10 09:40:01.554: INFO: (4) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 38.1198ms)
Feb 10 09:40:01.554: INFO: (4) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 38.4105ms)
Feb 10 09:40:01.554: INFO: (4) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 37.70332ms)
Feb 10 09:40:01.554: INFO: (4) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 38.3334ms)
Feb 10 09:40:01.562: INFO: (4) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 45.8552ms)
Feb 10 09:40:01.565: INFO: (4) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 49.01606ms)
Feb 10 09:40:01.566: INFO: (4) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 49.42432ms)
Feb 10 09:40:01.566: INFO: (4) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 49.47026ms)
Feb 10 09:40:01.566: INFO: (4) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 49.26388ms)
Feb 10 09:40:01.566: INFO: (4) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 50.43948ms)
Feb 10 09:40:01.571: INFO: (4) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 54.87306ms)
Feb 10 09:40:01.592: INFO: (5) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 20.41474ms)
Feb 10 09:40:01.597: INFO: (5) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 24.96322ms)
Feb 10 09:40:01.598: INFO: (5) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 25.62388ms)
Feb 10 09:40:01.598: INFO: (5) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 26.28482ms)
Feb 10 09:40:01.600: INFO: (5) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 28.202321ms)
Feb 10 09:40:01.600: INFO: (5) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 28.161581ms)
Feb 10 09:40:01.600: INFO: (5) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 28.682361ms)
Feb 10 09:40:01.600: INFO: (5) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 28.224141ms)
Feb 10 09:40:01.600: INFO: (5) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 28.574841ms)
Feb 10 09:40:01.604: INFO: (5) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 31.655421ms)
Feb 10 09:40:01.604: INFO: (5) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 32.727841ms)
Feb 10 09:40:01.612: INFO: (5) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 40.437501ms)
Feb 10 09:40:01.615: INFO: (5) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 43.033461ms)
Feb 10 09:40:01.615: INFO: (5) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 43.069221ms)
Feb 10 09:40:01.617: INFO: (5) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 45.464021ms)
Feb 10 09:40:01.620: INFO: (5) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 48.193501ms)
Feb 10 09:40:01.633: INFO: (6) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 13.62568ms)
Feb 10 09:40:01.641: INFO: (6) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 20.60502ms)
Feb 10 09:40:01.641: INFO: (6) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 20.21794ms)
Feb 10 09:40:01.647: INFO: (6) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 26.53952ms)
Feb 10 09:40:01.648: INFO: (6) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 27.2158ms)
Feb 10 09:40:01.649: INFO: (6) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 28.66968ms)
Feb 10 09:40:01.650: INFO: (6) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 28.022ms)
Feb 10 09:40:01.653: INFO: (6) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 33.06358ms)
Feb 10 09:40:01.654: INFO: (6) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 34.0013ms)
Feb 10 09:40:01.655: INFO: (6) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 34.02488ms)
Feb 10 09:40:01.656: INFO: (6) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 35.11432ms)
Feb 10 09:40:01.659: INFO: (6) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 38.00586ms)
Feb 10 09:40:01.659: INFO: (6) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 38.29898ms)
Feb 10 09:40:01.664: INFO: (6) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 42.34714ms)
Feb 10 09:40:01.666: INFO: (6) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 44.47704ms)
Feb 10 09:40:01.666: INFO: (6) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 45.52802ms)
Feb 10 09:40:01.695: INFO: (7) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 28.419621ms)
Feb 10 09:40:01.695: INFO: (7) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 28.695941ms)
Feb 10 09:40:01.696: INFO: (7) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 28.689601ms)
Feb 10 09:40:01.696: INFO: (7) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 30.088241ms)
Feb 10 09:40:01.696: INFO: (7) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 29.391181ms)
Feb 10 09:40:01.697: INFO: (7) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 29.953841ms)
Feb 10 09:40:01.697: INFO: (7) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 29.491801ms)
Feb 10 09:40:01.697: INFO: (7) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 30.099701ms)
Feb 10 09:40:01.697: INFO: (7) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 30.470781ms)
Feb 10 09:40:01.697: INFO: (7) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 30.467981ms)
Feb 10 09:40:01.699: INFO: (7) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 31.982721ms)
Feb 10 09:40:01.703: INFO: (7) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 36.454021ms)
Feb 10 09:40:01.707: INFO: (7) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 40.084061ms)
Feb 10 09:40:01.707: INFO: (7) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 40.163901ms)
Feb 10 09:40:01.707: INFO: (7) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 40.838901ms)
Feb 10 09:40:01.708: INFO: (7) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 41.358721ms)
Feb 10 09:40:01.728: INFO: (8) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 17.74068ms)
Feb 10 09:40:01.729: INFO: (8) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 19.04684ms)
Feb 10 09:40:01.734: INFO: (8) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 23.47ms)
Feb 10 09:40:01.739: INFO: (8) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 30.12094ms)
Feb 10 09:40:01.741: INFO: (8) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 31.25294ms)
Feb 10 09:40:01.741: INFO: (8) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 32.76854ms)
Feb 10 09:40:01.741: INFO: (8) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 32.9216ms)
Feb 10 09:40:01.743: INFO: (8) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 34.10812ms)
Feb 10 09:40:01.743: INFO: (8) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 33.95838ms)
Feb 10 09:40:01.743: INFO: (8) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 33.79388ms)
Feb 10 09:40:01.744: INFO: (8) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 34.35832ms)
Feb 10 09:40:01.745: INFO: (8) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 35.66588ms)
Feb 10 09:40:01.745: INFO: (8) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 34.19544ms)
Feb 10 09:40:01.745: INFO: (8) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 34.17122ms)
Feb 10 09:40:01.746: INFO: (8) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 36.64752ms)
Feb 10 09:40:01.754: INFO: (8) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 43.34096ms)
Feb 10 09:40:01.775: INFO: (9) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 19.621121ms)
Feb 10 09:40:01.779: INFO: (9) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 24.399541ms)
Feb 10 09:40:01.783: INFO: (9) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 26.078441ms)
Feb 10 09:40:01.784: INFO: (9) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 28.156581ms)
Feb 10 09:40:01.784: INFO: (9) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 29.551621ms)
Feb 10 09:40:01.784: INFO: (9) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 28.764421ms)
Feb 10 09:40:01.784: INFO: (9) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 29.777841ms)
Feb 10 09:40:01.784: INFO: (9) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 29.606781ms)
Feb 10 09:40:01.788: INFO: (9) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 33.878821ms)
Feb 10 09:40:01.788: INFO: (9) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 32.583741ms)
Feb 10 09:40:01.788: INFO: (9) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 34.591501ms)
Feb 10 09:40:01.790: INFO: (9) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 34.695801ms)
Feb 10 09:40:01.794: INFO: (9) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 39.230421ms)
Feb 10 09:40:01.795: INFO: (9) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 38.991101ms)
Feb 10 09:40:01.795: INFO: (9) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 39.121921ms)
Feb 10 09:40:01.797: INFO: (9) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 42.315461ms)
Feb 10 09:40:01.822: INFO: (10) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 24.658ms)
Feb 10 09:40:01.823: INFO: (10) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 24.85156ms)
Feb 10 09:40:01.825: INFO: (10) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 26.3928ms)
Feb 10 09:40:01.825: INFO: (10) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 27.28866ms)
Feb 10 09:40:01.826: INFO: (10) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 28.1118ms)
Feb 10 09:40:01.827: INFO: (10) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 28.80166ms)
Feb 10 09:40:01.833: INFO: (10) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 34.86072ms)
Feb 10 09:40:01.837: INFO: (10) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 39.30022ms)
Feb 10 09:40:01.838: INFO: (10) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 40.30622ms)
Feb 10 09:40:01.838: INFO: (10) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 40.61298ms)
Feb 10 09:40:01.838: INFO: (10) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 40.2336ms)
Feb 10 09:40:01.838: INFO: (10) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 40.4619ms)
Feb 10 09:40:01.839: INFO: (10) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 41.4967ms)
Feb 10 09:40:01.839: INFO: (10) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 40.98122ms)
Feb 10 09:40:01.839: INFO: (10) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 41.26252ms)
Feb 10 09:40:01.841: INFO: (10) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 42.34222ms)
Feb 10 09:40:01.856: INFO: (11) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 15.521261ms)
Feb 10 09:40:01.858: INFO: (11) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 16.348641ms)
Feb 10 09:40:01.862: INFO: (11) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 18.960081ms)
Feb 10 09:40:01.868: INFO: (11) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 25.802861ms)
Feb 10 09:40:01.876: INFO: (11) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 33.943141ms)
Feb 10 09:40:01.876: INFO: (11) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 33.341241ms)
Feb 10 09:40:01.876: INFO: (11) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 34.145661ms)
Feb 10 09:40:01.876: INFO: (11) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 33.523121ms)
Feb 10 09:40:01.891: INFO: (11) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 48.129341ms)
Feb 10 09:40:01.921: INFO: (11) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 78.238161ms)
Feb 10 09:40:01.921: INFO: (11) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 78.156681ms)
Feb 10 09:40:01.922: INFO: (11) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 79.643541ms)
Feb 10 09:40:01.926: INFO: (11) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 83.502021ms)
Feb 10 09:40:01.927: INFO: (11) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 85.453061ms)
Feb 10 09:40:01.934: INFO: (11) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 91.570081ms)
Feb 10 09:40:01.934: INFO: (11) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 91.317881ms)
Feb 10 09:40:01.958: INFO: (12) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 21.7362ms)
Feb 10 09:40:01.958: INFO: (12) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 23.355461ms)
Feb 10 09:40:01.958: INFO: (12) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 22.52042ms)
Feb 10 09:40:01.958: INFO: (12) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 22.81472ms)
Feb 10 09:40:01.969: INFO: (12) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 32.85342ms)
Feb 10 09:40:01.970: INFO: (12) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 32.54718ms)
Feb 10 09:40:01.975: INFO: (12) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 38.06282ms)
Feb 10 09:40:01.975: INFO: (12) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 37.98376ms)
Feb 10 09:40:01.975: INFO: (12) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 38.20758ms)
Feb 10 09:40:01.975: INFO: (12) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 38.5837ms)
Feb 10 09:40:01.975: INFO: (12) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 38.80832ms)
Feb 10 09:40:01.975: INFO: (12) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 38.70194ms)
Feb 10 09:40:01.975: INFO: (12) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 38.54812ms)
Feb 10 09:40:01.975: INFO: (12) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 39.16408ms)
Feb 10 09:40:01.975: INFO: (12) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 38.5045ms)
Feb 10 09:40:01.975: INFO: (12) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 39.11484ms)
Feb 10 09:40:02.005: INFO: (13) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 29.13668ms)
Feb 10 09:40:02.017: INFO: (13) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 39.67132ms)
Feb 10 09:40:02.018: INFO: (13) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 40.4925ms)
Feb 10 09:40:02.018: INFO: (13) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 41.2669ms)
Feb 10 09:40:02.018: INFO: (13) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 40.6537ms)
Feb 10 09:40:02.018: INFO: (13) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 41.34722ms)
Feb 10 09:40:02.019: INFO: (13) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 41.22092ms)
Feb 10 09:40:02.019: INFO: (13) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 42.22528ms)
Feb 10 09:40:02.020: INFO: (13) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 42.62758ms)
Feb 10 09:40:02.044: INFO: (13) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 66.040461ms)
Feb 10 09:40:02.044: INFO: (13) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 66.752101ms)
Feb 10 09:40:02.044: INFO: (13) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 66.377381ms)
Feb 10 09:40:02.044: INFO: (13) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 66.805581ms)
Feb 10 09:40:02.048: INFO: (13) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 71.304861ms)
Feb 10 09:40:02.048: INFO: (13) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 70.528661ms)
Feb 10 09:40:02.048: INFO: (13) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 71.545001ms)
Feb 10 09:40:02.158: INFO: (14) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 106.395241ms)
Feb 10 09:40:02.158: INFO: (14) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 109.774581ms)
Feb 10 09:40:02.158: INFO: (14) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 109.109421ms)
Feb 10 09:40:02.158: INFO: (14) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 108.315961ms)
Feb 10 09:40:02.158: INFO: (14) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 107.966221ms)
Feb 10 09:40:02.158: INFO: (14) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 108.924161ms)
Feb 10 09:40:02.158: INFO: (14) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 107.659181ms)
Feb 10 09:40:02.159: INFO: (14) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 107.485901ms)
Feb 10 09:40:02.159: INFO: (14) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 108.108401ms)
Feb 10 09:40:02.159: INFO: (14) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 108.884301ms)
Feb 10 09:40:02.159: INFO: (14) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 108.667061ms)
Feb 10 09:40:02.159: INFO: (14) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 107.680021ms)
Feb 10 09:40:02.173: INFO: (14) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 121.159121ms)
Feb 10 09:40:02.185: INFO: (14) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 133.074241ms)
Feb 10 09:40:02.195: INFO: (14) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 143.002321ms)
Feb 10 09:40:02.195: INFO: (14) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 142.989621ms)
Feb 10 09:40:02.215: INFO: (15) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 19.201381ms)
Feb 10 09:40:02.223: INFO: (15) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 24.254281ms)
Feb 10 09:40:02.226: INFO: (15) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 27.441801ms)
Feb 10 09:40:02.227: INFO: (15) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 29.904101ms)
Feb 10 09:40:02.227: INFO: (15) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 25.888281ms)
Feb 10 09:40:02.228: INFO: (15) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 23.152341ms)
Feb 10 09:40:02.228: INFO: (15) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 30.080721ms)
Feb 10 09:40:02.230: INFO: (15) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 27.293201ms)
Feb 10 09:40:02.231: INFO: (15) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 32.419321ms)
Feb 10 09:40:02.232: INFO: (15) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 35.867561ms)
Feb 10 09:40:02.232: INFO: (15) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 24.624401ms)
Feb 10 09:40:02.233: INFO: (15) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 29.595981ms)
Feb 10 09:40:02.233: INFO: (15) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 28.898321ms)
Feb 10 09:40:02.233: INFO: (15) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 29.432141ms)
Feb 10 09:40:02.239: INFO: (15) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 36.488961ms)
Feb 10 09:40:02.239: INFO: (15) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 38.353221ms)
Feb 10 09:40:02.259: INFO: (16) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 19.5647ms)
Feb 10 09:40:02.279: INFO: (16) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 38.07092ms)
Feb 10 09:40:02.280: INFO: (16) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 38.87964ms)
Feb 10 09:40:02.280: INFO: (16) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 38.09928ms)
Feb 10 09:40:02.280: INFO: (16) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 37.95902ms)
Feb 10 09:40:02.280: INFO: (16) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 40.0412ms)
Feb 10 09:40:02.280: INFO: (16) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 40.06892ms)
Feb 10 09:40:02.280: INFO: (16) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 37.49536ms)
Feb 10 09:40:02.280: INFO: (16) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 38.0589ms)
Feb 10 09:40:02.280: INFO: (16) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 38.81548ms)
Feb 10 09:40:02.280: INFO: (16) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 39.91554ms)
Feb 10 09:40:02.281: INFO: (16) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 38.43374ms)
Feb 10 09:40:02.281: INFO: (16) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 39.51248ms)
Feb 10 09:40:02.281: INFO: (16) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 38.85772ms)
Feb 10 09:40:02.281: INFO: (16) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 39.813ms)
Feb 10 09:40:02.281: INFO: (16) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 39.59578ms)
Feb 10 09:40:02.305: INFO: (17) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 24.45082ms)
Feb 10 09:40:02.315: INFO: (17) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 32.869601ms)
Feb 10 09:40:02.315: INFO: (17) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 33.521401ms)
Feb 10 09:40:02.315: INFO: (17) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 32.691201ms)
Feb 10 09:40:02.316: INFO: (17) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 32.194381ms)
Feb 10 09:40:02.323: INFO: (17) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 39.883001ms)
Feb 10 09:40:02.323: INFO: (17) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 39.283221ms)
Feb 10 09:40:02.323: INFO: (17) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 40.500561ms)
Feb 10 09:40:02.323: INFO: (17) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 39.320701ms)
Feb 10 09:40:02.324: INFO: (17) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 42.085681ms)
Feb 10 09:40:02.335: INFO: (17) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 53.117041ms)
Feb 10 09:40:02.336: INFO: (17) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 52.584381ms)
Feb 10 09:40:02.336: INFO: (17) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 52.256041ms)
Feb 10 09:40:02.337: INFO: (17) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 54.086501ms)
Feb 10 09:40:02.344: INFO: (17) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 54.812941ms)
Feb 10 09:40:02.344: INFO: (17) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 60.920081ms)
Feb 10 09:40:02.373: INFO: (18) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 25.77716ms)
Feb 10 09:40:02.373: INFO: (18) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 25.44352ms)
Feb 10 09:40:02.373: INFO: (18) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 27.91016ms)
Feb 10 09:40:02.373: INFO: (18) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 25.73516ms)
Feb 10 09:40:02.374: INFO: (18) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 26.71354ms)
Feb 10 09:40:02.384: INFO: (18) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 37.04088ms)
Feb 10 09:40:02.385: INFO: (18) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 38.16556ms)
Feb 10 09:40:02.385: INFO: (18) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 36.4145ms)
Feb 10 09:40:02.385: INFO: (18) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 38.58544ms)
Feb 10 09:40:02.385: INFO: (18) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 38.087ms)
Feb 10 09:40:02.385: INFO: (18) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 37.80916ms)
Feb 10 09:40:02.385: INFO: (18) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 38.00868ms)
Feb 10 09:40:02.385: INFO: (18) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 37.8572ms)
Feb 10 09:40:02.392: INFO: (18) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 47.28066ms)
Feb 10 09:40:02.392: INFO: (18) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 43.66282ms)
Feb 10 09:40:02.489: INFO: (18) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 141.572921ms)
Feb 10 09:40:02.513: INFO: (19) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 22.424981ms)
Feb 10 09:40:02.514: INFO: (19) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">... (200; 24.012121ms)
Feb 10 09:40:02.514: INFO: (19) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:443/proxy/tlsrewritem... (200; 23.212421ms)
Feb 10 09:40:02.514: INFO: (19) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:1080/proxy/rewriteme">test<... (200; 23.400261ms)
Feb 10 09:40:02.514: INFO: (19) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/: <a href="/api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz/proxy/rewriteme">test</a> (200; 23.700741ms)
Feb 10 09:40:02.514: INFO: (19) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:460/proxy/: tls baz (200; 22.405461ms)
Feb 10 09:40:02.515: INFO: (19) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:162/proxy/: bar (200; 23.654001ms)
Feb 10 09:40:02.517: INFO: (19) /api/v1/namespaces/proxy-6259/pods/http:proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 27.204121ms)
Feb 10 09:40:02.517: INFO: (19) /api/v1/namespaces/proxy-6259/pods/proxy-service-tcd67-cgrcz:160/proxy/: foo (200; 25.296381ms)
Feb 10 09:40:02.517: INFO: (19) /api/v1/namespaces/proxy-6259/pods/https:proxy-service-tcd67-cgrcz:462/proxy/: tls qux (200; 26.038981ms)
Feb 10 09:40:02.529: INFO: (19) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname2/proxy/: bar (200; 37.940081ms)
Feb 10 09:40:02.529: INFO: (19) /api/v1/namespaces/proxy-6259/services/http:proxy-service-tcd67:portname1/proxy/: foo (200; 38.341561ms)
Feb 10 09:40:02.529: INFO: (19) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname1/proxy/: foo (200; 37.604821ms)
Feb 10 09:40:02.531: INFO: (19) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname2/proxy/: tls qux (200; 39.612581ms)
Feb 10 09:40:02.531: INFO: (19) /api/v1/namespaces/proxy-6259/services/proxy-service-tcd67:portname2/proxy/: bar (200; 40.587181ms)
Feb 10 09:40:02.572: INFO: (19) /api/v1/namespaces/proxy-6259/services/https:proxy-service-tcd67:tlsportname1/proxy/: tls baz (200; 80.430381ms)
STEP: deleting ReplicationController proxy-service-tcd67 in namespace proxy-6259, will wait for the garbage collector to delete the pods
Feb 10 09:40:02.685: INFO: Deleting ReplicationController proxy-service-tcd67 took: 42.99776ms
Feb 10 09:40:03.086: INFO: Terminating ReplicationController proxy-service-tcd67 pods took: 401.462025ms
[AfterEach] version v1
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:40:06.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6259" for this suite.

• [SLOW TEST:15.510 seconds]
[sig-network] Proxy
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":278,"completed":208,"skipped":3390,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:40:07.024: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:40:39.615: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 09:40:41.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924439, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924439, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924439, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924439, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:40:43.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924439, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924439, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924439, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716924439, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:40:46.712: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 10 09:40:46.917: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:40:46.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7840" for this suite.
STEP: Destroying namespace "webhook-7840-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:40.202 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":278,"completed":209,"skipped":3403,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:40:47.229: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4503
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-4503
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4503
Feb 10 09:40:47.723: INFO: Found 0 stateful pods, waiting for 1
Feb 10 09:40:57.742: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 10 09:40:57.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4503 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 10 09:40:59.158: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 10 09:40:59.159: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 10 09:40:59.159: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 10 09:40:59.175: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 10 09:41:09.196: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 10 09:41:09.196: INFO: Waiting for statefulset status.replicas updated to 0
Feb 10 09:41:09.242: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Feb 10 09:41:09.243: INFO: ss-0  master3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  }]
Feb 10 09:41:09.243: INFO: 
Feb 10 09:41:09.243: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 10 09:41:10.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987209999s
Feb 10 09:41:11.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.965606479s
Feb 10 09:41:12.312: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.937160042s
Feb 10 09:41:13.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.918265465s
Feb 10 09:41:14.340: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.902917908s
Feb 10 09:41:15.356: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.890062813s
Feb 10 09:41:16.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.87427636s
Feb 10 09:41:17.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.856628887s
Feb 10 09:41:18.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 843.148256ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4503
Feb 10 09:41:19.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4503 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 10 09:41:20.765: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 10 09:41:20.766: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 10 09:41:20.766: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 10 09:41:20.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4503 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 10 09:41:22.126: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: cannot stat '/tmp/index.html': No such file or directory\n+ true\n"
Feb 10 09:41:22.127: INFO: stdout: ""
Feb 10 09:41:22.127: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Feb 10 09:41:22.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 10 09:41:23.449: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: cannot stat '/tmp/index.html': No such file or directory\n+ true\n"
Feb 10 09:41:23.450: INFO: stdout: ""
Feb 10 09:41:23.450: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Feb 10 09:41:23.469: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:41:23.469: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:41:23.470: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 10 09:41:23.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4503 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 10 09:41:24.976: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 10 09:41:24.976: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 10 09:41:24.976: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 10 09:41:24.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4503 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 10 09:41:26.370: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 10 09:41:26.370: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 10 09:41:26.370: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 10 09:41:26.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4503 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 10 09:41:27.861: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 10 09:41:27.861: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 10 09:41:27.861: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 10 09:41:27.861: INFO: Waiting for statefulset status.replicas updated to 0
Feb 10 09:41:27.876: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 10 09:41:37.908: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 10 09:41:37.908: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 10 09:41:37.908: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 10 09:41:37.949: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Feb 10 09:41:37.949: INFO: ss-0  master3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  }]
Feb 10 09:41:37.949: INFO: ss-1  master1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:37.950: INFO: ss-2  master2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:37.950: INFO: 
Feb 10 09:41:37.950: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 10 09:41:38.966: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Feb 10 09:41:38.966: INFO: ss-0  master3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  }]
Feb 10 09:41:38.966: INFO: ss-1  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:38.967: INFO: ss-2  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:38.967: INFO: 
Feb 10 09:41:38.967: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 10 09:41:39.993: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Feb 10 09:41:39.994: INFO: ss-0  master3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  }]
Feb 10 09:41:39.994: INFO: ss-1  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:39.994: INFO: ss-2  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:39.994: INFO: 
Feb 10 09:41:39.994: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 10 09:41:41.010: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Feb 10 09:41:41.010: INFO: ss-0  master3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  }]
Feb 10 09:41:41.010: INFO: ss-1  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:41.010: INFO: ss-2  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:41.010: INFO: 
Feb 10 09:41:41.010: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 10 09:41:42.026: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Feb 10 09:41:42.027: INFO: ss-0  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:40:47 +0000 UTC  }]
Feb 10 09:41:42.027: INFO: ss-1  master1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:42.027: INFO: ss-2  master2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:42.027: INFO: 
Feb 10 09:41:42.027: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 10 09:41:43.043: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Feb 10 09:41:43.043: INFO: ss-2  master2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:43.043: INFO: 
Feb 10 09:41:43.043: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 10 09:41:44.057: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Feb 10 09:41:44.057: INFO: ss-2  master2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-10 09:41:09 +0000 UTC  }]
Feb 10 09:41:44.058: INFO: 
Feb 10 09:41:44.058: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 10 09:41:45.070: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.875728561s
Feb 10 09:41:46.083: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.863731783s
Feb 10 09:41:47.100: INFO: Verifying statefulset ss doesn't scale past 0 for another 849.867865ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4503
Feb 10 09:41:48.118: INFO: Scaling statefulset ss to 0
Feb 10 09:41:48.153: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 10 09:41:48.162: INFO: Deleting all statefulset in ns statefulset-4503
Feb 10 09:41:48.171: INFO: Scaling statefulset ss to 0
Feb 10 09:41:48.198: INFO: Waiting for statefulset status.replicas updated to 0
Feb 10 09:41:48.206: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:41:48.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4503" for this suite.

• [SLOW TEST:61.057 seconds]
[sig-apps] StatefulSet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":278,"completed":210,"skipped":3406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:41:48.292: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 10 09:41:48.723: INFO: Waiting up to 5m0s for pod "pod-eb46d45c-04b0-484a-b225-06884f2e97cb" in namespace "emptydir-5296" to be "success or failure"
Feb 10 09:41:48.735: INFO: Pod "pod-eb46d45c-04b0-484a-b225-06884f2e97cb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012698ms
Feb 10 09:41:50.746: INFO: Pod "pod-eb46d45c-04b0-484a-b225-06884f2e97cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023132207s
Feb 10 09:41:52.759: INFO: Pod "pod-eb46d45c-04b0-484a-b225-06884f2e97cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035374192s
Feb 10 09:41:54.772: INFO: Pod "pod-eb46d45c-04b0-484a-b225-06884f2e97cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048903931s
STEP: Saw pod success
Feb 10 09:41:54.772: INFO: Pod "pod-eb46d45c-04b0-484a-b225-06884f2e97cb" satisfied condition "success or failure"
Feb 10 09:41:54.782: INFO: Trying to get logs from node master3 pod pod-eb46d45c-04b0-484a-b225-06884f2e97cb container test-container: <nil>
STEP: delete the pod
Feb 10 09:41:55.245: INFO: Waiting for pod pod-eb46d45c-04b0-484a-b225-06884f2e97cb to disappear
Feb 10 09:41:55.256: INFO: Pod pod-eb46d45c-04b0-484a-b225-06884f2e97cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:41:55.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5296" for this suite.

• [SLOW TEST:7.002 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":211,"skipped":3432,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:41:55.295: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-942790b0-e552-41fe-baba-a234f2076223
[AfterEach] [sig-node] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:41:55.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9773" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":278,"completed":212,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:41:55.782: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3792
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3792
I0210 09:41:56.309738      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-3792, replica count: 2
I0210 09:41:59.360985      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 10 09:42:02.361: INFO: Creating new exec pod
I0210 09:42:02.361544      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 10 09:42:09.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-3792 execpod6srqv -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 10 09:42:10.840: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/*] succeeded!\n"
Feb 10 09:42:10.841: INFO: stdout: ""
Feb 10 09:42:10.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-3792 execpod6srqv -- /bin/sh -x -c nc -zv -t -w 2 10.155.120.234 80'
Feb 10 09:42:12.192: INFO: stderr: "+ nc -zv -t -w 2 10.155.120.234 80\nConnection to 10.155.120.234 80 port [tcp/*] succeeded!\n"
Feb 10 09:42:12.193: INFO: stdout: ""
Feb 10 09:42:12.193: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:42:12.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3792" for this suite.
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:16.540 seconds]
[sig-network] Services
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":278,"completed":213,"skipped":3458,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:42:12.325: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-3a5d8778-4822-4880-8bd2-85b9164348f8 in namespace container-probe-9139
Feb 10 09:42:18.811: INFO: Started pod busybox-3a5d8778-4822-4880-8bd2-85b9164348f8 in namespace container-probe-9139
STEP: checking the pod's current state and verifying that restartCount is present
Feb 10 09:42:18.824: INFO: Initial restart count of pod busybox-3a5d8778-4822-4880-8bd2-85b9164348f8 is 0
Feb 10 09:43:11.253: INFO: Restart count of pod container-probe-9139/busybox-3a5d8778-4822-4880-8bd2-85b9164348f8 is now 1 (52.429489258s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:43:11.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9139" for this suite.

• [SLOW TEST:59.023 seconds]
[k8s.io] Probing container
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":278,"completed":214,"skipped":3473,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:43:11.366: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 10 09:43:11.799: INFO: Waiting up to 5m0s for pod "pod-ee494dc8-2596-4d7a-8633-e97e24476633" in namespace "emptydir-1824" to be "success or failure"
Feb 10 09:43:11.811: INFO: Pod "pod-ee494dc8-2596-4d7a-8633-e97e24476633": Phase="Pending", Reason="", readiness=false. Elapsed: 11.641917ms
Feb 10 09:43:13.826: INFO: Pod "pod-ee494dc8-2596-4d7a-8633-e97e24476633": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027236611s
Feb 10 09:43:15.841: INFO: Pod "pod-ee494dc8-2596-4d7a-8633-e97e24476633": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04186604s
Feb 10 09:43:17.855: INFO: Pod "pod-ee494dc8-2596-4d7a-8633-e97e24476633": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055762286s
STEP: Saw pod success
Feb 10 09:43:17.855: INFO: Pod "pod-ee494dc8-2596-4d7a-8633-e97e24476633" satisfied condition "success or failure"
Feb 10 09:43:17.867: INFO: Trying to get logs from node master3 pod pod-ee494dc8-2596-4d7a-8633-e97e24476633 container test-container: <nil>
STEP: delete the pod
Feb 10 09:43:17.943: INFO: Waiting for pod pod-ee494dc8-2596-4d7a-8633-e97e24476633 to disappear
Feb 10 09:43:17.954: INFO: Pod pod-ee494dc8-2596-4d7a-8633-e97e24476633 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:43:17.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1824" for this suite.

• [SLOW TEST:6.631 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":215,"skipped":3493,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:43:17.999: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-ltrv
STEP: Creating a pod to test atomic-volume-subpath
Feb 10 09:43:18.518: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ltrv" in namespace "subpath-3441" to be "success or failure"
Feb 10 09:43:18.528: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.423597ms
Feb 10 09:43:20.542: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023823178s
Feb 10 09:43:22.554: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036317016s
Feb 10 09:43:24.570: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Running", Reason="", readiness=true. Elapsed: 6.051968328s
Feb 10 09:43:26.581: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Running", Reason="", readiness=true. Elapsed: 8.062784319s
Feb 10 09:43:28.594: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Running", Reason="", readiness=true. Elapsed: 10.075683444s
Feb 10 09:43:30.606: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Running", Reason="", readiness=true. Elapsed: 12.088498426s
Feb 10 09:43:32.618: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Running", Reason="", readiness=true. Elapsed: 14.100360245s
Feb 10 09:43:34.636: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Running", Reason="", readiness=true. Elapsed: 16.117683698s
Feb 10 09:43:36.650: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Running", Reason="", readiness=true. Elapsed: 18.131958649s
Feb 10 09:43:38.664: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Running", Reason="", readiness=true. Elapsed: 20.145967495s
Feb 10 09:43:40.676: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Running", Reason="", readiness=true. Elapsed: 22.158417159s
Feb 10 09:43:42.693: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Running", Reason="", readiness=true. Elapsed: 24.174787198s
Feb 10 09:43:44.709: INFO: Pod "pod-subpath-test-projected-ltrv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.191196673s
STEP: Saw pod success
Feb 10 09:43:44.709: INFO: Pod "pod-subpath-test-projected-ltrv" satisfied condition "success or failure"
Feb 10 09:43:44.722: INFO: Trying to get logs from node master3 pod pod-subpath-test-projected-ltrv container test-container-subpath-projected-ltrv: <nil>
STEP: delete the pod
Feb 10 09:43:44.818: INFO: Waiting for pod pod-subpath-test-projected-ltrv to disappear
Feb 10 09:43:44.830: INFO: Pod pod-subpath-test-projected-ltrv no longer exists
STEP: Deleting pod pod-subpath-test-projected-ltrv
Feb 10 09:43:44.830: INFO: Deleting pod "pod-subpath-test-projected-ltrv" in namespace "subpath-3441"
[AfterEach] [sig-storage] Subpath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:43:44.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3441" for this suite.

• [SLOW TEST:26.880 seconds]
[sig-storage] Subpath
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":278,"completed":216,"skipped":3512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:43:44.897: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5077
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:43:45.363: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 10 09:44:03.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-5077 create -f -'
Feb 10 09:44:07.101: INFO: stderr: ""
Feb 10 09:44:07.102: INFO: stdout: "e2e-test-crd-publish-openapi-1743-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 10 09:44:07.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-5077 delete e2e-test-crd-publish-openapi-1743-crds test-cr'
Feb 10 09:44:07.635: INFO: stderr: ""
Feb 10 09:44:07.636: INFO: stdout: "e2e-test-crd-publish-openapi-1743-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 10 09:44:07.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-5077 apply -f -'
Feb 10 09:44:08.651: INFO: stderr: ""
Feb 10 09:44:08.651: INFO: stdout: "e2e-test-crd-publish-openapi-1743-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 10 09:44:08.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-5077 delete e2e-test-crd-publish-openapi-1743-crds test-cr'
Feb 10 09:44:09.203: INFO: stderr: ""
Feb 10 09:44:09.204: INFO: stdout: "e2e-test-crd-publish-openapi-1743-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 10 09:44:09.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 explain e2e-test-crd-publish-openapi-1743-crds'
Feb 10 09:44:10.498: INFO: stderr: ""
Feb 10 09:44:10.498: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1743-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:44:23.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5077" for this suite.

• [SLOW TEST:38.745 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":278,"completed":217,"skipped":3580,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:44:23.644: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1980
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1980
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Feb 10 09:44:24.123: INFO: Found 0 stateful pods, waiting for 3
Feb 10 09:44:34.143: INFO: Found 2 stateful pods, waiting for 3
Feb 10 09:44:44.139: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:44:44.139: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:44:44.139: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 09:44:44.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-1980 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 10 09:44:45.605: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 10 09:44:45.605: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 10 09:44:45.605: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 10 09:44:55.714: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 10 09:45:05.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-1980 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 10 09:45:07.189: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 10 09:45:07.189: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 10 09:45:07.189: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 10 09:45:17.279: INFO: Waiting for StatefulSet statefulset-1980/ss2 to complete update
Feb 10 09:45:17.279: INFO: Waiting for Pod statefulset-1980/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 10 09:45:17.280: INFO: Waiting for Pod statefulset-1980/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 10 09:45:27.308: INFO: Waiting for StatefulSet statefulset-1980/ss2 to complete update
Feb 10 09:45:27.309: INFO: Waiting for Pod statefulset-1980/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 10 09:45:37.309: INFO: Waiting for StatefulSet statefulset-1980/ss2 to complete update
STEP: Rolling back to a previous revision
Feb 10 09:45:47.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-1980 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 10 09:45:48.704: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 10 09:45:48.704: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 10 09:45:48.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 10 09:45:58.804: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 10 09:46:08.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-1980 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 10 09:46:10.298: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 10 09:46:10.298: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 10 09:46:10.298: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 10 09:46:20.398: INFO: Waiting for StatefulSet statefulset-1980/ss2 to complete update
Feb 10 09:46:20.398: INFO: Waiting for Pod statefulset-1980/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 10 09:46:20.398: INFO: Waiting for Pod statefulset-1980/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 10 09:46:20.398: INFO: Waiting for Pod statefulset-1980/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 10 09:46:30.433: INFO: Waiting for StatefulSet statefulset-1980/ss2 to complete update
Feb 10 09:46:30.433: INFO: Waiting for Pod statefulset-1980/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 10 09:46:30.433: INFO: Waiting for Pod statefulset-1980/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 10 09:46:40.427: INFO: Waiting for StatefulSet statefulset-1980/ss2 to complete update
Feb 10 09:46:40.428: INFO: Waiting for Pod statefulset-1980/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 10 09:46:50.426: INFO: Waiting for StatefulSet statefulset-1980/ss2 to complete update
Feb 10 09:46:50.427: INFO: Waiting for Pod statefulset-1980/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 10 09:47:00.429: INFO: Waiting for StatefulSet statefulset-1980/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 10 09:47:10.431: INFO: Deleting all statefulset in ns statefulset-1980
Feb 10 09:47:10.440: INFO: Scaling statefulset ss2 to 0
Feb 10 09:47:50.501: INFO: Waiting for statefulset status.replicas updated to 0
Feb 10 09:47:50.514: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:47:50.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1980" for this suite.

• [SLOW TEST:206.985 seconds]
[sig-apps] StatefulSet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":278,"completed":218,"skipped":3598,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:47:50.634: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6638
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:48:07.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6638" for this suite.

• [SLOW TEST:16.791 seconds]
[sig-api-machinery] ResourceQuota
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":278,"completed":219,"skipped":3606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:48:07.429: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3828.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3828.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3828.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3828.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3828.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3828.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3828.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3828.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3828.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3828.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 10 09:48:15.957: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local from pod dns-3828/dns-test-804653d4-03c5-488d-9dac-d0dab567ab06: the server could not find the requested resource (get pods dns-test-804653d4-03c5-488d-9dac-d0dab567ab06)
Feb 10 09:48:15.971: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local from pod dns-3828/dns-test-804653d4-03c5-488d-9dac-d0dab567ab06: the server could not find the requested resource (get pods dns-test-804653d4-03c5-488d-9dac-d0dab567ab06)
Feb 10 09:48:15.983: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3828.svc.cluster.local from pod dns-3828/dns-test-804653d4-03c5-488d-9dac-d0dab567ab06: the server could not find the requested resource (get pods dns-test-804653d4-03c5-488d-9dac-d0dab567ab06)
Feb 10 09:48:16.000: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3828.svc.cluster.local from pod dns-3828/dns-test-804653d4-03c5-488d-9dac-d0dab567ab06: the server could not find the requested resource (get pods dns-test-804653d4-03c5-488d-9dac-d0dab567ab06)
Feb 10 09:48:16.041: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local from pod dns-3828/dns-test-804653d4-03c5-488d-9dac-d0dab567ab06: the server could not find the requested resource (get pods dns-test-804653d4-03c5-488d-9dac-d0dab567ab06)
Feb 10 09:48:16.054: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local from pod dns-3828/dns-test-804653d4-03c5-488d-9dac-d0dab567ab06: the server could not find the requested resource (get pods dns-test-804653d4-03c5-488d-9dac-d0dab567ab06)
Feb 10 09:48:16.076: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3828.svc.cluster.local from pod dns-3828/dns-test-804653d4-03c5-488d-9dac-d0dab567ab06: the server could not find the requested resource (get pods dns-test-804653d4-03c5-488d-9dac-d0dab567ab06)
Feb 10 09:48:16.094: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3828.svc.cluster.local from pod dns-3828/dns-test-804653d4-03c5-488d-9dac-d0dab567ab06: the server could not find the requested resource (get pods dns-test-804653d4-03c5-488d-9dac-d0dab567ab06)
Feb 10 09:48:16.129: INFO: Lookups using dns-3828/dns-test-804653d4-03c5-488d-9dac-d0dab567ab06 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3828.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3828.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3828.svc.cluster.local jessie_udp@dns-test-service-2.dns-3828.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3828.svc.cluster.local]

Feb 10 09:48:21.290: INFO: DNS probes using dns-3828/dns-test-804653d4-03c5-488d-9dac-d0dab567ab06 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:48:21.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3828" for this suite.

• [SLOW TEST:14.027 seconds]
[sig-network] DNS
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":278,"completed":220,"skipped":3633,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:48:21.457: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 10 09:48:21.946: INFO: Waiting up to 5m0s for pod "pod-6d9d453c-1049-4bd3-9199-a497eb074742" in namespace "emptydir-6171" to be "success or failure"
Feb 10 09:48:21.961: INFO: Pod "pod-6d9d453c-1049-4bd3-9199-a497eb074742": Phase="Pending", Reason="", readiness=false. Elapsed: 14.361477ms
Feb 10 09:48:23.978: INFO: Pod "pod-6d9d453c-1049-4bd3-9199-a497eb074742": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030982181s
Feb 10 09:48:25.991: INFO: Pod "pod-6d9d453c-1049-4bd3-9199-a497eb074742": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044510006s
Feb 10 09:48:28.004: INFO: Pod "pod-6d9d453c-1049-4bd3-9199-a497eb074742": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057126275s
STEP: Saw pod success
Feb 10 09:48:28.004: INFO: Pod "pod-6d9d453c-1049-4bd3-9199-a497eb074742" satisfied condition "success or failure"
Feb 10 09:48:28.015: INFO: Trying to get logs from node master3 pod pod-6d9d453c-1049-4bd3-9199-a497eb074742 container test-container: <nil>
STEP: delete the pod
Feb 10 09:48:28.392: INFO: Waiting for pod pod-6d9d453c-1049-4bd3-9199-a497eb074742 to disappear
Feb 10 09:48:28.403: INFO: Pod pod-6d9d453c-1049-4bd3-9199-a497eb074742 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:48:28.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6171" for this suite.

• [SLOW TEST:6.980 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":221,"skipped":3634,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:48:28.439: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 10 09:48:41.104: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 10 09:48:41.116: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 10 09:48:43.116: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 10 09:48:43.133: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 10 09:48:45.116: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 10 09:48:45.131: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 10 09:48:47.116: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 10 09:48:47.129: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 10 09:48:49.116: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 10 09:48:49.128: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:48:49.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6554" for this suite.

• [SLOW TEST:20.731 seconds]
[k8s.io] Container Lifecycle Hook
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":278,"completed":222,"skipped":3648,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:48:49.170: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6263
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 10 09:48:49.684: INFO: Waiting up to 5m0s for pod "pod-4bf47be6-f994-4b57-8a00-36c8259762ca" in namespace "emptydir-6263" to be "success or failure"
Feb 10 09:48:49.705: INFO: Pod "pod-4bf47be6-f994-4b57-8a00-36c8259762ca": Phase="Pending", Reason="", readiness=false. Elapsed: 20.862303ms
Feb 10 09:48:51.720: INFO: Pod "pod-4bf47be6-f994-4b57-8a00-36c8259762ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035952486s
Feb 10 09:48:53.739: INFO: Pod "pod-4bf47be6-f994-4b57-8a00-36c8259762ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054758799s
Feb 10 09:48:55.756: INFO: Pod "pod-4bf47be6-f994-4b57-8a00-36c8259762ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071756236s
STEP: Saw pod success
Feb 10 09:48:55.756: INFO: Pod "pod-4bf47be6-f994-4b57-8a00-36c8259762ca" satisfied condition "success or failure"
Feb 10 09:48:55.767: INFO: Trying to get logs from node master1 pod pod-4bf47be6-f994-4b57-8a00-36c8259762ca container test-container: <nil>
STEP: delete the pod
Feb 10 09:48:56.163: INFO: Waiting for pod pod-4bf47be6-f994-4b57-8a00-36c8259762ca to disappear
Feb 10 09:48:56.173: INFO: Pod pod-4bf47be6-f994-4b57-8a00-36c8259762ca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:48:56.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6263" for this suite.

• [SLOW TEST:7.042 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":223,"skipped":3659,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:48:56.213: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1729
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1729
STEP: creating replication controller externalsvc in namespace services-1729
I0210 09:48:56.764175      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-1729, replica count: 2
I0210 09:48:59.816887      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 09:49:02.817353      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 10 09:49:02.878: INFO: Creating new exec pod
Feb 10 09:49:08.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-1729 execpodst5f9 -- /bin/sh -x -c nslookup clusterip-service'
Feb 10 09:49:11.335: INFO: stderr: "+ nslookup clusterip-service\n"
Feb 10 09:49:11.335: INFO: stdout: "Server:    10.155.0.3\nAddress 1: 10.155.0.3 coredns.kube-system.svc.cluster.local\n\nName:      clusterip-service\nAddress 1: 10.155.241.70 externalsvc.services-1729.svc.cluster.local\n"
STEP: deleting ReplicationController externalsvc in namespace services-1729, will wait for the garbage collector to delete the pods
Feb 10 09:49:11.426: INFO: Deleting ReplicationController externalsvc took: 25.510425ms
Feb 10 09:49:11.828: INFO: Terminating ReplicationController externalsvc pods took: 401.781099ms
Feb 10 09:49:28.740: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:49:28.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1729" for this suite.
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:32.620 seconds]
[sig-network] Services
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":278,"completed":224,"skipped":3660,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:49:28.834: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 09:49:29.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac3ab5be-8669-4fc6-90e8-bc8fdac7bde5" in namespace "projected-2771" to be "success or failure"
Feb 10 09:49:29.337: INFO: Pod "downwardapi-volume-ac3ab5be-8669-4fc6-90e8-bc8fdac7bde5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009353ms
Feb 10 09:49:31.349: INFO: Pod "downwardapi-volume-ac3ab5be-8669-4fc6-90e8-bc8fdac7bde5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025784268s
Feb 10 09:49:33.362: INFO: Pod "downwardapi-volume-ac3ab5be-8669-4fc6-90e8-bc8fdac7bde5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039027911s
Feb 10 09:49:35.374: INFO: Pod "downwardapi-volume-ac3ab5be-8669-4fc6-90e8-bc8fdac7bde5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05099266s
STEP: Saw pod success
Feb 10 09:49:35.374: INFO: Pod "downwardapi-volume-ac3ab5be-8669-4fc6-90e8-bc8fdac7bde5" satisfied condition "success or failure"
Feb 10 09:49:35.388: INFO: Trying to get logs from node master1 pod downwardapi-volume-ac3ab5be-8669-4fc6-90e8-bc8fdac7bde5 container client-container: <nil>
STEP: delete the pod
Feb 10 09:49:35.486: INFO: Waiting for pod downwardapi-volume-ac3ab5be-8669-4fc6-90e8-bc8fdac7bde5 to disappear
Feb 10 09:49:35.497: INFO: Pod downwardapi-volume-ac3ab5be-8669-4fc6-90e8-bc8fdac7bde5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:49:35.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2771" for this suite.

• [SLOW TEST:6.706 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":278,"completed":225,"skipped":3667,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:49:35.542: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 10 09:49:42.548: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7559 pod-service-account-1aeccfd5-f428-48e2-98c2-64d02c6b3895 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 10 09:49:43.878: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7559 pod-service-account-1aeccfd5-f428-48e2-98c2-64d02c6b3895 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 10 09:49:45.279: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7559 pod-service-account-1aeccfd5-f428-48e2-98c2-64d02c6b3895 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:49:46.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7559" for this suite.

• [SLOW TEST:11.178 seconds]
[sig-auth] ServiceAccounts
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":278,"completed":226,"skipped":3668,"failed":0}
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:49:46.720: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run pod
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1841
[It] should create a pod from an image when restart is Never  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 10 09:49:47.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1252'
Feb 10 09:49:47.692: INFO: stderr: ""
Feb 10 09:49:47.692: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1846
Feb 10 09:49:47.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete pods e2e-test-httpd-pod --namespace=kubectl-1252'
Feb 10 09:49:50.998: INFO: stderr: ""
Feb 10 09:49:50.999: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:49:50.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1252" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":278,"completed":227,"skipped":3668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:49:51.094: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5335
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-00092032-4d38-48fb-93ba-084ba408eae9
STEP: Creating a pod to test consume configMaps
Feb 10 09:49:51.608: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef2e6676-9b82-456e-ade3-c1113239dfc2" in namespace "configmap-5335" to be "success or failure"
Feb 10 09:49:51.619: INFO: Pod "pod-configmaps-ef2e6676-9b82-456e-ade3-c1113239dfc2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.728109ms
Feb 10 09:49:53.632: INFO: Pod "pod-configmaps-ef2e6676-9b82-456e-ade3-c1113239dfc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023304324s
Feb 10 09:49:55.650: INFO: Pod "pod-configmaps-ef2e6676-9b82-456e-ade3-c1113239dfc2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041550771s
Feb 10 09:49:57.665: INFO: Pod "pod-configmaps-ef2e6676-9b82-456e-ade3-c1113239dfc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056396783s
STEP: Saw pod success
Feb 10 09:49:57.665: INFO: Pod "pod-configmaps-ef2e6676-9b82-456e-ade3-c1113239dfc2" satisfied condition "success or failure"
Feb 10 09:49:57.677: INFO: Trying to get logs from node master1 pod pod-configmaps-ef2e6676-9b82-456e-ade3-c1113239dfc2 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 09:49:57.771: INFO: Waiting for pod pod-configmaps-ef2e6676-9b82-456e-ade3-c1113239dfc2 to disappear
Feb 10 09:49:57.785: INFO: Pod pod-configmaps-ef2e6676-9b82-456e-ade3-c1113239dfc2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:49:57.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5335" for this suite.

• [SLOW TEST:6.729 seconds]
[sig-storage] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":228,"skipped":3696,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:49:57.828: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:50:24.053: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 09:50:26.105: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925024, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925024, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925024, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925024, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:50:28.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925024, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925024, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925024, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925024, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:50:31.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:50:31.175: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:50:38.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-690" for this suite.
STEP: Destroying namespace "webhook-690-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:40.609 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":278,"completed":229,"skipped":3733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:50:38.443: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-77b266b2-d16a-41f9-8088-83e193823fc9 in namespace container-probe-6363
Feb 10 09:50:44.979: INFO: Started pod liveness-77b266b2-d16a-41f9-8088-83e193823fc9 in namespace container-probe-6363
STEP: checking the pod's current state and verifying that restartCount is present
Feb 10 09:50:44.997: INFO: Initial restart count of pod liveness-77b266b2-d16a-41f9-8088-83e193823fc9 is 0
Feb 10 09:51:09.179: INFO: Restart count of pod container-probe-6363/liveness-77b266b2-d16a-41f9-8088-83e193823fc9 is now 1 (24.181758382s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:51:09.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6363" for this suite.

• [SLOW TEST:30.826 seconds]
[k8s.io] Probing container
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":278,"completed":230,"skipped":3794,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:51:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:51:35.361: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 09:51:37.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:51:39.415: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:51:41.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925095, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:51:44.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:51:44.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5180" for this suite.
STEP: Destroying namespace "webhook-5180-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:35.898 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":278,"completed":231,"skipped":3796,"failed":0}
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:51:45.176: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 10 09:51:52.204: INFO: Successfully updated pod "adopt-release-48c5t"
STEP: Checking that the Job readopts the Pod
Feb 10 09:51:52.204: INFO: Waiting up to 15m0s for pod "adopt-release-48c5t" in namespace "job-4363" to be "adopted"
Feb 10 09:51:52.215: INFO: Pod "adopt-release-48c5t": Phase="Running", Reason="", readiness=true. Elapsed: 10.541332ms
Feb 10 09:51:54.225: INFO: Pod "adopt-release-48c5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.02048742s
Feb 10 09:51:54.225: INFO: Pod "adopt-release-48c5t" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 10 09:51:54.755: INFO: Successfully updated pod "adopt-release-48c5t"
STEP: Checking that the Job releases the Pod
Feb 10 09:51:54.756: INFO: Waiting up to 15m0s for pod "adopt-release-48c5t" in namespace "job-4363" to be "released"
Feb 10 09:51:54.770: INFO: Pod "adopt-release-48c5t": Phase="Running", Reason="", readiness=true. Elapsed: 14.726857ms
Feb 10 09:51:56.786: INFO: Pod "adopt-release-48c5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.0308102s
Feb 10 09:51:56.787: INFO: Pod "adopt-release-48c5t" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:51:56.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4363" for this suite.

• [SLOW TEST:11.646 seconds]
[sig-apps] Job
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":278,"completed":232,"skipped":3796,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:51:56.826: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5504.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5504.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5504.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5504.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 10 09:52:05.334: INFO: DNS probes using dns-test-2b4dc0ef-a3b2-4811-ba85-532181120f22 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5504.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5504.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5504.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5504.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 10 09:52:13.529: INFO: DNS probes using dns-test-6b367cc0-e652-4fdc-b6e5-afe571525c80 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5504.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5504.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5504.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5504.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 10 09:52:21.743: INFO: DNS probes using dns-test-82536adc-79ea-4d62-a307-d9862cd4a906 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:52:21.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5504" for this suite.

• [SLOW TEST:25.067 seconds]
[sig-network] DNS
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":278,"completed":233,"skipped":3805,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:52:21.893: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9100
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-f1bbe9d2-71f5-435e-ad3e-4d505f8195f7 in namespace container-probe-9100
Feb 10 09:52:28.374: INFO: Started pod test-webserver-f1bbe9d2-71f5-435e-ad3e-4d505f8195f7 in namespace container-probe-9100
STEP: checking the pod's current state and verifying that restartCount is present
Feb 10 09:52:28.385: INFO: Initial restart count of pod test-webserver-f1bbe9d2-71f5-435e-ad3e-4d505f8195f7 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:56:30.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9100" for this suite.

• [SLOW TEST:248.284 seconds]
[k8s.io] Probing container
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":278,"completed":234,"skipped":3811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:56:30.179: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 10 09:56:30.588: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 10 09:56:30.638: INFO: Waiting for terminating namespaces to be deleted...
Feb 10 09:56:30.646: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Feb 10 09:56:30.999: INFO: kube-scheduler-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container kube-scheduler ready: true, restart count 572
Feb 10 09:56:31.000: INFO: nginx-proxy-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container nginx-proxy ready: true, restart count 845
Feb 10 09:56:31.000: INFO: coredns-hfdns from kube-system started at 2020-01-15 01:54:01 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container coredns ready: true, restart count 1
Feb 10 09:56:31.000: INFO: agnhost-deployment-54964f567d-nvhtm from kube-system started at 2020-02-10 07:28:38 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container agnhost ready: true, restart count 0
Feb 10 09:56:31.000: INFO: sonobuoy-e2e-job-26d59f1c3cea4685 from sonobuoy started at 2020-02-10 08:27:25 +0000 UTC (2 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container e2e ready: true, restart count 0
Feb 10 09:56:31.000: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 10 09:56:31.000: INFO: kube-proxy-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 09:56:31.000: INFO: localpv-provisioner-ds-nt4r5 from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container provisioner ready: true, restart count 1
Feb 10 09:56:31.000: INFO: kube-apiserver-master1 from kube-system started at 2020-02-05 16:29:54 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container kube-apiserver ready: true, restart count 3021
Feb 10 09:56:31.000: INFO: ss2-1 from statefulset-6054 started at 2020-02-10 05:24:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container webserver ready: true, restart count 0
Feb 10 09:56:31.000: INFO: kube-controller-manager-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container kube-controller-manager ready: true, restart count 565
Feb 10 09:56:31.000: INFO: ntp-zj7fc from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container ntp ready: true, restart count 1
Feb 10 09:56:31.000: INFO: calico-node-p64wz from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container calico-node ready: true, restart count 1
Feb 10 09:56:31.000: INFO: simpletest-rc-to-be-deleted-f48d7 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.000: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:56:31.000: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Feb 10 09:56:31.349: INFO: nginx-proxy-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.350: INFO: 	Container nginx-proxy ready: true, restart count 846
Feb 10 09:56:31.350: INFO: kube-apiserver-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.350: INFO: 	Container kube-apiserver ready: true, restart count 563
Feb 10 09:56:31.350: INFO: localpv-provisioner-ds-sd5nt from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.350: INFO: 	Container provisioner ready: true, restart count 3
Feb 10 09:56:31.350: INFO: tiller-deploy-75cd95b6b4-tkpds from kube-system started at 2020-02-07 13:38:14 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.350: INFO: 	Container tiller ready: true, restart count 0
Feb 10 09:56:31.350: INFO: simpletest-rc-to-be-deleted-hvgr7 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.350: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:56:31.350: INFO: kube-scheduler-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.351: INFO: 	Container kube-scheduler ready: true, restart count 561
Feb 10 09:56:31.351: INFO: kube-controller-manager-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.351: INFO: 	Container kube-controller-manager ready: true, restart count 581
Feb 10 09:56:31.351: INFO: ntp-g2jhr from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.351: INFO: 	Container ntp ready: true, restart count 1
Feb 10 09:56:31.351: INFO: kube-proxy-master2 from kube-system started at 2020-02-05 16:28:09 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.351: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 09:56:31.351: INFO: coredns-sv7qc from kube-system started at 2020-01-15 02:50:54 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.351: INFO: 	Container coredns ready: true, restart count 1
Feb 10 09:56:31.351: INFO: calico-node-q6h5s from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.352: INFO: 	Container calico-node ready: true, restart count 2
Feb 10 09:56:31.352: INFO: ss2-2 from statefulset-6054 started at 2020-02-10 05:25:22 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.352: INFO: 	Container webserver ready: true, restart count 0
Feb 10 09:56:31.352: INFO: simpletest-rc-to-be-deleted-62s49 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.352: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:56:31.352: INFO: sonobuoy from sonobuoy started at 2020-02-10 08:27:21 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.352: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 10 09:56:31.352: INFO: metrics-server-6c6fd884-md7t8 from kube-system started at 2020-01-15 01:54:18 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.352: INFO: 	Container metrics-server ready: true, restart count 1
Feb 10 09:56:31.352: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Feb 10 09:56:31.699: INFO: calico-node-6fr4n from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.699: INFO: 	Container calico-node ready: true, restart count 2
Feb 10 09:56:31.699: INFO: simpletest-rc-to-be-deleted-clkth from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.699: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:56:31.699: INFO: localpv-provisioner-ds-vhkpq from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.699: INFO: 	Container provisioner ready: true, restart count 3
Feb 10 09:56:31.699: INFO: simpletest-rc-to-be-deleted-g226k from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.699: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:56:31.699: INFO: ntp-c9lwf from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.700: INFO: 	Container ntp ready: true, restart count 1
Feb 10 09:56:31.700: INFO: calico-kube-controllers-6fcbd694f7-zs7dw from kube-system started at 2020-02-07 13:38:14 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.700: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 10 09:56:31.700: INFO: ss2-0 from statefulset-6054 started at 2020-02-10 05:25:17 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.700: INFO: 	Container webserver ready: true, restart count 0
Feb 10 09:56:31.700: INFO: kube-apiserver-master3 from kube-system started at 2020-02-05 16:28:26 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.700: INFO: 	Container kube-apiserver ready: true, restart count 563
Feb 10 09:56:31.700: INFO: kube-controller-manager-master3 from kube-system started at 2020-02-05 16:28:26 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.700: INFO: 	Container kube-controller-manager ready: true, restart count 553
Feb 10 09:56:31.700: INFO: kube-proxy-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.700: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 09:56:31.700: INFO: kube-scheduler-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.700: INFO: 	Container kube-scheduler ready: true, restart count 574
Feb 10 09:56:31.700: INFO: nginx-proxy-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.700: INFO: 	Container nginx-proxy ready: true, restart count 851
Feb 10 09:56:31.700: INFO: coredns-bsmzw from kube-system started at 2020-01-15 01:54:55 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.700: INFO: 	Container coredns ready: true, restart count 1
Feb 10 09:56:31.700: INFO: ntp-server-8cbc779-hj2rj from kube-system started at 2020-01-15 03:48:56 +0000 UTC (1 container statuses recorded)
Feb 10 09:56:31.700: INFO: 	Container ntp-server ready: true, restart count 1
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b7c9d821-4c26-455d-b27a-cbafadecec1d 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-b7c9d821-4c26-455d-b27a-cbafadecec1d off the node master3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b7c9d821-4c26-455d-b27a-cbafadecec1d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:56:56.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6235" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:25.914 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":278,"completed":235,"skipped":3850,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:56:56.095: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 10 09:57:21.518: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 10 09:57:23.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925441, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925441, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925441, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925441, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 10 09:57:25.572: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925441, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925441, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925441, loc:(*time.Location)(0x127bc8cc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63716925441, loc:(*time.Location)(0x127bc8cc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 10 09:57:28.626: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 09:57:28.640: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9215-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:57:35.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1012" for this suite.
STEP: Destroying namespace "webhook-1012-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:39.913 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":278,"completed":236,"skipped":3864,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:57:36.008: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 10 09:57:36.620: INFO: Waiting up to 5m0s for pod "pod-a9300e23-0d3d-414c-b2fc-bc7a971e2b31" in namespace "emptydir-730" to be "success or failure"
Feb 10 09:57:36.635: INFO: Pod "pod-a9300e23-0d3d-414c-b2fc-bc7a971e2b31": Phase="Pending", Reason="", readiness=false. Elapsed: 15.358137ms
Feb 10 09:57:38.677: INFO: Pod "pod-a9300e23-0d3d-414c-b2fc-bc7a971e2b31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057091079s
Feb 10 09:57:40.694: INFO: Pod "pod-a9300e23-0d3d-414c-b2fc-bc7a971e2b31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074412609s
Feb 10 09:57:42.711: INFO: Pod "pod-a9300e23-0d3d-414c-b2fc-bc7a971e2b31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090987733s
STEP: Saw pod success
Feb 10 09:57:42.711: INFO: Pod "pod-a9300e23-0d3d-414c-b2fc-bc7a971e2b31" satisfied condition "success or failure"
Feb 10 09:57:42.725: INFO: Trying to get logs from node master3 pod pod-a9300e23-0d3d-414c-b2fc-bc7a971e2b31 container test-container: <nil>
STEP: delete the pod
Feb 10 09:57:42.808: INFO: Waiting for pod pod-a9300e23-0d3d-414c-b2fc-bc7a971e2b31 to disappear
Feb 10 09:57:42.821: INFO: Pod pod-a9300e23-0d3d-414c-b2fc-bc7a971e2b31 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:57:42.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-730" for this suite.

• [SLOW TEST:6.847 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":237,"skipped":3871,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:57:42.857: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-242
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-caf505db-7ce3-4117-a142-1df30e0a23e3
STEP: Creating secret with name s-test-opt-upd-cba47dc9-8a21-4449-927a-dec78bc8a3c1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-caf505db-7ce3-4117-a142-1df30e0a23e3
STEP: Updating secret s-test-opt-upd-cba47dc9-8a21-4449-927a-dec78bc8a3c1
STEP: Creating secret with name s-test-opt-create-c7461321-8087-4885-9700-35ca54de0fe4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 09:57:53.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-242" for this suite.

• [SLOW TEST:10.860 seconds]
[sig-storage] Projected secret
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":238,"skipped":3891,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 09:57:53.718: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 10 09:57:54.139: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 10 09:57:54.193: INFO: Waiting for terminating namespaces to be deleted...
Feb 10 09:57:54.204: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Feb 10 09:57:54.250: INFO: kube-proxy-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.250: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 09:57:54.250: INFO: localpv-provisioner-ds-nt4r5 from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container provisioner ready: true, restart count 1
Feb 10 09:57:54.251: INFO: kube-apiserver-master1 from kube-system started at 2020-02-05 16:29:54 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container kube-apiserver ready: true, restart count 3021
Feb 10 09:57:54.251: INFO: ss2-1 from statefulset-6054 started at 2020-02-10 05:24:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container webserver ready: true, restart count 0
Feb 10 09:57:54.251: INFO: pod-projected-secrets-81195d6d-2c36-4bc2-b90d-002a84f9c4df from projected-242 started at 2020-02-10 09:57:43 +0000 UTC (3 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container creates-volume-test ready: true, restart count 0
Feb 10 09:57:54.251: INFO: 	Container dels-volume-test ready: true, restart count 0
Feb 10 09:57:54.251: INFO: 	Container upds-volume-test ready: true, restart count 0
Feb 10 09:57:54.251: INFO: kube-controller-manager-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container kube-controller-manager ready: true, restart count 565
Feb 10 09:57:54.251: INFO: ntp-zj7fc from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container ntp ready: true, restart count 1
Feb 10 09:57:54.251: INFO: calico-node-p64wz from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container calico-node ready: true, restart count 1
Feb 10 09:57:54.251: INFO: simpletest-rc-to-be-deleted-f48d7 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:57:54.251: INFO: kube-scheduler-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container kube-scheduler ready: true, restart count 572
Feb 10 09:57:54.251: INFO: nginx-proxy-master1 from kube-system started at 2020-02-10 04:12:59 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container nginx-proxy ready: true, restart count 845
Feb 10 09:57:54.251: INFO: coredns-hfdns from kube-system started at 2020-01-15 01:54:01 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container coredns ready: true, restart count 1
Feb 10 09:57:54.251: INFO: agnhost-deployment-54964f567d-nvhtm from kube-system started at 2020-02-10 07:28:38 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container agnhost ready: true, restart count 0
Feb 10 09:57:54.251: INFO: sonobuoy-e2e-job-26d59f1c3cea4685 from sonobuoy started at 2020-02-10 08:27:25 +0000 UTC (2 container statuses recorded)
Feb 10 09:57:54.251: INFO: 	Container e2e ready: true, restart count 0
Feb 10 09:57:54.251: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 10 09:57:54.251: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Feb 10 09:57:54.292: INFO: kube-controller-manager-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container kube-controller-manager ready: true, restart count 581
Feb 10 09:57:54.292: INFO: ntp-g2jhr from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container ntp ready: true, restart count 1
Feb 10 09:57:54.292: INFO: kube-scheduler-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container kube-scheduler ready: true, restart count 561
Feb 10 09:57:54.292: INFO: coredns-sv7qc from kube-system started at 2020-01-15 02:50:54 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container coredns ready: true, restart count 1
Feb 10 09:57:54.292: INFO: calico-node-q6h5s from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container calico-node ready: true, restart count 2
Feb 10 09:57:54.292: INFO: ss2-2 from statefulset-6054 started at 2020-02-10 05:25:22 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container webserver ready: true, restart count 0
Feb 10 09:57:54.292: INFO: simpletest-rc-to-be-deleted-62s49 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:57:54.292: INFO: sonobuoy from sonobuoy started at 2020-02-10 08:27:21 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 10 09:57:54.292: INFO: kube-proxy-master2 from kube-system started at 2020-02-05 16:28:09 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 09:57:54.292: INFO: metrics-server-6c6fd884-md7t8 from kube-system started at 2020-01-15 01:54:18 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container metrics-server ready: true, restart count 1
Feb 10 09:57:54.292: INFO: kube-apiserver-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container kube-apiserver ready: true, restart count 563
Feb 10 09:57:54.292: INFO: localpv-provisioner-ds-sd5nt from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container provisioner ready: true, restart count 3
Feb 10 09:57:54.292: INFO: tiller-deploy-75cd95b6b4-tkpds from kube-system started at 2020-02-07 13:38:14 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container tiller ready: true, restart count 0
Feb 10 09:57:54.292: INFO: simpletest-rc-to-be-deleted-hvgr7 from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:57:54.292: INFO: nginx-proxy-master2 from kube-system started at 2020-02-05 16:42:35 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.292: INFO: 	Container nginx-proxy ready: true, restart count 846
Feb 10 09:57:54.292: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Feb 10 09:57:54.331: INFO: simpletest-rc-to-be-deleted-clkth from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.331: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:57:54.332: INFO: calico-node-6fr4n from kube-system started at 2020-01-17 02:14:08 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.332: INFO: 	Container calico-node ready: true, restart count 2
Feb 10 09:57:54.332: INFO: simpletest-rc-to-be-deleted-g226k from gc-821 started at 2020-02-10 08:16:41 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.332: INFO: 	Container nginx ready: true, restart count 0
Feb 10 09:57:54.332: INFO: localpv-provisioner-ds-vhkpq from kube-system started at 2020-02-03 08:48:15 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.332: INFO: 	Container provisioner ready: true, restart count 3
Feb 10 09:57:54.332: INFO: calico-kube-controllers-6fcbd694f7-zs7dw from kube-system started at 2020-02-07 13:38:14 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.332: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 10 09:57:54.333: INFO: ss2-0 from statefulset-6054 started at 2020-02-10 05:25:17 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.333: INFO: 	Container webserver ready: true, restart count 0
Feb 10 09:57:54.333: INFO: ntp-c9lwf from kube-system started at 2020-01-15 03:48:51 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.333: INFO: 	Container ntp ready: true, restart count 1
Feb 10 09:57:54.333: INFO: kube-controller-manager-master3 from kube-system started at 2020-02-05 16:28:26 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.333: INFO: 	Container kube-controller-manager ready: true, restart count 553
Feb 10 09:57:54.333: INFO: kube-proxy-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.333: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 10 09:57:54.333: INFO: kube-scheduler-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.333: INFO: 	Container kube-scheduler ready: true, restart count 574
Feb 10 09:57:54.334: INFO: nginx-proxy-master3 from kube-system started at 2020-02-05 16:42:30 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.334: INFO: 	Container nginx-proxy ready: true, restart count 851
Feb 10 09:57:54.334: INFO: coredns-bsmzw from kube-system started at 2020-01-15 01:54:55 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.334: INFO: 	Container coredns ready: true, restart count 1
Feb 10 09:57:54.334: INFO: ntp-server-8cbc779-hj2rj from kube-system started at 2020-01-15 03:48:56 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.334: INFO: 	Container ntp-server ready: true, restart count 1
Feb 10 09:57:54.334: INFO: kube-apiserver-master3 from kube-system started at 2020-02-05 16:28:26 +0000 UTC (1 container statuses recorded)
Feb 10 09:57:54.335: INFO: 	Container kube-apiserver ready: true, restart count 563
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f206a2d0-2ecb-4ab2-ab20-443976ca7435 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-f206a2d0-2ecb-4ab2-ab20-443976ca7435 off the node master3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f206a2d0-2ecb-4ab2-ab20-443976ca7435
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:03:06.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1780" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:312.986 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":278,"completed":239,"skipped":3897,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:03:06.705: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6095
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6095
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6095
STEP: creating replication controller externalsvc in namespace services-6095
I0210 10:03:07.219811      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-6095, replica count: 2
I0210 10:03:10.270911      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 10:03:13.271516      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 10 10:03:13.376: INFO: Creating new exec pod
Feb 10 10:03:19.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-6095 execpodbbrwm -- /bin/sh -x -c nslookup nodeport-service'
Feb 10 10:03:23.552: INFO: stderr: "+ nslookup nodeport-service\n"
Feb 10 10:03:23.552: INFO: stdout: "Server:    10.155.0.3\nAddress 1: 10.155.0.3 coredns.kube-system.svc.cluster.local\n\nName:      nodeport-service\nAddress 1: 10.155.64.55 externalsvc.services-6095.svc.cluster.local\n"
STEP: deleting ReplicationController externalsvc in namespace services-6095, will wait for the garbage collector to delete the pods
Feb 10 10:03:23.655: INFO: Deleting ReplicationController externalsvc took: 30.462623ms
Feb 10 10:03:24.055: INFO: Terminating ReplicationController externalsvc pods took: 400.704169ms
Feb 10 10:03:38.743: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:03:38.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6095" for this suite.
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:32.141 seconds]
[sig-network] Services
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":278,"completed":240,"skipped":3905,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:03:38.851: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:03:45.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6154" for this suite.

• [SLOW TEST:6.856 seconds]
[k8s.io] Docker Containers
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":278,"completed":241,"skipped":3915,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:03:45.708: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5115
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-5115
I0210 10:03:46.236056      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-5115, replica count: 2
I0210 10:03:49.287104      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 10:03:52.290110      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 10 10:03:52.290: INFO: Creating new exec pod
Feb 10 10:03:59.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-5115 execpodfbzjt -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 10 10:04:00.713: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/*] succeeded!\n"
Feb 10 10:04:00.713: INFO: stdout: ""
Feb 10 10:04:00.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-5115 execpodfbzjt -- /bin/sh -x -c nc -zv -t -w 2 10.155.227.71 80'
Feb 10 10:04:02.097: INFO: stderr: "+ nc -zv -t -w 2 10.155.227.71 80\nConnection to 10.155.227.71 80 port [tcp/*] succeeded!\n"
Feb 10 10:04:02.097: INFO: stdout: ""
Feb 10 10:04:02.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-5115 execpodfbzjt -- /bin/sh -x -c nc -zv -t -w 2 172.16.3.137 30105'
Feb 10 10:04:03.489: INFO: stderr: "+ nc -zv -t -w 2 172.16.3.137 30105\nConnection to 172.16.3.137 30105 port [tcp/*] succeeded!\n"
Feb 10 10:04:03.490: INFO: stdout: ""
Feb 10 10:04:03.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=services-5115 execpodfbzjt -- /bin/sh -x -c nc -zv -t -w 2 172.16.3.136 30105'
Feb 10 10:04:04.877: INFO: stderr: "+ nc -zv -t -w 2 172.16.3.136 30105\nConnection to 172.16.3.136 30105 port [tcp/*] succeeded!\n"
Feb 10 10:04:04.877: INFO: stdout: ""
Feb 10 10:04:04.877: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:04:04.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5115" for this suite.
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:19.307 seconds]
[sig-network] Services
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":278,"completed":242,"skipped":3927,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:04:05.028: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl label
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1362
STEP: creating the pod
Feb 10 10:04:05.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-4327'
Feb 10 10:04:06.796: INFO: stderr: ""
Feb 10 10:04:06.797: INFO: stdout: "pod/pause created\n"
Feb 10 10:04:06.797: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 10 10:04:06.797: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4327" to be "running and ready"
Feb 10 10:04:06.814: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 17.115692ms
Feb 10 10:04:08.827: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030113451s
Feb 10 10:04:10.846: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049034711s
Feb 10 10:04:12.864: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.067095508s
Feb 10 10:04:12.864: INFO: Pod "pause" satisfied condition "running and ready"
Feb 10 10:04:12.865: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 10 10:04:12.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 label pods pause testing-label=testing-label-value --namespace=kubectl-4327'
Feb 10 10:04:13.427: INFO: stderr: ""
Feb 10 10:04:13.427: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 10 10:04:13.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pod pause -L testing-label --namespace=kubectl-4327'
Feb 10 10:04:13.986: INFO: stderr: ""
Feb 10 10:04:13.987: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 10 10:04:13.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 label pods pause testing-label- --namespace=kubectl-4327'
Feb 10 10:04:14.579: INFO: stderr: ""
Feb 10 10:04:14.579: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 10 10:04:14.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pod pause -L testing-label --namespace=kubectl-4327'
Feb 10 10:04:15.155: INFO: stderr: ""
Feb 10 10:04:15.155: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          9s    \n"
[AfterEach] Kubectl label
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1369
STEP: using delete to clean up resources
Feb 10 10:04:15.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 delete --grace-period=0 --force -f - --namespace=kubectl-4327'
Feb 10 10:04:15.731: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 10 10:04:15.732: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 10 10:04:15.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get rc,svc -l name=pause --no-headers --namespace=kubectl-4327'
Feb 10 10:04:16.274: INFO: stderr: "No resources found in kubectl-4327 namespace.\n"
Feb 10 10:04:16.275: INFO: stdout: ""
Feb 10 10:04:16.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 get pods -l name=pause --namespace=kubectl-4327 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 10 10:04:16.758: INFO: stderr: ""
Feb 10 10:04:16.758: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:04:16.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4327" for this suite.

• [SLOW TEST:11.772 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
    should update the label on a resource  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":278,"completed":243,"skipped":3982,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:04:16.803: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-8203/configmap-test-0de27607-58c6-4b03-9b26-d233aaf82c8c
STEP: Creating a pod to test consume configMaps
Feb 10 10:04:17.265: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad5e973d-d878-40c5-a6d8-9d86f041c8c4" in namespace "configmap-8203" to be "success or failure"
Feb 10 10:04:17.279: INFO: Pod "pod-configmaps-ad5e973d-d878-40c5-a6d8-9d86f041c8c4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.347509ms
Feb 10 10:04:19.293: INFO: Pod "pod-configmaps-ad5e973d-d878-40c5-a6d8-9d86f041c8c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027791013s
Feb 10 10:04:21.306: INFO: Pod "pod-configmaps-ad5e973d-d878-40c5-a6d8-9d86f041c8c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040895392s
Feb 10 10:04:23.323: INFO: Pod "pod-configmaps-ad5e973d-d878-40c5-a6d8-9d86f041c8c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057640171s
STEP: Saw pod success
Feb 10 10:04:23.323: INFO: Pod "pod-configmaps-ad5e973d-d878-40c5-a6d8-9d86f041c8c4" satisfied condition "success or failure"
Feb 10 10:04:23.332: INFO: Trying to get logs from node master3 pod pod-configmaps-ad5e973d-d878-40c5-a6d8-9d86f041c8c4 container env-test: <nil>
STEP: delete the pod
Feb 10 10:04:23.413: INFO: Waiting for pod pod-configmaps-ad5e973d-d878-40c5-a6d8-9d86f041c8c4 to disappear
Feb 10 10:04:23.424: INFO: Pod pod-configmaps-ad5e973d-d878-40c5-a6d8-9d86f041c8c4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:04:23.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8203" for this suite.

• [SLOW TEST:6.658 seconds]
[sig-node] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":278,"completed":244,"skipped":4003,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:04:23.462: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9334
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:04:23.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9334" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":278,"completed":245,"skipped":4023,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:04:23.946: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-4584
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 10 10:04:24.389: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 10 10:04:50.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.156.32.17:8080/dial?request=hostname&protocol=http&host=10.156.161.243&port=8080&tries=1'] Namespace:pod-network-test-4584 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 10:04:50.712: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 10:04:51.545: INFO: Waiting for responses: map[]
Feb 10 10:04:51.556: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.156.32.17:8080/dial?request=hostname&protocol=http&host=10.156.208.107&port=8080&tries=1'] Namespace:pod-network-test-4584 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 10:04:51.556: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 10:04:52.432: INFO: Waiting for responses: map[]
Feb 10 10:04:52.445: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.156.32.17:8080/dial?request=hostname&protocol=http&host=10.156.32.16&port=8080&tries=1'] Namespace:pod-network-test-4584 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 10:04:52.445: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 10:04:53.283: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:04:53.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4584" for this suite.

• [SLOW TEST:29.382 seconds]
[sig-network] Networking
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":246,"skipped":4046,"failed":0}
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:04:53.328: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-6550
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6550 to expose endpoints map[]
Feb 10 10:04:53.805: INFO: successfully validated that service multi-endpoint-test in namespace services-6550 exposes endpoints map[] (27.948359ms elapsed)
STEP: Creating pod pod1 in namespace services-6550
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6550 to expose endpoints map[pod1:[100]]
Feb 10 10:04:57.969: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.125255198s elapsed, will retry)
Feb 10 10:04:59.011: INFO: successfully validated that service multi-endpoint-test in namespace services-6550 exposes endpoints map[pod1:[100]] (5.167753644s elapsed)
STEP: Creating pod pod2 in namespace services-6550
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6550 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 10 10:05:03.251: INFO: Unexpected endpoints: found map[7c9287e3-9e49-4b40-b6f8-9b7a5cbcf390:[100]], expected map[pod1:[100] pod2:[101]] (4.218945387s elapsed, will retry)
Feb 10 10:05:05.321: INFO: successfully validated that service multi-endpoint-test in namespace services-6550 exposes endpoints map[pod1:[100] pod2:[101]] (6.289257079s elapsed)
STEP: Deleting pod pod1 in namespace services-6550
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6550 to expose endpoints map[pod2:[101]]
Feb 10 10:05:06.407: INFO: successfully validated that service multi-endpoint-test in namespace services-6550 exposes endpoints map[pod2:[101]] (1.061226913s elapsed)
STEP: Deleting pod pod2 in namespace services-6550
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6550 to expose endpoints map[]
Feb 10 10:05:07.459: INFO: successfully validated that service multi-endpoint-test in namespace services-6550 exposes endpoints map[] (1.02940231s elapsed)
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:05:07.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6550" for this suite.
[AfterEach] [sig-network] Services
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:14.267 seconds]
[sig-network] Services
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":278,"completed":247,"skipped":4046,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:05:07.597: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 10:05:08.009: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1803
I0210 10:05:08.073496      23 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1803, replica count: 1
I0210 10:05:09.124540      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 10:05:10.124959      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 10:05:11.125928      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 10:05:12.126386      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0210 10:05:13.126754      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 10 10:05:13.268: INFO: Created: latency-svc-cmcpg
Feb 10 10:05:13.298: INFO: Got endpoints: latency-svc-cmcpg [71.745389ms]
Feb 10 10:05:13.343: INFO: Created: latency-svc-lzrph
Feb 10 10:05:13.368: INFO: Got endpoints: latency-svc-lzrph [68.316227ms]
Feb 10 10:05:13.392: INFO: Created: latency-svc-hsnd9
Feb 10 10:05:13.409: INFO: Created: latency-svc-mr5k7
Feb 10 10:05:13.434: INFO: Got endpoints: latency-svc-hsnd9 [134.182152ms]
Feb 10 10:05:13.436: INFO: Got endpoints: latency-svc-mr5k7 [135.911193ms]
Feb 10 10:05:13.454: INFO: Created: latency-svc-n44sq
Feb 10 10:05:13.472: INFO: Got endpoints: latency-svc-n44sq [171.748998ms]
Feb 10 10:05:13.482: INFO: Created: latency-svc-fhsml
Feb 10 10:05:13.505: INFO: Got endpoints: latency-svc-fhsml [204.32854ms]
Feb 10 10:05:13.522: INFO: Created: latency-svc-4vxgz
Feb 10 10:05:13.541: INFO: Got endpoints: latency-svc-4vxgz [240.284065ms]
Feb 10 10:05:13.561: INFO: Created: latency-svc-wxfq8
Feb 10 10:05:13.585: INFO: Got endpoints: latency-svc-wxfq8 [283.178254ms]
Feb 10 10:05:13.603: INFO: Created: latency-svc-rxhqt
Feb 10 10:05:13.631: INFO: Got endpoints: latency-svc-rxhqt [328.954525ms]
Feb 10 10:05:13.637: INFO: Created: latency-svc-jwqrw
Feb 10 10:05:13.654: INFO: Got endpoints: latency-svc-jwqrw [352.600621ms]
Feb 10 10:05:13.683: INFO: Created: latency-svc-5hrkt
Feb 10 10:05:13.702: INFO: Got endpoints: latency-svc-5hrkt [400.507995ms]
Feb 10 10:05:13.714: INFO: Created: latency-svc-5n5vt
Feb 10 10:05:13.734: INFO: Got endpoints: latency-svc-5n5vt [431.316695ms]
Feb 10 10:05:13.745: INFO: Created: latency-svc-kzjm7
Feb 10 10:05:13.762: INFO: Got endpoints: latency-svc-kzjm7 [460.199015ms]
Feb 10 10:05:13.779: INFO: Created: latency-svc-m94bs
Feb 10 10:05:13.812: INFO: Created: latency-svc-g7b6g
Feb 10 10:05:13.812: INFO: Got endpoints: latency-svc-m94bs [510.082349ms]
Feb 10 10:05:13.835: INFO: Got endpoints: latency-svc-g7b6g [533.423605ms]
Feb 10 10:05:13.844: INFO: Created: latency-svc-5m6jj
Feb 10 10:05:13.863: INFO: Got endpoints: latency-svc-5m6jj [560.536124ms]
Feb 10 10:05:13.868: INFO: Created: latency-svc-cqn24
Feb 10 10:05:13.890: INFO: Got endpoints: latency-svc-cqn24 [521.931437ms]
Feb 10 10:05:13.899: INFO: Created: latency-svc-4vvqk
Feb 10 10:05:13.915: INFO: Got endpoints: latency-svc-4vvqk [481.14109ms]
Feb 10 10:05:13.934: INFO: Created: latency-svc-l2hqz
Feb 10 10:05:13.957: INFO: Got endpoints: latency-svc-l2hqz [520.563776ms]
Feb 10 10:05:13.978: INFO: Created: latency-svc-8pd4m
Feb 10 10:05:14.005: INFO: Got endpoints: latency-svc-8pd4m [532.588124ms]
Feb 10 10:05:14.025: INFO: Created: latency-svc-tp9fk
Feb 10 10:05:14.038: INFO: Got endpoints: latency-svc-tp9fk [532.172584ms]
Feb 10 10:05:14.048: INFO: Created: latency-svc-8ndhr
Feb 10 10:05:14.066: INFO: Got endpoints: latency-svc-8ndhr [523.985839ms]
Feb 10 10:05:14.077: INFO: Created: latency-svc-x4hpn
Feb 10 10:05:14.099: INFO: Got endpoints: latency-svc-x4hpn [514.671773ms]
Feb 10 10:05:14.108: INFO: Created: latency-svc-bbnqz
Feb 10 10:05:14.128: INFO: Got endpoints: latency-svc-bbnqz [497.07806ms]
Feb 10 10:05:14.138: INFO: Created: latency-svc-fr7b9
Feb 10 10:05:14.161: INFO: Got endpoints: latency-svc-fr7b9 [506.867487ms]
Feb 10 10:05:14.166: INFO: Created: latency-svc-tx9pk
Feb 10 10:05:14.182: INFO: Got endpoints: latency-svc-tx9pk [480.454989ms]
Feb 10 10:05:14.193: INFO: Created: latency-svc-msrlh
Feb 10 10:05:14.211: INFO: Got endpoints: latency-svc-msrlh [476.847806ms]
Feb 10 10:05:14.224: INFO: Created: latency-svc-pbdpx
Feb 10 10:05:14.241: INFO: Got endpoints: latency-svc-pbdpx [479.030308ms]
Feb 10 10:05:14.258: INFO: Created: latency-svc-98c9w
Feb 10 10:05:14.277: INFO: Got endpoints: latency-svc-98c9w [464.642318ms]
Feb 10 10:05:14.288: INFO: Created: latency-svc-v64rm
Feb 10 10:05:14.320: INFO: Got endpoints: latency-svc-v64rm [483.919891ms]
Feb 10 10:05:14.333: INFO: Created: latency-svc-tdrz9
Feb 10 10:05:14.359: INFO: Got endpoints: latency-svc-tdrz9 [495.611379ms]
Feb 10 10:05:14.381: INFO: Created: latency-svc-k7svw
Feb 10 10:05:14.407: INFO: Got endpoints: latency-svc-k7svw [517.433374ms]
Feb 10 10:05:14.417: INFO: Created: latency-svc-zqmxg
Feb 10 10:05:14.442: INFO: Got endpoints: latency-svc-zqmxg [526.74418ms]
Feb 10 10:05:14.494: INFO: Created: latency-svc-tmr8s
Feb 10 10:05:14.495: INFO: Created: latency-svc-cmfwz
Feb 10 10:05:14.495: INFO: Got endpoints: latency-svc-cmfwz [538.168569ms]
Feb 10 10:05:14.499: INFO: Got endpoints: latency-svc-tmr8s [493.913058ms]
Feb 10 10:05:14.519: INFO: Created: latency-svc-qgz9f
Feb 10 10:05:14.529: INFO: Got endpoints: latency-svc-qgz9f [489.734215ms]
Feb 10 10:05:14.537: INFO: Created: latency-svc-tp2j8
Feb 10 10:05:14.551: INFO: Got endpoints: latency-svc-tp2j8 [484.880731ms]
Feb 10 10:05:14.573: INFO: Created: latency-svc-q6544
Feb 10 10:05:14.596: INFO: Got endpoints: latency-svc-q6544 [496.73644ms]
Feb 10 10:05:14.615: INFO: Created: latency-svc-rqpk4
Feb 10 10:05:14.642: INFO: Got endpoints: latency-svc-rqpk4 [513.501792ms]
Feb 10 10:05:14.652: INFO: Created: latency-svc-lsc9k
Feb 10 10:05:14.679: INFO: Created: latency-svc-42f28
Feb 10 10:05:14.710: INFO: Got endpoints: latency-svc-42f28 [527.804321ms]
Feb 10 10:05:14.710: INFO: Got endpoints: latency-svc-lsc9k [548.565156ms]
Feb 10 10:05:14.714: INFO: Created: latency-svc-9bpln
Feb 10 10:05:14.740: INFO: Got endpoints: latency-svc-9bpln [528.883862ms]
Feb 10 10:05:14.748: INFO: Created: latency-svc-lkzzq
Feb 10 10:05:14.775: INFO: Got endpoints: latency-svc-lkzzq [533.224685ms]
Feb 10 10:05:14.783: INFO: Created: latency-svc-9kkvf
Feb 10 10:05:14.811: INFO: Got endpoints: latency-svc-9kkvf [533.648305ms]
Feb 10 10:05:14.823: INFO: Created: latency-svc-55xl8
Feb 10 10:05:14.843: INFO: Got endpoints: latency-svc-55xl8 [523.522679ms]
Feb 10 10:05:14.862: INFO: Created: latency-svc-zt7jc
Feb 10 10:05:14.877: INFO: Got endpoints: latency-svc-zt7jc [518.138615ms]
Feb 10 10:05:14.892: INFO: Created: latency-svc-dbsmv
Feb 10 10:05:14.939: INFO: Got endpoints: latency-svc-dbsmv [531.005723ms]
Feb 10 10:05:14.957: INFO: Created: latency-svc-b7cm6
Feb 10 10:05:14.975: INFO: Got endpoints: latency-svc-b7cm6 [532.398224ms]
Feb 10 10:05:15.013: INFO: Created: latency-svc-4b6dg
Feb 10 10:05:15.044: INFO: Got endpoints: latency-svc-4b6dg [105.339112ms]
Feb 10 10:05:15.048: INFO: Created: latency-svc-2nx9b
Feb 10 10:05:15.065: INFO: Got endpoints: latency-svc-2nx9b [569.763429ms]
Feb 10 10:05:15.079: INFO: Created: latency-svc-c6s6c
Feb 10 10:05:15.116: INFO: Got endpoints: latency-svc-c6s6c [616.746102ms]
Feb 10 10:05:15.120: INFO: Created: latency-svc-p6mf7
Feb 10 10:05:15.150: INFO: Got endpoints: latency-svc-p6mf7 [620.130184ms]
Feb 10 10:05:15.154: INFO: Created: latency-svc-xms9g
Feb 10 10:05:15.170: INFO: Got endpoints: latency-svc-xms9g [619.636243ms]
Feb 10 10:05:15.197: INFO: Created: latency-svc-pb4s6
Feb 10 10:05:15.223: INFO: Got endpoints: latency-svc-pb4s6 [626.879508ms]
Feb 10 10:05:15.251: INFO: Created: latency-svc-sqxqm
Feb 10 10:05:15.277: INFO: Got endpoints: latency-svc-sqxqm [635.098334ms]
Feb 10 10:05:15.291: INFO: Created: latency-svc-cq242
Feb 10 10:05:15.316: INFO: Got endpoints: latency-svc-cq242 [605.409094ms]
Feb 10 10:05:15.329: INFO: Created: latency-svc-jmqzp
Feb 10 10:05:15.347: INFO: Got endpoints: latency-svc-jmqzp [635.904335ms]
Feb 10 10:05:15.359: INFO: Created: latency-svc-5qt7m
Feb 10 10:05:15.378: INFO: Got endpoints: latency-svc-5qt7m [638.474376ms]
Feb 10 10:05:15.395: INFO: Created: latency-svc-f9dx8
Feb 10 10:05:15.416: INFO: Created: latency-svc-r7whq
Feb 10 10:05:15.422: INFO: Got endpoints: latency-svc-f9dx8 [647.455502ms]
Feb 10 10:05:15.444: INFO: Got endpoints: latency-svc-r7whq [633.108412ms]
Feb 10 10:05:15.470: INFO: Created: latency-svc-mnxrk
Feb 10 10:05:15.483: INFO: Got endpoints: latency-svc-mnxrk [639.374297ms]
Feb 10 10:05:15.503: INFO: Created: latency-svc-jxkdz
Feb 10 10:05:15.521: INFO: Got endpoints: latency-svc-jxkdz [643.249039ms]
Feb 10 10:05:15.536: INFO: Created: latency-svc-7hdwf
Feb 10 10:05:15.562: INFO: Got endpoints: latency-svc-7hdwf [586.543121ms]
Feb 10 10:05:15.565: INFO: Created: latency-svc-sdhlc
Feb 10 10:05:15.584: INFO: Got endpoints: latency-svc-sdhlc [539.689729ms]
Feb 10 10:05:15.594: INFO: Created: latency-svc-cc6fd
Feb 10 10:05:15.615: INFO: Got endpoints: latency-svc-cc6fd [549.592816ms]
Feb 10 10:05:15.648: INFO: Created: latency-svc-6bwzw
Feb 10 10:05:15.673: INFO: Got endpoints: latency-svc-6bwzw [556.507181ms]
Feb 10 10:05:15.686: INFO: Created: latency-svc-t6jmr
Feb 10 10:05:15.713: INFO: Got endpoints: latency-svc-t6jmr [563.327505ms]
Feb 10 10:05:15.725: INFO: Created: latency-svc-s4fvz
Feb 10 10:05:15.747: INFO: Got endpoints: latency-svc-s4fvz [575.941174ms]
Feb 10 10:05:15.753: INFO: Created: latency-svc-hjfx6
Feb 10 10:05:15.771: INFO: Got endpoints: latency-svc-hjfx6 [547.118333ms]
Feb 10 10:05:15.788: INFO: Created: latency-svc-9mcl6
Feb 10 10:05:15.816: INFO: Got endpoints: latency-svc-9mcl6 [538.943488ms]
Feb 10 10:05:15.824: INFO: Created: latency-svc-2vz5x
Feb 10 10:05:15.842: INFO: Got endpoints: latency-svc-2vz5x [526.181119ms]
Feb 10 10:05:15.854: INFO: Created: latency-svc-qnp22
Feb 10 10:05:15.878: INFO: Got endpoints: latency-svc-qnp22 [531.307003ms]
Feb 10 10:05:15.896: INFO: Created: latency-svc-twvnv
Feb 10 10:05:15.920: INFO: Created: latency-svc-554tq
Feb 10 10:05:15.928: INFO: Got endpoints: latency-svc-twvnv [549.052855ms]
Feb 10 10:05:15.942: INFO: Got endpoints: latency-svc-554tq [518.806794ms]
Feb 10 10:05:15.954: INFO: Created: latency-svc-bwbhp
Feb 10 10:05:15.971: INFO: Got endpoints: latency-svc-bwbhp [526.31592ms]
Feb 10 10:05:15.989: INFO: Created: latency-svc-htz8h
Feb 10 10:05:16.005: INFO: Got endpoints: latency-svc-htz8h [522.326317ms]
Feb 10 10:05:16.033: INFO: Created: latency-svc-9s5nn
Feb 10 10:05:16.053: INFO: Got endpoints: latency-svc-9s5nn [532.294803ms]
Feb 10 10:05:16.065: INFO: Created: latency-svc-tzxj9
Feb 10 10:05:16.093: INFO: Got endpoints: latency-svc-tzxj9 [531.151863ms]
Feb 10 10:05:16.110: INFO: Created: latency-svc-fj9dp
Feb 10 10:05:16.135: INFO: Got endpoints: latency-svc-fj9dp [550.519236ms]
Feb 10 10:05:16.144: INFO: Created: latency-svc-sghpb
Feb 10 10:05:16.161: INFO: Got endpoints: latency-svc-sghpb [546.227693ms]
Feb 10 10:05:16.167: INFO: Created: latency-svc-6xnjz
Feb 10 10:05:16.193: INFO: Got endpoints: latency-svc-6xnjz [519.639175ms]
Feb 10 10:05:16.200: INFO: Created: latency-svc-n4wkv
Feb 10 10:05:16.241: INFO: Got endpoints: latency-svc-n4wkv [527.840061ms]
Feb 10 10:05:16.246: INFO: Created: latency-svc-9bct2
Feb 10 10:05:16.272: INFO: Created: latency-svc-sbxcf
Feb 10 10:05:16.272: INFO: Got endpoints: latency-svc-9bct2 [525.231659ms]
Feb 10 10:05:16.292: INFO: Got endpoints: latency-svc-sbxcf [521.538637ms]
Feb 10 10:05:16.310: INFO: Created: latency-svc-h2x9j
Feb 10 10:05:16.332: INFO: Got endpoints: latency-svc-h2x9j [516.063033ms]
Feb 10 10:05:16.341: INFO: Created: latency-svc-gddj7
Feb 10 10:05:16.365: INFO: Got endpoints: latency-svc-gddj7 [522.031257ms]
Feb 10 10:05:16.376: INFO: Created: latency-svc-6qh4g
Feb 10 10:05:16.405: INFO: Created: latency-svc-lhsg6
Feb 10 10:05:16.415: INFO: Got endpoints: latency-svc-6qh4g [536.366086ms]
Feb 10 10:05:16.441: INFO: Created: latency-svc-ph5tz
Feb 10 10:05:16.460: INFO: Got endpoints: latency-svc-lhsg6 [531.848163ms]
Feb 10 10:05:16.472: INFO: Created: latency-svc-lct7v
Feb 10 10:05:16.480: INFO: Got endpoints: latency-svc-ph5tz [537.925147ms]
Feb 10 10:05:16.497: INFO: Got endpoints: latency-svc-lct7v [526.419999ms]
Feb 10 10:05:16.506: INFO: Created: latency-svc-qtbhw
Feb 10 10:05:16.533: INFO: Got endpoints: latency-svc-qtbhw [526.91564ms]
Feb 10 10:05:16.536: INFO: Created: latency-svc-sfk8x
Feb 10 10:05:16.568: INFO: Got endpoints: latency-svc-sfk8x [514.369292ms]
Feb 10 10:05:16.570: INFO: Created: latency-svc-w6kjk
Feb 10 10:05:16.595: INFO: Got endpoints: latency-svc-w6kjk [501.864882ms]
Feb 10 10:05:16.605: INFO: Created: latency-svc-9pp4v
Feb 10 10:05:16.628: INFO: Got endpoints: latency-svc-9pp4v [493.360017ms]
Feb 10 10:05:16.641: INFO: Created: latency-svc-dlgwv
Feb 10 10:05:16.660: INFO: Got endpoints: latency-svc-dlgwv [498.594101ms]
Feb 10 10:05:16.666: INFO: Created: latency-svc-qfcqp
Feb 10 10:05:16.692: INFO: Got endpoints: latency-svc-qfcqp [499.219001ms]
Feb 10 10:05:16.698: INFO: Created: latency-svc-m649d
Feb 10 10:05:16.723: INFO: Got endpoints: latency-svc-m649d [481.417288ms]
Feb 10 10:05:16.726: INFO: Created: latency-svc-cxjcp
Feb 10 10:05:16.749: INFO: Got endpoints: latency-svc-cxjcp [476.553305ms]
Feb 10 10:05:16.756: INFO: Created: latency-svc-nsx87
Feb 10 10:05:16.771: INFO: Got endpoints: latency-svc-nsx87 [478.623266ms]
Feb 10 10:05:16.797: INFO: Created: latency-svc-sjhhq
Feb 10 10:05:16.823: INFO: Got endpoints: latency-svc-sjhhq [490.804035ms]
Feb 10 10:05:16.838: INFO: Created: latency-svc-hqw56
Feb 10 10:05:16.861: INFO: Got endpoints: latency-svc-hqw56 [496.323039ms]
Feb 10 10:05:16.867: INFO: Created: latency-svc-6gq5k
Feb 10 10:05:16.907: INFO: Created: latency-svc-bmv4c
Feb 10 10:05:16.920: INFO: Got endpoints: latency-svc-6gq5k [504.815965ms]
Feb 10 10:05:16.940: INFO: Created: latency-svc-c4q6r
Feb 10 10:05:16.963: INFO: Got endpoints: latency-svc-bmv4c [502.337003ms]
Feb 10 10:05:16.967: INFO: Created: latency-svc-qlx4z
Feb 10 10:05:16.999: INFO: Created: latency-svc-d69j2
Feb 10 10:05:17.020: INFO: Got endpoints: latency-svc-c4q6r [540.209728ms]
Feb 10 10:05:17.030: INFO: Created: latency-svc-vjdl9
Feb 10 10:05:17.065: INFO: Got endpoints: latency-svc-qlx4z [567.463348ms]
Feb 10 10:05:17.070: INFO: Created: latency-svc-jwqq6
Feb 10 10:05:17.079: INFO: Created: latency-svc-pvx9n
Feb 10 10:05:17.112: INFO: Created: latency-svc-6fhbx
Feb 10 10:05:17.118: INFO: Got endpoints: latency-svc-d69j2 [585.250539ms]
Feb 10 10:05:17.142: INFO: Created: latency-svc-r4nx4
Feb 10 10:05:17.166: INFO: Got endpoints: latency-svc-vjdl9 [598.586408ms]
Feb 10 10:05:17.185: INFO: Created: latency-svc-cxjsh
Feb 10 10:05:17.210: INFO: Created: latency-svc-qfrlm
Feb 10 10:05:17.216: INFO: Got endpoints: latency-svc-jwqq6 [620.346943ms]
Feb 10 10:05:17.231: INFO: Created: latency-svc-knzpp
Feb 10 10:05:17.258: INFO: Got endpoints: latency-svc-pvx9n [629.90759ms]
Feb 10 10:05:17.267: INFO: Created: latency-svc-nxzvf
Feb 10 10:05:17.310: INFO: Created: latency-svc-hbmqp
Feb 10 10:05:17.327: INFO: Got endpoints: latency-svc-6fhbx [666.760135ms]
Feb 10 10:05:17.354: INFO: Created: latency-svc-lkw57
Feb 10 10:05:17.369: INFO: Got endpoints: latency-svc-r4nx4 [676.559921ms]
Feb 10 10:05:17.396: INFO: Created: latency-svc-69gzm
Feb 10 10:05:17.415: INFO: Got endpoints: latency-svc-cxjsh [691.423111ms]
Feb 10 10:05:17.422: INFO: Created: latency-svc-2dr2d
Feb 10 10:05:17.446: INFO: Created: latency-svc-wqhhl
Feb 10 10:05:17.462: INFO: Got endpoints: latency-svc-qfrlm [713.681887ms]
Feb 10 10:05:17.478: INFO: Created: latency-svc-zpkwx
Feb 10 10:05:17.510: INFO: Created: latency-svc-k7k2h
Feb 10 10:05:17.512: INFO: Got endpoints: latency-svc-knzpp [741.220506ms]
Feb 10 10:05:17.544: INFO: Created: latency-svc-t4zdw
Feb 10 10:05:17.560: INFO: Got endpoints: latency-svc-nxzvf [736.526282ms]
Feb 10 10:05:17.569: INFO: Created: latency-svc-4cxfs
Feb 10 10:05:17.599: INFO: Created: latency-svc-6rcs8
Feb 10 10:05:17.615: INFO: Got endpoints: latency-svc-hbmqp [753.438694ms]
Feb 10 10:05:17.627: INFO: Created: latency-svc-84tqv
Feb 10 10:05:17.651: INFO: Created: latency-svc-k4vsc
Feb 10 10:05:17.677: INFO: Got endpoints: latency-svc-lkw57 [757.296216ms]
Feb 10 10:05:17.686: INFO: Created: latency-svc-jhsh8
Feb 10 10:05:17.720: INFO: Got endpoints: latency-svc-69gzm [757.042356ms]
Feb 10 10:05:17.735: INFO: Created: latency-svc-96rnt
Feb 10 10:05:17.757: INFO: Got endpoints: latency-svc-2dr2d [737.175823ms]
Feb 10 10:05:17.768: INFO: Created: latency-svc-gzlkt
Feb 10 10:05:17.801: INFO: Created: latency-svc-9c5j8
Feb 10 10:05:17.815: INFO: Got endpoints: latency-svc-wqhhl [749.815931ms]
Feb 10 10:05:17.835: INFO: Created: latency-svc-ltk62
Feb 10 10:05:17.872: INFO: Got endpoints: latency-svc-zpkwx [753.868214ms]
Feb 10 10:05:17.915: INFO: Got endpoints: latency-svc-k7k2h [748.07127ms]
Feb 10 10:05:17.956: INFO: Created: latency-svc-jdcjd
Feb 10 10:05:17.959: INFO: Got endpoints: latency-svc-t4zdw [743.138547ms]
Feb 10 10:05:17.985: INFO: Created: latency-svc-fjxvk
Feb 10 10:05:18.022: INFO: Created: latency-svc-8hb9x
Feb 10 10:05:18.031: INFO: Got endpoints: latency-svc-4cxfs [772.004846ms]
Feb 10 10:05:18.064: INFO: Got endpoints: latency-svc-6rcs8 [736.690202ms]
Feb 10 10:05:18.078: INFO: Created: latency-svc-l9h76
Feb 10 10:05:18.102: INFO: Created: latency-svc-w74wx
Feb 10 10:05:18.118: INFO: Got endpoints: latency-svc-84tqv [748.839511ms]
Feb 10 10:05:18.139: INFO: Created: latency-svc-4qwlb
Feb 10 10:05:18.179: INFO: Got endpoints: latency-svc-k4vsc [763.983521ms]
Feb 10 10:05:18.192: INFO: Created: latency-svc-jb8nd
Feb 10 10:05:18.222: INFO: Created: latency-svc-nc456
Feb 10 10:05:18.228: INFO: Got endpoints: latency-svc-jhsh8 [764.987401ms]
Feb 10 10:05:18.256: INFO: Created: latency-svc-jgm7z
Feb 10 10:05:18.262: INFO: Got endpoints: latency-svc-96rnt [749.143091ms]
Feb 10 10:05:18.289: INFO: Created: latency-svc-whbgd
Feb 10 10:05:18.311: INFO: Got endpoints: latency-svc-gzlkt [750.704791ms]
Feb 10 10:05:18.318: INFO: Created: latency-svc-4rshv
Feb 10 10:05:18.359: INFO: Created: latency-svc-twnwt
Feb 10 10:05:18.369: INFO: Got endpoints: latency-svc-9c5j8 [754.086374ms]
Feb 10 10:05:18.409: INFO: Created: latency-svc-vghz6
Feb 10 10:05:18.414: INFO: Got endpoints: latency-svc-ltk62 [736.896323ms]
Feb 10 10:05:18.448: INFO: Created: latency-svc-vkfgs
Feb 10 10:05:18.462: INFO: Got endpoints: latency-svc-jdcjd [741.983225ms]
Feb 10 10:05:18.482: INFO: Created: latency-svc-9wtw5
Feb 10 10:05:18.507: INFO: Created: latency-svc-kd5qz
Feb 10 10:05:18.513: INFO: Got endpoints: latency-svc-fjxvk [755.173755ms]
Feb 10 10:05:18.549: INFO: Created: latency-svc-wlnx2
Feb 10 10:05:18.583: INFO: Got endpoints: latency-svc-8hb9x [768.341784ms]
Feb 10 10:05:18.625: INFO: Got endpoints: latency-svc-l9h76 [752.404253ms]
Feb 10 10:05:18.634: INFO: Created: latency-svc-t94kv
Feb 10 10:05:18.667: INFO: Got endpoints: latency-svc-w74wx [752.134513ms]
Feb 10 10:05:18.683: INFO: Created: latency-svc-8zqg7
Feb 10 10:05:18.709: INFO: Got endpoints: latency-svc-4qwlb [749.71491ms]
Feb 10 10:05:18.718: INFO: Created: latency-svc-26qv9
Feb 10 10:05:18.751: INFO: Created: latency-svc-bvsjl
Feb 10 10:05:18.762: INFO: Got endpoints: latency-svc-jb8nd [730.819598ms]
Feb 10 10:05:18.812: INFO: Created: latency-svc-n9clp
Feb 10 10:05:18.817: INFO: Got endpoints: latency-svc-nc456 [753.107193ms]
Feb 10 10:05:18.865: INFO: Got endpoints: latency-svc-jgm7z [746.524848ms]
Feb 10 10:05:18.865: INFO: Created: latency-svc-6g5wz
Feb 10 10:05:18.912: INFO: Created: latency-svc-bcgqc
Feb 10 10:05:18.916: INFO: Got endpoints: latency-svc-whbgd [737.296383ms]
Feb 10 10:05:18.964: INFO: Created: latency-svc-hjrwb
Feb 10 10:05:18.973: INFO: Got endpoints: latency-svc-4rshv [745.511547ms]
Feb 10 10:05:19.014: INFO: Got endpoints: latency-svc-twnwt [751.710392ms]
Feb 10 10:05:19.023: INFO: Created: latency-svc-qkzbf
Feb 10 10:05:19.065: INFO: Created: latency-svc-tzpsc
Feb 10 10:05:19.065: INFO: Got endpoints: latency-svc-vghz6 [754.225434ms]
Feb 10 10:05:19.113: INFO: Created: latency-svc-5ltlk
Feb 10 10:05:19.122: INFO: Got endpoints: latency-svc-vkfgs [753.223293ms]
Feb 10 10:05:19.166: INFO: Got endpoints: latency-svc-9wtw5 [751.957492ms]
Feb 10 10:05:19.174: INFO: Created: latency-svc-jxs8k
Feb 10 10:05:19.210: INFO: Got endpoints: latency-svc-kd5qz [748.26239ms]
Feb 10 10:05:19.220: INFO: Created: latency-svc-f4mkn
Feb 10 10:05:19.261: INFO: Created: latency-svc-7svn8
Feb 10 10:05:19.272: INFO: Got endpoints: latency-svc-wlnx2 [759.627817ms]
Feb 10 10:05:19.314: INFO: Got endpoints: latency-svc-t94kv [730.565697ms]
Feb 10 10:05:19.325: INFO: Created: latency-svc-6bpxj
Feb 10 10:05:19.373: INFO: Got endpoints: latency-svc-8zqg7 [748.212129ms]
Feb 10 10:05:19.377: INFO: Created: latency-svc-fhp8s
Feb 10 10:05:19.413: INFO: Got endpoints: latency-svc-26qv9 [745.718108ms]
Feb 10 10:05:19.424: INFO: Created: latency-svc-vpmd9
Feb 10 10:05:19.459: INFO: Created: latency-svc-7ggjn
Feb 10 10:05:19.467: INFO: Got endpoints: latency-svc-bvsjl [757.345775ms]
Feb 10 10:05:19.521: INFO: Created: latency-svc-qh79w
Feb 10 10:05:19.524: INFO: Got endpoints: latency-svc-n9clp [762.694539ms]
Feb 10 10:05:19.572: INFO: Created: latency-svc-hf9hl
Feb 10 10:05:19.576: INFO: Got endpoints: latency-svc-6g5wz [758.020556ms]
Feb 10 10:05:19.611: INFO: Got endpoints: latency-svc-bcgqc [746.385188ms]
Feb 10 10:05:19.615: INFO: Created: latency-svc-gr8bb
Feb 10 10:05:19.655: INFO: Created: latency-svc-6bxcm
Feb 10 10:05:19.661: INFO: Got endpoints: latency-svc-hjrwb [743.805226ms]
Feb 10 10:05:19.701: INFO: Created: latency-svc-l7zsc
Feb 10 10:05:19.714: INFO: Got endpoints: latency-svc-qkzbf [739.890963ms]
Feb 10 10:05:19.752: INFO: Created: latency-svc-pgs5m
Feb 10 10:05:19.763: INFO: Got endpoints: latency-svc-tzpsc [749.46829ms]
Feb 10 10:05:19.812: INFO: Got endpoints: latency-svc-5ltlk [745.960148ms]
Feb 10 10:05:19.816: INFO: Created: latency-svc-r5tz8
Feb 10 10:05:19.865: INFO: Got endpoints: latency-svc-jxs8k [742.081965ms]
Feb 10 10:05:19.867: INFO: Created: latency-svc-hn54f
Feb 10 10:05:19.906: INFO: Created: latency-svc-zl8cn
Feb 10 10:05:19.912: INFO: Got endpoints: latency-svc-f4mkn [744.886527ms]
Feb 10 10:05:19.951: INFO: Created: latency-svc-2vp6v
Feb 10 10:05:19.961: INFO: Got endpoints: latency-svc-7svn8 [749.180409ms]
Feb 10 10:05:20.003: INFO: Created: latency-svc-5kxr7
Feb 10 10:05:20.010: INFO: Got endpoints: latency-svc-6bpxj [736.016141ms]
Feb 10 10:05:20.049: INFO: Created: latency-svc-pggmq
Feb 10 10:05:20.064: INFO: Got endpoints: latency-svc-fhp8s [749.64227ms]
Feb 10 10:05:20.112: INFO: Got endpoints: latency-svc-vpmd9 [739.127743ms]
Feb 10 10:05:20.112: INFO: Created: latency-svc-whskq
Feb 10 10:05:20.158: INFO: Created: latency-svc-njlcd
Feb 10 10:05:20.165: INFO: Got endpoints: latency-svc-7ggjn [751.395591ms]
Feb 10 10:05:20.204: INFO: Created: latency-svc-s5d5z
Feb 10 10:05:20.220: INFO: Got endpoints: latency-svc-qh79w [752.841532ms]
Feb 10 10:05:20.258: INFO: Got endpoints: latency-svc-hf9hl [733.670919ms]
Feb 10 10:05:20.269: INFO: Created: latency-svc-tppvb
Feb 10 10:05:20.302: INFO: Created: latency-svc-t9fqx
Feb 10 10:05:20.310: INFO: Got endpoints: latency-svc-gr8bb [734.171699ms]
Feb 10 10:05:20.354: INFO: Created: latency-svc-9g5ds
Feb 10 10:05:20.360: INFO: Got endpoints: latency-svc-6bxcm [749.09789ms]
Feb 10 10:05:20.403: INFO: Created: latency-svc-lqxqk
Feb 10 10:05:20.412: INFO: Got endpoints: latency-svc-l7zsc [750.873631ms]
Feb 10 10:05:20.452: INFO: Created: latency-svc-fnsn8
Feb 10 10:05:20.459: INFO: Got endpoints: latency-svc-pgs5m [744.746167ms]
Feb 10 10:05:20.500: INFO: Created: latency-svc-s7v9t
Feb 10 10:05:20.517: INFO: Got endpoints: latency-svc-r5tz8 [753.428172ms]
Feb 10 10:05:20.564: INFO: Created: latency-svc-8x4sj
Feb 10 10:05:20.566: INFO: Got endpoints: latency-svc-hn54f [754.171413ms]
Feb 10 10:05:20.607: INFO: Created: latency-svc-zsbtc
Feb 10 10:05:20.624: INFO: Got endpoints: latency-svc-zl8cn [758.189136ms]
Feb 10 10:05:20.662: INFO: Created: latency-svc-ps7sj
Feb 10 10:05:20.664: INFO: Got endpoints: latency-svc-2vp6v [751.572591ms]
Feb 10 10:05:20.707: INFO: Created: latency-svc-fq82x
Feb 10 10:05:20.718: INFO: Got endpoints: latency-svc-5kxr7 [756.180314ms]
Feb 10 10:05:20.763: INFO: Got endpoints: latency-svc-pggmq [752.203991ms]
Feb 10 10:05:20.769: INFO: Created: latency-svc-5mftx
Feb 10 10:05:20.820: INFO: Got endpoints: latency-svc-whskq [755.831213ms]
Feb 10 10:05:20.830: INFO: Created: latency-svc-qjw4m
Feb 10 10:05:20.866: INFO: Got endpoints: latency-svc-njlcd [753.474992ms]
Feb 10 10:05:20.871: INFO: Created: latency-svc-bl7q9
Feb 10 10:05:20.918: INFO: Created: latency-svc-dc4wh
Feb 10 10:05:20.923: INFO: Got endpoints: latency-svc-s5d5z [757.903095ms]
Feb 10 10:05:20.959: INFO: Created: latency-svc-zdp84
Feb 10 10:05:20.966: INFO: Got endpoints: latency-svc-tppvb [745.967307ms]
Feb 10 10:05:21.004: INFO: Created: latency-svc-wr4dl
Feb 10 10:05:21.011: INFO: Got endpoints: latency-svc-t9fqx [752.770451ms]
Feb 10 10:05:21.054: INFO: Created: latency-svc-8tsb9
Feb 10 10:05:21.062: INFO: Got endpoints: latency-svc-9g5ds [752.349911ms]
Feb 10 10:05:21.101: INFO: Created: latency-svc-4zq94
Feb 10 10:05:21.111: INFO: Got endpoints: latency-svc-lqxqk [750.52159ms]
Feb 10 10:05:21.151: INFO: Created: latency-svc-tdzfk
Feb 10 10:05:21.163: INFO: Got endpoints: latency-svc-fnsn8 [750.28365ms]
Feb 10 10:05:21.209: INFO: Got endpoints: latency-svc-s7v9t [750.208009ms]
Feb 10 10:05:21.258: INFO: Got endpoints: latency-svc-8x4sj [740.590483ms]
Feb 10 10:05:21.310: INFO: Got endpoints: latency-svc-zsbtc [743.083745ms]
Feb 10 10:05:21.369: INFO: Got endpoints: latency-svc-ps7sj [745.051066ms]
Feb 10 10:05:21.415: INFO: Got endpoints: latency-svc-fq82x [751.39507ms]
Feb 10 10:05:21.466: INFO: Got endpoints: latency-svc-5mftx [747.978368ms]
Feb 10 10:05:21.511: INFO: Got endpoints: latency-svc-qjw4m [748.210968ms]
Feb 10 10:05:21.567: INFO: Got endpoints: latency-svc-bl7q9 [744.045426ms]
Feb 10 10:05:21.611: INFO: Got endpoints: latency-svc-dc4wh [744.730606ms]
Feb 10 10:05:21.658: INFO: Got endpoints: latency-svc-zdp84 [734.872599ms]
Feb 10 10:05:21.718: INFO: Got endpoints: latency-svc-wr4dl [751.59839ms]
Feb 10 10:05:21.761: INFO: Got endpoints: latency-svc-8tsb9 [749.961349ms]
Feb 10 10:05:21.810: INFO: Got endpoints: latency-svc-4zq94 [747.681547ms]
Feb 10 10:05:21.860: INFO: Got endpoints: latency-svc-tdzfk [748.381608ms]
Feb 10 10:05:21.860: INFO: Latencies: [68.316227ms 105.339112ms 134.182152ms 135.911193ms 171.748998ms 204.32854ms 240.284065ms 283.178254ms 328.954525ms 352.600621ms 400.507995ms 431.316695ms 460.199015ms 464.642318ms 476.553305ms 476.847806ms 478.623266ms 479.030308ms 480.454989ms 481.14109ms 481.417288ms 483.919891ms 484.880731ms 489.734215ms 490.804035ms 493.360017ms 493.913058ms 495.611379ms 496.323039ms 496.73644ms 497.07806ms 498.594101ms 499.219001ms 501.864882ms 502.337003ms 504.815965ms 506.867487ms 510.082349ms 513.501792ms 514.369292ms 514.671773ms 516.063033ms 517.433374ms 518.138615ms 518.806794ms 519.639175ms 520.563776ms 521.538637ms 521.931437ms 522.031257ms 522.326317ms 523.522679ms 523.985839ms 525.231659ms 526.181119ms 526.31592ms 526.419999ms 526.74418ms 526.91564ms 527.804321ms 527.840061ms 528.883862ms 531.005723ms 531.151863ms 531.307003ms 531.848163ms 532.172584ms 532.294803ms 532.398224ms 532.588124ms 533.224685ms 533.423605ms 533.648305ms 536.366086ms 537.925147ms 538.168569ms 538.943488ms 539.689729ms 540.209728ms 546.227693ms 547.118333ms 548.565156ms 549.052855ms 549.592816ms 550.519236ms 556.507181ms 560.536124ms 563.327505ms 567.463348ms 569.763429ms 575.941174ms 585.250539ms 586.543121ms 598.586408ms 605.409094ms 616.746102ms 619.636243ms 620.130184ms 620.346943ms 626.879508ms 629.90759ms 633.108412ms 635.098334ms 635.904335ms 638.474376ms 639.374297ms 643.249039ms 647.455502ms 666.760135ms 676.559921ms 691.423111ms 713.681887ms 730.565697ms 730.819598ms 733.670919ms 734.171699ms 734.872599ms 736.016141ms 736.526282ms 736.690202ms 736.896323ms 737.175823ms 737.296383ms 739.127743ms 739.890963ms 740.590483ms 741.220506ms 741.983225ms 742.081965ms 743.083745ms 743.138547ms 743.805226ms 744.045426ms 744.730606ms 744.746167ms 744.886527ms 745.051066ms 745.511547ms 745.718108ms 745.960148ms 745.967307ms 746.385188ms 746.524848ms 747.681547ms 747.978368ms 748.07127ms 748.210968ms 748.212129ms 748.26239ms 748.381608ms 748.839511ms 749.09789ms 749.143091ms 749.180409ms 749.46829ms 749.64227ms 749.71491ms 749.815931ms 749.961349ms 750.208009ms 750.28365ms 750.52159ms 750.704791ms 750.873631ms 751.39507ms 751.395591ms 751.572591ms 751.59839ms 751.710392ms 751.957492ms 752.134513ms 752.203991ms 752.349911ms 752.404253ms 752.770451ms 752.841532ms 753.107193ms 753.223293ms 753.428172ms 753.438694ms 753.474992ms 753.868214ms 754.086374ms 754.171413ms 754.225434ms 755.173755ms 755.831213ms 756.180314ms 757.042356ms 757.296216ms 757.345775ms 757.903095ms 758.020556ms 758.189136ms 759.627817ms 762.694539ms 763.983521ms 764.987401ms 768.341784ms 772.004846ms]
Feb 10 10:05:21.861: INFO: 50 %ile: 629.90759ms
Feb 10 10:05:21.861: INFO: 90 %ile: 753.474992ms
Feb 10 10:05:21.861: INFO: 99 %ile: 768.341784ms
Feb 10 10:05:21.861: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:05:21.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1803" for this suite.

• [SLOW TEST:14.313 seconds]
[sig-network] Service endpoints latency
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":278,"completed":248,"skipped":4056,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:05:21.910: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-19
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 10:05:22.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-19'
Feb 10 10:05:23.676: INFO: stderr: ""
Feb 10 10:05:23.676: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Feb 10 10:05:23.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 create -f - --namespace=kubectl-19'
Feb 10 10:05:24.640: INFO: stderr: ""
Feb 10 10:05:24.640: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 10 10:05:25.667: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 10:05:25.667: INFO: Found 0 / 1
Feb 10 10:05:26.655: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 10:05:26.655: INFO: Found 0 / 1
Feb 10 10:05:27.655: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 10:05:27.655: INFO: Found 0 / 1
Feb 10 10:05:28.656: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 10:05:28.656: INFO: Found 0 / 1
Feb 10 10:05:29.662: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 10:05:29.662: INFO: Found 1 / 1
Feb 10 10:05:29.662: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 10 10:05:29.690: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 10 10:05:29.690: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 10 10:05:29.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 describe pod agnhost-master-dzmgh --namespace=kubectl-19'
Feb 10 10:05:30.381: INFO: stderr: ""
Feb 10 10:05:30.381: INFO: stdout: "Name:         agnhost-master-dzmgh\nNamespace:    kubectl-19\nPriority:     0\nNode:         master3/172.16.3.137\nStart Time:   Mon, 10 Feb 2020 10:05:23 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           10.156.32.21\nIPs:\n  IP:           10.156.32.21\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://70064584eb72bb7da03dbca52926aca106cbc8197103c6fe578548e7f24f7966\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker://sha256:f776b6dbcc5c7d8ced560ee9b62a355abee3dc26b9d94bac8f751586f53da21c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 10 Feb 2020 10:05:27 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bsktf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-bsktf:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-bsktf\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned kubectl-19/agnhost-master-dzmgh to master3\n  Normal  Pulled     4s         kubelet, master3   Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    3s         kubelet, master3   Created container agnhost-master\n  Normal  Started    2s         kubelet, master3   Started container agnhost-master\n"
Feb 10 10:05:30.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 describe rc agnhost-master --namespace=kubectl-19'
Feb 10 10:05:31.124: INFO: stderr: ""
Feb 10 10:05:31.124: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-19\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  8s    replication-controller  Created pod: agnhost-master-dzmgh\n"
Feb 10 10:05:31.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 describe service agnhost-master --namespace=kubectl-19'
Feb 10 10:05:31.756: INFO: stderr: ""
Feb 10 10:05:31.757: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-19\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.155.106.134\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.156.32.21:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 10 10:05:31.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 describe node master1'
Feb 10 10:05:32.539: INFO: stderr: ""
Feb 10 10:05:32.540: INFO: stdout: "Name:               master1\nRoles:              master,monitor,node\nLabels:             beta.kubernetes.io/arch=mips64le\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=mips64le\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/monitor=true\n                    node-role.kubernetes.io/node=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    node.beta.kubernetes.io/node-resources: 4C/8G/200G\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 14 Jan 2020 08:56:40 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 10 Feb 2020 10:05:26 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 17 Jan 2020 02:14:12 +0000   Fri, 17 Jan 2020 02:14:12 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 10 Feb 2020 10:03:02 +0000   Mon, 10 Feb 2020 04:13:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 10 Feb 2020 10:03:02 +0000   Mon, 10 Feb 2020 04:13:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 10 Feb 2020 10:03:02 +0000   Mon, 10 Feb 2020 04:13:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 10 Feb 2020 10:03:02 +0000   Mon, 10 Feb 2020 04:13:16 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.16.3.135\n  Hostname:    master1\nCapacity:\n  cpu:                8\n  ephemeral-storage:  961299168Ki\n  hugepages-32Mi:     0\n  memory:             66156096Ki\n  pods:               60\nAllocatable:\n  cpu:                7\n  ephemeral-storage:  950813408Ki\n  hugepages-32Mi:     0\n  memory:             63117Mi\n  pods:               60\nSystem Info:\n  Machine ID:                 349467abee654601b57da0a75b1b53a8\n  System UUID:                349467abee654601b57da0a75b1b53a8\n  Boot ID:                    f2b5cca4-d835-4bef-a8ef-8a9bd3d23cf5\n  Kernel Version:             3.10.0-862.9.1.ns7_4.37.mips64el\n  OS Image:                   NeoKylin Linux Server 7.0 (loongson)\n  Operating System:           linux\n  Architecture:               mips64le\n  Container Runtime Version:  docker://18.9.8\n  Kubelet Version:            v1.17.0\n  Kube-Proxy Version:         v1.17.0\nNon-terminated Pods:          (14 in total)\n  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---\n  gc-821                      simpletest-rc-to-be-deleted-f48d7      0 (0%)        0 (0%)      0 (0%)           0 (0%)         108m\n  kube-system                 agnhost-deployment-54964f567d-nvhtm    0 (0%)        0 (0%)      0 (0%)           0 (0%)         156m\n  kube-system                 calico-node-p64wz                      150m (2%)     1 (14%)     64Mi (0%)        4Gi (6%)       24d\n  kube-system                 coredns-hfdns                          100m (1%)     0 (0%)      70Mi (0%)        500Mi (0%)     26d\n  kube-system                 kube-apiserver-master1                 100m (1%)     6 (85%)     256Mi (0%)       6Gi (9%)       4d17h\n  kube-system                 kube-controller-manager-master1        100m (1%)     2 (28%)     100Mi (0%)       6Gi (9%)       4d17h\n  kube-system                 kube-proxy-master1                     150m (2%)     500m (7%)   64M (0%)         2G (3%)        4d17h\n  kube-system                 kube-scheduler-master1                 80m (1%)      2 (28%)     170Mi (0%)       6Gi (9%)       4d17h\n  kube-system                 localpv-provisioner-ds-nt4r5           0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d1h\n  kube-system                 nginx-proxy-master1                    25m (0%)      500m (7%)   32M (0%)         512M (0%)      4d17h\n  kube-system                 ntp-zj7fc                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         26d\n  sonobuoy                    sonobuoy-e2e-job-26d59f1c3cea4685      0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  statefulset-6054            ss2-1                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h40m\n  svc-latency-1803            svc-latency-rc-p6lrf                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         24s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                705m (10%)     12 (171%)\n  memory             769590Ki (1%)  26033797Ki (40%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
Feb 10 10:05:32.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 describe namespace kubectl-19'
Feb 10 10:05:33.184: INFO: stderr: ""
Feb 10 10:05:33.185: INFO: stdout: "Name:         kubectl-19\nLabels:       e2e-framework=kubectl\n              e2e-run=18d479c9-9dfb-4697-a934-ba3fe3414b0a\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:05:33.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-19" for this suite.

• [SLOW TEST:11.322 seconds]
[sig-cli] Kubectl client
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":278,"completed":249,"skipped":4063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:05:33.239: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8425.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8425.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 10 10:05:41.996: INFO: DNS probes using dns-8425/dns-test-8a3ebae8-88c0-41fb-acd7-04d1d9245235 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:05:42.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8425" for this suite.

• [SLOW TEST:8.883 seconds]
[sig-network] DNS
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":278,"completed":250,"skipped":4111,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:05:42.123: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4388
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 10 10:05:42.643: INFO: Waiting up to 5m0s for pod "pod-bcbded22-99ab-4f73-a26f-bf40ce159301" in namespace "emptydir-4388" to be "success or failure"
Feb 10 10:05:42.656: INFO: Pod "pod-bcbded22-99ab-4f73-a26f-bf40ce159301": Phase="Pending", Reason="", readiness=false. Elapsed: 12.592129ms
Feb 10 10:05:44.677: INFO: Pod "pod-bcbded22-99ab-4f73-a26f-bf40ce159301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033703908s
Feb 10 10:05:46.694: INFO: Pod "pod-bcbded22-99ab-4f73-a26f-bf40ce159301": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050443802s
Feb 10 10:05:48.708: INFO: Pod "pod-bcbded22-99ab-4f73-a26f-bf40ce159301": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064496532s
Feb 10 10:05:50.719: INFO: Pod "pod-bcbded22-99ab-4f73-a26f-bf40ce159301": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.075812396s
STEP: Saw pod success
Feb 10 10:05:50.719: INFO: Pod "pod-bcbded22-99ab-4f73-a26f-bf40ce159301" satisfied condition "success or failure"
Feb 10 10:05:50.728: INFO: Trying to get logs from node master1 pod pod-bcbded22-99ab-4f73-a26f-bf40ce159301 container test-container: <nil>
STEP: delete the pod
Feb 10 10:05:51.179: INFO: Waiting for pod pod-bcbded22-99ab-4f73-a26f-bf40ce159301 to disappear
Feb 10 10:05:51.190: INFO: Pod pod-bcbded22-99ab-4f73-a26f-bf40ce159301 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:05:51.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4388" for this suite.

• [SLOW TEST:9.112 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":251,"skipped":4118,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:05:51.238: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3917
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e3f71fcb-526d-432d-a683-16a818bebebe
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-e3f71fcb-526d-432d-a683-16a818bebebe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:07:23.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3917" for this suite.

• [SLOW TEST:92.669 seconds]
[sig-storage] Projected configMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":252,"skipped":4123,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:07:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 10 10:07:30.381: INFO: &Pod{ObjectMeta:{send-events-74f6223b-45c1-4d9e-9640-b467eba35569  events-8445 /api/v1/namespaces/events-8445/pods/send-events-74f6223b-45c1-4d9e-9640-b467eba35569 57f453eb-94f4-4201-a863-7bb68e4a527a 4674144 0 2020-02-10 10:07:24 +0000 UTC <nil> <nil> map[name:foo time:298839740] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bsvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bsvf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bsvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 10:07:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 10:07:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 10:07:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-10 10:07:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.3.135,PodIP:10.156.161.246,StartTime:2020-02-10 10:07:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-10 10:07:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker://sha256:01966e4b12a05550df97ce433c8309c377e2de6d6e52b3454fb80c5f9b6f8212,ContainerID:docker://67b2625973e2316d1b5e32ab6f9f0459db4f8a585c874eef976bfb94ce726a81,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.156.161.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 10 10:07:32.395: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 10 10:07:34.409: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:07:34.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8445" for this suite.

• [SLOW TEST:10.561 seconds]
[k8s.io] [sig-node] Events
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":278,"completed":253,"skipped":4124,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:07:34.471: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-23c91bca-5b1c-428c-bec1-ceea2a75236a
STEP: Creating a pod to test consume secrets
Feb 10 10:07:34.930: INFO: Waiting up to 5m0s for pod "pod-secrets-ce0a720e-734b-4b27-9b10-fabcddcaab27" in namespace "secrets-8363" to be "success or failure"
Feb 10 10:07:34.962: INFO: Pod "pod-secrets-ce0a720e-734b-4b27-9b10-fabcddcaab27": Phase="Pending", Reason="", readiness=false. Elapsed: 31.334218ms
Feb 10 10:07:36.978: INFO: Pod "pod-secrets-ce0a720e-734b-4b27-9b10-fabcddcaab27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047660122s
Feb 10 10:07:38.997: INFO: Pod "pod-secrets-ce0a720e-734b-4b27-9b10-fabcddcaab27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067059104s
Feb 10 10:07:41.015: INFO: Pod "pod-secrets-ce0a720e-734b-4b27-9b10-fabcddcaab27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.084561183s
STEP: Saw pod success
Feb 10 10:07:41.015: INFO: Pod "pod-secrets-ce0a720e-734b-4b27-9b10-fabcddcaab27" satisfied condition "success or failure"
Feb 10 10:07:41.025: INFO: Trying to get logs from node master3 pod pod-secrets-ce0a720e-734b-4b27-9b10-fabcddcaab27 container secret-volume-test: <nil>
STEP: delete the pod
Feb 10 10:07:41.145: INFO: Waiting for pod pod-secrets-ce0a720e-734b-4b27-9b10-fabcddcaab27 to disappear
Feb 10 10:07:41.153: INFO: Pod pod-secrets-ce0a720e-734b-4b27-9b10-fabcddcaab27 no longer exists
[AfterEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:07:41.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8363" for this suite.

• [SLOW TEST:6.713 seconds]
[sig-storage] Secrets
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":254,"skipped":4128,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:07:41.189: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1494
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:07:48.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1494" for this suite.

• [SLOW TEST:7.550 seconds]
[sig-apps] ReplicationController
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":278,"completed":255,"skipped":4145,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:07:48.740: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2908
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 10 10:07:49.174: INFO: Waiting up to 5m0s for pod "pod-372bb24d-2412-4211-8e2b-95af319da850" in namespace "emptydir-2908" to be "success or failure"
Feb 10 10:07:49.184: INFO: Pod "pod-372bb24d-2412-4211-8e2b-95af319da850": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015166ms
Feb 10 10:07:51.200: INFO: Pod "pod-372bb24d-2412-4211-8e2b-95af319da850": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026705551s
Feb 10 10:07:53.214: INFO: Pod "pod-372bb24d-2412-4211-8e2b-95af319da850": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040194052s
Feb 10 10:07:55.226: INFO: Pod "pod-372bb24d-2412-4211-8e2b-95af319da850": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05274351s
STEP: Saw pod success
Feb 10 10:07:55.226: INFO: Pod "pod-372bb24d-2412-4211-8e2b-95af319da850" satisfied condition "success or failure"
Feb 10 10:07:55.238: INFO: Trying to get logs from node master3 pod pod-372bb24d-2412-4211-8e2b-95af319da850 container test-container: <nil>
STEP: delete the pod
Feb 10 10:07:55.328: INFO: Waiting for pod pod-372bb24d-2412-4211-8e2b-95af319da850 to disappear
Feb 10 10:07:55.338: INFO: Pod pod-372bb24d-2412-4211-8e2b-95af319da850 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:07:55.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2908" for this suite.

• [SLOW TEST:6.628 seconds]
[sig-storage] EmptyDir volumes
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":256,"skipped":4150,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:07:55.369: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5603
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 10:07:55.786: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:08:02.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5603" for this suite.

• [SLOW TEST:7.042 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    creating/deleting custom resource definition objects works  [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":278,"completed":257,"skipped":4158,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:08:02.412: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9893
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 10:08:02.809: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 10 10:08:21.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9893 create -f -'
Feb 10 10:08:25.505: INFO: stderr: ""
Feb 10 10:08:25.505: INFO: stdout: "e2e-test-crd-publish-openapi-9195-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 10 10:08:25.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9893 delete e2e-test-crd-publish-openapi-9195-crds test-foo'
Feb 10 10:08:26.074: INFO: stderr: ""
Feb 10 10:08:26.075: INFO: stdout: "e2e-test-crd-publish-openapi-9195-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 10 10:08:26.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9893 apply -f -'
Feb 10 10:08:27.225: INFO: stderr: ""
Feb 10 10:08:27.225: INFO: stdout: "e2e-test-crd-publish-openapi-9195-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 10 10:08:27.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9893 delete e2e-test-crd-publish-openapi-9195-crds test-foo'
Feb 10 10:08:27.768: INFO: stderr: ""
Feb 10 10:08:27.769: INFO: stdout: "e2e-test-crd-publish-openapi-9195-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 10 10:08:27.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9893 create -f -'
Feb 10 10:08:29.004: INFO: rc: 1
Feb 10 10:08:29.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9893 apply -f -'
Feb 10 10:08:30.278: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 10 10:08:30.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9893 create -f -'
Feb 10 10:08:31.231: INFO: rc: 1
Feb 10 10:08:31.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 --namespace=crd-publish-openapi-9893 apply -f -'
Feb 10 10:08:32.251: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 10 10:08:32.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 explain e2e-test-crd-publish-openapi-9195-crds'
Feb 10 10:08:33.555: INFO: stderr: ""
Feb 10 10:08:33.555: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9195-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 10 10:08:33.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 explain e2e-test-crd-publish-openapi-9195-crds.metadata'
Feb 10 10:08:34.604: INFO: stderr: ""
Feb 10 10:08:34.605: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9195-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 10 10:08:34.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 explain e2e-test-crd-publish-openapi-9195-crds.spec'
Feb 10 10:08:35.579: INFO: stderr: ""
Feb 10 10:08:35.580: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9195-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 10 10:08:35.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 explain e2e-test-crd-publish-openapi-9195-crds.spec.bars'
Feb 10 10:08:36.575: INFO: stderr: ""
Feb 10 10:08:36.576: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9195-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 10 10:08:36.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 explain e2e-test-crd-publish-openapi-9195-crds.spec.bars2'
Feb 10 10:08:37.962: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:08:51.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9893" for this suite.

• [SLOW TEST:49.415 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":278,"completed":258,"skipped":4183,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:08:51.828: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Feb 10 10:08:52.271: INFO: Waiting up to 5m0s for pod "var-expansion-a6867381-b8a2-4e61-8790-18beaf1627f4" in namespace "var-expansion-8427" to be "success or failure"
Feb 10 10:08:52.280: INFO: Pod "var-expansion-a6867381-b8a2-4e61-8790-18beaf1627f4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.071064ms
Feb 10 10:08:54.294: INFO: Pod "var-expansion-a6867381-b8a2-4e61-8790-18beaf1627f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02251029s
Feb 10 10:08:56.307: INFO: Pod "var-expansion-a6867381-b8a2-4e61-8790-18beaf1627f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035441732s
Feb 10 10:08:58.320: INFO: Pod "var-expansion-a6867381-b8a2-4e61-8790-18beaf1627f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048866193s
STEP: Saw pod success
Feb 10 10:08:58.320: INFO: Pod "var-expansion-a6867381-b8a2-4e61-8790-18beaf1627f4" satisfied condition "success or failure"
Feb 10 10:08:58.330: INFO: Trying to get logs from node master3 pod var-expansion-a6867381-b8a2-4e61-8790-18beaf1627f4 container dapi-container: <nil>
STEP: delete the pod
Feb 10 10:08:58.411: INFO: Waiting for pod var-expansion-a6867381-b8a2-4e61-8790-18beaf1627f4 to disappear
Feb 10 10:08:58.421: INFO: Pod var-expansion-a6867381-b8a2-4e61-8790-18beaf1627f4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:08:58.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8427" for this suite.

• [SLOW TEST:6.637 seconds]
[k8s.io] Variable Expansion
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":278,"completed":259,"skipped":4191,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:08:58.468: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:09:15.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6008" for this suite.

• [SLOW TEST:16.610 seconds]
[sig-api-machinery] ResourceQuota
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":278,"completed":260,"skipped":4191,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:09:15.079: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-8645
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 10 10:09:15.534: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 10 10:09:45.836: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.156.32.26:8080/dial?request=hostname&protocol=udp&host=10.156.161.247&port=8081&tries=1'] Namespace:pod-network-test-8645 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 10:09:45.836: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 10:09:46.757: INFO: Waiting for responses: map[]
Feb 10 10:09:46.771: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.156.32.26:8080/dial?request=hostname&protocol=udp&host=10.156.208.110&port=8081&tries=1'] Namespace:pod-network-test-8645 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 10:09:46.771: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 10:09:47.663: INFO: Waiting for responses: map[]
Feb 10 10:09:47.675: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.156.32.26:8080/dial?request=hostname&protocol=udp&host=10.156.32.25&port=8081&tries=1'] Namespace:pod-network-test-8645 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 10:09:47.675: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 10:09:48.532: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:09:48.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8645" for this suite.

• [SLOW TEST:33.492 seconds]
[sig-network] Networking
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":261,"skipped":4202,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:09:48.572: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-500da536-65c7-4f71-9343-900d40982f55
STEP: Creating a pod to test consume secrets
Feb 10 10:09:49.030: INFO: Waiting up to 5m0s for pod "pod-secrets-f5a7deda-1a6f-4f07-9f22-c021f5efb806" in namespace "secrets-3914" to be "success or failure"
Feb 10 10:09:49.042: INFO: Pod "pod-secrets-f5a7deda-1a6f-4f07-9f22-c021f5efb806": Phase="Pending", Reason="", readiness=false. Elapsed: 12.223912ms
Feb 10 10:09:51.060: INFO: Pod "pod-secrets-f5a7deda-1a6f-4f07-9f22-c021f5efb806": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030141155s
Feb 10 10:09:53.075: INFO: Pod "pod-secrets-f5a7deda-1a6f-4f07-9f22-c021f5efb806": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045043406s
Feb 10 10:09:55.090: INFO: Pod "pod-secrets-f5a7deda-1a6f-4f07-9f22-c021f5efb806": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060437377s
Feb 10 10:09:57.107: INFO: Pod "pod-secrets-f5a7deda-1a6f-4f07-9f22-c021f5efb806": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.076837268s
STEP: Saw pod success
Feb 10 10:09:57.108: INFO: Pod "pod-secrets-f5a7deda-1a6f-4f07-9f22-c021f5efb806" satisfied condition "success or failure"
Feb 10 10:09:57.119: INFO: Trying to get logs from node master2 pod pod-secrets-f5a7deda-1a6f-4f07-9f22-c021f5efb806 container secret-volume-test: <nil>
STEP: delete the pod
Feb 10 10:09:57.527: INFO: Waiting for pod pod-secrets-f5a7deda-1a6f-4f07-9f22-c021f5efb806 to disappear
Feb 10 10:09:57.541: INFO: Pod pod-secrets-f5a7deda-1a6f-4f07-9f22-c021f5efb806 no longer exists
[AfterEach] [sig-storage] Secrets
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:09:57.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3914" for this suite.

• [SLOW TEST:9.006 seconds]
[sig-storage] Secrets
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":278,"completed":262,"skipped":4204,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:09:57.581: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7559
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 10 10:09:58.029: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 10 10:10:28.333: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.156.161.248:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7559 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 10:10:28.334: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 10:10:29.194: INFO: Found all expected endpoints: [netserver-0]
Feb 10 10:10:29.206: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.156.208.112:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7559 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 10:10:29.206: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 10:10:30.094: INFO: Found all expected endpoints: [netserver-1]
Feb 10 10:10:30.106: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.156.32.28:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7559 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 10 10:10:30.106: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 10:10:30.955: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:10:30.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7559" for this suite.

• [SLOW TEST:33.416 seconds]
[sig-network] Networking
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":263,"skipped":4233,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:10:30.997: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9721
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 10 10:10:31.428: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
Feb 10 10:10:50.234: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:11:47.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9721" for this suite.

• [SLOW TEST:76.384 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":278,"completed":264,"skipped":4233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:11:47.386: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4923
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4923
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4923
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4923
Feb 10 10:11:47.867: INFO: Found 0 stateful pods, waiting for 1
Feb 10 10:11:57.884: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 10 10:11:57.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4923 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 10 10:11:59.250: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 10 10:11:59.251: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 10 10:11:59.251: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 10 10:11:59.273: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 10 10:12:09.299: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 10 10:12:09.299: INFO: Waiting for statefulset status.replicas updated to 0
Feb 10 10:12:09.362: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999704s
Feb 10 10:12:10.375: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.986497361s
Feb 10 10:12:11.388: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.973302364s
Feb 10 10:12:12.402: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.960860009s
Feb 10 10:12:13.416: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.94685953s
Feb 10 10:12:14.430: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.932685752s
Feb 10 10:12:15.443: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.918370174s
Feb 10 10:12:16.456: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.90542582s
Feb 10 10:12:17.468: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.892051985s
Feb 10 10:12:18.483: INFO: Verifying statefulset ss doesn't scale past 1 for another 879.675653ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4923
Feb 10 10:12:19.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4923 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 10 10:12:21.104: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 10 10:12:21.105: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 10 10:12:21.105: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 10 10:12:21.122: INFO: Found 1 stateful pods, waiting for 3
Feb 10 10:12:31.143: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 10:12:31.144: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 10:12:31.144: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 10 10:12:41.151: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 10:12:41.151: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 10 10:12:41.151: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 10 10:12:41.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4923 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 10 10:12:42.587: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 10 10:12:42.588: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 10 10:12:42.588: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 10 10:12:42.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4923 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 10 10:12:43.993: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 10 10:12:43.993: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 10 10:12:43.993: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 10 10:12:43.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4923 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 10 10:12:45.481: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 10 10:12:45.482: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 10 10:12:45.482: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 10 10:12:45.482: INFO: Waiting for statefulset status.replicas updated to 0
Feb 10 10:12:45.498: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 10 10:12:55.528: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 10 10:12:55.528: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 10 10:12:55.528: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 10 10:12:55.568: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999562s
Feb 10 10:12:56.583: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98496818s
Feb 10 10:12:57.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.970384041s
Feb 10 10:12:58.615: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.952993656s
Feb 10 10:12:59.629: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.938410058s
Feb 10 10:13:00.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.92327262s
Feb 10 10:13:01.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.889030913s
Feb 10 10:13:02.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.871020488s
Feb 10 10:13:03.714: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.853984646s
Feb 10 10:13:04.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 838.433609ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4923
Feb 10 10:13:05.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4923 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 10 10:13:07.215: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 10 10:13:07.215: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 10 10:13:07.215: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 10 10:13:07.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4923 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 10 10:13:08.587: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 10 10:13:08.588: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 10 10:13:08.588: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 10 10:13:08.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-754024871 exec --namespace=statefulset-4923 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 10 10:13:10.023: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 10 10:13:10.023: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 10 10:13:10.023: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 10 10:13:10.023: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 10 10:13:50.088: INFO: Deleting all statefulset in ns statefulset-4923
Feb 10 10:13:50.106: INFO: Scaling statefulset ss to 0
Feb 10 10:13:50.139: INFO: Waiting for statefulset status.replicas updated to 0
Feb 10 10:13:50.151: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:13:50.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4923" for this suite.

• [SLOW TEST:122.855 seconds]
[sig-apps] StatefulSet
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":278,"completed":265,"skipped":4255,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:13:50.245: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 10:13:50.685: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a068b8ba-02be-43f4-bf6e-e79366312628" in namespace "projected-4391" to be "success or failure"
Feb 10 10:13:50.697: INFO: Pod "downwardapi-volume-a068b8ba-02be-43f4-bf6e-e79366312628": Phase="Pending", Reason="", readiness=false. Elapsed: 12.261211ms
Feb 10 10:13:52.728: INFO: Pod "downwardapi-volume-a068b8ba-02be-43f4-bf6e-e79366312628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043059802s
Feb 10 10:13:54.739: INFO: Pod "downwardapi-volume-a068b8ba-02be-43f4-bf6e-e79366312628": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05380944s
Feb 10 10:13:56.751: INFO: Pod "downwardapi-volume-a068b8ba-02be-43f4-bf6e-e79366312628": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0661878s
STEP: Saw pod success
Feb 10 10:13:56.751: INFO: Pod "downwardapi-volume-a068b8ba-02be-43f4-bf6e-e79366312628" satisfied condition "success or failure"
Feb 10 10:13:56.762: INFO: Trying to get logs from node master3 pod downwardapi-volume-a068b8ba-02be-43f4-bf6e-e79366312628 container client-container: <nil>
STEP: delete the pod
Feb 10 10:13:57.176: INFO: Waiting for pod downwardapi-volume-a068b8ba-02be-43f4-bf6e-e79366312628 to disappear
Feb 10 10:13:57.187: INFO: Pod downwardapi-volume-a068b8ba-02be-43f4-bf6e-e79366312628 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:13:57.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4391" for this suite.

• [SLOW TEST:6.976 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":278,"completed":266,"skipped":4281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:13:57.223: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-a9bf1713-302b-42e2-b6a6-c326b6b79dde
STEP: Creating a pod to test consume secrets
Feb 10 10:13:57.683: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-21f9ef0d-fa31-4511-ba02-f8d94aeb1f8b" in namespace "projected-7154" to be "success or failure"
Feb 10 10:13:57.695: INFO: Pod "pod-projected-secrets-21f9ef0d-fa31-4511-ba02-f8d94aeb1f8b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.88835ms
Feb 10 10:13:59.713: INFO: Pod "pod-projected-secrets-21f9ef0d-fa31-4511-ba02-f8d94aeb1f8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030317523s
Feb 10 10:14:01.730: INFO: Pod "pod-projected-secrets-21f9ef0d-fa31-4511-ba02-f8d94aeb1f8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04721661s
Feb 10 10:14:03.743: INFO: Pod "pod-projected-secrets-21f9ef0d-fa31-4511-ba02-f8d94aeb1f8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060188526s
STEP: Saw pod success
Feb 10 10:14:03.744: INFO: Pod "pod-projected-secrets-21f9ef0d-fa31-4511-ba02-f8d94aeb1f8b" satisfied condition "success or failure"
Feb 10 10:14:03.754: INFO: Trying to get logs from node master3 pod pod-projected-secrets-21f9ef0d-fa31-4511-ba02-f8d94aeb1f8b container secret-volume-test: <nil>
STEP: delete the pod
Feb 10 10:14:03.850: INFO: Waiting for pod pod-projected-secrets-21f9ef0d-fa31-4511-ba02-f8d94aeb1f8b to disappear
Feb 10 10:14:03.862: INFO: Pod pod-projected-secrets-21f9ef0d-fa31-4511-ba02-f8d94aeb1f8b no longer exists
[AfterEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:14:03.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7154" for this suite.

• [SLOW TEST:6.679 seconds]
[sig-storage] Projected secret
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":278,"completed":267,"skipped":4305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:14:03.906: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 10 10:14:09.461: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:14:09.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7864" for this suite.

• [SLOW TEST:5.668 seconds]
[k8s.io] Container Runtime
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":268,"skipped":4337,"failed":0}
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:14:09.574: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-0732f2aa-171b-4577-9b31-69c320d70cf7 in namespace container-probe-5120
Feb 10 10:14:16.042: INFO: Started pod busybox-0732f2aa-171b-4577-9b31-69c320d70cf7 in namespace container-probe-5120
STEP: checking the pod's current state and verifying that restartCount is present
Feb 10 10:14:16.053: INFO: Initial restart count of pod busybox-0732f2aa-171b-4577-9b31-69c320d70cf7 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:18:17.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5120" for this suite.

• [SLOW TEST:248.240 seconds]
[k8s.io] Probing container
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":278,"completed":269,"skipped":4337,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:18:17.816: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0210 10:18:58.390299      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 10 10:18:58.390: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:18:58.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7771" for this suite.

• [SLOW TEST:40.615 seconds]
[sig-api-machinery] Garbage collector
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":278,"completed":270,"skipped":4379,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:18:58.432: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4520
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:18:58.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4520" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":278,"completed":271,"skipped":4383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:18:59.030: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-c01721b1-e1f9-4cbd-a32d-3e973cadbe4d
STEP: Creating a pod to test consume configMaps
Feb 10 10:18:59.498: INFO: Waiting up to 5m0s for pod "pod-configmaps-d535e995-7915-47ba-9491-5e21aa8511bb" in namespace "configmap-2643" to be "success or failure"
Feb 10 10:18:59.508: INFO: Pod "pod-configmaps-d535e995-7915-47ba-9491-5e21aa8511bb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.768566ms
Feb 10 10:19:01.521: INFO: Pod "pod-configmaps-d535e995-7915-47ba-9491-5e21aa8511bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023482829s
Feb 10 10:19:03.533: INFO: Pod "pod-configmaps-d535e995-7915-47ba-9491-5e21aa8511bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035143104s
Feb 10 10:19:05.548: INFO: Pod "pod-configmaps-d535e995-7915-47ba-9491-5e21aa8511bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050022686s
STEP: Saw pod success
Feb 10 10:19:05.548: INFO: Pod "pod-configmaps-d535e995-7915-47ba-9491-5e21aa8511bb" satisfied condition "success or failure"
Feb 10 10:19:05.571: INFO: Trying to get logs from node master1 pod pod-configmaps-d535e995-7915-47ba-9491-5e21aa8511bb container configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 10:19:06.275: INFO: Waiting for pod pod-configmaps-d535e995-7915-47ba-9491-5e21aa8511bb to disappear
Feb 10 10:19:06.293: INFO: Pod pod-configmaps-d535e995-7915-47ba-9491-5e21aa8511bb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:19:06.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2643" for this suite.

• [SLOW TEST:7.312 seconds]
[sig-storage] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":272,"skipped":4406,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:19:06.343: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 10:19:06.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c65adb7c-93bf-43e9-abf2-ad6e7816c006" in namespace "projected-9672" to be "success or failure"
Feb 10 10:19:06.865: INFO: Pod "downwardapi-volume-c65adb7c-93bf-43e9-abf2-ad6e7816c006": Phase="Pending", Reason="", readiness=false. Elapsed: 13.439395ms
Feb 10 10:19:08.895: INFO: Pod "downwardapi-volume-c65adb7c-93bf-43e9-abf2-ad6e7816c006": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044168515s
Feb 10 10:19:10.907: INFO: Pod "downwardapi-volume-c65adb7c-93bf-43e9-abf2-ad6e7816c006": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055829343s
Feb 10 10:19:12.923: INFO: Pod "downwardapi-volume-c65adb7c-93bf-43e9-abf2-ad6e7816c006": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072258461s
Feb 10 10:19:14.936: INFO: Pod "downwardapi-volume-c65adb7c-93bf-43e9-abf2-ad6e7816c006": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.085053008s
STEP: Saw pod success
Feb 10 10:19:14.936: INFO: Pod "downwardapi-volume-c65adb7c-93bf-43e9-abf2-ad6e7816c006" satisfied condition "success or failure"
Feb 10 10:19:14.947: INFO: Trying to get logs from node master3 pod downwardapi-volume-c65adb7c-93bf-43e9-abf2-ad6e7816c006 container client-container: <nil>
STEP: delete the pod
Feb 10 10:19:15.381: INFO: Waiting for pod downwardapi-volume-c65adb7c-93bf-43e9-abf2-ad6e7816c006 to disappear
Feb 10 10:19:15.392: INFO: Pod downwardapi-volume-c65adb7c-93bf-43e9-abf2-ad6e7816c006 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:19:15.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9672" for this suite.

• [SLOW TEST:9.092 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":273,"skipped":4426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:19:15.437: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-19131792-5858-4019-b138-54281eefe864
STEP: Creating a pod to test consume configMaps
Feb 10 10:19:15.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-2fb98391-e3da-4561-9219-13a01d2119fd" in namespace "configmap-8258" to be "success or failure"
Feb 10 10:19:15.898: INFO: Pod "pod-configmaps-2fb98391-e3da-4561-9219-13a01d2119fd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.993006ms
Feb 10 10:19:17.912: INFO: Pod "pod-configmaps-2fb98391-e3da-4561-9219-13a01d2119fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023909372s
Feb 10 10:19:19.931: INFO: Pod "pod-configmaps-2fb98391-e3da-4561-9219-13a01d2119fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04271773s
Feb 10 10:19:21.947: INFO: Pod "pod-configmaps-2fb98391-e3da-4561-9219-13a01d2119fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058678497s
STEP: Saw pod success
Feb 10 10:19:21.947: INFO: Pod "pod-configmaps-2fb98391-e3da-4561-9219-13a01d2119fd" satisfied condition "success or failure"
Feb 10 10:19:21.958: INFO: Trying to get logs from node master3 pod pod-configmaps-2fb98391-e3da-4561-9219-13a01d2119fd container configmap-volume-test: <nil>
STEP: delete the pod
Feb 10 10:19:22.054: INFO: Waiting for pod pod-configmaps-2fb98391-e3da-4561-9219-13a01d2119fd to disappear
Feb 10 10:19:22.067: INFO: Pod pod-configmaps-2fb98391-e3da-4561-9219-13a01d2119fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:19:22.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8258" for this suite.

• [SLOW TEST:6.668 seconds]
[sig-storage] ConfigMap
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":274,"skipped":4455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:19:22.115: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-8qll
STEP: Creating a pod to test atomic-volume-subpath
Feb 10 10:19:22.616: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8qll" in namespace "subpath-9739" to be "success or failure"
Feb 10 10:19:22.635: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Pending", Reason="", readiness=false. Elapsed: 18.834049ms
Feb 10 10:19:24.647: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030843644s
Feb 10 10:19:26.662: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045327143s
Feb 10 10:19:28.678: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Running", Reason="", readiness=true. Elapsed: 6.061551565s
Feb 10 10:19:30.694: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Running", Reason="", readiness=true. Elapsed: 8.077175582s
Feb 10 10:19:32.706: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Running", Reason="", readiness=true. Elapsed: 10.089172969s
Feb 10 10:19:34.717: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Running", Reason="", readiness=true. Elapsed: 12.100738312s
Feb 10 10:19:36.730: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Running", Reason="", readiness=true. Elapsed: 14.113511677s
Feb 10 10:19:38.743: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Running", Reason="", readiness=true. Elapsed: 16.127056101s
Feb 10 10:19:40.761: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Running", Reason="", readiness=true. Elapsed: 18.144566913s
Feb 10 10:19:42.776: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Running", Reason="", readiness=true. Elapsed: 20.159122076s
Feb 10 10:19:44.788: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Running", Reason="", readiness=true. Elapsed: 22.171198351s
Feb 10 10:19:46.800: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Running", Reason="", readiness=true. Elapsed: 24.183140663s
Feb 10 10:19:48.822: INFO: Pod "pod-subpath-test-configmap-8qll": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.20551532s
STEP: Saw pod success
Feb 10 10:19:48.822: INFO: Pod "pod-subpath-test-configmap-8qll" satisfied condition "success or failure"
Feb 10 10:19:48.841: INFO: Trying to get logs from node master3 pod pod-subpath-test-configmap-8qll container test-container-subpath-configmap-8qll: <nil>
STEP: delete the pod
Feb 10 10:19:48.927: INFO: Waiting for pod pod-subpath-test-configmap-8qll to disappear
Feb 10 10:19:48.937: INFO: Pod pod-subpath-test-configmap-8qll no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8qll
Feb 10 10:19:48.937: INFO: Deleting pod "pod-subpath-test-configmap-8qll" in namespace "subpath-9739"
[AfterEach] [sig-storage] Subpath
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:19:48.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9739" for this suite.

• [SLOW TEST:26.873 seconds]
[sig-storage] Subpath
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":278,"completed":275,"skipped":4492,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:19:48.989: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6709
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-b1793dd8-62ae-4a37-beef-0a43ece34599
STEP: Creating a pod to test consume secrets
Feb 10 10:19:49.477: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-091f59cf-ce07-480e-951f-f06e6b4d91c4" in namespace "projected-6709" to be "success or failure"
Feb 10 10:19:49.495: INFO: Pod "pod-projected-secrets-091f59cf-ce07-480e-951f-f06e6b4d91c4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.076387ms
Feb 10 10:19:51.536: INFO: Pod "pod-projected-secrets-091f59cf-ce07-480e-951f-f06e6b4d91c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058594328s
Feb 10 10:19:53.547: INFO: Pod "pod-projected-secrets-091f59cf-ce07-480e-951f-f06e6b4d91c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069981492s
STEP: Saw pod success
Feb 10 10:19:53.548: INFO: Pod "pod-projected-secrets-091f59cf-ce07-480e-951f-f06e6b4d91c4" satisfied condition "success or failure"
Feb 10 10:19:53.556: INFO: Trying to get logs from node master3 pod pod-projected-secrets-091f59cf-ce07-480e-951f-f06e6b4d91c4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 10 10:19:53.624: INFO: Waiting for pod pod-projected-secrets-091f59cf-ce07-480e-951f-f06e6b4d91c4 to disappear
Feb 10 10:19:53.634: INFO: Pod pod-projected-secrets-091f59cf-ce07-480e-951f-f06e6b4d91c4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:19:53.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6709" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":276,"skipped":4493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:19:53.684: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 10 10:19:54.130: INFO: Waiting up to 5m0s for pod "downwardapi-volume-830edbb7-7a7a-42b9-a1b6-7876111ac147" in namespace "projected-444" to be "success or failure"
Feb 10 10:19:54.140: INFO: Pod "downwardapi-volume-830edbb7-7a7a-42b9-a1b6-7876111ac147": Phase="Pending", Reason="", readiness=false. Elapsed: 9.899086ms
Feb 10 10:19:56.156: INFO: Pod "downwardapi-volume-830edbb7-7a7a-42b9-a1b6-7876111ac147": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025887459s
Feb 10 10:19:58.171: INFO: Pod "downwardapi-volume-830edbb7-7a7a-42b9-a1b6-7876111ac147": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040273526s
Feb 10 10:20:00.183: INFO: Pod "downwardapi-volume-830edbb7-7a7a-42b9-a1b6-7876111ac147": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052212125s
STEP: Saw pod success
Feb 10 10:20:00.183: INFO: Pod "downwardapi-volume-830edbb7-7a7a-42b9-a1b6-7876111ac147" satisfied condition "success or failure"
Feb 10 10:20:00.193: INFO: Trying to get logs from node master3 pod downwardapi-volume-830edbb7-7a7a-42b9-a1b6-7876111ac147 container client-container: <nil>
STEP: delete the pod
Feb 10 10:20:00.267: INFO: Waiting for pod downwardapi-volume-830edbb7-7a7a-42b9-a1b6-7876111ac147 to disappear
Feb 10 10:20:00.281: INFO: Pod downwardapi-volume-830edbb7-7a7a-42b9-a1b6-7876111ac147 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:20:00.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-444" for this suite.

• [SLOW TEST:6.636 seconds]
[sig-storage] Projected downwardAPI
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":278,"completed":277,"skipped":4516,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 10 10:20:00.322: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 10 10:20:00.736: INFO: >>> kubeConfig: /tmp/kubeconfig-754024871
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 10 10:20:07.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3875" for this suite.

• [SLOW TEST:7.379 seconds]
[k8s.io] Pods
/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":278,"completed":278,"skipped":4523,"failed":0}
SSSSSSSSSSSSSFeb 10 10:20:07.703: INFO: Running AfterSuite actions on all nodes
Feb 10 10:20:07.703: INFO: Running AfterSuite actions on node 1
Feb 10 10:20:07.703: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":278,"completed":278,"skipped":4536,"failed":0}

Ran 278 of 4814 Specs in 6746.841 seconds
SUCCESS! -- 278 Passed | 0 Failed | 0 Pending | 4536 Skipped
PASS

Ginkgo ran 1 suite in 1h52m38.42865034s
Test Suite Passed
