I0830 16:29:41.536470      23 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-962893509
I0830 16:29:41.536489      23 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0830 16:29:41.536584      23 e2e.go:109] Starting e2e run "eda2017a-4ba1-4abb-bd48-e853704860b3" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1598804980 - Will randomize all specs
Will run 280 of 4843 specs

Aug 30 16:29:41.544: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:29:41.546: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 30 16:29:41.573: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 30 16:29:41.609: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 30 16:29:41.609: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Aug 30 16:29:41.610: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 30 16:29:41.620: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Aug 30 16:29:41.620: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 30 16:29:41.620: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'logrotate' (0 seconds elapsed)
Aug 30 16:29:41.620: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Aug 30 16:29:41.620: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'user-ssh-keys-agent' (0 seconds elapsed)
Aug 30 16:29:41.620: INFO: e2e test version: v1.17.9
Aug 30 16:29:41.623: INFO: kube-apiserver version: v1.17.9
Aug 30 16:29:41.623: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:29:41.632: INFO: Cluster IP family: ipv4
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:29:41.632: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
Aug 30 16:29:41.694: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Aug 30 16:29:41.716: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 16:29:42.097: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 16:29:44.113: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734401782, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734401782, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734401782, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734401782, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:29:46.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734401782, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734401782, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734401782, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734401782, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:29:49.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:00.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2721" for this suite.
STEP: Destroying namespace "webhook-2721-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:18.548 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":1,"skipped":11,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:00.182: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 16:30:00.770: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:30:03.798: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:04.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-667" for this suite.
STEP: Destroying namespace "webhook-667-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":2,"skipped":18,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:04.210: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-8053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:04.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8053" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":3,"skipped":42,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:04.402: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:30:04.612: INFO: Creating deployment "test-recreate-deployment"
Aug 30 16:30:04.619: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 30 16:30:04.639: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 30 16:30:06.687: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 30 16:30:06.709: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 30 16:30:06.721: INFO: Updating deployment test-recreate-deployment
Aug 30 16:30:06.721: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Aug 30 16:30:07.022: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5519 /apis/apps/v1/namespaces/deployment-5519/deployments/test-recreate-deployment 8dbf4b47-805e-4a54-b8f3-82fbc3010540 2100 2 2020-08-30 16:30:04 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002826d08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-30 16:30:06 +0000 UTC,LastTransitionTime:2020-08-30 16:30:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-08-30 16:30:06 +0000 UTC,LastTransitionTime:2020-08-30 16:30:04 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 30 16:30:07.031: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-5519 /apis/apps/v1/namespaces/deployment-5519/replicasets/test-recreate-deployment-5f94c574ff 2c30a2a2-d8b5-467f-b9a3-1746b909c66a 2099 1 2020-08-30 16:30:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8dbf4b47-805e-4a54-b8f3-82fbc3010540 0xc002827337 0xc002827338}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002827398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:30:07.031: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 30 16:30:07.031: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-5519 /apis/apps/v1/namespaces/deployment-5519/replicasets/test-recreate-deployment-799c574856 dcb78500-58b3-4fc5-9748-4b422e8bb25c 2087 2 2020-08-30 16:30:04 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8dbf4b47-805e-4a54-b8f3-82fbc3010540 0xc0028273f7 0xc0028273f8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002827468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:30:07.057: INFO: Pod "test-recreate-deployment-5f94c574ff-ldg5l" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-ldg5l test-recreate-deployment-5f94c574ff- deployment-5519 /api/v1/namespaces/deployment-5519/pods/test-recreate-deployment-5f94c574ff-ldg5l 12cd748b-0b2e-4706-8ef5-327df40a3a4d 2098 0 2020-08-30 16:30:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 2c30a2a2-d8b5-467f-b9a3-1746b909c66a 0xc0028278c7 0xc0028278c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-df8nz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-df8nz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-df8nz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:30:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:30:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:30:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:30:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 16:30:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:07.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5519" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":4,"skipped":45,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:07.079: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7845
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-7845/configmap-test-665aab27-47b5-482c-81e3-f6ea3fb39ce2
STEP: Creating a pod to test consume configMaps
Aug 30 16:30:07.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-3d7dd31f-c195-4eee-93f4-a4395d094f3d" in namespace "configmap-7845" to be "success or failure"
Aug 30 16:30:07.316: INFO: Pod "pod-configmaps-3d7dd31f-c195-4eee-93f4-a4395d094f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.456934ms
Aug 30 16:30:09.329: INFO: Pod "pod-configmaps-3d7dd31f-c195-4eee-93f4-a4395d094f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019920308s
Aug 30 16:30:11.338: INFO: Pod "pod-configmaps-3d7dd31f-c195-4eee-93f4-a4395d094f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029670313s
Aug 30 16:30:13.344: INFO: Pod "pod-configmaps-3d7dd31f-c195-4eee-93f4-a4395d094f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034850169s
Aug 30 16:30:15.351: INFO: Pod "pod-configmaps-3d7dd31f-c195-4eee-93f4-a4395d094f3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.041734266s
STEP: Saw pod success
Aug 30 16:30:15.351: INFO: Pod "pod-configmaps-3d7dd31f-c195-4eee-93f4-a4395d094f3d" satisfied condition "success or failure"
Aug 30 16:30:15.355: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-configmaps-3d7dd31f-c195-4eee-93f4-a4395d094f3d container env-test: <nil>
STEP: delete the pod
Aug 30 16:30:15.429: INFO: Waiting for pod pod-configmaps-3d7dd31f-c195-4eee-93f4-a4395d094f3d to disappear
Aug 30 16:30:15.433: INFO: Pod pod-configmaps-3d7dd31f-c195-4eee-93f4-a4395d094f3d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:15.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7845" for this suite.

â€¢ [SLOW TEST:8.368 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":5,"skipped":47,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:15.447: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1357
STEP: creating an pod
Aug 30 16:30:15.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-3537 -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 30 16:30:15.825: INFO: stderr: ""
Aug 30 16:30:15.825: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Aug 30 16:30:15.825: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 30 16:30:15.825: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3537" to be "running and ready, or succeeded"
Aug 30 16:30:15.835: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.107724ms
Aug 30 16:30:17.841: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015708335s
Aug 30 16:30:19.846: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.020860013s
Aug 30 16:30:19.846: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 30 16:30:19.846: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 30 16:30:19.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 logs logs-generator logs-generator --namespace=kubectl-3537'
Aug 30 16:30:19.942: INFO: stderr: ""
Aug 30 16:30:19.942: INFO: stdout: "I0830 16:30:17.059896       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/j4g6 334\nI0830 16:30:17.260067       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/rhsr 202\nI0830 16:30:17.460060       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/lhrc 456\nI0830 16:30:17.660104       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/pw8z 405\nI0830 16:30:17.860045       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/kkgn 394\nI0830 16:30:18.060078       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/g97 308\nI0830 16:30:18.260067       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/x2j 488\nI0830 16:30:18.460012       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/qlcn 545\nI0830 16:30:18.660076       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/2wxn 209\nI0830 16:30:18.860075       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/plg 432\nI0830 16:30:19.060078       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/ftjf 276\nI0830 16:30:19.260176       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/lzh4 499\nI0830 16:30:19.460198       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/qnmh 257\nI0830 16:30:19.660149       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/hvbp 503\nI0830 16:30:19.860090       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/pstk 435\n"
STEP: limiting log lines
Aug 30 16:30:19.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 logs logs-generator logs-generator --namespace=kubectl-3537 --tail=1'
Aug 30 16:30:20.157: INFO: stderr: ""
Aug 30 16:30:20.157: INFO: stdout: "I0830 16:30:20.060091       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/gf4 349\n"
Aug 30 16:30:20.157: INFO: got output "I0830 16:30:20.060091       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/gf4 349\n"
STEP: limiting log bytes
Aug 30 16:30:20.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 logs logs-generator logs-generator --namespace=kubectl-3537 --limit-bytes=1'
Aug 30 16:30:20.241: INFO: stderr: ""
Aug 30 16:30:20.241: INFO: stdout: "I"
Aug 30 16:30:20.241: INFO: got output "I"
STEP: exposing timestamps
Aug 30 16:30:20.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 logs logs-generator logs-generator --namespace=kubectl-3537 --tail=1 --timestamps'
Aug 30 16:30:20.333: INFO: stderr: ""
Aug 30 16:30:20.333: INFO: stdout: "2020-08-30T16:30:20.260116997Z I0830 16:30:20.260000       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/kfg6 210\n"
Aug 30 16:30:20.333: INFO: got output "2020-08-30T16:30:20.260116997Z I0830 16:30:20.260000       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/kfg6 210\n"
STEP: restricting to a time range
Aug 30 16:30:22.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 logs logs-generator logs-generator --namespace=kubectl-3537 --since=1s'
Aug 30 16:30:22.919: INFO: stderr: ""
Aug 30 16:30:22.919: INFO: stdout: "I0830 16:30:22.060133       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/d7k4 517\nI0830 16:30:22.259997       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/msm 375\nI0830 16:30:22.460144       1 logs_generator.go:76] 27 GET /api/v1/namespaces/default/pods/v8zq 384\nI0830 16:30:22.660081       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/9n6 331\nI0830 16:30:22.860156       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/g6g 284\n"
Aug 30 16:30:22.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 logs logs-generator logs-generator --namespace=kubectl-3537 --since=24h'
Aug 30 16:30:23.003: INFO: stderr: ""
Aug 30 16:30:23.003: INFO: stdout: "I0830 16:30:17.059896       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/j4g6 334\nI0830 16:30:17.260067       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/rhsr 202\nI0830 16:30:17.460060       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/lhrc 456\nI0830 16:30:17.660104       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/pw8z 405\nI0830 16:30:17.860045       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/kkgn 394\nI0830 16:30:18.060078       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/g97 308\nI0830 16:30:18.260067       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/x2j 488\nI0830 16:30:18.460012       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/qlcn 545\nI0830 16:30:18.660076       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/2wxn 209\nI0830 16:30:18.860075       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/plg 432\nI0830 16:30:19.060078       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/ftjf 276\nI0830 16:30:19.260176       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/lzh4 499\nI0830 16:30:19.460198       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/qnmh 257\nI0830 16:30:19.660149       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/hvbp 503\nI0830 16:30:19.860090       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/pstk 435\nI0830 16:30:20.060091       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/gf4 349\nI0830 16:30:20.260000       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/kfg6 210\nI0830 16:30:20.460022       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/t7h 363\nI0830 16:30:20.660114       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/c6l 526\nI0830 16:30:20.860084       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/7sd 506\nI0830 16:30:21.060022       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/qqt 453\nI0830 16:30:21.260008       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/5rqp 293\nI0830 16:30:21.460017       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/pvzn 361\nI0830 16:30:21.660070       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/ckpg 491\nI0830 16:30:21.860075       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/rkf 366\nI0830 16:30:22.060133       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/d7k4 517\nI0830 16:30:22.259997       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/msm 375\nI0830 16:30:22.460144       1 logs_generator.go:76] 27 GET /api/v1/namespaces/default/pods/v8zq 384\nI0830 16:30:22.660081       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/9n6 331\nI0830 16:30:22.860156       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/g6g 284\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1363
Aug 30 16:30:23.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete pod logs-generator --namespace=kubectl-3537'
Aug 30 16:30:25.036: INFO: stderr: ""
Aug 30 16:30:25.036: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:25.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3537" for this suite.

â€¢ [SLOW TEST:9.605 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1353
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":6,"skipped":59,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:25.052: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:32.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4080" for this suite.

â€¢ [SLOW TEST:7.202 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":7,"skipped":59,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:32.254: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Aug 30 16:30:32.430: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:36.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2085" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":8,"skipped":114,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:36.437: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:47.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7369" for this suite.

â€¢ [SLOW TEST:11.283 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":9,"skipped":120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:47.723: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2046
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:51.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2046" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":10,"skipped":163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:51.987: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4887
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-db12fb4d-5722-41ae-82cb-9d52ea8f8579
STEP: Creating secret with name secret-projected-all-test-volume-b5a5de62-7b54-4f91-b3e1-ac2da66c0996
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 30 16:30:52.199: INFO: Waiting up to 5m0s for pod "projected-volume-7226a450-af9a-4914-b623-0e7157923755" in namespace "projected-4887" to be "success or failure"
Aug 30 16:30:52.205: INFO: Pod "projected-volume-7226a450-af9a-4914-b623-0e7157923755": Phase="Pending", Reason="", readiness=false. Elapsed: 5.386609ms
Aug 30 16:30:54.210: INFO: Pod "projected-volume-7226a450-af9a-4914-b623-0e7157923755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010488812s
STEP: Saw pod success
Aug 30 16:30:54.210: INFO: Pod "projected-volume-7226a450-af9a-4914-b623-0e7157923755" satisfied condition "success or failure"
Aug 30 16:30:54.214: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod projected-volume-7226a450-af9a-4914-b623-0e7157923755 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 30 16:30:54.280: INFO: Waiting for pod projected-volume-7226a450-af9a-4914-b623-0e7157923755 to disappear
Aug 30 16:30:54.285: INFO: Pod projected-volume-7226a450-af9a-4914-b623-0e7157923755 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:54.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4887" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":11,"skipped":191,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:54.300: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Aug 30 16:30:54.494: INFO: Waiting up to 5m0s for pod "var-expansion-96457c9d-db75-4b2f-8c7a-d9550f80356f" in namespace "var-expansion-6793" to be "success or failure"
Aug 30 16:30:54.502: INFO: Pod "var-expansion-96457c9d-db75-4b2f-8c7a-d9550f80356f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.243971ms
Aug 30 16:30:56.508: INFO: Pod "var-expansion-96457c9d-db75-4b2f-8c7a-d9550f80356f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014528319s
STEP: Saw pod success
Aug 30 16:30:56.509: INFO: Pod "var-expansion-96457c9d-db75-4b2f-8c7a-d9550f80356f" satisfied condition "success or failure"
Aug 30 16:30:56.515: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod var-expansion-96457c9d-db75-4b2f-8c7a-d9550f80356f container dapi-container: <nil>
STEP: delete the pod
Aug 30 16:30:56.589: INFO: Waiting for pod var-expansion-96457c9d-db75-4b2f-8c7a-d9550f80356f to disappear
Aug 30 16:30:56.595: INFO: Pod var-expansion-96457c9d-db75-4b2f-8c7a-d9550f80356f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:30:56.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6793" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":12,"skipped":196,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:30:56.609: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:30:56.813: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 30 16:30:56.827: INFO: Number of nodes with available pods: 0
Aug 30 16:30:56.827: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 30 16:30:56.862: INFO: Number of nodes with available pods: 0
Aug 30 16:30:56.862: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:30:57.869: INFO: Number of nodes with available pods: 0
Aug 30 16:30:57.869: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:30:58.868: INFO: Number of nodes with available pods: 0
Aug 30 16:30:58.868: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:30:59.873: INFO: Number of nodes with available pods: 0
Aug 30 16:30:59.873: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:00.868: INFO: Number of nodes with available pods: 0
Aug 30 16:31:00.868: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:01.867: INFO: Number of nodes with available pods: 0
Aug 30 16:31:01.867: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:02.868: INFO: Number of nodes with available pods: 0
Aug 30 16:31:02.868: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:03.868: INFO: Number of nodes with available pods: 1
Aug 30 16:31:03.868: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 30 16:31:03.902: INFO: Number of nodes with available pods: 1
Aug 30 16:31:03.902: INFO: Number of running nodes: 0, number of available pods: 1
Aug 30 16:31:04.909: INFO: Number of nodes with available pods: 0
Aug 30 16:31:04.909: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 30 16:31:04.923: INFO: Number of nodes with available pods: 0
Aug 30 16:31:04.924: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:05.930: INFO: Number of nodes with available pods: 0
Aug 30 16:31:05.930: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:06.929: INFO: Number of nodes with available pods: 0
Aug 30 16:31:06.929: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:07.930: INFO: Number of nodes with available pods: 0
Aug 30 16:31:07.930: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:08.930: INFO: Number of nodes with available pods: 0
Aug 30 16:31:08.931: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:09.930: INFO: Number of nodes with available pods: 0
Aug 30 16:31:09.930: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:10.931: INFO: Number of nodes with available pods: 0
Aug 30 16:31:10.931: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:11.930: INFO: Number of nodes with available pods: 0
Aug 30 16:31:11.930: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:31:12.929: INFO: Number of nodes with available pods: 1
Aug 30 16:31:12.929: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7433, will wait for the garbage collector to delete the pods
Aug 30 16:31:13.007: INFO: Deleting DaemonSet.extensions daemon-set took: 14.360742ms
Aug 30 16:31:13.507: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.179115ms
Aug 30 16:31:16.113: INFO: Number of nodes with available pods: 0
Aug 30 16:31:16.113: INFO: Number of running nodes: 0, number of available pods: 0
Aug 30 16:31:16.119: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7433/daemonsets","resourceVersion":"2742"},"items":null}

Aug 30 16:31:16.125: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7433/pods","resourceVersion":"2742"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:31:16.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7433" for this suite.

â€¢ [SLOW TEST:19.557 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":13,"skipped":197,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:31:16.166: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-9bf8d6d9-21dd-487a-ae9a-7eca3f01ded2 in namespace container-probe-1373
Aug 30 16:31:20.362: INFO: Started pod test-webserver-9bf8d6d9-21dd-487a-ae9a-7eca3f01ded2 in namespace container-probe-1373
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 16:31:20.367: INFO: Initial restart count of pod test-webserver-9bf8d6d9-21dd-487a-ae9a-7eca3f01ded2 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:35:22.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1373" for this suite.

â€¢ [SLOW TEST:245.992 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":14,"skipped":213,"failed":0}
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:35:22.165: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 30 16:35:28.880: INFO: Successfully updated pod "pod-update-21b6066a-33bf-407b-86b1-7ebe3b5e5df0"
STEP: verifying the updated pod is in kubernetes
Aug 30 16:35:28.888: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:35:28.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9381" for this suite.

â€¢ [SLOW TEST:6.737 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":15,"skipped":213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:35:28.902: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:35:29.114: INFO: (0) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 47.011998ms)
Aug 30 16:35:29.162: INFO: (1) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 47.607247ms)
Aug 30 16:35:29.209: INFO: (2) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 47.436353ms)
Aug 30 16:35:29.218: INFO: (3) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.281031ms)
Aug 30 16:35:29.251: INFO: (4) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 32.713884ms)
Aug 30 16:35:29.323: INFO: (5) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 71.603236ms)
Aug 30 16:35:29.333: INFO: (6) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.81989ms)
Aug 30 16:35:29.341: INFO: (7) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.36592ms)
Aug 30 16:35:29.351: INFO: (8) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.066191ms)
Aug 30 16:35:29.361: INFO: (9) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.186453ms)
Aug 30 16:35:29.442: INFO: (10) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 81.678026ms)
Aug 30 16:35:29.457: INFO: (11) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 15.113422ms)
Aug 30 16:35:29.507: INFO: (12) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 49.204417ms)
Aug 30 16:35:29.515: INFO: (13) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.32563ms)
Aug 30 16:35:29.562: INFO: (14) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 46.991215ms)
Aug 30 16:35:29.571: INFO: (15) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.204257ms)
Aug 30 16:35:29.619: INFO: (16) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 47.767621ms)
Aug 30 16:35:29.627: INFO: (17) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.286886ms)
Aug 30 16:35:29.640: INFO: (18) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.918541ms)
Aug 30 16:35:29.654: INFO: (19) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.179856ms)
[AfterEach] version v1
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:35:29.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2480" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":16,"skipped":236,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:35:29.669: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2205
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1626
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 30 16:35:29.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-2205'
Aug 30 16:35:29.926: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 30 16:35:29.926: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1631
Aug 30 16:35:31.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete deployment e2e-test-httpd-deployment --namespace=kubectl-2205'
Aug 30 16:35:32.026: INFO: stderr: ""
Aug 30 16:35:32.026: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:35:32.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2205" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":17,"skipped":242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:35:32.040: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 30 16:35:32.223: INFO: Waiting up to 5m0s for pod "pod-96fa5c45-57f0-4d25-ae74-e26638eecfa2" in namespace "emptydir-9275" to be "success or failure"
Aug 30 16:35:32.227: INFO: Pod "pod-96fa5c45-57f0-4d25-ae74-e26638eecfa2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.614183ms
Aug 30 16:35:34.232: INFO: Pod "pod-96fa5c45-57f0-4d25-ae74-e26638eecfa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009352529s
Aug 30 16:35:36.238: INFO: Pod "pod-96fa5c45-57f0-4d25-ae74-e26638eecfa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015030796s
STEP: Saw pod success
Aug 30 16:35:36.238: INFO: Pod "pod-96fa5c45-57f0-4d25-ae74-e26638eecfa2" satisfied condition "success or failure"
Aug 30 16:35:36.242: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod pod-96fa5c45-57f0-4d25-ae74-e26638eecfa2 container test-container: <nil>
STEP: delete the pod
Aug 30 16:35:36.315: INFO: Waiting for pod pod-96fa5c45-57f0-4d25-ae74-e26638eecfa2 to disappear
Aug 30 16:35:36.319: INFO: Pod pod-96fa5c45-57f0-4d25-ae74-e26638eecfa2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:35:36.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9275" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":18,"skipped":301,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:35:36.332: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 30 16:35:36.530: INFO: Waiting up to 5m0s for pod "pod-ec5f4dfc-6bb3-40de-8689-f13268185958" in namespace "emptydir-2665" to be "success or failure"
Aug 30 16:35:36.540: INFO: Pod "pod-ec5f4dfc-6bb3-40de-8689-f13268185958": Phase="Pending", Reason="", readiness=false. Elapsed: 9.284805ms
Aug 30 16:35:38.545: INFO: Pod "pod-ec5f4dfc-6bb3-40de-8689-f13268185958": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014578115s
STEP: Saw pod success
Aug 30 16:35:38.545: INFO: Pod "pod-ec5f4dfc-6bb3-40de-8689-f13268185958" satisfied condition "success or failure"
Aug 30 16:35:38.554: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod pod-ec5f4dfc-6bb3-40de-8689-f13268185958 container test-container: <nil>
STEP: delete the pod
Aug 30 16:35:38.619: INFO: Waiting for pod pod-ec5f4dfc-6bb3-40de-8689-f13268185958 to disappear
Aug 30 16:35:38.623: INFO: Pod pod-ec5f4dfc-6bb3-40de-8689-f13268185958 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:35:38.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2665" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":19,"skipped":307,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:35:38.637: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1208
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Aug 30 16:35:38.853: INFO: Waiting up to 5m0s for pod "client-containers-e1247519-d5fe-4cad-b30d-17c4a3d4be23" in namespace "containers-1208" to be "success or failure"
Aug 30 16:35:38.861: INFO: Pod "client-containers-e1247519-d5fe-4cad-b30d-17c4a3d4be23": Phase="Pending", Reason="", readiness=false. Elapsed: 7.158436ms
Aug 30 16:35:40.866: INFO: Pod "client-containers-e1247519-d5fe-4cad-b30d-17c4a3d4be23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012555535s
Aug 30 16:35:42.872: INFO: Pod "client-containers-e1247519-d5fe-4cad-b30d-17c4a3d4be23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018374469s
Aug 30 16:35:44.878: INFO: Pod "client-containers-e1247519-d5fe-4cad-b30d-17c4a3d4be23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024668885s
STEP: Saw pod success
Aug 30 16:35:44.878: INFO: Pod "client-containers-e1247519-d5fe-4cad-b30d-17c4a3d4be23" satisfied condition "success or failure"
Aug 30 16:35:44.884: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod client-containers-e1247519-d5fe-4cad-b30d-17c4a3d4be23 container test-container: <nil>
STEP: delete the pod
Aug 30 16:35:44.958: INFO: Waiting for pod client-containers-e1247519-d5fe-4cad-b30d-17c4a3d4be23 to disappear
Aug 30 16:35:44.962: INFO: Pod client-containers-e1247519-d5fe-4cad-b30d-17c4a3d4be23 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:35:44.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1208" for this suite.

â€¢ [SLOW TEST:6.341 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":20,"skipped":318,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:35:44.977: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7703
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:35:45.139: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 30 16:35:45.153: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 30 16:35:50.159: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 30 16:35:50.159: INFO: Creating deployment "test-rolling-update-deployment"
Aug 30 16:35:50.167: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 30 16:35:50.175: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 30 16:35:52.192: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 30 16:35:52.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402150, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402150, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402152, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402150, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:35:54.204: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Aug 30 16:35:54.217: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7703 /apis/apps/v1/namespaces/deployment-7703/deployments/test-rolling-update-deployment 0d319f55-eb95-4372-8547-f6a910ff43eb 4011 1 2020-08-30 16:35:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00270c4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-30 16:35:50 +0000 UTC,LastTransitionTime:2020-08-30 16:35:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-08-30 16:35:52 +0000 UTC,LastTransitionTime:2020-08-30 16:35:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 30 16:35:54.225: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-7703 /apis/apps/v1/namespaces/deployment-7703/replicasets/test-rolling-update-deployment-67cf4f6444 484db303-d776-4217-a4cb-61fcd92b29df 4000 1 2020-08-30 16:35:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 0d319f55-eb95-4372-8547-f6a910ff43eb 0xc00270c977 0xc00270c978}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00270c9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:35:54.225: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 30 16:35:54.225: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7703 /apis/apps/v1/namespaces/deployment-7703/replicasets/test-rolling-update-controller f3737a56-3430-4db2-8c46-ae6e09459458 4010 2 2020-08-30 16:35:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 0d319f55-eb95-4372-8547-f6a910ff43eb 0xc00270c89f 0xc00270c8b0}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00270c918 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:35:54.231: INFO: Pod "test-rolling-update-deployment-67cf4f6444-j2cph" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-j2cph test-rolling-update-deployment-67cf4f6444- deployment-7703 /api/v1/namespaces/deployment-7703/pods/test-rolling-update-deployment-67cf4f6444-j2cph 1926da9e-826b-4f3a-a520-186fb324c8e8 3998 0 2020-08-30 16:35:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:172.25.0.22/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 484db303-d776-4217-a4cb-61fcd92b29df 0xc00270ce57 0xc00270ce58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7hw7x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7hw7x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7hw7x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:35:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:35:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:35:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:172.25.0.22,StartTime:2020-08-30 16:35:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 16:35:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://0023a7180c84950d710b154874ef8a1c76343ef074258908c16c6d5a41270990,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:35:54.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7703" for this suite.

â€¢ [SLOW TEST:9.266 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":21,"skipped":324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:35:54.245: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-3334840d-d4d8-4c7c-bc43-319194a79a1f
STEP: Creating a pod to test consume secrets
Aug 30 16:35:54.431: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f3a9b070-e56f-4950-b6d5-2a3f146979f6" in namespace "projected-267" to be "success or failure"
Aug 30 16:35:54.436: INFO: Pod "pod-projected-secrets-f3a9b070-e56f-4950-b6d5-2a3f146979f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.677535ms
Aug 30 16:35:56.442: INFO: Pod "pod-projected-secrets-f3a9b070-e56f-4950-b6d5-2a3f146979f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011651506s
Aug 30 16:35:58.448: INFO: Pod "pod-projected-secrets-f3a9b070-e56f-4950-b6d5-2a3f146979f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017324748s
STEP: Saw pod success
Aug 30 16:35:58.448: INFO: Pod "pod-projected-secrets-f3a9b070-e56f-4950-b6d5-2a3f146979f6" satisfied condition "success or failure"
Aug 30 16:35:58.452: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-projected-secrets-f3a9b070-e56f-4950-b6d5-2a3f146979f6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 30 16:35:58.530: INFO: Waiting for pod pod-projected-secrets-f3a9b070-e56f-4950-b6d5-2a3f146979f6 to disappear
Aug 30 16:35:58.534: INFO: Pod pod-projected-secrets-f3a9b070-e56f-4950-b6d5-2a3f146979f6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:35:58.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-267" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":22,"skipped":418,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:35:58.548: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-8854
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8854 to expose endpoints map[]
Aug 30 16:35:58.733: INFO: successfully validated that service multi-endpoint-test in namespace services-8854 exposes endpoints map[] (9.310679ms elapsed)
STEP: Creating pod pod1 in namespace services-8854
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8854 to expose endpoints map[pod1:[100]]
Aug 30 16:36:00.799: INFO: successfully validated that service multi-endpoint-test in namespace services-8854 exposes endpoints map[pod1:[100]] (2.049916569s elapsed)
STEP: Creating pod pod2 in namespace services-8854
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8854 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 30 16:36:03.872: INFO: successfully validated that service multi-endpoint-test in namespace services-8854 exposes endpoints map[pod1:[100] pod2:[101]] (3.065281094s elapsed)
STEP: Deleting pod pod1 in namespace services-8854
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8854 to expose endpoints map[pod2:[101]]
Aug 30 16:36:03.896: INFO: successfully validated that service multi-endpoint-test in namespace services-8854 exposes endpoints map[pod2:[101]] (13.877505ms elapsed)
STEP: Deleting pod pod2 in namespace services-8854
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8854 to expose endpoints map[]
Aug 30 16:36:04.919: INFO: successfully validated that service multi-endpoint-test in namespace services-8854 exposes endpoints map[] (1.01204984s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:36:04.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8854" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:6.412 seconds]
[sig-network] Services
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":23,"skipped":431,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:36:04.960: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1790
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 30 16:36:05.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6018'
Aug 30 16:36:05.218: INFO: stderr: ""
Aug 30 16:36:05.218: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 30 16:36:10.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pod e2e-test-httpd-pod --namespace=kubectl-6018 -o json'
Aug 30 16:36:10.339: INFO: stderr: ""
Aug 30 16:36:10.339: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.25.0.25/32\"\n        },\n        \"creationTimestamp\": \"2020-08-30T16:36:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6018\",\n        \"resourceVersion\": \"4197\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6018/pods/e2e-test-httpd-pod\",\n        \"uid\": \"349d9913-c5a6-4c56-96ed-19e7c7403449\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-ntlvp\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"adoring-wozniak-54dcfd79fc-948mf\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-ntlvp\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-ntlvp\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-30T16:36:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-30T16:36:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-30T16:36:07Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-30T16:36:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://87d387d33b0b449db9e1ac96dede0e40eed305b8f75ca8eebd0a67e723a6ff5f\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-08-30T16:36:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"46.101.135.210\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.0.25\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.0.25\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-08-30T16:36:05Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 30 16:36:10.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 replace -f - --namespace=kubectl-6018'
Aug 30 16:36:10.555: INFO: stderr: ""
Aug 30 16:36:10.555: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1795
Aug 30 16:36:10.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete pods e2e-test-httpd-pod --namespace=kubectl-6018'
Aug 30 16:36:14.276: INFO: stderr: ""
Aug 30 16:36:14.276: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:36:14.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6018" for this suite.

â€¢ [SLOW TEST:9.330 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1786
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":24,"skipped":443,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:36:14.291: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 30 16:36:18.517: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 30 16:36:18.522: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 30 16:36:20.522: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 30 16:36:20.527: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 30 16:36:22.522: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 30 16:36:22.528: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:36:22.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4176" for this suite.

â€¢ [SLOW TEST:8.303 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":25,"skipped":444,"failed":0}
SSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:36:22.595: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-556
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Aug 30 16:36:22.779: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-556" to be "success or failure"
Aug 30 16:36:22.786: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.828789ms
Aug 30 16:36:24.791: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01206146s
STEP: Saw pod success
Aug 30 16:36:24.791: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 30 16:36:24.796: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 30 16:36:24.861: INFO: Waiting for pod pod-host-path-test to disappear
Aug 30 16:36:24.865: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:36:24.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-556" for this suite.
â€¢{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":26,"skipped":447,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:36:24.879: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:36:25.074: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 30 16:36:25.094: INFO: Number of nodes with available pods: 0
Aug 30 16:36:25.094: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:36:26.107: INFO: Number of nodes with available pods: 0
Aug 30 16:36:26.107: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:36:27.104: INFO: Number of nodes with available pods: 2
Aug 30 16:36:27.104: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 30 16:36:27.151: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:27.151: INFO: Wrong image for pod: daemon-set-pmlp8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:28.163: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:28.163: INFO: Wrong image for pod: daemon-set-pmlp8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:29.165: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:29.165: INFO: Wrong image for pod: daemon-set-pmlp8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:30.161: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:30.161: INFO: Wrong image for pod: daemon-set-pmlp8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:30.161: INFO: Pod daemon-set-pmlp8 is not available
Aug 30 16:36:31.163: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:31.163: INFO: Pod daemon-set-hqn6x is not available
Aug 30 16:36:32.164: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:32.164: INFO: Pod daemon-set-hqn6x is not available
Aug 30 16:36:33.163: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:34.163: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:34.163: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:35.162: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:35.162: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:36.164: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:36.164: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:37.163: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:37.163: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:38.163: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:38.163: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:39.162: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:39.162: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:40.163: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:40.163: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:41.163: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:41.163: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:42.164: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:42.164: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:43.189: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:43.189: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:44.163: INFO: Wrong image for pod: daemon-set-7pjdd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 30 16:36:44.163: INFO: Pod daemon-set-7pjdd is not available
Aug 30 16:36:45.163: INFO: Pod daemon-set-v5tnr is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 30 16:36:45.177: INFO: Number of nodes with available pods: 1
Aug 30 16:36:45.177: INFO: Node adoring-wozniak-54dcfd79fc-948mf is running more than one daemon pod
Aug 30 16:36:46.187: INFO: Number of nodes with available pods: 1
Aug 30 16:36:46.187: INFO: Node adoring-wozniak-54dcfd79fc-948mf is running more than one daemon pod
Aug 30 16:36:47.188: INFO: Number of nodes with available pods: 2
Aug 30 16:36:47.188: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6694, will wait for the garbage collector to delete the pods
Aug 30 16:36:47.277: INFO: Deleting DaemonSet.extensions daemon-set took: 9.846717ms
Aug 30 16:36:47.777: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.155457ms
Aug 30 16:36:59.982: INFO: Number of nodes with available pods: 0
Aug 30 16:36:59.982: INFO: Number of running nodes: 0, number of available pods: 0
Aug 30 16:36:59.987: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6694/daemonsets","resourceVersion":"4610"},"items":null}

Aug 30 16:36:59.991: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6694/pods","resourceVersion":"4610"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:37:00.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6694" for this suite.

â€¢ [SLOW TEST:35.140 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":27,"skipped":467,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:37:00.019: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Aug 30 16:37:00.188: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 16:37:00.203: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 16:37:00.208: INFO: 
Logging pods the kubelet thinks is on node adoring-wozniak-54dcfd79fc-6rshr before test
Aug 30 16:37:00.266: INFO: canal-54glj from kube-system started at 2020-08-30 16:28:14 +0000 UTC (2 container statuses recorded)
Aug 30 16:37:00.266: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:37:00.266: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 30 16:37:00.266: INFO: kube-proxy-wxdxv from kube-system started at 2020-08-30 16:28:14 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.266: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 16:37:00.266: INFO: user-ssh-keys-agent-xwrzj from kube-system started at 2020-08-30 16:28:14 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.266: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Aug 30 16:37:00.266: INFO: node-local-dns-cvxnf from kube-system started at 2020-08-30 16:28:34 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.266: INFO: 	Container node-cache ready: true, restart count 0
Aug 30 16:37:00.266: INFO: logrotate-57bmz from kube-system started at 2020-08-30 16:28:34 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.266: INFO: 	Container logrotate ready: true, restart count 0
Aug 30 16:37:00.266: INFO: sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-nd5nc from sonobuoy started at 2020-08-30 16:29:24 +0000 UTC (2 container statuses recorded)
Aug 30 16:37:00.266: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:37:00.266: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 16:37:00.266: INFO: coredns-54457d966b-fbnz4 from kube-system started at 2020-08-30 16:28:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.266: INFO: 	Container coredns ready: true, restart count 0
Aug 30 16:37:00.266: INFO: sonobuoy from sonobuoy started at 2020-08-30 16:29:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.266: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 16:37:00.266: INFO: sonobuoy-e2e-job-cf49606f646f4c8a from sonobuoy started at 2020-08-30 16:29:23 +0000 UTC (2 container statuses recorded)
Aug 30 16:37:00.266: INFO: 	Container e2e ready: true, restart count 0
Aug 30 16:37:00.266: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:37:00.266: INFO: 
Logging pods the kubelet thinks is on node adoring-wozniak-54dcfd79fc-948mf before test
Aug 30 16:37:00.324: INFO: dashboard-metrics-scraper-59bfc65dc9-94mm2 from kubernetes-dashboard started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.324: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 30 16:37:00.324: INFO: coredns-54457d966b-vr27r from kube-system started at 2020-08-30 16:28:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.324: INFO: 	Container coredns ready: true, restart count 0
Aug 30 16:37:00.324: INFO: canal-lg5hn from kube-system started at 2020-08-30 16:28:09 +0000 UTC (2 container statuses recorded)
Aug 30 16:37:00.324: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:37:00.324: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 30 16:37:00.324: INFO: user-ssh-keys-agent-vkbs9 from kube-system started at 2020-08-30 16:28:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.324: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Aug 30 16:37:00.324: INFO: openvpn-client-78d595f58b-pht8r from kube-system started at 2020-08-30 16:28:29 +0000 UTC (2 container statuses recorded)
Aug 30 16:37:00.324: INFO: 	Container dnat-controller ready: true, restart count 0
Aug 30 16:37:00.324: INFO: 	Container openvpn-client ready: true, restart count 0
Aug 30 16:37:00.324: INFO: dashboard-metrics-scraper-59bfc65dc9-fhfrl from kubernetes-dashboard started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.324: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 30 16:37:00.324: INFO: node-local-dns-wrs2k from kube-system started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.324: INFO: 	Container node-cache ready: true, restart count 0
Aug 30 16:37:00.324: INFO: logrotate-pw2ff from kube-system started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.324: INFO: 	Container logrotate ready: true, restart count 0
Aug 30 16:37:00.324: INFO: sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-cdm8v from sonobuoy started at 2020-08-30 16:29:23 +0000 UTC (2 container statuses recorded)
Aug 30 16:37:00.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:37:00.324: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 16:37:00.324: INFO: kube-proxy-v88gx from kube-system started at 2020-08-30 16:28:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:37:00.324: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-326c569c-5555-4d60-af2a-9b273bd14ec5 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-326c569c-5555-4d60-af2a-9b273bd14ec5 off the node adoring-wozniak-54dcfd79fc-948mf
STEP: verifying the node doesn't have the label kubernetes.io/e2e-326c569c-5555-4d60-af2a-9b273bd14ec5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:37:06.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8955" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:6.419 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":28,"skipped":468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:37:06.439: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-74156fc0-7904-4b99-92b4-13943f680892 in namespace container-probe-7084
Aug 30 16:37:10.630: INFO: Started pod busybox-74156fc0-7904-4b99-92b4-13943f680892 in namespace container-probe-7084
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 16:37:10.636: INFO: Initial restart count of pod busybox-74156fc0-7904-4b99-92b4-13943f680892 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:41:11.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7084" for this suite.

â€¢ [SLOW TEST:244.962 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":29,"skipped":498,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:41:11.401: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8348
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:41:11.570: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 30 16:41:16.575: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 30 16:41:16.575: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 30 16:41:18.581: INFO: Creating deployment "test-rollover-deployment"
Aug 30 16:41:18.593: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 30 16:41:20.604: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 30 16:41:20.614: INFO: Ensure that both replica sets have 1 created replica
Aug 30 16:41:20.624: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 30 16:41:20.635: INFO: Updating deployment test-rollover-deployment
Aug 30 16:41:20.635: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 30 16:41:22.646: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 30 16:41:22.657: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 30 16:41:22.667: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:41:22.667: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402480, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:41:24.677: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:41:24.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402482, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:41:26.679: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:41:26.680: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402482, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:41:28.677: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:41:28.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402482, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:41:30.678: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:41:30.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402482, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:41:32.679: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:41:32.680: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402482, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402478, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:41:34.679: INFO: 
Aug 30 16:41:34.679: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Aug 30 16:41:34.908: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8348 /apis/apps/v1/namespaces/deployment-8348/deployments/test-rollover-deployment 55db255b-9dc1-484b-ba21-a5e1e8916064 5756 2 2020-08-30 16:41:18 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0000547c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-30 16:41:18 +0000 UTC,LastTransitionTime:2020-08-30 16:41:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-08-30 16:41:32 +0000 UTC,LastTransitionTime:2020-08-30 16:41:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 30 16:41:34.913: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-8348 /apis/apps/v1/namespaces/deployment-8348/replicasets/test-rollover-deployment-574d6dfbff 95ae23a1-fdbc-43ee-b549-3241c7546eb4 5745 2 2020-08-30 16:41:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 55db255b-9dc1-484b-ba21-a5e1e8916064 0xc0000552d7 0xc0000552d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0000553a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:41:34.913: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 30 16:41:34.913: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8348 /apis/apps/v1/namespaces/deployment-8348/replicasets/test-rollover-controller 5f130d62-ae1d-4830-b32e-368e38c61ffd 5755 2 2020-08-30 16:41:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 55db255b-9dc1-484b-ba21-a5e1e8916064 0xc00005506f 0xc0000550d0}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000055168 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:41:34.913: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-8348 /apis/apps/v1/namespaces/deployment-8348/replicasets/test-rollover-deployment-f6c94f66c 17632baf-bd21-4657-9047-262037f9f705 5691 2 2020-08-30 16:41:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 55db255b-9dc1-484b-ba21-a5e1e8916064 0xc0000554a0 0xc0000554a1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000055578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:41:35.140: INFO: Pod "test-rollover-deployment-574d6dfbff-98nwc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-98nwc test-rollover-deployment-574d6dfbff- deployment-8348 /api/v1/namespaces/deployment-8348/pods/test-rollover-deployment-574d6dfbff-98nwc bf49bf93-e84f-4685-b47f-ed5b53c2deaf 5706 0 2020-08-30 16:41:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:172.25.1.16/32] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 95ae23a1-fdbc-43ee-b549-3241c7546eb4 0xc000786827 0xc000786828}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pmpwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pmpwl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pmpwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:41:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:41:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:41:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:41:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:172.25.1.16,StartTime:2020-08-30 16:41:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 16:41:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://d0dda35f351dd9b7f0522f5680db98e55e2a84af41f7e7a6ce765ddf0e2faeea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:41:35.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8348" for this suite.

â€¢ [SLOW TEST:23.756 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":30,"skipped":499,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:41:35.158: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3676
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Aug 30 16:41:36.604: INFO: created pod pod-service-account-defaultsa
Aug 30 16:41:36.604: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 30 16:41:36.613: INFO: created pod pod-service-account-mountsa
Aug 30 16:41:36.613: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 30 16:41:36.621: INFO: created pod pod-service-account-nomountsa
Aug 30 16:41:36.621: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 30 16:41:36.630: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 30 16:41:36.630: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 30 16:41:36.641: INFO: created pod pod-service-account-mountsa-mountspec
Aug 30 16:41:36.641: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 30 16:41:36.664: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 30 16:41:36.664: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 30 16:41:36.681: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 30 16:41:36.681: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 30 16:41:36.701: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 30 16:41:36.702: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 30 16:41:36.710: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 30 16:41:36.710: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:41:36.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3676" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":31,"skipped":527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:41:36.734: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 16:41:36.955: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1388654e-32f8-48ef-8d90-3fe1f2c32367" in namespace "downward-api-8207" to be "success or failure"
Aug 30 16:41:36.965: INFO: Pod "downwardapi-volume-1388654e-32f8-48ef-8d90-3fe1f2c32367": Phase="Pending", Reason="", readiness=false. Elapsed: 10.360808ms
Aug 30 16:41:38.971: INFO: Pod "downwardapi-volume-1388654e-32f8-48ef-8d90-3fe1f2c32367": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015932416s
Aug 30 16:41:40.977: INFO: Pod "downwardapi-volume-1388654e-32f8-48ef-8d90-3fe1f2c32367": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021601996s
Aug 30 16:41:42.982: INFO: Pod "downwardapi-volume-1388654e-32f8-48ef-8d90-3fe1f2c32367": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02662219s
STEP: Saw pod success
Aug 30 16:41:42.982: INFO: Pod "downwardapi-volume-1388654e-32f8-48ef-8d90-3fe1f2c32367" satisfied condition "success or failure"
Aug 30 16:41:42.987: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod downwardapi-volume-1388654e-32f8-48ef-8d90-3fe1f2c32367 container client-container: <nil>
STEP: delete the pod
Aug 30 16:41:43.054: INFO: Waiting for pod downwardapi-volume-1388654e-32f8-48ef-8d90-3fe1f2c32367 to disappear
Aug 30 16:41:43.058: INFO: Pod downwardapi-volume-1388654e-32f8-48ef-8d90-3fe1f2c32367 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:41:43.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8207" for this suite.

â€¢ [SLOW TEST:6.337 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":32,"skipped":598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:41:43.072: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Aug 30 16:41:43.237: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 16:41:43.254: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 16:41:43.258: INFO: 
Logging pods the kubelet thinks is on node adoring-wozniak-54dcfd79fc-6rshr before test
Aug 30 16:41:43.314: INFO: sonobuoy from sonobuoy started at 2020-08-30 16:29:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.314: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 16:41:43.314: INFO: logrotate-57bmz from kube-system started at 2020-08-30 16:28:34 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.314: INFO: 	Container logrotate ready: true, restart count 0
Aug 30 16:41:43.314: INFO: sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-nd5nc from sonobuoy started at 2020-08-30 16:29:24 +0000 UTC (2 container statuses recorded)
Aug 30 16:41:43.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:41:43.314: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 16:41:43.314: INFO: kube-proxy-wxdxv from kube-system started at 2020-08-30 16:28:14 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.314: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 16:41:43.314: INFO: node-local-dns-cvxnf from kube-system started at 2020-08-30 16:28:34 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.314: INFO: 	Container node-cache ready: true, restart count 0
Aug 30 16:41:43.314: INFO: coredns-54457d966b-fbnz4 from kube-system started at 2020-08-30 16:28:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.314: INFO: 	Container coredns ready: true, restart count 0
Aug 30 16:41:43.314: INFO: sonobuoy-e2e-job-cf49606f646f4c8a from sonobuoy started at 2020-08-30 16:29:23 +0000 UTC (2 container statuses recorded)
Aug 30 16:41:43.314: INFO: 	Container e2e ready: true, restart count 0
Aug 30 16:41:43.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:41:43.314: INFO: canal-54glj from kube-system started at 2020-08-30 16:28:14 +0000 UTC (2 container statuses recorded)
Aug 30 16:41:43.314: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:41:43.314: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 30 16:41:43.314: INFO: user-ssh-keys-agent-xwrzj from kube-system started at 2020-08-30 16:28:14 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.314: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Aug 30 16:41:43.314: INFO: 
Logging pods the kubelet thinks is on node adoring-wozniak-54dcfd79fc-948mf before test
Aug 30 16:41:43.412: INFO: dashboard-metrics-scraper-59bfc65dc9-94mm2 from kubernetes-dashboard started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.412: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 30 16:41:43.412: INFO: kube-proxy-v88gx from kube-system started at 2020-08-30 16:28:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.412: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 16:41:43.412: INFO: user-ssh-keys-agent-vkbs9 from kube-system started at 2020-08-30 16:28:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.412: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Aug 30 16:41:43.412: INFO: openvpn-client-78d595f58b-pht8r from kube-system started at 2020-08-30 16:28:29 +0000 UTC (2 container statuses recorded)
Aug 30 16:41:43.412: INFO: 	Container dnat-controller ready: true, restart count 0
Aug 30 16:41:43.412: INFO: 	Container openvpn-client ready: true, restart count 0
Aug 30 16:41:43.412: INFO: dashboard-metrics-scraper-59bfc65dc9-fhfrl from kubernetes-dashboard started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.412: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 30 16:41:43.412: INFO: logrotate-pw2ff from kube-system started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.412: INFO: 	Container logrotate ready: true, restart count 0
Aug 30 16:41:43.412: INFO: sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-cdm8v from sonobuoy started at 2020-08-30 16:29:23 +0000 UTC (2 container statuses recorded)
Aug 30 16:41:43.412: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:41:43.412: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 16:41:43.412: INFO: canal-lg5hn from kube-system started at 2020-08-30 16:28:09 +0000 UTC (2 container statuses recorded)
Aug 30 16:41:43.412: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:41:43.412: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 30 16:41:43.412: INFO: coredns-54457d966b-vr27r from kube-system started at 2020-08-30 16:28:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.412: INFO: 	Container coredns ready: true, restart count 0
Aug 30 16:41:43.412: INFO: node-local-dns-wrs2k from kube-system started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:41:43.412: INFO: 	Container node-cache ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3d26398b-7c08-48c2-8a84-60b584cd313f 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-3d26398b-7c08-48c2-8a84-60b584cd313f off the node adoring-wozniak-54dcfd79fc-948mf
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3d26398b-7c08-48c2-8a84-60b584cd313f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:46:47.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8930" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:304.497 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":33,"skipped":652,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:46:47.569: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4100
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 16:46:47.770: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69f49be4-38bf-416a-a85f-27586485f6d6" in namespace "downward-api-4100" to be "success or failure"
Aug 30 16:46:47.776: INFO: Pod "downwardapi-volume-69f49be4-38bf-416a-a85f-27586485f6d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.555073ms
Aug 30 16:46:49.782: INFO: Pod "downwardapi-volume-69f49be4-38bf-416a-a85f-27586485f6d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012197904s
Aug 30 16:46:51.788: INFO: Pod "downwardapi-volume-69f49be4-38bf-416a-a85f-27586485f6d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018192108s
STEP: Saw pod success
Aug 30 16:46:51.788: INFO: Pod "downwardapi-volume-69f49be4-38bf-416a-a85f-27586485f6d6" satisfied condition "success or failure"
Aug 30 16:46:51.793: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-69f49be4-38bf-416a-a85f-27586485f6d6 container client-container: <nil>
STEP: delete the pod
Aug 30 16:46:51.863: INFO: Waiting for pod downwardapi-volume-69f49be4-38bf-416a-a85f-27586485f6d6 to disappear
Aug 30 16:46:51.867: INFO: Pod downwardapi-volume-69f49be4-38bf-416a-a85f-27586485f6d6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:46:51.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4100" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":34,"skipped":666,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:46:51.889: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8680
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-162574cd-903d-4d50-a5e8-1cbf4afce40a
STEP: Creating configMap with name cm-test-opt-upd-8220f7d1-abe6-4029-b935-9c73e5c83adb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-162574cd-903d-4d50-a5e8-1cbf4afce40a
STEP: Updating configmap cm-test-opt-upd-8220f7d1-abe6-4029-b935-9c73e5c83adb
STEP: Creating configMap with name cm-test-opt-create-c29ce3f8-28c0-46e3-9338-78377a0fd8f4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:48:27.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8680" for this suite.

â€¢ [SLOW TEST:95.480 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":35,"skipped":675,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:48:27.369: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6694
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 30 16:48:27.548: INFO: Waiting up to 5m0s for pod "pod-d441700d-ec20-4ffb-933a-0a927c2b57bf" in namespace "emptydir-6694" to be "success or failure"
Aug 30 16:48:27.559: INFO: Pod "pod-d441700d-ec20-4ffb-933a-0a927c2b57bf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.626048ms
Aug 30 16:48:29.565: INFO: Pod "pod-d441700d-ec20-4ffb-933a-0a927c2b57bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016472409s
STEP: Saw pod success
Aug 30 16:48:29.565: INFO: Pod "pod-d441700d-ec20-4ffb-933a-0a927c2b57bf" satisfied condition "success or failure"
Aug 30 16:48:29.569: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod pod-d441700d-ec20-4ffb-933a-0a927c2b57bf container test-container: <nil>
STEP: delete the pod
Aug 30 16:48:29.679: INFO: Waiting for pod pod-d441700d-ec20-4ffb-933a-0a927c2b57bf to disappear
Aug 30 16:48:29.683: INFO: Pod pod-d441700d-ec20-4ffb-933a-0a927c2b57bf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:48:29.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6694" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":36,"skipped":696,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:48:29.699: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-634
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Aug 30 16:48:29.868: INFO: Waiting up to 5m0s for pod "downward-api-543e9512-0410-4819-997b-3036c613f168" in namespace "downward-api-634" to be "success or failure"
Aug 30 16:48:29.873: INFO: Pod "downward-api-543e9512-0410-4819-997b-3036c613f168": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650776ms
Aug 30 16:48:31.878: INFO: Pod "downward-api-543e9512-0410-4819-997b-3036c613f168": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009927127s
Aug 30 16:48:33.885: INFO: Pod "downward-api-543e9512-0410-4819-997b-3036c613f168": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016495012s
Aug 30 16:48:35.890: INFO: Pod "downward-api-543e9512-0410-4819-997b-3036c613f168": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022259619s
STEP: Saw pod success
Aug 30 16:48:35.890: INFO: Pod "downward-api-543e9512-0410-4819-997b-3036c613f168" satisfied condition "success or failure"
Aug 30 16:48:35.895: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod downward-api-543e9512-0410-4819-997b-3036c613f168 container dapi-container: <nil>
STEP: delete the pod
Aug 30 16:48:35.978: INFO: Waiting for pod downward-api-543e9512-0410-4819-997b-3036c613f168 to disappear
Aug 30 16:48:35.988: INFO: Pod downward-api-543e9512-0410-4819-997b-3036c613f168 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:48:35.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-634" for this suite.

â€¢ [SLOW TEST:6.303 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":37,"skipped":715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:48:36.002: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7824
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 16:48:36.544: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 16:48:38.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402916, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402916, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402916, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402916, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:48:41.580: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Aug 30 16:48:41.610: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:48:41.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7824" for this suite.
STEP: Destroying namespace "webhook-7824-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.075 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":38,"skipped":739,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:48:42.080: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:48:42.251: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 30 16:48:47.257: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 30 16:48:47.257: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Aug 30 16:48:47.288: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4729 /apis/apps/v1/namespaces/deployment-4729/deployments/test-cleanup-deployment fe6bc564-b116-474e-8ace-b9bb3ccd4d28 7750 1 2020-08-30 16:48:47 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002827678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 30 16:48:47.294: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-4729 /apis/apps/v1/namespaces/deployment-4729/replicasets/test-cleanup-deployment-55ffc6b7b6 97db64fd-daeb-42d8-87c2-f835b7a9bcfa 7752 1 2020-08-30 16:48:47 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment fe6bc564-b116-474e-8ace-b9bb3ccd4d28 0xc002827a97 0xc002827a98}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002827b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:48:47.294: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 30 16:48:47.294: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4729 /apis/apps/v1/namespaces/deployment-4729/replicasets/test-cleanup-controller 69c539d3-9a11-4dda-a8a4-2d14706b2924 7751 1 2020-08-30 16:48:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment fe6bc564-b116-474e-8ace-b9bb3ccd4d28 0xc0028279d7 0xc0028279d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002827a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:48:47.302: INFO: Pod "test-cleanup-controller-h74rt" is available:
&Pod{ObjectMeta:{test-cleanup-controller-h74rt test-cleanup-controller- deployment-4729 /api/v1/namespaces/deployment-4729/pods/test-cleanup-controller-h74rt 883cfb7c-577d-4eff-94d0-6d249565be11 7721 0 2020-08-30 16:48:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.25.0.46/32] [{apps/v1 ReplicaSet test-cleanup-controller 69c539d3-9a11-4dda-a8a4-2d14706b2924 0xc002ab61cf 0xc002ab61f0}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x26qx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x26qx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x26qx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:48:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:48:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:48:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 16:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:172.25.0.46,StartTime:2020-08-30 16:48:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 16:48:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6f88b8c52d91ed05a581934074df975718614b90410e86b8f3c0d3ba7cf5d373,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:48:47.302: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-zn4r8" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-zn4r8 test-cleanup-deployment-55ffc6b7b6- deployment-4729 /api/v1/namespaces/deployment-4729/pods/test-cleanup-deployment-55ffc6b7b6-zn4r8 d05e5e8e-86be-44e3-a827-3a806f3a90c8 7754 0 2020-08-30 16:48:47 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 97db64fd-daeb-42d8-87c2-f835b7a9bcfa 0xc002ab6387 0xc002ab6388}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x26qx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x26qx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x26qx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:48:47.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4729" for this suite.

â€¢ [SLOW TEST:5.245 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":39,"skipped":741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:48:47.327: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7104
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-e174f66e-dc2d-4809-b01d-87718e87c79a
STEP: Creating a pod to test consume secrets
Aug 30 16:48:47.513: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-48126f7d-46f3-4b8f-9c56-4321a8ea34f1" in namespace "projected-7104" to be "success or failure"
Aug 30 16:48:47.518: INFO: Pod "pod-projected-secrets-48126f7d-46f3-4b8f-9c56-4321a8ea34f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.213617ms
Aug 30 16:48:49.524: INFO: Pod "pod-projected-secrets-48126f7d-46f3-4b8f-9c56-4321a8ea34f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010572817s
STEP: Saw pod success
Aug 30 16:48:49.524: INFO: Pod "pod-projected-secrets-48126f7d-46f3-4b8f-9c56-4321a8ea34f1" satisfied condition "success or failure"
Aug 30 16:48:49.528: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod pod-projected-secrets-48126f7d-46f3-4b8f-9c56-4321a8ea34f1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 30 16:48:49.596: INFO: Waiting for pod pod-projected-secrets-48126f7d-46f3-4b8f-9c56-4321a8ea34f1 to disappear
Aug 30 16:48:49.601: INFO: Pod pod-projected-secrets-48126f7d-46f3-4b8f-9c56-4321a8ea34f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:48:49.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7104" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":40,"skipped":763,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:48:49.620: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Aug 30 16:48:49.778: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 16:48:49.793: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 16:48:49.798: INFO: 
Logging pods the kubelet thinks is on node adoring-wozniak-54dcfd79fc-6rshr before test
Aug 30 16:48:49.855: INFO: kube-proxy-wxdxv from kube-system started at 2020-08-30 16:28:14 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.855: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 16:48:49.855: INFO: node-local-dns-cvxnf from kube-system started at 2020-08-30 16:28:34 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.855: INFO: 	Container node-cache ready: true, restart count 0
Aug 30 16:48:49.855: INFO: logrotate-57bmz from kube-system started at 2020-08-30 16:28:34 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.855: INFO: 	Container logrotate ready: true, restart count 0
Aug 30 16:48:49.855: INFO: sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-nd5nc from sonobuoy started at 2020-08-30 16:29:24 +0000 UTC (2 container statuses recorded)
Aug 30 16:48:49.855: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:48:49.855: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 16:48:49.856: INFO: coredns-54457d966b-fbnz4 from kube-system started at 2020-08-30 16:28:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.856: INFO: 	Container coredns ready: true, restart count 0
Aug 30 16:48:49.856: INFO: sonobuoy-e2e-job-cf49606f646f4c8a from sonobuoy started at 2020-08-30 16:29:23 +0000 UTC (2 container statuses recorded)
Aug 30 16:48:49.856: INFO: 	Container e2e ready: true, restart count 0
Aug 30 16:48:49.856: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:48:49.856: INFO: canal-54glj from kube-system started at 2020-08-30 16:28:14 +0000 UTC (2 container statuses recorded)
Aug 30 16:48:49.856: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:48:49.856: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 30 16:48:49.856: INFO: user-ssh-keys-agent-xwrzj from kube-system started at 2020-08-30 16:28:14 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.856: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Aug 30 16:48:49.856: INFO: sonobuoy from sonobuoy started at 2020-08-30 16:29:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.856: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 16:48:49.856: INFO: 
Logging pods the kubelet thinks is on node adoring-wozniak-54dcfd79fc-948mf before test
Aug 30 16:48:49.916: INFO: node-local-dns-wrs2k from kube-system started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container node-cache ready: true, restart count 0
Aug 30 16:48:49.916: INFO: test-cleanup-deployment-55ffc6b7b6-zn4r8 from deployment-4729 started at 2020-08-30 16:48:47 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container agnhost ready: true, restart count 0
Aug 30 16:48:49.916: INFO: dashboard-metrics-scraper-59bfc65dc9-94mm2 from kubernetes-dashboard started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 30 16:48:49.916: INFO: kube-proxy-v88gx from kube-system started at 2020-08-30 16:28:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 16:48:49.916: INFO: user-ssh-keys-agent-vkbs9 from kube-system started at 2020-08-30 16:28:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Aug 30 16:48:49.916: INFO: openvpn-client-78d595f58b-pht8r from kube-system started at 2020-08-30 16:28:29 +0000 UTC (2 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container dnat-controller ready: true, restart count 0
Aug 30 16:48:49.916: INFO: 	Container openvpn-client ready: true, restart count 0
Aug 30 16:48:49.916: INFO: dashboard-metrics-scraper-59bfc65dc9-fhfrl from kubernetes-dashboard started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 30 16:48:49.916: INFO: logrotate-pw2ff from kube-system started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container logrotate ready: true, restart count 0
Aug 30 16:48:49.916: INFO: sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-cdm8v from sonobuoy started at 2020-08-30 16:29:23 +0000 UTC (2 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:48:49.916: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 16:48:49.916: INFO: canal-lg5hn from kube-system started at 2020-08-30 16:28:09 +0000 UTC (2 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:48:49.916: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 30 16:48:49.916: INFO: coredns-54457d966b-vr27r from kube-system started at 2020-08-30 16:28:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:48:49.916: INFO: 	Container coredns ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-aca8055d-bfed-4b9b-ae16-87b75752b18c 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-aca8055d-bfed-4b9b-ae16-87b75752b18c off the node adoring-wozniak-54dcfd79fc-948mf
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aca8055d-bfed-4b9b-ae16-87b75752b18c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:02.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9339" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:12.453 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":41,"skipped":770,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:02.074: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-393
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Aug 30 16:49:02.243: INFO: Waiting up to 5m0s for pod "var-expansion-27cca060-9c17-4f7d-9379-19f1e5fe5dc9" in namespace "var-expansion-393" to be "success or failure"
Aug 30 16:49:02.247: INFO: Pod "var-expansion-27cca060-9c17-4f7d-9379-19f1e5fe5dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.162286ms
Aug 30 16:49:04.252: INFO: Pod "var-expansion-27cca060-9c17-4f7d-9379-19f1e5fe5dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009390887s
Aug 30 16:49:06.258: INFO: Pod "var-expansion-27cca060-9c17-4f7d-9379-19f1e5fe5dc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01570666s
STEP: Saw pod success
Aug 30 16:49:06.258: INFO: Pod "var-expansion-27cca060-9c17-4f7d-9379-19f1e5fe5dc9" satisfied condition "success or failure"
Aug 30 16:49:06.263: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod var-expansion-27cca060-9c17-4f7d-9379-19f1e5fe5dc9 container dapi-container: <nil>
STEP: delete the pod
Aug 30 16:49:06.333: INFO: Waiting for pod var-expansion-27cca060-9c17-4f7d-9379-19f1e5fe5dc9 to disappear
Aug 30 16:49:06.338: INFO: Pod var-expansion-27cca060-9c17-4f7d-9379-19f1e5fe5dc9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:06.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-393" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":42,"skipped":772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:06.356: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1616
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-09ba011e-d647-4e57-9544-1e0510178fac
STEP: Creating a pod to test consume configMaps
Aug 30 16:49:06.533: INFO: Waiting up to 5m0s for pod "pod-configmaps-fcd7409b-b213-4b80-aa89-2bde8b7f62b9" in namespace "configmap-1616" to be "success or failure"
Aug 30 16:49:06.537: INFO: Pod "pod-configmaps-fcd7409b-b213-4b80-aa89-2bde8b7f62b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189363ms
Aug 30 16:49:08.543: INFO: Pod "pod-configmaps-fcd7409b-b213-4b80-aa89-2bde8b7f62b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010092674s
Aug 30 16:49:10.548: INFO: Pod "pod-configmaps-fcd7409b-b213-4b80-aa89-2bde8b7f62b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015152772s
STEP: Saw pod success
Aug 30 16:49:10.548: INFO: Pod "pod-configmaps-fcd7409b-b213-4b80-aa89-2bde8b7f62b9" satisfied condition "success or failure"
Aug 30 16:49:10.555: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod pod-configmaps-fcd7409b-b213-4b80-aa89-2bde8b7f62b9 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 16:49:10.620: INFO: Waiting for pod pod-configmaps-fcd7409b-b213-4b80-aa89-2bde8b7f62b9 to disappear
Aug 30 16:49:10.625: INFO: Pod pod-configmaps-fcd7409b-b213-4b80-aa89-2bde8b7f62b9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:10.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1616" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":43,"skipped":831,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:10.643: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Aug 30 16:49:10.827: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 16:49:10.841: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 16:49:10.846: INFO: 
Logging pods the kubelet thinks is on node adoring-wozniak-54dcfd79fc-6rshr before test
Aug 30 16:49:10.908: INFO: kube-proxy-wxdxv from kube-system started at 2020-08-30 16:28:14 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.908: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 16:49:10.908: INFO: node-local-dns-cvxnf from kube-system started at 2020-08-30 16:28:34 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.908: INFO: 	Container node-cache ready: true, restart count 0
Aug 30 16:49:10.908: INFO: logrotate-57bmz from kube-system started at 2020-08-30 16:28:34 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.908: INFO: 	Container logrotate ready: true, restart count 0
Aug 30 16:49:10.908: INFO: sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-nd5nc from sonobuoy started at 2020-08-30 16:29:24 +0000 UTC (2 container statuses recorded)
Aug 30 16:49:10.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:49:10.908: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 16:49:10.908: INFO: coredns-54457d966b-fbnz4 from kube-system started at 2020-08-30 16:28:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.908: INFO: 	Container coredns ready: true, restart count 0
Aug 30 16:49:10.908: INFO: sonobuoy-e2e-job-cf49606f646f4c8a from sonobuoy started at 2020-08-30 16:29:23 +0000 UTC (2 container statuses recorded)
Aug 30 16:49:10.908: INFO: 	Container e2e ready: true, restart count 0
Aug 30 16:49:10.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:49:10.908: INFO: canal-54glj from kube-system started at 2020-08-30 16:28:14 +0000 UTC (2 container statuses recorded)
Aug 30 16:49:10.908: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:49:10.908: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 30 16:49:10.908: INFO: user-ssh-keys-agent-xwrzj from kube-system started at 2020-08-30 16:28:14 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.908: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Aug 30 16:49:10.908: INFO: sonobuoy from sonobuoy started at 2020-08-30 16:29:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.908: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 16:49:10.908: INFO: 
Logging pods the kubelet thinks is on node adoring-wozniak-54dcfd79fc-948mf before test
Aug 30 16:49:10.995: INFO: dashboard-metrics-scraper-59bfc65dc9-94mm2 from kubernetes-dashboard started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.995: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 30 16:49:10.995: INFO: pod1 from sched-pred-9339 started at 2020-08-30 16:48:51 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.995: INFO: 	Container pod1 ready: false, restart count 0
Aug 30 16:49:10.995: INFO: dashboard-metrics-scraper-59bfc65dc9-fhfrl from kubernetes-dashboard started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.995: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 30 16:49:10.995: INFO: logrotate-pw2ff from kube-system started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.995: INFO: 	Container logrotate ready: true, restart count 0
Aug 30 16:49:10.995: INFO: sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-cdm8v from sonobuoy started at 2020-08-30 16:29:23 +0000 UTC (2 container statuses recorded)
Aug 30 16:49:10.995: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:49:10.995: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 16:49:10.995: INFO: kube-proxy-v88gx from kube-system started at 2020-08-30 16:28:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.995: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 16:49:10.995: INFO: user-ssh-keys-agent-vkbs9 from kube-system started at 2020-08-30 16:28:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.995: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Aug 30 16:49:10.995: INFO: openvpn-client-78d595f58b-pht8r from kube-system started at 2020-08-30 16:28:29 +0000 UTC (2 container statuses recorded)
Aug 30 16:49:10.995: INFO: 	Container dnat-controller ready: true, restart count 0
Aug 30 16:49:10.995: INFO: 	Container openvpn-client ready: true, restart count 0
Aug 30 16:49:10.995: INFO: pod3 from sched-pred-9339 started at 2020-08-30 16:48:58 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.996: INFO: 	Container pod3 ready: false, restart count 0
Aug 30 16:49:10.996: INFO: canal-lg5hn from kube-system started at 2020-08-30 16:28:09 +0000 UTC (2 container statuses recorded)
Aug 30 16:49:10.996: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:49:10.996: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 30 16:49:10.996: INFO: coredns-54457d966b-vr27r from kube-system started at 2020-08-30 16:28:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.996: INFO: 	Container coredns ready: true, restart count 0
Aug 30 16:49:10.996: INFO: pod2 from sched-pred-9339 started at 2020-08-30 16:48:53 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.996: INFO: 	Container pod2 ready: false, restart count 0
Aug 30 16:49:10.996: INFO: node-local-dns-wrs2k from kube-system started at 2020-08-30 16:28:29 +0000 UTC (1 container statuses recorded)
Aug 30 16:49:10.996: INFO: 	Container node-cache ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.163019b937d891cc], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:12.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3398" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":44,"skipped":846,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:12.046: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3042
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Aug 30 16:49:12.203: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:29.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3042" for this suite.

â€¢ [SLOW TEST:17.759 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":45,"skipped":864,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:29.805: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Aug 30 16:49:29.962: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-962893509 proxy --unix-socket=/tmp/kubectl-proxy-unix984500412/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:30.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9346" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":46,"skipped":874,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:30.023: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:49:30.189: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:32.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6855" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":886,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:32.258: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2052
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 16:49:32.922: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 16:49:34.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402972, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402972, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402972, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734402972, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:49:37.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:38.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2052" for this suite.
STEP: Destroying namespace "webhook-2052-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.150 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":48,"skipped":906,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:38.408: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:54.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5625" for this suite.

â€¢ [SLOW TEST:16.237 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":49,"skipped":914,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:54.645: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-bf5659d9-be44-4a12-86fc-b7bc3c1a9464
STEP: Creating a pod to test consume configMaps
Aug 30 16:49:54.823: INFO: Waiting up to 5m0s for pod "pod-configmaps-2a726753-e88b-405a-b5aa-a4a72bd355c9" in namespace "configmap-8225" to be "success or failure"
Aug 30 16:49:54.830: INFO: Pod "pod-configmaps-2a726753-e88b-405a-b5aa-a4a72bd355c9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.148696ms
Aug 30 16:49:56.837: INFO: Pod "pod-configmaps-2a726753-e88b-405a-b5aa-a4a72bd355c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013710561s
STEP: Saw pod success
Aug 30 16:49:56.837: INFO: Pod "pod-configmaps-2a726753-e88b-405a-b5aa-a4a72bd355c9" satisfied condition "success or failure"
Aug 30 16:49:56.841: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-configmaps-2a726753-e88b-405a-b5aa-a4a72bd355c9 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 16:49:56.911: INFO: Waiting for pod pod-configmaps-2a726753-e88b-405a-b5aa-a4a72bd355c9 to disappear
Aug 30 16:49:56.915: INFO: Pod pod-configmaps-2a726753-e88b-405a-b5aa-a4a72bd355c9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:56.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8225" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":50,"skipped":918,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:56.930: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7092
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Aug 30 16:49:57.091: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-962893509 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:49:57.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7092" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":51,"skipped":923,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:49:57.162: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8920
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:49:57.328: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 30 16:50:00.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-8920 create -f -'
Aug 30 16:50:01.059: INFO: stderr: ""
Aug 30 16:50:01.059: INFO: stdout: "e2e-test-crd-publish-openapi-8397-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 30 16:50:01.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-8920 delete e2e-test-crd-publish-openapi-8397-crds test-cr'
Aug 30 16:50:01.186: INFO: stderr: ""
Aug 30 16:50:01.186: INFO: stdout: "e2e-test-crd-publish-openapi-8397-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 30 16:50:01.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-8920 apply -f -'
Aug 30 16:50:01.343: INFO: stderr: ""
Aug 30 16:50:01.343: INFO: stdout: "e2e-test-crd-publish-openapi-8397-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 30 16:50:01.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-8920 delete e2e-test-crd-publish-openapi-8397-crds test-cr'
Aug 30 16:50:01.418: INFO: stderr: ""
Aug 30 16:50:01.418: INFO: stdout: "e2e-test-crd-publish-openapi-8397-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 30 16:50:01.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 explain e2e-test-crd-publish-openapi-8397-crds'
Aug 30 16:50:01.643: INFO: stderr: ""
Aug 30 16:50:01.643: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8397-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:50:04.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8920" for this suite.

â€¢ [SLOW TEST:7.339 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":52,"skipped":944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:50:04.511: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:50:08.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-984" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":53,"skipped":974,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:50:08.732: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-2c7b1eb4-84fb-478f-9827-1801e51a849c
STEP: Creating a pod to test consume secrets
Aug 30 16:50:08.923: INFO: Waiting up to 5m0s for pod "pod-secrets-fe951ddf-90c7-4a4f-aa9c-b0b0207bd45f" in namespace "secrets-2846" to be "success or failure"
Aug 30 16:50:08.930: INFO: Pod "pod-secrets-fe951ddf-90c7-4a4f-aa9c-b0b0207bd45f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.833296ms
Aug 30 16:50:10.936: INFO: Pod "pod-secrets-fe951ddf-90c7-4a4f-aa9c-b0b0207bd45f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012307109s
Aug 30 16:50:12.941: INFO: Pod "pod-secrets-fe951ddf-90c7-4a4f-aa9c-b0b0207bd45f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017855898s
STEP: Saw pod success
Aug 30 16:50:12.941: INFO: Pod "pod-secrets-fe951ddf-90c7-4a4f-aa9c-b0b0207bd45f" satisfied condition "success or failure"
Aug 30 16:50:12.946: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod pod-secrets-fe951ddf-90c7-4a4f-aa9c-b0b0207bd45f container secret-env-test: <nil>
STEP: delete the pod
Aug 30 16:50:13.012: INFO: Waiting for pod pod-secrets-fe951ddf-90c7-4a4f-aa9c-b0b0207bd45f to disappear
Aug 30 16:50:13.017: INFO: Pod pod-secrets-fe951ddf-90c7-4a4f-aa9c-b0b0207bd45f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:50:13.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2846" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":54,"skipped":981,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:50:13.033: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Aug 30 16:50:13.203: INFO: Created pod &Pod{ObjectMeta:{dns-7306  dns-7306 /api/v1/namespaces/dns-7306/pods/dns-7306 aa2c0257-2bcc-427b-903b-35c2c5e6a861 8644 0 2020-08-30 16:50:13 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wtr57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wtr57,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wtr57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Aug 30 16:50:15.215: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7306 PodName:dns-7306 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:50:15.215: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Verifying customized DNS server is configured on pod...
Aug 30 16:50:15.729: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7306 PodName:dns-7306 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:50:15.729: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:50:16.219: INFO: Deleting pod dns-7306...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:50:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7306" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":55,"skipped":997,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:50:16.248: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7014
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Aug 30 16:50:18.438: INFO: Pod pod-hostip-56164592-8487-4f96-8d59-771151151a01 has hostIP: 46.101.135.210
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:50:18.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7014" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":56,"skipped":1012,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:50:18.458: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-2a0b2d9e-7701-4c95-a3b8-80b14889492e in namespace container-probe-120
Aug 30 16:50:20.633: INFO: Started pod liveness-2a0b2d9e-7701-4c95-a3b8-80b14889492e in namespace container-probe-120
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 16:50:20.639: INFO: Initial restart count of pod liveness-2a0b2d9e-7701-4c95-a3b8-80b14889492e is 0
Aug 30 16:50:42.712: INFO: Restart count of pod container-probe-120/liveness-2a0b2d9e-7701-4c95-a3b8-80b14889492e is now 1 (22.073414768s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:50:42.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-120" for this suite.

â€¢ [SLOW TEST:24.300 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":57,"skipped":1034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:50:42.759: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-3260
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3260
STEP: Deleting pre-stop pod
Aug 30 16:50:52.073: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:50:52.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3260" for this suite.

â€¢ [SLOW TEST:9.340 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":58,"skipped":1059,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:50:52.099: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6093
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6093.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6093.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6093.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6093.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6093.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6093.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 16:51:05.036: INFO: DNS probes using dns-6093/dns-test-841d7211-0b87-45a8-9d4c-344d51dbecd2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:51:05.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6093" for this suite.

â€¢ [SLOW TEST:12.973 seconds]
[sig-network] DNS
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":59,"skipped":1060,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:51:05.072: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-a7c7870a-1c5d-41cd-815e-4f9b603f1e73
Aug 30 16:51:05.247: INFO: Pod name my-hostname-basic-a7c7870a-1c5d-41cd-815e-4f9b603f1e73: Found 0 pods out of 1
Aug 30 16:51:10.254: INFO: Pod name my-hostname-basic-a7c7870a-1c5d-41cd-815e-4f9b603f1e73: Found 1 pods out of 1
Aug 30 16:51:10.254: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a7c7870a-1c5d-41cd-815e-4f9b603f1e73" are running
Aug 30 16:51:10.260: INFO: Pod "my-hostname-basic-a7c7870a-1c5d-41cd-815e-4f9b603f1e73-2b4c6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-30 16:51:05 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-30 16:51:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-30 16:51:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-30 16:51:05 +0000 UTC Reason: Message:}])
Aug 30 16:51:10.260: INFO: Trying to dial the pod
Aug 30 16:51:15.405: INFO: Controller my-hostname-basic-a7c7870a-1c5d-41cd-815e-4f9b603f1e73: Got expected result from replica 1 [my-hostname-basic-a7c7870a-1c5d-41cd-815e-4f9b603f1e73-2b4c6]: "my-hostname-basic-a7c7870a-1c5d-41cd-815e-4f9b603f1e73-2b4c6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:51:15.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9123" for this suite.

â€¢ [SLOW TEST:10.347 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":60,"skipped":1071,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:51:15.420: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6837
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6837
STEP: creating replication controller externalsvc in namespace services-6837
I0830 16:51:15.615884      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-6837, replica count: 2
I0830 16:51:18.668516      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 30 16:51:18.700: INFO: Creating new exec pod
Aug 30 16:51:20.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-6837 execpodnnq5g -- /bin/sh -x -c nslookup nodeport-service'
Aug 30 16:51:21.260: INFO: stderr: "+ nslookup nodeport-service\n"
Aug 30 16:51:21.260: INFO: stdout: "Server:\t\t10.240.16.10\nAddress:\t10.240.16.10#53\n\nnodeport-service.services-6837.svc.cluster.local\tcanonical name = externalsvc.services-6837.svc.cluster.local.\nName:\texternalsvc.services-6837.svc.cluster.local\nAddress: 10.240.30.4\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6837, will wait for the garbage collector to delete the pods
Aug 30 16:51:21.326: INFO: Deleting ReplicationController externalsvc took: 10.301369ms
Aug 30 16:51:21.826: INFO: Terminating ReplicationController externalsvc pods took: 500.159597ms
Aug 30 16:51:34.348: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:51:34.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6837" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:18.966 seconds]
[sig-network] Services
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":61,"skipped":1075,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:51:34.386: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 16:51:34.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6fff232-6368-4e2f-89eb-fb26c6670adf" in namespace "projected-1005" to be "success or failure"
Aug 30 16:51:34.572: INFO: Pod "downwardapi-volume-e6fff232-6368-4e2f-89eb-fb26c6670adf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.228356ms
Aug 30 16:51:36.577: INFO: Pod "downwardapi-volume-e6fff232-6368-4e2f-89eb-fb26c6670adf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014290644s
STEP: Saw pod success
Aug 30 16:51:36.577: INFO: Pod "downwardapi-volume-e6fff232-6368-4e2f-89eb-fb26c6670adf" satisfied condition "success or failure"
Aug 30 16:51:36.581: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-e6fff232-6368-4e2f-89eb-fb26c6670adf container client-container: <nil>
STEP: delete the pod
Aug 30 16:51:36.651: INFO: Waiting for pod downwardapi-volume-e6fff232-6368-4e2f-89eb-fb26c6670adf to disappear
Aug 30 16:51:36.657: INFO: Pod downwardapi-volume-e6fff232-6368-4e2f-89eb-fb26c6670adf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:51:36.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1005" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":62,"skipped":1087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:51:36.673: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8075
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-8075
Aug 30 16:51:36.873: INFO: Found 0 stateful pods, waiting for 1
Aug 30 16:51:46.880: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 30 16:51:46.909: INFO: Deleting all statefulset in ns statefulset-8075
Aug 30 16:51:46.914: INFO: Scaling statefulset ss to 0
Aug 30 16:51:56.980: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 16:51:56.986: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:51:57.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8075" for this suite.

â€¢ [SLOW TEST:20.345 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":63,"skipped":1125,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:51:57.018: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-5565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 30 16:52:03.232: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5565 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:52:03.232: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:52:03.750: INFO: Exec stderr: ""
Aug 30 16:52:03.750: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5565 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:52:03.750: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:52:04.279: INFO: Exec stderr: ""
Aug 30 16:52:04.279: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5565 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:52:04.280: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:52:04.782: INFO: Exec stderr: ""
Aug 30 16:52:04.782: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5565 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:52:04.782: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:52:05.319: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 30 16:52:05.319: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5565 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:52:05.319: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:52:05.848: INFO: Exec stderr: ""
Aug 30 16:52:05.848: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5565 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:52:05.848: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:52:06.428: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 30 16:52:06.428: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5565 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:52:06.428: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:52:06.983: INFO: Exec stderr: ""
Aug 30 16:52:06.983: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5565 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:52:06.983: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:52:07.499: INFO: Exec stderr: ""
Aug 30 16:52:07.499: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5565 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:52:07.499: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:52:08.003: INFO: Exec stderr: ""
Aug 30 16:52:08.003: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5565 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:52:08.003: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:52:08.513: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:52:08.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5565" for this suite.

â€¢ [SLOW TEST:11.509 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":64,"skipped":1134,"failed":0}
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:52:08.528: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Aug 30 16:52:08.703: INFO: Waiting up to 5m0s for pod "client-containers-b9dbd7eb-84bd-4cba-91e7-954a85c5c62e" in namespace "containers-6565" to be "success or failure"
Aug 30 16:52:08.710: INFO: Pod "client-containers-b9dbd7eb-84bd-4cba-91e7-954a85c5c62e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.243744ms
Aug 30 16:52:10.716: INFO: Pod "client-containers-b9dbd7eb-84bd-4cba-91e7-954a85c5c62e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012844861s
STEP: Saw pod success
Aug 30 16:52:10.721: INFO: Pod "client-containers-b9dbd7eb-84bd-4cba-91e7-954a85c5c62e" satisfied condition "success or failure"
Aug 30 16:52:10.729: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod client-containers-b9dbd7eb-84bd-4cba-91e7-954a85c5c62e container test-container: <nil>
STEP: delete the pod
Aug 30 16:52:10.801: INFO: Waiting for pod client-containers-b9dbd7eb-84bd-4cba-91e7-954a85c5c62e to disappear
Aug 30 16:52:10.806: INFO: Pod client-containers-b9dbd7eb-84bd-4cba-91e7-954a85c5c62e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:52:10.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6565" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":65,"skipped":1134,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:52:10.818: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:52:13.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-814" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":66,"skipped":1141,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:52:13.044: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Aug 30 16:52:13.207: INFO: Waiting up to 5m0s for pod "downward-api-2e855c8a-2429-4f62-a059-9de181b518ca" in namespace "downward-api-8421" to be "success or failure"
Aug 30 16:52:13.211: INFO: Pod "downward-api-2e855c8a-2429-4f62-a059-9de181b518ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.9841ms
Aug 30 16:52:15.219: INFO: Pod "downward-api-2e855c8a-2429-4f62-a059-9de181b518ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011110875s
STEP: Saw pod success
Aug 30 16:52:15.219: INFO: Pod "downward-api-2e855c8a-2429-4f62-a059-9de181b518ca" satisfied condition "success or failure"
Aug 30 16:52:15.224: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod downward-api-2e855c8a-2429-4f62-a059-9de181b518ca container dapi-container: <nil>
STEP: delete the pod
Aug 30 16:52:15.293: INFO: Waiting for pod downward-api-2e855c8a-2429-4f62-a059-9de181b518ca to disappear
Aug 30 16:52:15.297: INFO: Pod downward-api-2e855c8a-2429-4f62-a059-9de181b518ca no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:52:15.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8421" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":67,"skipped":1171,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:52:15.312: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:52:15.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5489" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":68,"skipped":1192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:52:15.513: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Aug 30 16:52:18.225: INFO: Successfully updated pod "labelsupdate57159e48-ae0c-4f9e-bfda-e8de55c6ce6e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:52:22.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7724" for this suite.

â€¢ [SLOW TEST:6.779 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":69,"skipped":1215,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:52:22.293: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 30 16:52:22.920: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 30 16:52:24.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403142, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403142, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403142, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403142, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:52:27.958: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:52:27.964: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:52:29.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3986" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

â€¢ [SLOW TEST:7.365 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":70,"skipped":1230,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:52:29.658: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1356
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:52:29.816: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 30 16:52:32.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-1356 create -f -'
Aug 30 16:52:33.059: INFO: stderr: ""
Aug 30 16:52:33.059: INFO: stdout: "e2e-test-crd-publish-openapi-5414-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 30 16:52:33.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-1356 delete e2e-test-crd-publish-openapi-5414-crds test-cr'
Aug 30 16:52:33.136: INFO: stderr: ""
Aug 30 16:52:33.136: INFO: stdout: "e2e-test-crd-publish-openapi-5414-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 30 16:52:33.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-1356 apply -f -'
Aug 30 16:52:33.453: INFO: stderr: ""
Aug 30 16:52:33.453: INFO: stdout: "e2e-test-crd-publish-openapi-5414-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 30 16:52:33.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-1356 delete e2e-test-crd-publish-openapi-5414-crds test-cr'
Aug 30 16:52:33.528: INFO: stderr: ""
Aug 30 16:52:33.528: INFO: stdout: "e2e-test-crd-publish-openapi-5414-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 30 16:52:33.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 explain e2e-test-crd-publish-openapi-5414-crds'
Aug 30 16:52:33.664: INFO: stderr: ""
Aug 30 16:52:33.664: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5414-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:52:36.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1356" for this suite.

â€¢ [SLOW TEST:7.355 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":71,"skipped":1234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:52:37.013: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 30 16:52:37.224: INFO: Number of nodes with available pods: 0
Aug 30 16:52:37.224: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:38.235: INFO: Number of nodes with available pods: 0
Aug 30 16:52:38.235: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:39.234: INFO: Number of nodes with available pods: 1
Aug 30 16:52:39.234: INFO: Node adoring-wozniak-54dcfd79fc-948mf is running more than one daemon pod
Aug 30 16:52:40.236: INFO: Number of nodes with available pods: 2
Aug 30 16:52:40.236: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 30 16:52:40.267: INFO: Number of nodes with available pods: 1
Aug 30 16:52:40.267: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:41.278: INFO: Number of nodes with available pods: 1
Aug 30 16:52:41.278: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:42.276: INFO: Number of nodes with available pods: 1
Aug 30 16:52:42.276: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:43.277: INFO: Number of nodes with available pods: 1
Aug 30 16:52:43.277: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:44.278: INFO: Number of nodes with available pods: 1
Aug 30 16:52:44.279: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:45.279: INFO: Number of nodes with available pods: 1
Aug 30 16:52:45.279: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:46.277: INFO: Number of nodes with available pods: 1
Aug 30 16:52:46.277: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:47.280: INFO: Number of nodes with available pods: 1
Aug 30 16:52:47.280: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:48.278: INFO: Number of nodes with available pods: 1
Aug 30 16:52:48.278: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:49.279: INFO: Number of nodes with available pods: 1
Aug 30 16:52:49.279: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:50.279: INFO: Number of nodes with available pods: 1
Aug 30 16:52:50.279: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:51.278: INFO: Number of nodes with available pods: 1
Aug 30 16:52:51.278: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 16:52:52.279: INFO: Number of nodes with available pods: 2
Aug 30 16:52:52.279: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4831, will wait for the garbage collector to delete the pods
Aug 30 16:52:52.359: INFO: Deleting DaemonSet.extensions daemon-set took: 16.777252ms
Aug 30 16:52:52.859: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.225636ms
Aug 30 16:52:59.967: INFO: Number of nodes with available pods: 0
Aug 30 16:52:59.967: INFO: Number of running nodes: 0, number of available pods: 0
Aug 30 16:52:59.971: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4831/daemonsets","resourceVersion":"10136"},"items":null}

Aug 30 16:52:59.978: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4831/pods","resourceVersion":"10136"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:53:00.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4831" for this suite.

â€¢ [SLOW TEST:23.000 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":72,"skipped":1288,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:53:00.013: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-41312b37-d091-43bc-b555-b93585e2603f
STEP: Creating a pod to test consume configMaps
Aug 30 16:53:00.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-b4b12971-c9a1-4ee2-b6d5-ce8637435789" in namespace "configmap-3569" to be "success or failure"
Aug 30 16:53:00.212: INFO: Pod "pod-configmaps-b4b12971-c9a1-4ee2-b6d5-ce8637435789": Phase="Pending", Reason="", readiness=false. Elapsed: 10.465585ms
Aug 30 16:53:02.218: INFO: Pod "pod-configmaps-b4b12971-c9a1-4ee2-b6d5-ce8637435789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016764774s
STEP: Saw pod success
Aug 30 16:53:02.218: INFO: Pod "pod-configmaps-b4b12971-c9a1-4ee2-b6d5-ce8637435789" satisfied condition "success or failure"
Aug 30 16:53:02.222: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-configmaps-b4b12971-c9a1-4ee2-b6d5-ce8637435789 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 16:53:02.291: INFO: Waiting for pod pod-configmaps-b4b12971-c9a1-4ee2-b6d5-ce8637435789 to disappear
Aug 30 16:53:02.294: INFO: Pod pod-configmaps-b4b12971-c9a1-4ee2-b6d5-ce8637435789 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:53:02.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3569" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":73,"skipped":1291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:53:02.309: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7933
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 30 16:53:02.475: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:53:05.323: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:53:16.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7933" for this suite.

â€¢ [SLOW TEST:14.217 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":74,"skipped":1322,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:53:16.526: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8301
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-vhzl
STEP: Creating a pod to test atomic-volume-subpath
Aug 30 16:53:16.711: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vhzl" in namespace "subpath-8301" to be "success or failure"
Aug 30 16:53:16.717: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201288ms
Aug 30 16:53:18.722: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010600931s
Aug 30 16:53:20.728: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Running", Reason="", readiness=true. Elapsed: 4.016827865s
Aug 30 16:53:22.734: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Running", Reason="", readiness=true. Elapsed: 6.02238565s
Aug 30 16:53:24.741: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Running", Reason="", readiness=true. Elapsed: 8.029293134s
Aug 30 16:53:26.747: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Running", Reason="", readiness=true. Elapsed: 10.035929937s
Aug 30 16:53:28.753: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Running", Reason="", readiness=true. Elapsed: 12.041904235s
Aug 30 16:53:30.759: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Running", Reason="", readiness=true. Elapsed: 14.047499912s
Aug 30 16:53:32.765: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Running", Reason="", readiness=true. Elapsed: 16.053675131s
Aug 30 16:53:34.771: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Running", Reason="", readiness=true. Elapsed: 18.059842839s
Aug 30 16:53:36.776: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Running", Reason="", readiness=true. Elapsed: 20.06495855s
Aug 30 16:53:38.787: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Running", Reason="", readiness=true. Elapsed: 22.075882696s
Aug 30 16:53:40.794: INFO: Pod "pod-subpath-test-downwardapi-vhzl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.083037824s
STEP: Saw pod success
Aug 30 16:53:40.794: INFO: Pod "pod-subpath-test-downwardapi-vhzl" satisfied condition "success or failure"
Aug 30 16:53:40.799: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-subpath-test-downwardapi-vhzl container test-container-subpath-downwardapi-vhzl: <nil>
STEP: delete the pod
Aug 30 16:53:40.870: INFO: Waiting for pod pod-subpath-test-downwardapi-vhzl to disappear
Aug 30 16:53:40.876: INFO: Pod pod-subpath-test-downwardapi-vhzl no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vhzl
Aug 30 16:53:40.876: INFO: Deleting pod "pod-subpath-test-downwardapi-vhzl" in namespace "subpath-8301"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:53:40.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8301" for this suite.

â€¢ [SLOW TEST:24.378 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":75,"skipped":1340,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:53:40.905: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7904
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 30 16:53:41.089: INFO: Waiting up to 5m0s for pod "pod-3c8c1b16-2c11-4b59-ad67-ffde9ba8a61e" in namespace "emptydir-7904" to be "success or failure"
Aug 30 16:53:41.098: INFO: Pod "pod-3c8c1b16-2c11-4b59-ad67-ffde9ba8a61e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.896854ms
Aug 30 16:53:43.103: INFO: Pod "pod-3c8c1b16-2c11-4b59-ad67-ffde9ba8a61e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014370293s
Aug 30 16:53:45.110: INFO: Pod "pod-3c8c1b16-2c11-4b59-ad67-ffde9ba8a61e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021204517s
STEP: Saw pod success
Aug 30 16:53:45.110: INFO: Pod "pod-3c8c1b16-2c11-4b59-ad67-ffde9ba8a61e" satisfied condition "success or failure"
Aug 30 16:53:45.115: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-3c8c1b16-2c11-4b59-ad67-ffde9ba8a61e container test-container: <nil>
STEP: delete the pod
Aug 30 16:53:45.184: INFO: Waiting for pod pod-3c8c1b16-2c11-4b59-ad67-ffde9ba8a61e to disappear
Aug 30 16:53:45.188: INFO: Pod pod-3c8c1b16-2c11-4b59-ad67-ffde9ba8a61e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:53:45.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7904" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":76,"skipped":1360,"failed":0}
S
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:53:45.207: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 16:53:45.382: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a2a3c968-1e32-48f6-b0ed-6abaa2eb856a" in namespace "security-context-test-6148" to be "success or failure"
Aug 30 16:53:45.389: INFO: Pod "busybox-user-65534-a2a3c968-1e32-48f6-b0ed-6abaa2eb856a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.911547ms
Aug 30 16:53:47.394: INFO: Pod "busybox-user-65534-a2a3c968-1e32-48f6-b0ed-6abaa2eb856a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012328334s
Aug 30 16:53:47.394: INFO: Pod "busybox-user-65534-a2a3c968-1e32-48f6-b0ed-6abaa2eb856a" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:53:47.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6148" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":77,"skipped":1361,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:53:47.406: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7478
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 30 16:53:47.569: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 30 16:54:09.664: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.74:8080/dial?request=hostname&protocol=udp&host=172.25.1.37&port=8081&tries=1'] Namespace:pod-network-test-7478 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:54:09.664: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:54:10.149: INFO: Waiting for responses: map[]
Aug 30 16:54:10.155: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.74:8080/dial?request=hostname&protocol=udp&host=172.25.0.73&port=8081&tries=1'] Namespace:pod-network-test-7478 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:54:10.155: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:54:10.667: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:54:10.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7478" for this suite.

â€¢ [SLOW TEST:23.277 seconds]
[sig-network] Networking
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":78,"skipped":1363,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:54:10.684: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5806
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Aug 30 16:54:10.842: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:54:26.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5806" for this suite.

â€¢ [SLOW TEST:15.890 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":79,"skipped":1363,"failed":0}
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:54:26.574: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-bad71537-b448-4b23-8a08-43f9759a6ea4 in namespace container-probe-7936
Aug 30 16:54:28.763: INFO: Started pod liveness-bad71537-b448-4b23-8a08-43f9759a6ea4 in namespace container-probe-7936
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 16:54:28.768: INFO: Initial restart count of pod liveness-bad71537-b448-4b23-8a08-43f9759a6ea4 is 0
Aug 30 16:54:38.809: INFO: Restart count of pod container-probe-7936/liveness-bad71537-b448-4b23-8a08-43f9759a6ea4 is now 1 (10.041506124s elapsed)
Aug 30 16:55:00.874: INFO: Restart count of pod container-probe-7936/liveness-bad71537-b448-4b23-8a08-43f9759a6ea4 is now 2 (32.105789395s elapsed)
Aug 30 16:55:20.938: INFO: Restart count of pod container-probe-7936/liveness-bad71537-b448-4b23-8a08-43f9759a6ea4 is now 3 (52.170517379s elapsed)
Aug 30 16:55:40.998: INFO: Restart count of pod container-probe-7936/liveness-bad71537-b448-4b23-8a08-43f9759a6ea4 is now 4 (1m12.229727837s elapsed)
Aug 30 16:56:39.186: INFO: Restart count of pod container-probe-7936/liveness-bad71537-b448-4b23-8a08-43f9759a6ea4 is now 5 (2m10.417823754s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:56:39.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7936" for this suite.

â€¢ [SLOW TEST:132.640 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":80,"skipped":1363,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:56:39.214: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3522
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Aug 30 16:56:41.932: INFO: Successfully updated pod "labelsupdate035d0125-60f0-4cf0-b1d0-576719b51a12"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:56:43.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3522" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":81,"skipped":1368,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:56:43.979: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6565
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 16:56:44.143: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87f79ca0-3d98-4a27-b6b5-47bb123e0866" in namespace "downward-api-6565" to be "success or failure"
Aug 30 16:56:44.149: INFO: Pod "downwardapi-volume-87f79ca0-3d98-4a27-b6b5-47bb123e0866": Phase="Pending", Reason="", readiness=false. Elapsed: 5.561741ms
Aug 30 16:56:46.155: INFO: Pod "downwardapi-volume-87f79ca0-3d98-4a27-b6b5-47bb123e0866": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01125812s
Aug 30 16:56:48.161: INFO: Pod "downwardapi-volume-87f79ca0-3d98-4a27-b6b5-47bb123e0866": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017792728s
STEP: Saw pod success
Aug 30 16:56:48.161: INFO: Pod "downwardapi-volume-87f79ca0-3d98-4a27-b6b5-47bb123e0866" satisfied condition "success or failure"
Aug 30 16:56:48.166: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-87f79ca0-3d98-4a27-b6b5-47bb123e0866 container client-container: <nil>
STEP: delete the pod
Aug 30 16:56:48.232: INFO: Waiting for pod downwardapi-volume-87f79ca0-3d98-4a27-b6b5-47bb123e0866 to disappear
Aug 30 16:56:48.236: INFO: Pod downwardapi-volume-87f79ca0-3d98-4a27-b6b5-47bb123e0866 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:56:48.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6565" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":82,"skipped":1371,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:56:48.250: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-ec4477c7-364b-4b83-b066-71a96b830c06
STEP: Creating a pod to test consume configMaps
Aug 30 16:56:48.424: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-23666c77-0c71-4837-9d1b-abccaf248e7c" in namespace "projected-8356" to be "success or failure"
Aug 30 16:56:48.429: INFO: Pod "pod-projected-configmaps-23666c77-0c71-4837-9d1b-abccaf248e7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.91345ms
Aug 30 16:56:50.434: INFO: Pod "pod-projected-configmaps-23666c77-0c71-4837-9d1b-abccaf248e7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010011039s
Aug 30 16:56:52.440: INFO: Pod "pod-projected-configmaps-23666c77-0c71-4837-9d1b-abccaf248e7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015977144s
STEP: Saw pod success
Aug 30 16:56:52.440: INFO: Pod "pod-projected-configmaps-23666c77-0c71-4837-9d1b-abccaf248e7c" satisfied condition "success or failure"
Aug 30 16:56:52.445: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-projected-configmaps-23666c77-0c71-4837-9d1b-abccaf248e7c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 16:56:52.474: INFO: Waiting for pod pod-projected-configmaps-23666c77-0c71-4837-9d1b-abccaf248e7c to disappear
Aug 30 16:56:52.478: INFO: Pod pod-projected-configmaps-23666c77-0c71-4837-9d1b-abccaf248e7c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:56:52.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8356" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":83,"skipped":1371,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:56:52.493: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-213
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 30 16:56:54.690: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-213 PodName:pod-sharedvolume-29076d9e-a2fa-414f-8d5e-a8cd1624f8eb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 16:56:54.690: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 16:56:55.192: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:56:55.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-213" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":84,"skipped":1391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:56:55.210: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9437
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9437.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9437.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9437.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9437.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9437.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9437.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9437.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 26.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.26_udp@PTR;check="$$(dig +tcp +noall +answer +search 26.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.26_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9437.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9437.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9437.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9437.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9437.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9437.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9437.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 26.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.26_udp@PTR;check="$$(dig +tcp +noall +answer +search 26.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.26_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 16:56:59.577: INFO: Unable to read wheezy_udp@dns-test-service.dns-9437.svc.cluster.local from pod dns-9437/dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3: the server could not find the requested resource (get pods dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3)
Aug 30 16:56:59.623: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9437.svc.cluster.local from pod dns-9437/dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3: the server could not find the requested resource (get pods dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3)
Aug 30 16:56:59.632: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local from pod dns-9437/dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3: the server could not find the requested resource (get pods dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3)
Aug 30 16:56:59.641: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local from pod dns-9437/dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3: the server could not find the requested resource (get pods dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3)
Aug 30 16:57:00.170: INFO: Unable to read jessie_udp@dns-test-service.dns-9437.svc.cluster.local from pod dns-9437/dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3: the server could not find the requested resource (get pods dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3)
Aug 30 16:57:00.179: INFO: Unable to read jessie_tcp@dns-test-service.dns-9437.svc.cluster.local from pod dns-9437/dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3: the server could not find the requested resource (get pods dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3)
Aug 30 16:57:00.188: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local from pod dns-9437/dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3: the server could not find the requested resource (get pods dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3)
Aug 30 16:57:00.197: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local from pod dns-9437/dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3: the server could not find the requested resource (get pods dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3)
Aug 30 16:57:00.630: INFO: Lookups using dns-9437/dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3 failed for: [wheezy_udp@dns-test-service.dns-9437.svc.cluster.local wheezy_tcp@dns-test-service.dns-9437.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local jessie_udp@dns-test-service.dns-9437.svc.cluster.local jessie_tcp@dns-test-service.dns-9437.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9437.svc.cluster.local]

Aug 30 16:57:07.214: INFO: DNS probes using dns-9437/dns-test-2d85c08f-4123-4145-baff-a2f52667e8e3 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:57:07.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9437" for this suite.

â€¢ [SLOW TEST:12.099 seconds]
[sig-network] DNS
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":85,"skipped":1421,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:57:07.310: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8848
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 30 16:57:07.477: INFO: Waiting up to 5m0s for pod "pod-cbfd0dca-9d4f-4890-8f12-4ca438b14997" in namespace "emptydir-8848" to be "success or failure"
Aug 30 16:57:07.485: INFO: Pod "pod-cbfd0dca-9d4f-4890-8f12-4ca438b14997": Phase="Pending", Reason="", readiness=false. Elapsed: 7.375665ms
Aug 30 16:57:09.491: INFO: Pod "pod-cbfd0dca-9d4f-4890-8f12-4ca438b14997": Phase="Running", Reason="", readiness=true. Elapsed: 2.013221069s
Aug 30 16:57:11.496: INFO: Pod "pod-cbfd0dca-9d4f-4890-8f12-4ca438b14997": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019149205s
STEP: Saw pod success
Aug 30 16:57:11.496: INFO: Pod "pod-cbfd0dca-9d4f-4890-8f12-4ca438b14997" satisfied condition "success or failure"
Aug 30 16:57:11.501: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-cbfd0dca-9d4f-4890-8f12-4ca438b14997 container test-container: <nil>
STEP: delete the pod
Aug 30 16:57:11.571: INFO: Waiting for pod pod-cbfd0dca-9d4f-4890-8f12-4ca438b14997 to disappear
Aug 30 16:57:11.575: INFO: Pod pod-cbfd0dca-9d4f-4890-8f12-4ca438b14997 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:57:11.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8848" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":86,"skipped":1428,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:57:11.590: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1525
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 30 16:57:11.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2592'
Aug 30 16:57:11.825: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 30 16:57:11.825: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Aug 30 16:57:11.839: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-k2dhv]
Aug 30 16:57:11.839: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-k2dhv" in namespace "kubectl-2592" to be "running and ready"
Aug 30 16:57:11.843: INFO: Pod "e2e-test-httpd-rc-k2dhv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.772084ms
Aug 30 16:57:13.848: INFO: Pod "e2e-test-httpd-rc-k2dhv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008437497s
Aug 30 16:57:15.853: INFO: Pod "e2e-test-httpd-rc-k2dhv": Phase="Running", Reason="", readiness=true. Elapsed: 4.014136885s
Aug 30 16:57:15.853: INFO: Pod "e2e-test-httpd-rc-k2dhv" satisfied condition "running and ready"
Aug 30 16:57:15.853: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-k2dhv]
Aug 30 16:57:15.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 logs rc/e2e-test-httpd-rc --namespace=kubectl-2592'
Aug 30 16:57:16.076: INFO: stderr: ""
Aug 30 16:57:16.076: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.25.0.81. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.25.0.81. Set the 'ServerName' directive globally to suppress this message\n[Sun Aug 30 16:57:13.068258 2020] [mpm_event:notice] [pid 1:tid 139877614336872] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Sun Aug 30 16:57:13.068313 2020] [core:notice] [pid 1:tid 139877614336872] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1530
Aug 30 16:57:16.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete rc e2e-test-httpd-rc --namespace=kubectl-2592'
Aug 30 16:57:16.147: INFO: stderr: ""
Aug 30 16:57:16.147: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:57:16.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2592" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":87,"skipped":1436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:57:16.163: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Aug 30 16:57:16.331: INFO: Waiting up to 5m0s for pod "downward-api-cf7afb18-fe74-41b0-ae66-fff628d79318" in namespace "downward-api-6562" to be "success or failure"
Aug 30 16:57:16.337: INFO: Pod "downward-api-cf7afb18-fe74-41b0-ae66-fff628d79318": Phase="Pending", Reason="", readiness=false. Elapsed: 5.778091ms
Aug 30 16:57:18.342: INFO: Pod "downward-api-cf7afb18-fe74-41b0-ae66-fff628d79318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01134933s
STEP: Saw pod success
Aug 30 16:57:18.342: INFO: Pod "downward-api-cf7afb18-fe74-41b0-ae66-fff628d79318" satisfied condition "success or failure"
Aug 30 16:57:18.347: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod downward-api-cf7afb18-fe74-41b0-ae66-fff628d79318 container dapi-container: <nil>
STEP: delete the pod
Aug 30 16:57:18.419: INFO: Waiting for pod downward-api-cf7afb18-fe74-41b0-ae66-fff628d79318 to disappear
Aug 30 16:57:18.422: INFO: Pod downward-api-cf7afb18-fe74-41b0-ae66-fff628d79318 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:57:18.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6562" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:57:18.434: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-998083cf-6c9e-4b31-9220-2cdb3e446ff9
STEP: Creating a pod to test consume secrets
Aug 30 16:57:18.608: INFO: Waiting up to 5m0s for pod "pod-secrets-768e7354-39df-4083-b1ee-dde859a90c9b" in namespace "secrets-7144" to be "success or failure"
Aug 30 16:57:18.616: INFO: Pod "pod-secrets-768e7354-39df-4083-b1ee-dde859a90c9b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.644868ms
Aug 30 16:57:20.621: INFO: Pod "pod-secrets-768e7354-39df-4083-b1ee-dde859a90c9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012926395s
STEP: Saw pod success
Aug 30 16:57:20.621: INFO: Pod "pod-secrets-768e7354-39df-4083-b1ee-dde859a90c9b" satisfied condition "success or failure"
Aug 30 16:57:20.626: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-secrets-768e7354-39df-4083-b1ee-dde859a90c9b container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 16:57:20.729: INFO: Waiting for pod pod-secrets-768e7354-39df-4083-b1ee-dde859a90c9b to disappear
Aug 30 16:57:20.733: INFO: Pod pod-secrets-768e7354-39df-4083-b1ee-dde859a90c9b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:57:20.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7144" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":89,"skipped":1496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:57:20.749: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 30 16:57:28.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 30 16:57:28.995: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 30 16:57:30.995: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 30 16:57:31.003: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 30 16:57:32.995: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 30 16:57:33.000: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:57:33.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6365" for this suite.

â€¢ [SLOW TEST:12.267 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":90,"skipped":1575,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:57:33.016: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2060
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-c39561a7-f559-4b15-bd96-5be1619c61cb
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:57:33.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2060" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":91,"skipped":1590,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:57:33.202: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 16:57:33.380: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0cffc00-944b-4fd1-adb7-1d545e235695" in namespace "projected-803" to be "success or failure"
Aug 30 16:57:33.386: INFO: Pod "downwardapi-volume-b0cffc00-944b-4fd1-adb7-1d545e235695": Phase="Pending", Reason="", readiness=false. Elapsed: 5.086596ms
Aug 30 16:57:35.392: INFO: Pod "downwardapi-volume-b0cffc00-944b-4fd1-adb7-1d545e235695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011795847s
STEP: Saw pod success
Aug 30 16:57:35.392: INFO: Pod "downwardapi-volume-b0cffc00-944b-4fd1-adb7-1d545e235695" satisfied condition "success or failure"
Aug 30 16:57:35.397: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-b0cffc00-944b-4fd1-adb7-1d545e235695 container client-container: <nil>
STEP: delete the pod
Aug 30 16:57:35.466: INFO: Waiting for pod downwardapi-volume-b0cffc00-944b-4fd1-adb7-1d545e235695 to disappear
Aug 30 16:57:35.471: INFO: Pod downwardapi-volume-b0cffc00-944b-4fd1-adb7-1d545e235695 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:57:35.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-803" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":92,"skipped":1599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:57:35.490: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:57:35.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7234" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":93,"skipped":1625,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:57:35.671: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-6493
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6493
STEP: Creating statefulset with conflicting port in namespace statefulset-6493
STEP: Waiting until pod test-pod will start running in namespace statefulset-6493
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6493
Aug 30 16:57:39.926: INFO: Observed stateful pod in namespace: statefulset-6493, name: ss-0, uid: 461803bf-b314-4878-a95c-22533646ca30, status phase: Pending. Waiting for statefulset controller to delete.
Aug 30 16:57:40.115: INFO: Observed stateful pod in namespace: statefulset-6493, name: ss-0, uid: 461803bf-b314-4878-a95c-22533646ca30, status phase: Failed. Waiting for statefulset controller to delete.
Aug 30 16:57:40.125: INFO: Observed stateful pod in namespace: statefulset-6493, name: ss-0, uid: 461803bf-b314-4878-a95c-22533646ca30, status phase: Failed. Waiting for statefulset controller to delete.
Aug 30 16:57:40.132: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6493
STEP: Removing pod with conflicting port in namespace statefulset-6493
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6493 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 30 16:57:46.178: INFO: Deleting all statefulset in ns statefulset-6493
Aug 30 16:57:46.184: INFO: Scaling statefulset ss to 0
Aug 30 16:57:56.207: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 16:57:56.212: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:57:56.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6493" for this suite.

â€¢ [SLOW TEST:20.577 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":94,"skipped":1638,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:57:56.248: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2805
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-be64058a-324e-4ec6-8ffd-08034b855bd4
STEP: Creating a pod to test consume configMaps
Aug 30 16:57:56.429: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ba3572b-870a-4619-82d1-a08a8c8a845e" in namespace "configmap-2805" to be "success or failure"
Aug 30 16:57:56.434: INFO: Pod "pod-configmaps-6ba3572b-870a-4619-82d1-a08a8c8a845e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.434899ms
Aug 30 16:57:58.440: INFO: Pod "pod-configmaps-6ba3572b-870a-4619-82d1-a08a8c8a845e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010422495s
Aug 30 16:58:00.447: INFO: Pod "pod-configmaps-6ba3572b-870a-4619-82d1-a08a8c8a845e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017794371s
STEP: Saw pod success
Aug 30 16:58:00.447: INFO: Pod "pod-configmaps-6ba3572b-870a-4619-82d1-a08a8c8a845e" satisfied condition "success or failure"
Aug 30 16:58:00.452: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-configmaps-6ba3572b-870a-4619-82d1-a08a8c8a845e container configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 16:58:00.521: INFO: Waiting for pod pod-configmaps-6ba3572b-870a-4619-82d1-a08a8c8a845e to disappear
Aug 30 16:58:00.526: INFO: Pod pod-configmaps-6ba3572b-870a-4619-82d1-a08a8c8a845e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:58:00.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2805" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":95,"skipped":1644,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:58:00.547: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Aug 30 16:58:00.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-2382'
Aug 30 16:58:00.963: INFO: stderr: ""
Aug 30 16:58:00.963: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 30 16:58:00.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2382'
Aug 30 16:58:01.038: INFO: stderr: ""
Aug 30 16:58:01.038: INFO: stdout: "update-demo-nautilus-sdxbj update-demo-nautilus-vshvf "
Aug 30 16:58:01.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-sdxbj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2382'
Aug 30 16:58:01.099: INFO: stderr: ""
Aug 30 16:58:01.099: INFO: stdout: ""
Aug 30 16:58:01.099: INFO: update-demo-nautilus-sdxbj is created but not running
Aug 30 16:58:06.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2382'
Aug 30 16:58:06.166: INFO: stderr: ""
Aug 30 16:58:06.166: INFO: stdout: "update-demo-nautilus-sdxbj update-demo-nautilus-vshvf "
Aug 30 16:58:06.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-sdxbj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2382'
Aug 30 16:58:06.231: INFO: stderr: ""
Aug 30 16:58:06.231: INFO: stdout: "true"
Aug 30 16:58:06.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-sdxbj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2382'
Aug 30 16:58:06.293: INFO: stderr: ""
Aug 30 16:58:06.293: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 30 16:58:06.293: INFO: validating pod update-demo-nautilus-sdxbj
Aug 30 16:58:06.429: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 16:58:06.429: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 16:58:06.429: INFO: update-demo-nautilus-sdxbj is verified up and running
Aug 30 16:58:06.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-vshvf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2382'
Aug 30 16:58:06.498: INFO: stderr: ""
Aug 30 16:58:06.498: INFO: stdout: "true"
Aug 30 16:58:06.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-vshvf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2382'
Aug 30 16:58:06.563: INFO: stderr: ""
Aug 30 16:58:06.563: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 30 16:58:06.563: INFO: validating pod update-demo-nautilus-vshvf
Aug 30 16:58:06.669: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 16:58:06.669: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 16:58:06.669: INFO: update-demo-nautilus-vshvf is verified up and running
STEP: rolling-update to new replication controller
Aug 30 16:58:06.672: INFO: scanned /root for discovery docs: <nil>
Aug 30 16:58:06.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2382'
Aug 30 16:58:29.259: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 30 16:58:29.259: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 30 16:58:29.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2382'
Aug 30 16:58:29.334: INFO: stderr: ""
Aug 30 16:58:29.334: INFO: stdout: "update-demo-kitten-rq8n6 update-demo-kitten-xkh5s "
Aug 30 16:58:29.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-kitten-rq8n6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2382'
Aug 30 16:58:29.397: INFO: stderr: ""
Aug 30 16:58:29.397: INFO: stdout: "true"
Aug 30 16:58:29.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-kitten-rq8n6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2382'
Aug 30 16:58:29.467: INFO: stderr: ""
Aug 30 16:58:29.467: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 30 16:58:29.467: INFO: validating pod update-demo-kitten-rq8n6
Aug 30 16:58:29.601: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 30 16:58:29.601: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 30 16:58:29.601: INFO: update-demo-kitten-rq8n6 is verified up and running
Aug 30 16:58:29.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-kitten-xkh5s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2382'
Aug 30 16:58:29.671: INFO: stderr: ""
Aug 30 16:58:29.671: INFO: stdout: "true"
Aug 30 16:58:29.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-kitten-xkh5s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2382'
Aug 30 16:58:29.736: INFO: stderr: ""
Aug 30 16:58:29.736: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 30 16:58:29.736: INFO: validating pod update-demo-kitten-xkh5s
Aug 30 16:58:29.870: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 30 16:58:29.870: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 30 16:58:29.870: INFO: update-demo-kitten-xkh5s is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:58:29.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2382" for this suite.

â€¢ [SLOW TEST:29.338 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":96,"skipped":1654,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:58:29.885: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 16:58:30.437: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 16:58:32.460: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403510, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403510, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403510, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403510, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:58:35.480: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:58:35.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4481" for this suite.
STEP: Destroying namespace "webhook-4481-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.109 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":97,"skipped":1655,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:58:35.994: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 16:58:36.163: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f90bb7d-2770-42cf-8f72-773c196a8552" in namespace "downward-api-4964" to be "success or failure"
Aug 30 16:58:36.174: INFO: Pod "downwardapi-volume-7f90bb7d-2770-42cf-8f72-773c196a8552": Phase="Pending", Reason="", readiness=false. Elapsed: 10.474979ms
Aug 30 16:58:38.179: INFO: Pod "downwardapi-volume-7f90bb7d-2770-42cf-8f72-773c196a8552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01574016s
Aug 30 16:58:40.185: INFO: Pod "downwardapi-volume-7f90bb7d-2770-42cf-8f72-773c196a8552": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021856357s
STEP: Saw pod success
Aug 30 16:58:40.185: INFO: Pod "downwardapi-volume-7f90bb7d-2770-42cf-8f72-773c196a8552" satisfied condition "success or failure"
Aug 30 16:58:40.190: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod downwardapi-volume-7f90bb7d-2770-42cf-8f72-773c196a8552 container client-container: <nil>
STEP: delete the pod
Aug 30 16:58:40.260: INFO: Waiting for pod downwardapi-volume-7f90bb7d-2770-42cf-8f72-773c196a8552 to disappear
Aug 30 16:58:40.264: INFO: Pod downwardapi-volume-7f90bb7d-2770-42cf-8f72-773c196a8552 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:58:40.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4964" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":98,"skipped":1682,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:58:40.279: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:58:53.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2994" for this suite.

â€¢ [SLOW TEST:13.276 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":99,"skipped":1690,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:58:53.555: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-a4a0c10c-8d35-4a6b-b19e-ccf905550ae2
STEP: Creating a pod to test consume configMaps
Aug 30 16:58:53.757: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-18c0f743-8008-46e2-94f8-f65116ba1f47" in namespace "projected-1140" to be "success or failure"
Aug 30 16:58:53.764: INFO: Pod "pod-projected-configmaps-18c0f743-8008-46e2-94f8-f65116ba1f47": Phase="Pending", Reason="", readiness=false. Elapsed: 7.525193ms
Aug 30 16:58:55.771: INFO: Pod "pod-projected-configmaps-18c0f743-8008-46e2-94f8-f65116ba1f47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013654398s
STEP: Saw pod success
Aug 30 16:58:55.771: INFO: Pod "pod-projected-configmaps-18c0f743-8008-46e2-94f8-f65116ba1f47" satisfied condition "success or failure"
Aug 30 16:58:55.780: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-projected-configmaps-18c0f743-8008-46e2-94f8-f65116ba1f47 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 16:58:55.855: INFO: Waiting for pod pod-projected-configmaps-18c0f743-8008-46e2-94f8-f65116ba1f47 to disappear
Aug 30 16:58:55.861: INFO: Pod pod-projected-configmaps-18c0f743-8008-46e2-94f8-f65116ba1f47 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:58:55.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1140" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":100,"skipped":1706,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:58:55.879: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1754
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 30 16:58:56.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7394'
Aug 30 16:58:56.132: INFO: stderr: ""
Aug 30 16:58:56.132: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1759
Aug 30 16:58:56.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete pods e2e-test-httpd-pod --namespace=kubectl-7394'
Aug 30 16:59:04.282: INFO: stderr: ""
Aug 30 16:59:04.282: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:59:04.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7394" for this suite.

â€¢ [SLOW TEST:8.417 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1750
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":101,"skipped":1730,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:59:04.296: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4910
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-6476be7b-c3e8-4204-b0fe-d8148695f67e
STEP: Creating configMap with name cm-test-opt-upd-57b8491b-8647-4cf8-9712-00cff247cb95
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6476be7b-c3e8-4204-b0fe-d8148695f67e
STEP: Updating configmap cm-test-opt-upd-57b8491b-8647-4cf8-9712-00cff247cb95
STEP: Creating configMap with name cm-test-opt-create-67c4d135-d245-40ab-9741-6ba51f9e2989
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:59:08.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4910" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":102,"skipped":1748,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:59:09.018: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8456
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-86c10b45-ec9e-49be-aa9f-e6ab2b4ecad8
STEP: Creating secret with name s-test-opt-upd-dc86dfc2-25f2-48a1-a1d6-70d14f5a5dec
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-86c10b45-ec9e-49be-aa9f-e6ab2b4ecad8
STEP: Updating secret s-test-opt-upd-dc86dfc2-25f2-48a1-a1d6-70d14f5a5dec
STEP: Creating secret with name s-test-opt-create-12fa9a05-9d29-44fb-b216-109fbac5641b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:59:13.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8456" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":103,"skipped":1754,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:59:13.654: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 16:59:13.827: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2302d46c-df17-4be0-8acd-63a386e590a7" in namespace "projected-7210" to be "success or failure"
Aug 30 16:59:13.834: INFO: Pod "downwardapi-volume-2302d46c-df17-4be0-8acd-63a386e590a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.862177ms
Aug 30 16:59:15.839: INFO: Pod "downwardapi-volume-2302d46c-df17-4be0-8acd-63a386e590a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012580265s
STEP: Saw pod success
Aug 30 16:59:15.839: INFO: Pod "downwardapi-volume-2302d46c-df17-4be0-8acd-63a386e590a7" satisfied condition "success or failure"
Aug 30 16:59:15.844: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-2302d46c-df17-4be0-8acd-63a386e590a7 container client-container: <nil>
STEP: delete the pod
Aug 30 16:59:15.913: INFO: Waiting for pod downwardapi-volume-2302d46c-df17-4be0-8acd-63a386e590a7 to disappear
Aug 30 16:59:15.918: INFO: Pod downwardapi-volume-2302d46c-df17-4be0-8acd-63a386e590a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 16:59:15.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7210" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":104,"skipped":1756,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 16:59:15.930: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Aug 30 16:59:16.086: INFO: PodSpec: initContainers in spec.initContainers
Aug 30 17:00:02.521: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-14f33a98-b13d-4740-9b21-312839e6a27b", GenerateName:"", Namespace:"init-container-8547", SelfLink:"/api/v1/namespaces/init-container-8547/pods/pod-init-14f33a98-b13d-4740-9b21-312839e6a27b", UID:"297636c5-dd7f-4681-8839-3c901e827a97", ResourceVersion:"13160", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63734403556, loc:(*time.Location)(0x7925260)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"86902557"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.25.0.96/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-94bws", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003978b00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-94bws", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-94bws", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-94bws", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00412ddf8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"adoring-wozniak-54dcfd79fc-948mf", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00326eba0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00412de70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00412de90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00412de98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00412de9c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403556, loc:(*time.Location)(0x7925260)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403556, loc:(*time.Location)(0x7925260)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403556, loc:(*time.Location)(0x7925260)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403556, loc:(*time.Location)(0x7925260)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"46.101.135.210", PodIP:"172.25.0.96", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.0.96"}}, StartTime:(*v1.Time)(0xc002f69ec0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc002f69f00), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000207730)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://a2988fd2e6a5e8c4c5c111c1b1f6e4ebab8d018d17a8f3c5aa103f97f43c4adc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f69f40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f69ee0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00412df1f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:00:02.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8547" for this suite.

â€¢ [SLOW TEST:46.605 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":105,"skipped":1766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:00:02.538: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5776
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug 30 17:00:04.731: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-962893509 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 30 17:00:09.819: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:00:09.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5776" for this suite.

â€¢ [SLOW TEST:7.300 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":106,"skipped":1823,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:00:09.838: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4513
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:00:10.252: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 17:00:12.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403610, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403610, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403610, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403610, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:00:15.301: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
Aug 30 17:00:15.414: INFO: Waiting for webhook configuration to be ready...
Aug 30 17:00:15.537: INFO: Waiting for webhook configuration to be ready...
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:00:28.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4513" for this suite.
STEP: Destroying namespace "webhook-4513-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:18.587 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":107,"skipped":1825,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:00:28.426: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1639
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 30 17:00:28.583: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 30 17:00:39.694: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 17:00:42.545: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:00:54.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1639" for this suite.

â€¢ [SLOW TEST:25.811 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":108,"skipped":1827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:00:54.239: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3572
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:01:05.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3572" for this suite.

â€¢ [SLOW TEST:11.241 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":109,"skipped":1882,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:01:05.480: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:01:06.574: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 30 17:01:08.589: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403666, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403666, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403666, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403666, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:01:11.609: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:01:11.614: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:01:13.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1607" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

â€¢ [SLOW TEST:7.822 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":110,"skipped":1885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:01:13.302: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-57
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
STEP: creating the pod
Aug 30 17:01:13.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-57'
Aug 30 17:01:13.742: INFO: stderr: ""
Aug 30 17:01:13.742: INFO: stdout: "pod/pause created\n"
Aug 30 17:01:13.742: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 30 17:01:13.742: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-57" to be "running and ready"
Aug 30 17:01:13.747: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.3094ms
Aug 30 17:01:15.753: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011011236s
Aug 30 17:01:17.758: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.016095287s
Aug 30 17:01:17.758: INFO: Pod "pause" satisfied condition "running and ready"
Aug 30 17:01:17.758: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 30 17:01:17.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 label pods pause testing-label=testing-label-value --namespace=kubectl-57'
Aug 30 17:01:17.832: INFO: stderr: ""
Aug 30 17:01:17.832: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 30 17:01:17.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pod pause -L testing-label --namespace=kubectl-57'
Aug 30 17:01:17.915: INFO: stderr: ""
Aug 30 17:01:17.915: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 30 17:01:17.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 label pods pause testing-label- --namespace=kubectl-57'
Aug 30 17:01:17.990: INFO: stderr: ""
Aug 30 17:01:17.990: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 30 17:01:17.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pod pause -L testing-label --namespace=kubectl-57'
Aug 30 17:01:18.058: INFO: stderr: ""
Aug 30 17:01:18.058: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1282
STEP: using delete to clean up resources
Aug 30 17:01:18.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete --grace-period=0 --force -f - --namespace=kubectl-57'
Aug 30 17:01:18.135: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 17:01:18.135: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 30 17:01:18.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get rc,svc -l name=pause --no-headers --namespace=kubectl-57'
Aug 30 17:01:18.205: INFO: stderr: "No resources found in kubectl-57 namespace.\n"
Aug 30 17:01:18.205: INFO: stdout: ""
Aug 30 17:01:18.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -l name=pause --namespace=kubectl-57 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 17:01:18.267: INFO: stderr: ""
Aug 30 17:01:18.267: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:01:18.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-57" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":111,"skipped":1919,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:01:18.286: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4851
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-4851
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 30 17:01:18.445: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 30 17:01:40.571: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.44:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4851 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 17:01:40.571: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 17:01:41.062: INFO: Found all expected endpoints: [netserver-0]
Aug 30 17:01:41.068: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.101:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4851 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 17:01:41.068: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 17:01:41.625: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:01:41.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4851" for this suite.

â€¢ [SLOW TEST:23.354 seconds]
[sig-network] Networking
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":112,"skipped":1965,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:01:41.640: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:01:45.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-438" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":113,"skipped":1992,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:01:45.837: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-952c75f3-d0da-43e9-954e-44216fdaa816
STEP: Creating a pod to test consume configMaps
Aug 30 17:01:46.006: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-866744b3-90b6-4cdb-b8d9-46af8d4ec473" in namespace "projected-3218" to be "success or failure"
Aug 30 17:01:46.011: INFO: Pod "pod-projected-configmaps-866744b3-90b6-4cdb-b8d9-46af8d4ec473": Phase="Pending", Reason="", readiness=false. Elapsed: 4.851974ms
Aug 30 17:01:48.017: INFO: Pod "pod-projected-configmaps-866744b3-90b6-4cdb-b8d9-46af8d4ec473": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010980775s
STEP: Saw pod success
Aug 30 17:01:48.018: INFO: Pod "pod-projected-configmaps-866744b3-90b6-4cdb-b8d9-46af8d4ec473" satisfied condition "success or failure"
Aug 30 17:01:48.023: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod pod-projected-configmaps-866744b3-90b6-4cdb-b8d9-46af8d4ec473 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 17:01:48.089: INFO: Waiting for pod pod-projected-configmaps-866744b3-90b6-4cdb-b8d9-46af8d4ec473 to disappear
Aug 30 17:01:48.094: INFO: Pod pod-projected-configmaps-866744b3-90b6-4cdb-b8d9-46af8d4ec473 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:01:48.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3218" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":114,"skipped":1994,"failed":0}

------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:01:48.111: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6797
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 30 17:01:50.313: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:01:50.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6797" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":115,"skipped":1994,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:01:50.344: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-3363
STEP: creating replication controller nodeport-test in namespace services-3363
I0830 17:01:50.529824      23 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-3363, replica count: 2
I0830 17:01:53.580185      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 17:01:53.580: INFO: Creating new exec pod
Aug 30 17:01:58.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-3363 execpod6b9zn -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Aug 30 17:01:59.165: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 30 17:01:59.165: INFO: stdout: ""
Aug 30 17:01:59.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-3363 execpod6b9zn -- /bin/sh -x -c nc -zv -t -w 2 10.240.23.81 80'
Aug 30 17:01:59.774: INFO: stderr: "+ nc -zv -t -w 2 10.240.23.81 80\nConnection to 10.240.23.81 80 port [tcp/http] succeeded!\n"
Aug 30 17:01:59.774: INFO: stdout: ""
Aug 30 17:01:59.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-3363 execpod6b9zn -- /bin/sh -x -c nc -zv -t -w 2 46.101.153.64 31140'
Aug 30 17:02:00.410: INFO: stderr: "+ nc -zv -t -w 2 46.101.153.64 31140\nConnection to 46.101.153.64 31140 port [tcp/31140] succeeded!\n"
Aug 30 17:02:00.410: INFO: stdout: ""
Aug 30 17:02:00.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-3363 execpod6b9zn -- /bin/sh -x -c nc -zv -t -w 2 46.101.135.210 31140'
Aug 30 17:02:00.936: INFO: stderr: "+ nc -zv -t -w 2 46.101.135.210 31140\nConnection to 46.101.135.210 31140 port [tcp/31140] succeeded!\n"
Aug 30 17:02:00.937: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:00.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3363" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:10.607 seconds]
[sig-network] Services
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":116,"skipped":2002,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:00.951: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 30 17:02:01.123: INFO: Waiting up to 5m0s for pod "pod-11d5a9ec-5053-4477-9667-fc1f8b575e00" in namespace "emptydir-3687" to be "success or failure"
Aug 30 17:02:01.129: INFO: Pod "pod-11d5a9ec-5053-4477-9667-fc1f8b575e00": Phase="Pending", Reason="", readiness=false. Elapsed: 5.556649ms
Aug 30 17:02:03.134: INFO: Pod "pod-11d5a9ec-5053-4477-9667-fc1f8b575e00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010437215s
STEP: Saw pod success
Aug 30 17:02:03.134: INFO: Pod "pod-11d5a9ec-5053-4477-9667-fc1f8b575e00" satisfied condition "success or failure"
Aug 30 17:02:03.138: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-11d5a9ec-5053-4477-9667-fc1f8b575e00 container test-container: <nil>
STEP: delete the pod
Aug 30 17:02:03.206: INFO: Waiting for pod pod-11d5a9ec-5053-4477-9667-fc1f8b575e00 to disappear
Aug 30 17:02:03.209: INFO: Pod pod-11d5a9ec-5053-4477-9667-fc1f8b575e00 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:03.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3687" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":117,"skipped":2013,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:03.223: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6864
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Aug 30 17:02:03.384: INFO: namespace kubectl-6864
Aug 30 17:02:03.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-6864'
Aug 30 17:02:03.643: INFO: stderr: ""
Aug 30 17:02:03.643: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Aug 30 17:02:04.649: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 17:02:04.649: INFO: Found 0 / 1
Aug 30 17:02:05.649: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 17:02:05.649: INFO: Found 1 / 1
Aug 30 17:02:05.649: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 30 17:02:05.653: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 17:02:05.653: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 30 17:02:05.653: INFO: wait on agnhost-master startup in kubectl-6864 
Aug 30 17:02:05.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 logs agnhost-master-pfd9s agnhost-master --namespace=kubectl-6864'
Aug 30 17:02:05.877: INFO: stderr: ""
Aug 30 17:02:05.877: INFO: stdout: "Paused\n"
STEP: exposing RC
Aug 30 17:02:05.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6864'
Aug 30 17:02:05.978: INFO: stderr: ""
Aug 30 17:02:05.978: INFO: stdout: "service/rm2 exposed\n"
Aug 30 17:02:05.985: INFO: Service rm2 in namespace kubectl-6864 found.
STEP: exposing service
Aug 30 17:02:07.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6864'
Aug 30 17:02:08.078: INFO: stderr: ""
Aug 30 17:02:08.078: INFO: stdout: "service/rm3 exposed\n"
Aug 30 17:02:08.082: INFO: Service rm3 in namespace kubectl-6864 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:10.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6864" for this suite.

â€¢ [SLOW TEST:6.881 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1188
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":118,"skipped":2014,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:10.105: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 30 17:02:10.288: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3684 /api/v1/namespaces/watch-3684/configmaps/e2e-watch-test-label-changed 458597cf-d454-454b-98ec-41e7a65afcb6 14268 0 2020-08-30 17:02:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 30 17:02:10.289: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3684 /api/v1/namespaces/watch-3684/configmaps/e2e-watch-test-label-changed 458597cf-d454-454b-98ec-41e7a65afcb6 14269 0 2020-08-30 17:02:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 30 17:02:10.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3684 /api/v1/namespaces/watch-3684/configmaps/e2e-watch-test-label-changed 458597cf-d454-454b-98ec-41e7a65afcb6 14270 0 2020-08-30 17:02:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 30 17:02:20.336: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3684 /api/v1/namespaces/watch-3684/configmaps/e2e-watch-test-label-changed 458597cf-d454-454b-98ec-41e7a65afcb6 14330 0 2020-08-30 17:02:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 30 17:02:20.337: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3684 /api/v1/namespaces/watch-3684/configmaps/e2e-watch-test-label-changed 458597cf-d454-454b-98ec-41e7a65afcb6 14331 0 2020-08-30 17:02:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 30 17:02:20.337: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3684 /api/v1/namespaces/watch-3684/configmaps/e2e-watch-test-label-changed 458597cf-d454-454b-98ec-41e7a65afcb6 14332 0 2020-08-30 17:02:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:20.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3684" for this suite.

â€¢ [SLOW TEST:10.248 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":119,"skipped":2054,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:20.353: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1261
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:20.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1261" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":120,"skipped":2061,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:20.607: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-10024e4b-0673-46d4-a59c-168f9c3eb2bb
STEP: Creating a pod to test consume configMaps
Aug 30 17:02:20.820: INFO: Waiting up to 5m0s for pod "pod-configmaps-1f921bde-4a8d-436c-bc19-e69be507ca35" in namespace "configmap-8704" to be "success or failure"
Aug 30 17:02:20.828: INFO: Pod "pod-configmaps-1f921bde-4a8d-436c-bc19-e69be507ca35": Phase="Pending", Reason="", readiness=false. Elapsed: 8.262462ms
Aug 30 17:02:22.834: INFO: Pod "pod-configmaps-1f921bde-4a8d-436c-bc19-e69be507ca35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014015603s
STEP: Saw pod success
Aug 30 17:02:22.834: INFO: Pod "pod-configmaps-1f921bde-4a8d-436c-bc19-e69be507ca35" satisfied condition "success or failure"
Aug 30 17:02:22.840: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-configmaps-1f921bde-4a8d-436c-bc19-e69be507ca35 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 17:02:22.916: INFO: Waiting for pod pod-configmaps-1f921bde-4a8d-436c-bc19-e69be507ca35 to disappear
Aug 30 17:02:22.920: INFO: Pod pod-configmaps-1f921bde-4a8d-436c-bc19-e69be507ca35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:22.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8704" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":121,"skipped":2078,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:22.936: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1440
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 30 17:02:25.127: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:25.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1440" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":122,"skipped":2096,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:25.159: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2270
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:02:25.313: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:31.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2270" for this suite.

â€¢ [SLOW TEST:6.640 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":123,"skipped":2108,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:31.799: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:02:32.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8e12a98a-2731-4540-8ae6-a95603d3080d" in namespace "projected-6793" to be "success or failure"
Aug 30 17:02:32.039: INFO: Pod "downwardapi-volume-8e12a98a-2731-4540-8ae6-a95603d3080d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.358295ms
Aug 30 17:02:34.044: INFO: Pod "downwardapi-volume-8e12a98a-2731-4540-8ae6-a95603d3080d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025392523s
Aug 30 17:02:36.052: INFO: Pod "downwardapi-volume-8e12a98a-2731-4540-8ae6-a95603d3080d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033588598s
STEP: Saw pod success
Aug 30 17:02:36.053: INFO: Pod "downwardapi-volume-8e12a98a-2731-4540-8ae6-a95603d3080d" satisfied condition "success or failure"
Aug 30 17:02:36.057: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-8e12a98a-2731-4540-8ae6-a95603d3080d container client-container: <nil>
STEP: delete the pod
Aug 30 17:02:36.131: INFO: Waiting for pod downwardapi-volume-8e12a98a-2731-4540-8ae6-a95603d3080d to disappear
Aug 30 17:02:36.135: INFO: Pod downwardapi-volume-8e12a98a-2731-4540-8ae6-a95603d3080d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:36.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6793" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":124,"skipped":2115,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:36.152: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-7069/configmap-test-84365d81-32fb-4dfd-beb6-b4901b98ef50
STEP: Creating a pod to test consume configMaps
Aug 30 17:02:36.326: INFO: Waiting up to 5m0s for pod "pod-configmaps-2850384b-6feb-4d77-b16e-8d9349084fcb" in namespace "configmap-7069" to be "success or failure"
Aug 30 17:02:36.333: INFO: Pod "pod-configmaps-2850384b-6feb-4d77-b16e-8d9349084fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.26094ms
Aug 30 17:02:38.339: INFO: Pod "pod-configmaps-2850384b-6feb-4d77-b16e-8d9349084fcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012751914s
STEP: Saw pod success
Aug 30 17:02:38.339: INFO: Pod "pod-configmaps-2850384b-6feb-4d77-b16e-8d9349084fcb" satisfied condition "success or failure"
Aug 30 17:02:38.343: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-configmaps-2850384b-6feb-4d77-b16e-8d9349084fcb container env-test: <nil>
STEP: delete the pod
Aug 30 17:02:38.372: INFO: Waiting for pod pod-configmaps-2850384b-6feb-4d77-b16e-8d9349084fcb to disappear
Aug 30 17:02:38.376: INFO: Pod pod-configmaps-2850384b-6feb-4d77-b16e-8d9349084fcb no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:38.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7069" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":2130,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:38.389: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2403
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-0ac8bb3a-09b8-43a4-8e13-18b20290c084
STEP: Creating a pod to test consume configMaps
Aug 30 17:02:38.574: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e34b1de0-5978-4ed2-8961-37cccfe4ec83" in namespace "projected-2403" to be "success or failure"
Aug 30 17:02:38.582: INFO: Pod "pod-projected-configmaps-e34b1de0-5978-4ed2-8961-37cccfe4ec83": Phase="Pending", Reason="", readiness=false. Elapsed: 8.457295ms
Aug 30 17:02:40.588: INFO: Pod "pod-projected-configmaps-e34b1de0-5978-4ed2-8961-37cccfe4ec83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014286519s
Aug 30 17:02:42.593: INFO: Pod "pod-projected-configmaps-e34b1de0-5978-4ed2-8961-37cccfe4ec83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019620186s
STEP: Saw pod success
Aug 30 17:02:42.593: INFO: Pod "pod-projected-configmaps-e34b1de0-5978-4ed2-8961-37cccfe4ec83" satisfied condition "success or failure"
Aug 30 17:02:42.598: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-projected-configmaps-e34b1de0-5978-4ed2-8961-37cccfe4ec83 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 17:02:42.676: INFO: Waiting for pod pod-projected-configmaps-e34b1de0-5978-4ed2-8961-37cccfe4ec83 to disappear
Aug 30 17:02:42.681: INFO: Pod pod-projected-configmaps-e34b1de0-5978-4ed2-8961-37cccfe4ec83 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:42.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2403" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":126,"skipped":2151,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:42.695: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:46.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2522" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":2157,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:46.919: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-354
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 30 17:02:47.089: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 30 17:02:54.152: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:54.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-354" for this suite.

â€¢ [SLOW TEST:7.257 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":128,"skipped":2164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:54.179: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7321
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 30 17:02:54.396: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7321 /api/v1/namespaces/watch-7321/configmaps/e2e-watch-test-resource-version c3a994f0-d5a7-40e7-b9ea-4b145e214b1b 14791 0 2020-08-30 17:02:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 30 17:02:54.397: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7321 /api/v1/namespaces/watch-7321/configmaps/e2e-watch-test-resource-version c3a994f0-d5a7-40e7-b9ea-4b145e214b1b 14792 0 2020-08-30 17:02:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:02:54.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7321" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":129,"skipped":2198,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:02:54.411: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:02:55.081: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 17:02:57.097: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403775, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403775, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403775, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403775, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:03:00.117: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:03:00.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4521" for this suite.
STEP: Destroying namespace "webhook-4521-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.051 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":130,"skipped":2218,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:03:00.463: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 30 17:03:00.638: INFO: Waiting up to 5m0s for pod "pod-89aae775-d4b0-4c4a-8457-b2048dcefb07" in namespace "emptydir-8947" to be "success or failure"
Aug 30 17:03:00.643: INFO: Pod "pod-89aae775-d4b0-4c4a-8457-b2048dcefb07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.998337ms
Aug 30 17:03:02.649: INFO: Pod "pod-89aae775-d4b0-4c4a-8457-b2048dcefb07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01126009s
STEP: Saw pod success
Aug 30 17:03:02.649: INFO: Pod "pod-89aae775-d4b0-4c4a-8457-b2048dcefb07" satisfied condition "success or failure"
Aug 30 17:03:02.654: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-89aae775-d4b0-4c4a-8457-b2048dcefb07 container test-container: <nil>
STEP: delete the pod
Aug 30 17:03:02.720: INFO: Waiting for pod pod-89aae775-d4b0-4c4a-8457-b2048dcefb07 to disappear
Aug 30 17:03:02.724: INFO: Pod pod-89aae775-d4b0-4c4a-8457-b2048dcefb07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:03:02.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8947" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":131,"skipped":2224,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:03:02.747: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9326
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2227
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:03:16.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4938" for this suite.
STEP: Destroying namespace "nsdeletetest-9326" for this suite.
Aug 30 17:03:16.307: INFO: Namespace nsdeletetest-9326 was already deleted
STEP: Destroying namespace "nsdeletetest-2227" for this suite.

â€¢ [SLOW TEST:13.570 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":132,"skipped":2234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:03:16.318: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7038
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 30 17:03:16.473: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 17:03:19.326: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:03:31.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7038" for this suite.

â€¢ [SLOW TEST:14.787 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":133,"skipped":2283,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:03:31.105: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-5267/secret-test-9e1c267f-fb7b-45ca-bebe-6b0a560e851f
STEP: Creating a pod to test consume secrets
Aug 30 17:03:31.288: INFO: Waiting up to 5m0s for pod "pod-configmaps-a8b34ba1-4e78-40e0-9a49-61595e258d4b" in namespace "secrets-5267" to be "success or failure"
Aug 30 17:03:31.295: INFO: Pod "pod-configmaps-a8b34ba1-4e78-40e0-9a49-61595e258d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.259044ms
Aug 30 17:03:33.300: INFO: Pod "pod-configmaps-a8b34ba1-4e78-40e0-9a49-61595e258d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012362146s
Aug 30 17:03:35.307: INFO: Pod "pod-configmaps-a8b34ba1-4e78-40e0-9a49-61595e258d4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018984346s
STEP: Saw pod success
Aug 30 17:03:35.307: INFO: Pod "pod-configmaps-a8b34ba1-4e78-40e0-9a49-61595e258d4b" satisfied condition "success or failure"
Aug 30 17:03:35.311: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-configmaps-a8b34ba1-4e78-40e0-9a49-61595e258d4b container env-test: <nil>
STEP: delete the pod
Aug 30 17:03:35.390: INFO: Waiting for pod pod-configmaps-a8b34ba1-4e78-40e0-9a49-61595e258d4b to disappear
Aug 30 17:03:35.393: INFO: Pod pod-configmaps-a8b34ba1-4e78-40e0-9a49-61595e258d4b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:03:35.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5267" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2290,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:03:35.408: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-4474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 30 17:03:35.557: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Aug 30 17:03:36.120: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 30 17:03:38.220: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 17:03:40.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 17:03:42.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 17:03:44.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734403816, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 17:03:47.348: INFO: Waited 1.108543883s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:03:48.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4474" for this suite.

â€¢ [SLOW TEST:12.938 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":135,"skipped":2301,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:03:48.349: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9227
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0830 17:04:28.567640      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 30 17:04:28.567: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:04:28.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9227" for this suite.

â€¢ [SLOW TEST:40.231 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":136,"skipped":2317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:04:28.582: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7268
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7268
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 30 17:04:28.751: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 30 17:04:48.856: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7268 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 17:04:48.856: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 17:04:50.335: INFO: Found all expected endpoints: [netserver-0]
Aug 30 17:04:50.340: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.125 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7268 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 17:04:50.340: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 17:04:51.757: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:04:51.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7268" for this suite.

â€¢ [SLOW TEST:23.188 seconds]
[sig-network] Networking
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":137,"skipped":2354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:04:51.771: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-39cd66b7-9392-4a92-9f49-b427de43bcba
STEP: Creating a pod to test consume secrets
Aug 30 17:04:51.956: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-99cd0842-b006-4743-84fa-c9284dc44f32" in namespace "projected-9563" to be "success or failure"
Aug 30 17:04:51.964: INFO: Pod "pod-projected-secrets-99cd0842-b006-4743-84fa-c9284dc44f32": Phase="Pending", Reason="", readiness=false. Elapsed: 7.900014ms
Aug 30 17:04:53.970: INFO: Pod "pod-projected-secrets-99cd0842-b006-4743-84fa-c9284dc44f32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013665981s
STEP: Saw pod success
Aug 30 17:04:53.970: INFO: Pod "pod-projected-secrets-99cd0842-b006-4743-84fa-c9284dc44f32" satisfied condition "success or failure"
Aug 30 17:04:53.975: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod pod-projected-secrets-99cd0842-b006-4743-84fa-c9284dc44f32 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 30 17:04:54.042: INFO: Waiting for pod pod-projected-secrets-99cd0842-b006-4743-84fa-c9284dc44f32 to disappear
Aug 30 17:04:54.046: INFO: Pod pod-projected-secrets-99cd0842-b006-4743-84fa-c9284dc44f32 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:04:54.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9563" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":138,"skipped":2388,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:04:54.066: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8383
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-8383
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8383
Aug 30 17:04:54.244: INFO: Found 0 stateful pods, waiting for 1
Aug 30 17:05:04.254: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 30 17:05:05.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 17:05:05.751: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 17:05:05.751: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 17:05:05.751: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 17:05:05.756: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 30 17:05:15.763: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 17:05:15.763: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 17:05:15.788: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:15.788: INFO: ss-0  adoring-wozniak-54dcfd79fc-6rshr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:04:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:04:54 +0000 UTC  }]
Aug 30 17:05:15.788: INFO: 
Aug 30 17:05:15.788: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 30 17:05:16.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994154815s
Aug 30 17:05:17.801: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986909983s
Aug 30 17:05:18.807: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981038002s
Aug 30 17:05:19.813: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974754748s
Aug 30 17:05:20.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96874811s
Aug 30 17:05:21.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962073015s
Aug 30 17:05:22.833: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.956400676s
Aug 30 17:05:23.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949317203s
Aug 30 17:05:24.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.481667ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8383
Aug 30 17:05:25.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:05:26.404: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 17:05:26.405: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 17:05:26.405: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 17:05:26.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:05:26.979: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 30 17:05:26.979: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 17:05:26.979: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 17:05:26.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:05:27.553: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 30 17:05:27.553: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 17:05:27.553: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 17:05:27.560: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:05:27.560: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:05:27.560: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 30 17:05:27.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 17:05:28.098: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 17:05:28.098: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 17:05:28.098: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 17:05:28.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 17:05:28.682: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 17:05:28.682: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 17:05:28.682: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 17:05:28.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 17:05:29.238: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 17:05:29.238: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 17:05:29.238: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 17:05:29.238: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 17:05:29.243: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 30 17:05:39.254: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 17:05:39.254: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 17:05:39.254: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 17:05:39.271: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:39.271: INFO: ss-0  adoring-wozniak-54dcfd79fc-6rshr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:04:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:04:54 +0000 UTC  }]
Aug 30 17:05:39.271: INFO: ss-1  adoring-wozniak-54dcfd79fc-948mf  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:39.271: INFO: ss-2  adoring-wozniak-54dcfd79fc-948mf  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:39.271: INFO: 
Aug 30 17:05:39.271: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 30 17:05:40.278: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:40.278: INFO: ss-0  adoring-wozniak-54dcfd79fc-6rshr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:04:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:04:54 +0000 UTC  }]
Aug 30 17:05:40.278: INFO: ss-1  adoring-wozniak-54dcfd79fc-948mf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:40.279: INFO: ss-2  adoring-wozniak-54dcfd79fc-948mf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:40.279: INFO: 
Aug 30 17:05:40.279: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 30 17:05:41.284: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:41.284: INFO: ss-1  adoring-wozniak-54dcfd79fc-948mf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:41.284: INFO: ss-2  adoring-wozniak-54dcfd79fc-948mf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:41.287: INFO: 
Aug 30 17:05:41.287: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 30 17:05:42.294: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:42.294: INFO: ss-1  adoring-wozniak-54dcfd79fc-948mf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:42.294: INFO: ss-2  adoring-wozniak-54dcfd79fc-948mf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:42.294: INFO: 
Aug 30 17:05:42.294: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 30 17:05:43.300: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:43.300: INFO: ss-1  adoring-wozniak-54dcfd79fc-948mf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:43.300: INFO: ss-2  adoring-wozniak-54dcfd79fc-948mf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:43.300: INFO: 
Aug 30 17:05:43.300: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 30 17:05:44.305: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:44.305: INFO: ss-1  adoring-wozniak-54dcfd79fc-948mf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:44.305: INFO: 
Aug 30 17:05:44.305: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 30 17:05:45.312: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:45.312: INFO: ss-1  adoring-wozniak-54dcfd79fc-948mf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:45.312: INFO: 
Aug 30 17:05:45.312: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 30 17:05:46.318: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:46.318: INFO: ss-1  adoring-wozniak-54dcfd79fc-948mf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:46.318: INFO: 
Aug 30 17:05:46.318: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 30 17:05:47.325: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:47.325: INFO: ss-1  adoring-wozniak-54dcfd79fc-948mf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:47.325: INFO: 
Aug 30 17:05:47.325: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 30 17:05:48.332: INFO: POD   NODE                              PHASE    GRACE  CONDITIONS
Aug 30 17:05:48.332: INFO: ss-1  adoring-wozniak-54dcfd79fc-948mf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-30 17:05:15 +0000 UTC  }]
Aug 30 17:05:48.332: INFO: 
Aug 30 17:05:48.332: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8383
Aug 30 17:05:49.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:05:49.563: INFO: rc: 1
Aug 30 17:05:49.563: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Aug 30 17:05:59.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:05:59.631: INFO: rc: 1
Aug 30 17:05:59.631: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:06:09.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:06:09.698: INFO: rc: 1
Aug 30 17:06:09.698: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:06:19.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:06:19.766: INFO: rc: 1
Aug 30 17:06:19.766: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:06:29.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:06:29.833: INFO: rc: 1
Aug 30 17:06:29.833: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:06:39.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:06:39.912: INFO: rc: 1
Aug 30 17:06:39.912: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:06:49.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:06:49.990: INFO: rc: 1
Aug 30 17:06:49.990: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:06:59.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:07:00.068: INFO: rc: 1
Aug 30 17:07:00.068: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:07:10.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:07:10.141: INFO: rc: 1
Aug 30 17:07:10.141: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:07:20.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:07:20.209: INFO: rc: 1
Aug 30 17:07:20.209: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:07:30.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:07:30.275: INFO: rc: 1
Aug 30 17:07:30.275: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:07:40.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:07:40.341: INFO: rc: 1
Aug 30 17:07:40.341: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:07:50.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:07:50.411: INFO: rc: 1
Aug 30 17:07:50.411: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:08:00.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:08:00.495: INFO: rc: 1
Aug 30 17:08:00.495: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:08:10.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:08:10.563: INFO: rc: 1
Aug 30 17:08:10.563: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:08:20.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:08:20.634: INFO: rc: 1
Aug 30 17:08:20.634: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:08:30.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:08:30.701: INFO: rc: 1
Aug 30 17:08:30.701: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:08:40.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:08:40.768: INFO: rc: 1
Aug 30 17:08:40.768: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:08:50.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:08:50.836: INFO: rc: 1
Aug 30 17:08:50.836: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:09:00.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:09:00.903: INFO: rc: 1
Aug 30 17:09:00.903: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:09:10.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:09:10.983: INFO: rc: 1
Aug 30 17:09:10.983: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:09:20.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:09:21.058: INFO: rc: 1
Aug 30 17:09:21.058: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:09:31.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:09:31.130: INFO: rc: 1
Aug 30 17:09:31.130: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:09:41.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:09:41.199: INFO: rc: 1
Aug 30 17:09:41.199: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:09:51.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:09:51.266: INFO: rc: 1
Aug 30 17:09:51.266: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:10:01.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:10:01.344: INFO: rc: 1
Aug 30 17:10:01.344: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:10:11.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:10:11.420: INFO: rc: 1
Aug 30 17:10:11.420: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:10:21.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:10:21.494: INFO: rc: 1
Aug 30 17:10:21.494: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:10:31.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:10:31.564: INFO: rc: 1
Aug 30 17:10:31.564: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:10:41.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:10:41.633: INFO: rc: 1
Aug 30 17:10:41.633: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 30 17:10:51.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:10:51.705: INFO: rc: 1
Aug 30 17:10:51.705: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Aug 30 17:10:51.705: INFO: Scaling statefulset ss to 0
Aug 30 17:10:51.723: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 30 17:10:51.728: INFO: Deleting all statefulset in ns statefulset-8383
Aug 30 17:10:51.733: INFO: Scaling statefulset ss to 0
Aug 30 17:10:51.749: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 17:10:51.753: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:10:51.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8383" for this suite.

â€¢ [SLOW TEST:357.721 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":139,"skipped":2397,"failed":0}
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:10:51.787: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Aug 30 17:10:54.523: INFO: Successfully updated pod "annotationupdate2de23eec-5a56-45aa-800b-b70f41425341"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:10:58.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1949" for this suite.

â€¢ [SLOW TEST:6.797 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":140,"skipped":2397,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:10:58.585: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:10:59.469: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 30 17:11:01.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404259, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404259, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404259, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404259, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:11:04.508: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:11:04.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9188" for this suite.
STEP: Destroying namespace "webhook-9188-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.023 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":141,"skipped":2407,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:11:04.609: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:11:04.782: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a60e8e4a-34e1-47b8-8ed7-051d5b5d7447" in namespace "downward-api-6143" to be "success or failure"
Aug 30 17:11:04.788: INFO: Pod "downwardapi-volume-a60e8e4a-34e1-47b8-8ed7-051d5b5d7447": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035248ms
Aug 30 17:11:06.794: INFO: Pod "downwardapi-volume-a60e8e4a-34e1-47b8-8ed7-051d5b5d7447": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012018932s
STEP: Saw pod success
Aug 30 17:11:06.794: INFO: Pod "downwardapi-volume-a60e8e4a-34e1-47b8-8ed7-051d5b5d7447" satisfied condition "success or failure"
Aug 30 17:11:06.799: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod downwardapi-volume-a60e8e4a-34e1-47b8-8ed7-051d5b5d7447 container client-container: <nil>
STEP: delete the pod
Aug 30 17:11:06.869: INFO: Waiting for pod downwardapi-volume-a60e8e4a-34e1-47b8-8ed7-051d5b5d7447 to disappear
Aug 30 17:11:06.873: INFO: Pod downwardapi-volume-a60e8e4a-34e1-47b8-8ed7-051d5b5d7447 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:11:06.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6143" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":142,"skipped":2416,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:11:06.886: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1253
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 30 17:11:07.072: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1253 /api/v1/namespaces/watch-1253/configmaps/e2e-watch-test-watch-closed 3302fdde-e7aa-4496-9b46-e8565092ab38 17435 0 2020-08-30 17:11:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 30 17:11:07.073: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1253 /api/v1/namespaces/watch-1253/configmaps/e2e-watch-test-watch-closed 3302fdde-e7aa-4496-9b46-e8565092ab38 17436 0 2020-08-30 17:11:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 30 17:11:07.105: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1253 /api/v1/namespaces/watch-1253/configmaps/e2e-watch-test-watch-closed 3302fdde-e7aa-4496-9b46-e8565092ab38 17437 0 2020-08-30 17:11:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 30 17:11:07.105: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1253 /api/v1/namespaces/watch-1253/configmaps/e2e-watch-test-watch-closed 3302fdde-e7aa-4496-9b46-e8565092ab38 17438 0 2020-08-30 17:11:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:11:07.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1253" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":143,"skipped":2418,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:11:07.122: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3450
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3450.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3450.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3450.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3450.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3450.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3450.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 17:11:12.056: INFO: DNS probes using dns-3450/dns-test-b62cf087-422c-4373-9bf3-5dcaa266f63a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:11:12.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3450" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":144,"skipped":2421,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:11:12.120: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:11:12.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-196" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":145,"skipped":2425,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:11:12.309: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-9339
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 30 17:11:12.467: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 30 17:11:32.566: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.132:8080/dial?request=hostname&protocol=http&host=172.25.1.58&port=8080&tries=1'] Namespace:pod-network-test-9339 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 17:11:32.566: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 17:11:33.059: INFO: Waiting for responses: map[]
Aug 30 17:11:33.064: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.132:8080/dial?request=hostname&protocol=http&host=172.25.0.131&port=8080&tries=1'] Namespace:pod-network-test-9339 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 30 17:11:33.064: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
Aug 30 17:11:33.606: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:11:33.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9339" for this suite.

â€¢ [SLOW TEST:21.313 seconds]
[sig-network] Networking
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":146,"skipped":2425,"failed":0}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:11:33.622: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3270
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 30 17:11:37.862: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 17:11:37.867: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 17:11:39.868: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 17:11:39.874: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 17:11:41.868: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 17:11:41.873: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 17:11:43.868: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 17:11:43.873: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 17:11:45.867: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 17:11:45.875: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 17:11:47.868: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 17:11:47.874: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 17:11:49.867: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 17:11:49.873: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 17:11:51.867: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 17:11:51.874: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 17:11:53.868: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 17:11:53.874: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 17:11:55.868: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 17:11:55.874: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:11:55.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3270" for this suite.

â€¢ [SLOW TEST:22.326 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2427,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:11:55.949: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7121
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-25b95375-48b8-47e8-ab05-05e6da7b655c
STEP: Creating a pod to test consume configMaps
Aug 30 17:11:56.135: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba6c501f-446a-4d18-a11b-7caa195a3427" in namespace "projected-7121" to be "success or failure"
Aug 30 17:11:56.140: INFO: Pod "pod-projected-configmaps-ba6c501f-446a-4d18-a11b-7caa195a3427": Phase="Pending", Reason="", readiness=false. Elapsed: 5.427442ms
Aug 30 17:11:58.147: INFO: Pod "pod-projected-configmaps-ba6c501f-446a-4d18-a11b-7caa195a3427": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011633213s
STEP: Saw pod success
Aug 30 17:11:58.147: INFO: Pod "pod-projected-configmaps-ba6c501f-446a-4d18-a11b-7caa195a3427" satisfied condition "success or failure"
Aug 30 17:11:58.151: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-projected-configmaps-ba6c501f-446a-4d18-a11b-7caa195a3427 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 17:11:58.221: INFO: Waiting for pod pod-projected-configmaps-ba6c501f-446a-4d18-a11b-7caa195a3427 to disappear
Aug 30 17:11:58.226: INFO: Pod pod-projected-configmaps-ba6c501f-446a-4d18-a11b-7caa195a3427 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:11:58.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7121" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":148,"skipped":2451,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:11:58.243: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Aug 30 17:11:58.410: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:12:01.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2355" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":149,"skipped":2468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:12:01.365: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:12:01.558: INFO: Create a RollingUpdate DaemonSet
Aug 30 17:12:01.567: INFO: Check that daemon pods launch on every node of the cluster
Aug 30 17:12:01.578: INFO: Number of nodes with available pods: 0
Aug 30 17:12:01.578: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 17:12:02.589: INFO: Number of nodes with available pods: 0
Aug 30 17:12:02.589: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 17:12:03.588: INFO: Number of nodes with available pods: 2
Aug 30 17:12:03.588: INFO: Number of running nodes: 2, number of available pods: 2
Aug 30 17:12:03.588: INFO: Update the DaemonSet to trigger a rollout
Aug 30 17:12:03.602: INFO: Updating DaemonSet daemon-set
Aug 30 17:12:14.626: INFO: Roll back the DaemonSet before rollout is complete
Aug 30 17:12:14.641: INFO: Updating DaemonSet daemon-set
Aug 30 17:12:14.641: INFO: Make sure DaemonSet rollback is complete
Aug 30 17:12:14.647: INFO: Wrong image for pod: daemon-set-nkmgl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 30 17:12:14.647: INFO: Pod daemon-set-nkmgl is not available
Aug 30 17:12:15.658: INFO: Wrong image for pod: daemon-set-nkmgl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 30 17:12:15.658: INFO: Pod daemon-set-nkmgl is not available
Aug 30 17:12:17.663: INFO: Pod daemon-set-jk2v5 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-257, will wait for the garbage collector to delete the pods
Aug 30 17:12:17.751: INFO: Deleting DaemonSet.extensions daemon-set took: 13.73808ms
Aug 30 17:12:18.251: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.17222ms
Aug 30 17:13:34.357: INFO: Number of nodes with available pods: 0
Aug 30 17:13:34.357: INFO: Number of running nodes: 0, number of available pods: 0
Aug 30 17:13:34.364: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-257/daemonsets","resourceVersion":"18363"},"items":null}

Aug 30 17:13:34.370: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-257/pods","resourceVersion":"18363"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:13:34.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-257" for this suite.

â€¢ [SLOW TEST:93.036 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":150,"skipped":2508,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:13:34.402: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5036
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-zq6l
STEP: Creating a pod to test atomic-volume-subpath
Aug 30 17:13:34.627: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zq6l" in namespace "subpath-5036" to be "success or failure"
Aug 30 17:13:34.636: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Pending", Reason="", readiness=false. Elapsed: 8.678941ms
Aug 30 17:13:36.644: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 2.016952632s
Aug 30 17:13:38.663: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 4.036120864s
Aug 30 17:13:40.669: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 6.041687425s
Aug 30 17:13:42.675: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 8.048260073s
Aug 30 17:13:44.681: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 10.054519263s
Aug 30 17:13:46.687: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 12.060296521s
Aug 30 17:13:48.693: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 14.065941811s
Aug 30 17:13:50.699: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 16.07193876s
Aug 30 17:13:52.705: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 18.077899193s
Aug 30 17:13:54.711: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 20.083738302s
Aug 30 17:13:56.716: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Running", Reason="", readiness=true. Elapsed: 22.089390209s
Aug 30 17:13:58.722: INFO: Pod "pod-subpath-test-configmap-zq6l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.095037378s
STEP: Saw pod success
Aug 30 17:13:58.722: INFO: Pod "pod-subpath-test-configmap-zq6l" satisfied condition "success or failure"
Aug 30 17:13:58.727: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-subpath-test-configmap-zq6l container test-container-subpath-configmap-zq6l: <nil>
STEP: delete the pod
Aug 30 17:13:58.797: INFO: Waiting for pod pod-subpath-test-configmap-zq6l to disappear
Aug 30 17:13:58.802: INFO: Pod pod-subpath-test-configmap-zq6l no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zq6l
Aug 30 17:13:58.802: INFO: Deleting pod "pod-subpath-test-configmap-zq6l" in namespace "subpath-5036"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:13:58.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5036" for this suite.

â€¢ [SLOW TEST:24.422 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":151,"skipped":2519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:13:58.824: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3244
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-b4dee1d3-bfb6-4ee3-a653-214c28232fb9
STEP: Creating a pod to test consume configMaps
Aug 30 17:13:59.016: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b738ac2-fa26-4e00-9131-c8d53cf7a28e" in namespace "projected-3244" to be "success or failure"
Aug 30 17:13:59.022: INFO: Pod "pod-projected-configmaps-7b738ac2-fa26-4e00-9131-c8d53cf7a28e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.166681ms
Aug 30 17:14:01.029: INFO: Pod "pod-projected-configmaps-7b738ac2-fa26-4e00-9131-c8d53cf7a28e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01291461s
STEP: Saw pod success
Aug 30 17:14:01.029: INFO: Pod "pod-projected-configmaps-7b738ac2-fa26-4e00-9131-c8d53cf7a28e" satisfied condition "success or failure"
Aug 30 17:14:01.034: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-projected-configmaps-7b738ac2-fa26-4e00-9131-c8d53cf7a28e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 17:14:01.109: INFO: Waiting for pod pod-projected-configmaps-7b738ac2-fa26-4e00-9131-c8d53cf7a28e to disappear
Aug 30 17:14:01.114: INFO: Pod pod-projected-configmaps-7b738ac2-fa26-4e00-9131-c8d53cf7a28e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:01.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3244" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":152,"skipped":2543,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:01.129: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-3cc2fcc6-092b-47a8-8928-65bc4fa397b5
STEP: Creating a pod to test consume secrets
Aug 30 17:14:01.319: INFO: Waiting up to 5m0s for pod "pod-secrets-22a85b26-b08b-4472-bf08-b9c559bfc7a4" in namespace "secrets-8957" to be "success or failure"
Aug 30 17:14:01.327: INFO: Pod "pod-secrets-22a85b26-b08b-4472-bf08-b9c559bfc7a4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.705686ms
Aug 30 17:14:03.333: INFO: Pod "pod-secrets-22a85b26-b08b-4472-bf08-b9c559bfc7a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013768796s
STEP: Saw pod success
Aug 30 17:14:03.333: INFO: Pod "pod-secrets-22a85b26-b08b-4472-bf08-b9c559bfc7a4" satisfied condition "success or failure"
Aug 30 17:14:03.338: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-secrets-22a85b26-b08b-4472-bf08-b9c559bfc7a4 container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 17:14:03.409: INFO: Waiting for pod pod-secrets-22a85b26-b08b-4472-bf08-b9c559bfc7a4 to disappear
Aug 30 17:14:03.414: INFO: Pod pod-secrets-22a85b26-b08b-4472-bf08-b9c559bfc7a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:03.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8957" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":153,"skipped":2552,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:03.428: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2513
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:14:03.590: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:04.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2513" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":154,"skipped":2571,"failed":0}
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:04.845: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9598
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:14:05.121: INFO: (0) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 45.650215ms)
Aug 30 17:14:05.171: INFO: (1) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 49.26616ms)
Aug 30 17:14:05.181: INFO: (2) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.485906ms)
Aug 30 17:14:05.266: INFO: (3) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 84.735259ms)
Aug 30 17:14:05.276: INFO: (4) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.209102ms)
Aug 30 17:14:05.324: INFO: (5) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 47.961764ms)
Aug 30 17:14:05.334: INFO: (6) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.846851ms)
Aug 30 17:14:05.343: INFO: (7) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.728188ms)
Aug 30 17:14:05.351: INFO: (8) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.534128ms)
Aug 30 17:14:05.360: INFO: (9) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.518062ms)
Aug 30 17:14:05.368: INFO: (10) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.650015ms)
Aug 30 17:14:05.377: INFO: (11) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.084091ms)
Aug 30 17:14:05.385: INFO: (12) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.498956ms)
Aug 30 17:14:05.393: INFO: (13) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.098914ms)
Aug 30 17:14:05.401: INFO: (14) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.923056ms)
Aug 30 17:14:05.517: INFO: (15) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 115.609524ms)
Aug 30 17:14:05.561: INFO: (16) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 44.013959ms)
Aug 30 17:14:05.608: INFO: (17) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 47.327446ms)
Aug 30 17:14:05.618: INFO: (18) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.548809ms)
Aug 30 17:14:05.665: INFO: (19) /api/v1/nodes/adoring-wozniak-54dcfd79fc-948mf:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 47.195692ms)
[AfterEach] version v1
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:05.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9598" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":155,"skipped":2576,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:05.680: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:14:05.846: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:08.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-377" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":156,"skipped":2579,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:08.095: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4157
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:14:08.258: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Aug 30 17:14:11.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-4157 create -f -'
Aug 30 17:14:11.548: INFO: stderr: ""
Aug 30 17:14:11.548: INFO: stdout: "e2e-test-crd-publish-openapi-2881-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 30 17:14:11.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-4157 delete e2e-test-crd-publish-openapi-2881-crds test-foo'
Aug 30 17:14:11.623: INFO: stderr: ""
Aug 30 17:14:11.623: INFO: stdout: "e2e-test-crd-publish-openapi-2881-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 30 17:14:11.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-4157 apply -f -'
Aug 30 17:14:11.776: INFO: stderr: ""
Aug 30 17:14:11.776: INFO: stdout: "e2e-test-crd-publish-openapi-2881-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 30 17:14:11.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-4157 delete e2e-test-crd-publish-openapi-2881-crds test-foo'
Aug 30 17:14:11.854: INFO: stderr: ""
Aug 30 17:14:11.854: INFO: stdout: "e2e-test-crd-publish-openapi-2881-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 30 17:14:11.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-4157 create -f -'
Aug 30 17:14:12.076: INFO: rc: 1
Aug 30 17:14:12.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-4157 apply -f -'
Aug 30 17:14:12.278: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Aug 30 17:14:12.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-4157 create -f -'
Aug 30 17:14:12.408: INFO: rc: 1
Aug 30 17:14:12.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-4157 apply -f -'
Aug 30 17:14:12.620: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 30 17:14:12.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 explain e2e-test-crd-publish-openapi-2881-crds'
Aug 30 17:14:12.753: INFO: stderr: ""
Aug 30 17:14:12.753: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2881-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 30 17:14:12.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 explain e2e-test-crd-publish-openapi-2881-crds.metadata'
Aug 30 17:14:12.889: INFO: stderr: ""
Aug 30 17:14:12.889: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2881-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 30 17:14:12.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 explain e2e-test-crd-publish-openapi-2881-crds.spec'
Aug 30 17:14:13.100: INFO: stderr: ""
Aug 30 17:14:13.100: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2881-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 30 17:14:13.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 explain e2e-test-crd-publish-openapi-2881-crds.spec.bars'
Aug 30 17:14:13.232: INFO: stderr: ""
Aug 30 17:14:13.232: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2881-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 30 17:14:13.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 explain e2e-test-crd-publish-openapi-2881-crds.spec.bars2'
Aug 30 17:14:13.467: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:16.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4157" for this suite.

â€¢ [SLOW TEST:8.725 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":157,"skipped":2580,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:16.820: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 30 17:14:19.534: INFO: Successfully updated pod "pod-update-activedeadlineseconds-226b01c7-5b86-4a3b-9a1b-d77eb8642a3c"
Aug 30 17:14:19.534: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-226b01c7-5b86-4a3b-9a1b-d77eb8642a3c" in namespace "pods-4238" to be "terminated due to deadline exceeded"
Aug 30 17:14:19.538: INFO: Pod "pod-update-activedeadlineseconds-226b01c7-5b86-4a3b-9a1b-d77eb8642a3c": Phase="Running", Reason="", readiness=true. Elapsed: 4.086459ms
Aug 30 17:14:21.544: INFO: Pod "pod-update-activedeadlineseconds-226b01c7-5b86-4a3b-9a1b-d77eb8642a3c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010589436s
Aug 30 17:14:23.550: INFO: Pod "pod-update-activedeadlineseconds-226b01c7-5b86-4a3b-9a1b-d77eb8642a3c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.01605853s
Aug 30 17:14:23.550: INFO: Pod "pod-update-activedeadlineseconds-226b01c7-5b86-4a3b-9a1b-d77eb8642a3c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:23.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4238" for this suite.

â€¢ [SLOW TEST:6.745 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":158,"skipped":2582,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:23.565: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3557.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3557.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3557.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3557.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3557.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3557.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 17:14:25.918: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local from pod dns-3557/dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69: the server could not find the requested resource (get pods dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69)
Aug 30 17:14:25.965: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local from pod dns-3557/dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69: the server could not find the requested resource (get pods dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69)
Aug 30 17:14:25.975: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3557.svc.cluster.local from pod dns-3557/dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69: the server could not find the requested resource (get pods dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69)
Aug 30 17:14:25.985: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3557.svc.cluster.local from pod dns-3557/dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69: the server could not find the requested resource (get pods dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69)
Aug 30 17:14:26.164: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local from pod dns-3557/dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69: the server could not find the requested resource (get pods dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69)
Aug 30 17:14:26.173: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local from pod dns-3557/dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69: the server could not find the requested resource (get pods dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69)
Aug 30 17:14:26.182: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3557.svc.cluster.local from pod dns-3557/dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69: the server could not find the requested resource (get pods dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69)
Aug 30 17:14:26.191: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3557.svc.cluster.local from pod dns-3557/dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69: the server could not find the requested resource (get pods dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69)
Aug 30 17:14:26.287: INFO: Lookups using dns-3557/dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3557.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3557.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3557.svc.cluster.local jessie_udp@dns-test-service-2.dns-3557.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3557.svc.cluster.local]

Aug 30 17:14:32.256: INFO: DNS probes using dns-3557/dns-test-6bf33241-fa47-457f-98e2-2137b84d0b69 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:32.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3557" for this suite.

â€¢ [SLOW TEST:8.754 seconds]
[sig-network] DNS
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":159,"skipped":2589,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:32.320: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:36.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5652" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":160,"skipped":2591,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:36.557: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5022
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 30 17:14:42.830: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 17:14:42.835: INFO: Pod pod-with-poststart-http-hook still exists
Aug 30 17:14:44.835: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 17:14:44.841: INFO: Pod pod-with-poststart-http-hook still exists
Aug 30 17:14:46.835: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 17:14:46.841: INFO: Pod pod-with-poststart-http-hook still exists
Aug 30 17:14:48.835: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 17:14:48.841: INFO: Pod pod-with-poststart-http-hook still exists
Aug 30 17:14:50.835: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 17:14:50.845: INFO: Pod pod-with-poststart-http-hook still exists
Aug 30 17:14:52.835: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 17:14:52.841: INFO: Pod pod-with-poststart-http-hook still exists
Aug 30 17:14:54.835: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 17:14:54.841: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:54.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5022" for this suite.

â€¢ [SLOW TEST:18.300 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":161,"skipped":2614,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:54.857: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:14:55.035: INFO: Waiting up to 5m0s for pod "downwardapi-volume-49773cf5-70e4-4b5c-9792-e05720c5c08f" in namespace "projected-7422" to be "success or failure"
Aug 30 17:14:55.039: INFO: Pod "downwardapi-volume-49773cf5-70e4-4b5c-9792-e05720c5c08f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095563ms
Aug 30 17:14:57.046: INFO: Pod "downwardapi-volume-49773cf5-70e4-4b5c-9792-e05720c5c08f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01154145s
STEP: Saw pod success
Aug 30 17:14:57.047: INFO: Pod "downwardapi-volume-49773cf5-70e4-4b5c-9792-e05720c5c08f" satisfied condition "success or failure"
Aug 30 17:14:57.052: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-49773cf5-70e4-4b5c-9792-e05720c5c08f container client-container: <nil>
STEP: delete the pod
Aug 30 17:14:57.122: INFO: Waiting for pod downwardapi-volume-49773cf5-70e4-4b5c-9792-e05720c5c08f to disappear
Aug 30 17:14:57.126: INFO: Pod downwardapi-volume-49773cf5-70e4-4b5c-9792-e05720c5c08f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:57.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7422" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":162,"skipped":2622,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:57.143: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 30 17:14:59.350: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:14:59.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4092" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":163,"skipped":2634,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:14:59.388: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-eb3ab438-75b4-4390-8934-11f10a0e5841
STEP: Creating a pod to test consume secrets
Aug 30 17:14:59.579: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3c49fdb1-6205-4e8e-99d4-db4f7dd0c8e3" in namespace "projected-4736" to be "success or failure"
Aug 30 17:14:59.587: INFO: Pod "pod-projected-secrets-3c49fdb1-6205-4e8e-99d4-db4f7dd0c8e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.196416ms
Aug 30 17:15:01.593: INFO: Pod "pod-projected-secrets-3c49fdb1-6205-4e8e-99d4-db4f7dd0c8e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014686588s
STEP: Saw pod success
Aug 30 17:15:01.594: INFO: Pod "pod-projected-secrets-3c49fdb1-6205-4e8e-99d4-db4f7dd0c8e3" satisfied condition "success or failure"
Aug 30 17:15:01.599: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-projected-secrets-3c49fdb1-6205-4e8e-99d4-db4f7dd0c8e3 container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 17:15:01.666: INFO: Waiting for pod pod-projected-secrets-3c49fdb1-6205-4e8e-99d4-db4f7dd0c8e3 to disappear
Aug 30 17:15:01.671: INFO: Pod pod-projected-secrets-3c49fdb1-6205-4e8e-99d4-db4f7dd0c8e3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:15:01.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4736" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":164,"skipped":2650,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:15:01.686: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:15:01.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3737" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":165,"skipped":2658,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:15:01.889: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:15:02.059: INFO: Creating deployment "webserver-deployment"
Aug 30 17:15:02.068: INFO: Waiting for observed generation 1
Aug 30 17:15:04.081: INFO: Waiting for all required pods to come up
Aug 30 17:15:04.088: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 30 17:15:06.108: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 30 17:15:06.119: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 30 17:15:06.133: INFO: Updating deployment webserver-deployment
Aug 30 17:15:06.133: INFO: Waiting for observed generation 2
Aug 30 17:15:08.143: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 30 17:15:08.148: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 30 17:15:08.152: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 30 17:15:08.167: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 30 17:15:08.167: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 30 17:15:08.172: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 30 17:15:08.182: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 30 17:15:08.182: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 30 17:15:08.194: INFO: Updating deployment webserver-deployment
Aug 30 17:15:08.194: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 30 17:15:08.209: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 30 17:15:08.219: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Aug 30 17:15:10.245: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6481 /apis/apps/v1/namespaces/deployment-6481/deployments/webserver-deployment 680bcbb3-2940-48c8-b5d7-6eb06ee84d2a 19541 3 2020-08-30 17:15:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00486bb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-30 17:15:08 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-08-30 17:15:08 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 30 17:15:10.251: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-6481 /apis/apps/v1/namespaces/deployment-6481/replicasets/webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 19519 3 2020-08-30 17:15:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 680bcbb3-2940-48c8-b5d7-6eb06ee84d2a 0xc004728497 0xc004728498}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004728518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 17:15:10.251: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 30 17:15:10.251: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-6481 /apis/apps/v1/namespaces/deployment-6481/replicasets/webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 19529 3 2020-08-30 17:15:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 680bcbb3-2940-48c8-b5d7-6eb06ee84d2a 0xc004728347 0xc004728348}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004728418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 30 17:15:10.265: INFO: Pod "webserver-deployment-595b5b9587-2g2p9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2g2p9 webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-2g2p9 7bcb6784-a737-4312-a50e-ec5cb789b496 19339 0 2020-08-30 17:15:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.66/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc004728da7 0xc004728da8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:172.25.1.66,StartTime:2020-08-30 17:15:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 17:15:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9c8424db3be642029a8b23da2eb37fab730a5f3cf21cbf4340de6a4656f9a560,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.265: INFO: Pod "webserver-deployment-595b5b9587-4vq75" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4vq75 webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-4vq75 ed576391-a1e1-4733-9b35-3293e1eab4cb 19352 0 2020-08-30 17:15:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.153/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc004729030 0xc004729031}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:172.25.0.153,StartTime:2020-08-30 17:15:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 17:15:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://fa5e294c068129f3eeb713027e1bd4dd72b281b0369211f731f4226049ee7c0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.266: INFO: Pod "webserver-deployment-595b5b9587-5qkvb" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5qkvb webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-5qkvb 342ad11b-59e9-484c-b9ce-c2d77b263a63 19576 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.70/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0047292b7 0xc0047292b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.266: INFO: Pod "webserver-deployment-595b5b9587-6dd57" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6dd57 webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-6dd57 59f5ba7c-6537-4f78-862e-e08717b1b197 19355 0 2020-08-30 17:15:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.65/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0047294a7 0xc0047294a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:172.25.1.65,StartTime:2020-08-30 17:15:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 17:15:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f4bc01eafdf7c2dd8cd3fca5c3b95de547fed3a27c6057baf9b300efb875e057,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.266: INFO: Pod "webserver-deployment-595b5b9587-6rvh6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6rvh6 webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-6rvh6 edf7dc56-d928-4ac5-a9e7-cd31aa9f822a 19587 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.158/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0047296d0 0xc0047296d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.267: INFO: Pod "webserver-deployment-595b5b9587-8qkjq" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8qkjq webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-8qkjq e1ab339f-ea03-489e-8e55-3a1476198eef 19583 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0047299b7 0xc0047299b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.267: INFO: Pod "webserver-deployment-595b5b9587-gqnzf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gqnzf webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-gqnzf a3a07b9c-ffc6-4b6a-bc56-e485ee093684 19584 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.71/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc004729c77 0xc004729c78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.267: INFO: Pod "webserver-deployment-595b5b9587-jp6wd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jp6wd webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-jp6wd 2d4dfd70-98f2-4d06-978c-84530b6a6faf 19549 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc004729f07 0xc004729f08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.268: INFO: Pod "webserver-deployment-595b5b9587-lkh5h" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lkh5h webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-lkh5h 6071dcd6-14aa-47d4-bdad-0fbc421fc353 19524 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f0117 0xc0046f0118}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.269: INFO: Pod "webserver-deployment-595b5b9587-lrg65" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lrg65 webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-lrg65 0e89445c-7b40-44a1-b2fe-74e93049b4aa 19546 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f02b0 0xc0046f02b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.270: INFO: Pod "webserver-deployment-595b5b9587-n5pds" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n5pds webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-n5pds 1c2052f1-6343-464b-9dfc-1b8feadf9bb0 19304 0 2020-08-30 17:15:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.64/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f04a7 0xc0046f04a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:172.25.1.64,StartTime:2020-08-30 17:15:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 17:15:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d4a31967f40d671692ce7e803ab24b40ce5f1c23bb9455398d62c382d08b0cf6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.270: INFO: Pod "webserver-deployment-595b5b9587-q9qz8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-q9qz8 webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-q9qz8 9c9fc6ef-d3fa-4c57-9b1b-f81013588e0a 19296 0 2020-08-30 17:15:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.63/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f06c0 0xc0046f06c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:172.25.1.63,StartTime:2020-08-30 17:15:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 17:15:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://49a9df326d03568e2a7f93d96b7d1508fe84d42adc48e286b8e9392e6e2faf8d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.270: INFO: Pod "webserver-deployment-595b5b9587-qd2hl" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qd2hl webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-qd2hl bd542e9b-144f-431d-94b7-5f1cb14eeff2 19348 0 2020-08-30 17:15:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.150/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f0880 0xc0046f0881}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:172.25.0.150,StartTime:2020-08-30 17:15:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 17:15:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9a26a92d98685efaab0b7cd93db1e9c0f20fc3cc074c474b0d2e797cb7a625a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.270: INFO: Pod "webserver-deployment-595b5b9587-rph88" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rph88 webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-rph88 0d7401ac-6352-4eda-bdea-cbe7c2851f2f 19606 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.73/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f0a17 0xc0046f0a18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.270: INFO: Pod "webserver-deployment-595b5b9587-svsp8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-svsp8 webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-svsp8 2d103d51-e74e-4db5-a92b-8e086e96dc98 19611 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.160/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f0c47 0xc0046f0c48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.271: INFO: Pod "webserver-deployment-595b5b9587-tmslx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tmslx webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-tmslx d8a8ab71-e7cb-4c45-865f-624dded4991d 19346 0 2020-08-30 17:15:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.152/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f0f37 0xc0046f0f38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:172.25.0.152,StartTime:2020-08-30 17:15:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 17:15:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://fe56b2fa5b49b869818a2abeb39b7c41d71ae26f206572cc1e2b7f79fe8a72cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.271: INFO: Pod "webserver-deployment-595b5b9587-vw4rc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vw4rc webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-vw4rc 668aa491-cd19-4d23-8a84-6051a0c3aca3 19578 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.157/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f11c7 0xc0046f11c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.271: INFO: Pod "webserver-deployment-595b5b9587-vz75q" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vz75q webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-vz75q 0e5b59a0-526d-42a5-aa8b-de26d2495886 19335 0 2020-08-30 17:15:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.67/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f1387 0xc0046f1388}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:172.25.1.67,StartTime:2020-08-30 17:15:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 17:15:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ea2c33b947cabcaab164130f56f497797b9f4d9b08f5fda50957b3ccd962e540,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.271: INFO: Pod "webserver-deployment-595b5b9587-xvhrk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xvhrk webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-xvhrk fad69d70-8ba4-4338-b02f-e1148b5fe6b2 19599 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f15f0 0xc0046f15f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.271: INFO: Pod "webserver-deployment-595b5b9587-zdbf4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zdbf4 webserver-deployment-595b5b9587- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-595b5b9587-zdbf4 0e8491c9-9f70-4764-9048-1604c7f1e3ce 19581 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 52d387b8-02db-40c1-a5cd-f22a2165744c 0xc0046f1817 0xc0046f1818}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.272: INFO: Pod "webserver-deployment-c7997dcc8-4hkzf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4hkzf webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-4hkzf 5c866929-b11c-41f9-b994-5c8cf44540b8 19453 0 2020-08-30 17:15:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.0.155/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046f19e7 0xc0046f19e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.272: INFO: Pod "webserver-deployment-c7997dcc8-5bbk7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5bbk7 webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-5bbk7 4b3c9258-4351-4044-b7b4-13926db068f7 19594 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.1.72/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046f1c07 0xc0046f1c08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.272: INFO: Pod "webserver-deployment-c7997dcc8-6gr78" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6gr78 webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-6gr78 ccae3208-34c5-4bc9-ac0d-60a25daa5b9d 19604 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.0.159/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046f1e47 0xc0046f1e48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.272: INFO: Pod "webserver-deployment-c7997dcc8-b8lth" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-b8lth webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-b8lth e15886eb-c8f8-4dc3-948a-7cdfd8ee9aaf 19456 0 2020-08-30 17:15:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.0.156/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046b8077 0xc0046b8078}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.272: INFO: Pod "webserver-deployment-c7997dcc8-fz996" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fz996 webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-fz996 6417c552-c23d-41b5-901a-3b9f8191ed65 19548 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046b8277 0xc0046b8278}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.272: INFO: Pod "webserver-deployment-c7997dcc8-g2t2g" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-g2t2g webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-g2t2g 89ee1bb6-de37-4acc-ae09-9f57cf3eedf0 19608 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.1.74/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046b8477 0xc0046b8478}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.273: INFO: Pod "webserver-deployment-c7997dcc8-g75bp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-g75bp webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-g75bp fd498bf2-b1e4-45fa-ad90-227155676e36 19612 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.1.75/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046b86c7 0xc0046b86c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.273: INFO: Pod "webserver-deployment-c7997dcc8-gzqcn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gzqcn webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-gzqcn 70a264c0-3acc-4fcf-be57-da566256f8c5 19507 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046b8897 0xc0046b8898}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.273: INFO: Pod "webserver-deployment-c7997dcc8-kjtdz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-kjtdz webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-kjtdz 185ee828-93ac-4db8-9b35-125336d52a3f 19450 0 2020-08-30 17:15:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.1.69/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046b8b47 0xc0046b8b48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:,StartTime:2020-08-30 17:15:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.273: INFO: Pod "webserver-deployment-c7997dcc8-lbxl5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lbxl5 webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-lbxl5 01fb03d6-fde4-444c-80c8-439f917617d2 19449 0 2020-08-30 17:15:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.0.154/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046b8df7 0xc0046b8df8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.273: INFO: Pod "webserver-deployment-c7997dcc8-lfz5s" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lfz5s webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-lfz5s f0f19c92-9f4b-416e-ba86-077e8028e0ec 19603 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046b8fc7 0xc0046b8fc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:,StartTime:2020-08-30 17:15:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.274: INFO: Pod "webserver-deployment-c7997dcc8-p2fj7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-p2fj7 webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-p2fj7 91b476e6-ac25-4032-be3b-487e12419e81 19528 0 2020-08-30 17:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046b9287 0xc0046b9288}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 17:15:10.274: INFO: Pod "webserver-deployment-c7997dcc8-x9vzj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-x9vzj webserver-deployment-c7997dcc8- deployment-6481 /api/v1/namespaces/deployment-6481/pods/webserver-deployment-c7997dcc8-x9vzj 8efa2591-1b09-4bed-b08d-e3b1fc4a92bd 19615 0 2020-08-30 17:15:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.1.68/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5d470b43-d410-4c14-808f-a19570286625 0xc0046b9430 0xc0046b9431}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-42hvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-42hvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-42hvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-6rshr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:15:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.153.64,PodIP:172.25.1.68,StartTime:2020-08-30 17:15:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:15:10.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6481" for this suite.

â€¢ [SLOW TEST:8.400 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":166,"skipped":2664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:15:10.290: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:15:10.470: INFO: Waiting up to 5m0s for pod "downwardapi-volume-853f89c9-9240-4ab8-b5de-754d9ea9b2e9" in namespace "projected-7447" to be "success or failure"
Aug 30 17:15:10.476: INFO: Pod "downwardapi-volume-853f89c9-9240-4ab8-b5de-754d9ea9b2e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.498804ms
Aug 30 17:15:12.486: INFO: Pod "downwardapi-volume-853f89c9-9240-4ab8-b5de-754d9ea9b2e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015973423s
Aug 30 17:15:14.491: INFO: Pod "downwardapi-volume-853f89c9-9240-4ab8-b5de-754d9ea9b2e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020914463s
Aug 30 17:15:16.497: INFO: Pod "downwardapi-volume-853f89c9-9240-4ab8-b5de-754d9ea9b2e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027098723s
STEP: Saw pod success
Aug 30 17:15:16.497: INFO: Pod "downwardapi-volume-853f89c9-9240-4ab8-b5de-754d9ea9b2e9" satisfied condition "success or failure"
Aug 30 17:15:16.501: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-6rshr pod downwardapi-volume-853f89c9-9240-4ab8-b5de-754d9ea9b2e9 container client-container: <nil>
STEP: delete the pod
Aug 30 17:15:16.569: INFO: Waiting for pod downwardapi-volume-853f89c9-9240-4ab8-b5de-754d9ea9b2e9 to disappear
Aug 30 17:15:16.575: INFO: Pod downwardapi-volume-853f89c9-9240-4ab8-b5de-754d9ea9b2e9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:15:16.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7447" for this suite.

â€¢ [SLOW TEST:6.298 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":167,"skipped":2688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:15:16.589: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:15:32.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9218" for this suite.

â€¢ [SLOW TEST:16.336 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":168,"skipped":2714,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:15:32.926: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-cd80d43a-e83a-412e-a087-e171cd919837
STEP: Creating a pod to test consume secrets
Aug 30 17:15:33.104: INFO: Waiting up to 5m0s for pod "pod-secrets-2a311052-57ed-4d6e-b05c-84381ef6c902" in namespace "secrets-9947" to be "success or failure"
Aug 30 17:15:33.110: INFO: Pod "pod-secrets-2a311052-57ed-4d6e-b05c-84381ef6c902": Phase="Pending", Reason="", readiness=false. Elapsed: 6.221694ms
Aug 30 17:15:35.117: INFO: Pod "pod-secrets-2a311052-57ed-4d6e-b05c-84381ef6c902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012770344s
STEP: Saw pod success
Aug 30 17:15:35.117: INFO: Pod "pod-secrets-2a311052-57ed-4d6e-b05c-84381ef6c902" satisfied condition "success or failure"
Aug 30 17:15:35.123: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-secrets-2a311052-57ed-4d6e-b05c-84381ef6c902 container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 17:15:35.195: INFO: Waiting for pod pod-secrets-2a311052-57ed-4d6e-b05c-84381ef6c902 to disappear
Aug 30 17:15:35.199: INFO: Pod pod-secrets-2a311052-57ed-4d6e-b05c-84381ef6c902 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:15:35.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9947" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":169,"skipped":2715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:15:35.213: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 30 17:15:35.388: INFO: Waiting up to 5m0s for pod "pod-1c473627-b6a5-4d6e-801b-bffa346ca5af" in namespace "emptydir-2895" to be "success or failure"
Aug 30 17:15:35.401: INFO: Pod "pod-1c473627-b6a5-4d6e-801b-bffa346ca5af": Phase="Pending", Reason="", readiness=false. Elapsed: 13.458977ms
Aug 30 17:15:37.407: INFO: Pod "pod-1c473627-b6a5-4d6e-801b-bffa346ca5af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019230287s
STEP: Saw pod success
Aug 30 17:15:37.407: INFO: Pod "pod-1c473627-b6a5-4d6e-801b-bffa346ca5af" satisfied condition "success or failure"
Aug 30 17:15:37.411: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-1c473627-b6a5-4d6e-801b-bffa346ca5af container test-container: <nil>
STEP: delete the pod
Aug 30 17:15:37.491: INFO: Waiting for pod pod-1c473627-b6a5-4d6e-801b-bffa346ca5af to disappear
Aug 30 17:15:37.496: INFO: Pod pod-1c473627-b6a5-4d6e-801b-bffa346ca5af no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:15:37.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2895" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":170,"skipped":2772,"failed":0}

------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:15:37.510: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-9wvsq in namespace proxy-4269
I0830 17:15:37.697014      23 runners.go:189] Created replication controller with name: proxy-service-9wvsq, namespace: proxy-4269, replica count: 1
I0830 17:15:38.747528      23 runners.go:189] proxy-service-9wvsq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 17:15:39.747955      23 runners.go:189] proxy-service-9wvsq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0830 17:15:40.748170      23 runners.go:189] proxy-service-9wvsq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 17:15:40.755: INFO: setup took 3.081448192s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 55.292595ms)
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 55.461107ms)
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 55.175117ms)
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 55.531876ms)
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 55.529326ms)
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 55.415929ms)
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 55.031416ms)
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 55.319023ms)
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 55.378748ms)
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 55.409143ms)
Aug 30 17:15:40.811: INFO: (0) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 55.508125ms)
Aug 30 17:15:40.817: INFO: (0) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 61.908249ms)
Aug 30 17:15:40.817: INFO: (0) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 61.544929ms)
Aug 30 17:15:40.818: INFO: (0) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 62.118673ms)
Aug 30 17:15:40.818: INFO: (0) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 62.263628ms)
Aug 30 17:15:40.820: INFO: (0) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 64.529459ms)
Aug 30 17:15:40.833: INFO: (1) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 12.189276ms)
Aug 30 17:15:40.833: INFO: (1) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 12.772096ms)
Aug 30 17:15:40.833: INFO: (1) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 12.123468ms)
Aug 30 17:15:40.833: INFO: (1) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 12.844396ms)
Aug 30 17:15:40.833: INFO: (1) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 12.684967ms)
Aug 30 17:15:40.833: INFO: (1) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 12.567932ms)
Aug 30 17:15:40.833: INFO: (1) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 12.626477ms)
Aug 30 17:15:40.835: INFO: (1) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 14.043488ms)
Aug 30 17:15:40.837: INFO: (1) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 16.332829ms)
Aug 30 17:15:40.837: INFO: (1) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 16.747056ms)
Aug 30 17:15:40.837: INFO: (1) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 16.552193ms)
Aug 30 17:15:40.838: INFO: (1) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 17.210329ms)
Aug 30 17:15:40.838: INFO: (1) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 16.708935ms)
Aug 30 17:15:40.838: INFO: (1) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 17.514778ms)
Aug 30 17:15:40.838: INFO: (1) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 17.437199ms)
Aug 30 17:15:40.841: INFO: (1) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 20.298633ms)
Aug 30 17:15:40.855: INFO: (2) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 13.997734ms)
Aug 30 17:15:40.855: INFO: (2) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 13.74198ms)
Aug 30 17:15:40.856: INFO: (2) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 14.301493ms)
Aug 30 17:15:40.856: INFO: (2) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 13.837758ms)
Aug 30 17:15:40.856: INFO: (2) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 14.085813ms)
Aug 30 17:15:40.856: INFO: (2) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 14.70654ms)
Aug 30 17:15:40.858: INFO: (2) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 16.883174ms)
Aug 30 17:15:40.859: INFO: (2) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 16.956622ms)
Aug 30 17:15:40.859: INFO: (2) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 16.867176ms)
Aug 30 17:15:40.859: INFO: (2) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 16.947503ms)
Aug 30 17:15:40.859: INFO: (2) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 17.020736ms)
Aug 30 17:15:40.859: INFO: (2) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 17.422567ms)
Aug 30 17:15:40.859: INFO: (2) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 18.033669ms)
Aug 30 17:15:40.859: INFO: (2) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 17.38063ms)
Aug 30 17:15:40.859: INFO: (2) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 17.640874ms)
Aug 30 17:15:40.863: INFO: (2) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 21.370564ms)
Aug 30 17:15:40.871: INFO: (3) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 8.2623ms)
Aug 30 17:15:40.873: INFO: (3) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 10.147238ms)
Aug 30 17:15:40.873: INFO: (3) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 10.14417ms)
Aug 30 17:15:40.873: INFO: (3) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 10.244407ms)
Aug 30 17:15:40.874: INFO: (3) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 10.871874ms)
Aug 30 17:15:40.874: INFO: (3) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 10.953524ms)
Aug 30 17:15:40.874: INFO: (3) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 11.238809ms)
Aug 30 17:15:40.874: INFO: (3) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 11.014796ms)
Aug 30 17:15:40.874: INFO: (3) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 10.944493ms)
Aug 30 17:15:40.874: INFO: (3) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 11.279ms)
Aug 30 17:15:40.876: INFO: (3) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 12.722382ms)
Aug 30 17:15:40.876: INFO: (3) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 12.951222ms)
Aug 30 17:15:40.877: INFO: (3) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 13.907972ms)
Aug 30 17:15:40.878: INFO: (3) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 15.145606ms)
Aug 30 17:15:40.878: INFO: (3) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 15.101726ms)
Aug 30 17:15:40.879: INFO: (3) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 15.682346ms)
Aug 30 17:15:40.890: INFO: (4) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 11.506743ms)
Aug 30 17:15:40.890: INFO: (4) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 11.00304ms)
Aug 30 17:15:40.890: INFO: (4) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 11.60588ms)
Aug 30 17:15:40.890: INFO: (4) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 10.74146ms)
Aug 30 17:15:40.890: INFO: (4) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 11.343776ms)
Aug 30 17:15:40.890: INFO: (4) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 10.9826ms)
Aug 30 17:15:40.890: INFO: (4) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 11.287717ms)
Aug 30 17:15:40.890: INFO: (4) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 11.759739ms)
Aug 30 17:15:40.890: INFO: (4) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 11.258425ms)
Aug 30 17:15:40.890: INFO: (4) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 11.515753ms)
Aug 30 17:15:40.893: INFO: (4) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 14.22182ms)
Aug 30 17:15:40.894: INFO: (4) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 14.579136ms)
Aug 30 17:15:40.894: INFO: (4) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 14.347614ms)
Aug 30 17:15:40.894: INFO: (4) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 14.380459ms)
Aug 30 17:15:40.949: INFO: (4) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 69.291409ms)
Aug 30 17:15:40.949: INFO: (4) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 69.372985ms)
Aug 30 17:15:40.961: INFO: (5) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 11.965998ms)
Aug 30 17:15:40.961: INFO: (5) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 12.009181ms)
Aug 30 17:15:40.961: INFO: (5) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 12.096083ms)
Aug 30 17:15:40.961: INFO: (5) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 11.953383ms)
Aug 30 17:15:40.961: INFO: (5) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 12.113919ms)
Aug 30 17:15:40.961: INFO: (5) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 12.106247ms)
Aug 30 17:15:40.961: INFO: (5) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 11.929259ms)
Aug 30 17:15:40.961: INFO: (5) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 12.019159ms)
Aug 30 17:15:40.963: INFO: (5) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 13.89746ms)
Aug 30 17:15:40.963: INFO: (5) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 13.871851ms)
Aug 30 17:15:40.963: INFO: (5) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 14.062104ms)
Aug 30 17:15:40.965: INFO: (5) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 15.836325ms)
Aug 30 17:15:40.967: INFO: (5) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 17.658701ms)
Aug 30 17:15:40.967: INFO: (5) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 17.619377ms)
Aug 30 17:15:40.967: INFO: (5) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 17.684458ms)
Aug 30 17:15:40.967: INFO: (5) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 17.741951ms)
Aug 30 17:15:40.977: INFO: (6) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 9.845138ms)
Aug 30 17:15:40.977: INFO: (6) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 10.045084ms)
Aug 30 17:15:40.977: INFO: (6) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 10.004217ms)
Aug 30 17:15:40.977: INFO: (6) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 10.060649ms)
Aug 30 17:15:40.979: INFO: (6) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 11.921553ms)
Aug 30 17:15:40.979: INFO: (6) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 12.022087ms)
Aug 30 17:15:40.980: INFO: (6) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 12.999898ms)
Aug 30 17:15:40.980: INFO: (6) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 13.267485ms)
Aug 30 17:15:40.980: INFO: (6) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 13.381566ms)
Aug 30 17:15:40.980: INFO: (6) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 13.620613ms)
Aug 30 17:15:40.982: INFO: (6) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 15.474571ms)
Aug 30 17:15:40.983: INFO: (6) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 15.647318ms)
Aug 30 17:15:40.984: INFO: (6) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 17.285855ms)
Aug 30 17:15:40.984: INFO: (6) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 17.293668ms)
Aug 30 17:15:40.984: INFO: (6) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 17.289365ms)
Aug 30 17:15:40.984: INFO: (6) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 17.346754ms)
Aug 30 17:15:40.995: INFO: (7) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 10.286955ms)
Aug 30 17:15:40.995: INFO: (7) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 10.197303ms)
Aug 30 17:15:40.995: INFO: (7) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 10.367459ms)
Aug 30 17:15:40.995: INFO: (7) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 10.397277ms)
Aug 30 17:15:40.995: INFO: (7) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 10.343953ms)
Aug 30 17:15:40.995: INFO: (7) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 10.943902ms)
Aug 30 17:15:40.996: INFO: (7) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 11.424027ms)
Aug 30 17:15:40.997: INFO: (7) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 12.074236ms)
Aug 30 17:15:40.997: INFO: (7) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 11.905012ms)
Aug 30 17:15:40.997: INFO: (7) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 11.93122ms)
Aug 30 17:15:40.997: INFO: (7) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 12.376721ms)
Aug 30 17:15:41.049: INFO: (7) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 64.566214ms)
Aug 30 17:15:41.049: INFO: (7) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 64.300314ms)
Aug 30 17:15:41.051: INFO: (7) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 66.271094ms)
Aug 30 17:15:41.051: INFO: (7) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 66.456563ms)
Aug 30 17:15:41.052: INFO: (7) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 66.890005ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 14.676118ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 14.500391ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 14.147284ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 14.023738ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 14.430358ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 14.66756ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 14.82776ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 14.374748ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 14.253334ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 14.528493ms)
Aug 30 17:15:41.066: INFO: (8) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 14.206794ms)
Aug 30 17:15:41.067: INFO: (8) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 15.242471ms)
Aug 30 17:15:41.070: INFO: (8) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 17.796402ms)
Aug 30 17:15:41.070: INFO: (8) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 17.937886ms)
Aug 30 17:15:41.070: INFO: (8) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 17.881218ms)
Aug 30 17:15:41.072: INFO: (8) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 19.074874ms)
Aug 30 17:15:41.081: INFO: (9) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 8.754789ms)
Aug 30 17:15:41.081: INFO: (9) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 8.885706ms)
Aug 30 17:15:41.081: INFO: (9) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 9.019476ms)
Aug 30 17:15:41.081: INFO: (9) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 9.605693ms)
Aug 30 17:15:41.081: INFO: (9) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 9.394149ms)
Aug 30 17:15:41.081: INFO: (9) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 9.275028ms)
Aug 30 17:15:41.082: INFO: (9) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 9.932788ms)
Aug 30 17:15:41.082: INFO: (9) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 10.035001ms)
Aug 30 17:15:41.082: INFO: (9) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 10.910486ms)
Aug 30 17:15:41.082: INFO: (9) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 10.607868ms)
Aug 30 17:15:41.151: INFO: (9) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 79.071785ms)
Aug 30 17:15:41.151: INFO: (9) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 79.31495ms)
Aug 30 17:15:41.151: INFO: (9) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 79.0615ms)
Aug 30 17:15:41.151: INFO: (9) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 78.982617ms)
Aug 30 17:15:41.151: INFO: (9) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 79.642246ms)
Aug 30 17:15:41.153: INFO: (9) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 80.326231ms)
Aug 30 17:15:41.164: INFO: (10) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 10.830333ms)
Aug 30 17:15:41.164: INFO: (10) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 10.834875ms)
Aug 30 17:15:41.164: INFO: (10) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 10.701947ms)
Aug 30 17:15:41.164: INFO: (10) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 10.437597ms)
Aug 30 17:15:41.164: INFO: (10) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 10.387226ms)
Aug 30 17:15:41.164: INFO: (10) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 10.363391ms)
Aug 30 17:15:41.164: INFO: (10) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 10.64367ms)
Aug 30 17:15:41.164: INFO: (10) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 10.970383ms)
Aug 30 17:15:41.164: INFO: (10) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 11.000652ms)
Aug 30 17:15:41.166: INFO: (10) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 12.646118ms)
Aug 30 17:15:41.166: INFO: (10) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 12.699324ms)
Aug 30 17:15:41.167: INFO: (10) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 13.818686ms)
Aug 30 17:15:41.167: INFO: (10) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 13.86985ms)
Aug 30 17:15:41.167: INFO: (10) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 13.725175ms)
Aug 30 17:15:41.167: INFO: (10) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 13.586659ms)
Aug 30 17:15:41.167: INFO: (10) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 14.461871ms)
Aug 30 17:15:41.178: INFO: (11) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 10.906257ms)
Aug 30 17:15:41.179: INFO: (11) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 11.086138ms)
Aug 30 17:15:41.179: INFO: (11) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 11.264594ms)
Aug 30 17:15:41.179: INFO: (11) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 10.856153ms)
Aug 30 17:15:41.179: INFO: (11) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 11.016539ms)
Aug 30 17:15:41.180: INFO: (11) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 12.468631ms)
Aug 30 17:15:41.180: INFO: (11) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 12.411062ms)
Aug 30 17:15:41.180: INFO: (11) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 12.692482ms)
Aug 30 17:15:41.180: INFO: (11) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 12.258149ms)
Aug 30 17:15:41.180: INFO: (11) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 12.850746ms)
Aug 30 17:15:41.180: INFO: (11) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 12.826147ms)
Aug 30 17:15:41.182: INFO: (11) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 14.287773ms)
Aug 30 17:15:41.182: INFO: (11) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 14.43057ms)
Aug 30 17:15:41.182: INFO: (11) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 13.974868ms)
Aug 30 17:15:41.182: INFO: (11) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 14.011938ms)
Aug 30 17:15:41.184: INFO: (11) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 15.931239ms)
Aug 30 17:15:41.196: INFO: (12) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 11.57295ms)
Aug 30 17:15:41.196: INFO: (12) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 12.152778ms)
Aug 30 17:15:41.196: INFO: (12) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 12.097499ms)
Aug 30 17:15:41.196: INFO: (12) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 12.002259ms)
Aug 30 17:15:41.196: INFO: (12) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 11.800768ms)
Aug 30 17:15:41.249: INFO: (12) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 65.244225ms)
Aug 30 17:15:41.249: INFO: (12) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 64.399893ms)
Aug 30 17:15:41.249: INFO: (12) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 64.351797ms)
Aug 30 17:15:41.249: INFO: (12) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 64.59599ms)
Aug 30 17:15:41.249: INFO: (12) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 65.104824ms)
Aug 30 17:15:41.249: INFO: (12) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 64.628269ms)
Aug 30 17:15:41.253: INFO: (12) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 69.264626ms)
Aug 30 17:15:41.253: INFO: (12) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 69.226599ms)
Aug 30 17:15:41.254: INFO: (12) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 69.538214ms)
Aug 30 17:15:41.254: INFO: (12) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 69.010297ms)
Aug 30 17:15:41.256: INFO: (12) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 72.094679ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 12.715056ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 12.791438ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 12.957208ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 12.751387ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 12.858228ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 12.942005ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 12.984448ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 12.852064ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 12.900512ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 13.160968ms)
Aug 30 17:15:41.269: INFO: (13) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 13.065116ms)
Aug 30 17:15:41.273: INFO: (13) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 17.164618ms)
Aug 30 17:15:41.273: INFO: (13) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 17.135606ms)
Aug 30 17:15:41.273: INFO: (13) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 17.231616ms)
Aug 30 17:15:41.273: INFO: (13) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 17.095982ms)
Aug 30 17:15:41.273: INFO: (13) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 17.18489ms)
Aug 30 17:15:41.283: INFO: (14) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 9.756512ms)
Aug 30 17:15:41.283: INFO: (14) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 9.754309ms)
Aug 30 17:15:41.283: INFO: (14) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 9.794106ms)
Aug 30 17:15:41.283: INFO: (14) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 9.762324ms)
Aug 30 17:15:41.283: INFO: (14) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 9.91277ms)
Aug 30 17:15:41.283: INFO: (14) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 9.781502ms)
Aug 30 17:15:41.283: INFO: (14) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 9.895165ms)
Aug 30 17:15:41.284: INFO: (14) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 10.816323ms)
Aug 30 17:15:41.284: INFO: (14) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 10.965605ms)
Aug 30 17:15:41.284: INFO: (14) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 10.802395ms)
Aug 30 17:15:41.292: INFO: (14) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 18.777379ms)
Aug 30 17:15:41.293: INFO: (14) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 19.105807ms)
Aug 30 17:15:41.293: INFO: (14) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 19.13377ms)
Aug 30 17:15:41.293: INFO: (14) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 19.303629ms)
Aug 30 17:15:41.293: INFO: (14) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 19.376015ms)
Aug 30 17:15:41.293: INFO: (14) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 19.530142ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 59.038043ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 59.19108ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 58.8853ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 59.065ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 59.447423ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 59.521249ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 59.285047ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 59.356348ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 58.885708ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 59.22126ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 59.840846ms)
Aug 30 17:15:41.353: INFO: (15) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 59.212405ms)
Aug 30 17:15:41.357: INFO: (15) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 63.651302ms)
Aug 30 17:15:41.357: INFO: (15) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 63.002227ms)
Aug 30 17:15:41.357: INFO: (15) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 63.456629ms)
Aug 30 17:15:41.357: INFO: (15) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 63.923251ms)
Aug 30 17:15:41.369: INFO: (16) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 11.928258ms)
Aug 30 17:15:41.369: INFO: (16) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 12.094395ms)
Aug 30 17:15:41.369: INFO: (16) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 11.988511ms)
Aug 30 17:15:41.369: INFO: (16) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 11.964792ms)
Aug 30 17:15:41.369: INFO: (16) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 11.981988ms)
Aug 30 17:15:41.369: INFO: (16) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 12.102815ms)
Aug 30 17:15:41.369: INFO: (16) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 12.222212ms)
Aug 30 17:15:41.370: INFO: (16) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 12.702972ms)
Aug 30 17:15:41.373: INFO: (16) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 16.036069ms)
Aug 30 17:15:41.373: INFO: (16) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 16.004771ms)
Aug 30 17:15:41.373: INFO: (16) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 16.00063ms)
Aug 30 17:15:41.373: INFO: (16) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 15.979915ms)
Aug 30 17:15:41.373: INFO: (16) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 16.114104ms)
Aug 30 17:15:41.374: INFO: (16) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 16.419849ms)
Aug 30 17:15:41.374: INFO: (16) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 16.398462ms)
Aug 30 17:15:41.378: INFO: (16) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 20.78876ms)
Aug 30 17:15:41.389: INFO: (17) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 11.19479ms)
Aug 30 17:15:41.389: INFO: (17) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 11.317557ms)
Aug 30 17:15:41.389: INFO: (17) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 11.324859ms)
Aug 30 17:15:41.390: INFO: (17) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 11.685834ms)
Aug 30 17:15:41.390: INFO: (17) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 11.82859ms)
Aug 30 17:15:41.390: INFO: (17) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 11.800443ms)
Aug 30 17:15:41.390: INFO: (17) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 11.638098ms)
Aug 30 17:15:41.390: INFO: (17) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 11.702824ms)
Aug 30 17:15:41.390: INFO: (17) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 11.89906ms)
Aug 30 17:15:41.449: INFO: (17) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 71.468284ms)
Aug 30 17:15:41.449: INFO: (17) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 71.416649ms)
Aug 30 17:15:41.450: INFO: (17) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 71.374385ms)
Aug 30 17:15:41.454: INFO: (17) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 75.574083ms)
Aug 30 17:15:41.454: INFO: (17) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 75.685901ms)
Aug 30 17:15:41.454: INFO: (17) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 75.736185ms)
Aug 30 17:15:41.456: INFO: (17) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 77.549047ms)
Aug 30 17:15:41.467: INFO: (18) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 11.598233ms)
Aug 30 17:15:41.467: INFO: (18) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 11.591151ms)
Aug 30 17:15:41.467: INFO: (18) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 11.580405ms)
Aug 30 17:15:41.467: INFO: (18) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 11.665908ms)
Aug 30 17:15:41.467: INFO: (18) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 11.725961ms)
Aug 30 17:15:41.468: INFO: (18) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 11.890983ms)
Aug 30 17:15:41.468: INFO: (18) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 12.272112ms)
Aug 30 17:15:41.468: INFO: (18) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 12.329538ms)
Aug 30 17:15:41.469: INFO: (18) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 13.269398ms)
Aug 30 17:15:41.471: INFO: (18) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 14.707156ms)
Aug 30 17:15:41.471: INFO: (18) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 14.66879ms)
Aug 30 17:15:41.471: INFO: (18) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 14.6562ms)
Aug 30 17:15:41.471: INFO: (18) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 14.964936ms)
Aug 30 17:15:41.471: INFO: (18) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 15.130802ms)
Aug 30 17:15:41.471: INFO: (18) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 15.015835ms)
Aug 30 17:15:41.471: INFO: (18) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 15.087768ms)
Aug 30 17:15:41.482: INFO: (19) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7/proxy/rewriteme">test</a> (200; 10.817148ms)
Aug 30 17:15:41.483: INFO: (19) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:460/proxy/: tls baz (200; 11.509774ms)
Aug 30 17:15:41.483: INFO: (19) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:443/proxy/tlsrewritem... (200; 11.585422ms)
Aug 30 17:15:41.483: INFO: (19) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">test<... (200; 11.58009ms)
Aug 30 17:15:41.483: INFO: (19) /api/v1/namespaces/proxy-4269/pods/https:proxy-service-9wvsq-2m8h7:462/proxy/: tls qux (200; 11.512016ms)
Aug 30 17:15:41.483: INFO: (19) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 11.483573ms)
Aug 30 17:15:41.483: INFO: (19) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 12.117451ms)
Aug 30 17:15:41.483: INFO: (19) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:1080/proxy/rewriteme">... (200; 12.336066ms)
Aug 30 17:15:41.483: INFO: (19) /api/v1/namespaces/proxy-4269/pods/proxy-service-9wvsq-2m8h7:162/proxy/: bar (200; 12.232334ms)
Aug 30 17:15:41.483: INFO: (19) /api/v1/namespaces/proxy-4269/pods/http:proxy-service-9wvsq-2m8h7:160/proxy/: foo (200; 12.312142ms)
Aug 30 17:15:41.484: INFO: (19) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname1/proxy/: tls baz (200; 13.097417ms)
Aug 30 17:15:41.484: INFO: (19) /api/v1/namespaces/proxy-4269/services/https:proxy-service-9wvsq:tlsportname2/proxy/: tls qux (200; 13.129519ms)
Aug 30 17:15:41.486: INFO: (19) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname1/proxy/: foo (200; 15.301113ms)
Aug 30 17:15:41.486: INFO: (19) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname2/proxy/: bar (200; 15.338158ms)
Aug 30 17:15:41.486: INFO: (19) /api/v1/namespaces/proxy-4269/services/http:proxy-service-9wvsq:portname1/proxy/: foo (200; 15.417041ms)
Aug 30 17:15:41.486: INFO: (19) /api/v1/namespaces/proxy-4269/services/proxy-service-9wvsq:portname2/proxy/: bar (200; 15.492965ms)
STEP: deleting ReplicationController proxy-service-9wvsq in namespace proxy-4269, will wait for the garbage collector to delete the pods
Aug 30 17:15:41.553: INFO: Deleting ReplicationController proxy-service-9wvsq took: 11.810098ms
Aug 30 17:15:42.054: INFO: Terminating ReplicationController proxy-service-9wvsq pods took: 500.631149ms
[AfterEach] version v1
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:15:54.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4269" for this suite.

â€¢ [SLOW TEST:16.858 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":171,"skipped":2772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:15:54.369: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:15:55.232: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 17:15:57.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404555, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404555, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404555, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404555, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:16:00.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:16:00.283: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:16:01.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3020" for this suite.
STEP: Destroying namespace "webhook-3020-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.714 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":172,"skipped":2803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:16:02.083: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5030
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:16:02.261: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:16:03.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5030" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":173,"skipped":2841,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:16:03.354: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0830 17:16:04.641775      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 30 17:16:04.641: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:16:04.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5080" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":174,"skipped":2849,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:16:04.660: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8032
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 30 17:16:06.853: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:16:06.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8032" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":175,"skipped":2861,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:16:06.897: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-2343cc3c-147f-4370-a70c-11198a14fc41 in namespace container-probe-8778
Aug 30 17:16:09.083: INFO: Started pod busybox-2343cc3c-147f-4370-a70c-11198a14fc41 in namespace container-probe-8778
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 17:16:09.087: INFO: Initial restart count of pod busybox-2343cc3c-147f-4370-a70c-11198a14fc41 is 0
Aug 30 17:17:03.255: INFO: Restart count of pod container-probe-8778/busybox-2343cc3c-147f-4370-a70c-11198a14fc41 is now 1 (54.168115482s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:17:03.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8778" for this suite.

â€¢ [SLOW TEST:56.399 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":176,"skipped":2866,"failed":0}
SSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:17:03.296: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-5282
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Aug 30 17:17:03.476: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 17:18:03.505: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:18:03.510: INFO: Starting informer...
STEP: Starting pods...
Aug 30 17:18:03.736: INFO: Pod1 is running on adoring-wozniak-54dcfd79fc-948mf. Tainting Node
Aug 30 17:18:05.964: INFO: Pod2 is running on adoring-wozniak-54dcfd79fc-948mf. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 30 17:18:12.588: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 30 17:18:32.734: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:18:32.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5282" for this suite.

â€¢ [SLOW TEST:89.489 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":177,"skipped":2870,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:18:32.788: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-1c3b662a-1664-4e34-859d-f0795133201d
STEP: Creating a pod to test consume secrets
Aug 30 17:18:32.966: INFO: Waiting up to 5m0s for pod "pod-secrets-a4ae5a9c-c38f-4ed8-ad7a-4d1ce45e5ae6" in namespace "secrets-6642" to be "success or failure"
Aug 30 17:18:32.971: INFO: Pod "pod-secrets-a4ae5a9c-c38f-4ed8-ad7a-4d1ce45e5ae6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.66063ms
Aug 30 17:18:34.976: INFO: Pod "pod-secrets-a4ae5a9c-c38f-4ed8-ad7a-4d1ce45e5ae6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009804427s
Aug 30 17:18:36.982: INFO: Pod "pod-secrets-a4ae5a9c-c38f-4ed8-ad7a-4d1ce45e5ae6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015769934s
STEP: Saw pod success
Aug 30 17:18:36.982: INFO: Pod "pod-secrets-a4ae5a9c-c38f-4ed8-ad7a-4d1ce45e5ae6" satisfied condition "success or failure"
Aug 30 17:18:36.987: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-secrets-a4ae5a9c-c38f-4ed8-ad7a-4d1ce45e5ae6 container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 17:18:37.055: INFO: Waiting for pod pod-secrets-a4ae5a9c-c38f-4ed8-ad7a-4d1ce45e5ae6 to disappear
Aug 30 17:18:37.060: INFO: Pod pod-secrets-a4ae5a9c-c38f-4ed8-ad7a-4d1ce45e5ae6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:18:37.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6642" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":178,"skipped":2890,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:18:37.075: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9319
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 30 17:19:07.801: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0830 17:19:07.801010      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:19:07.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9319" for this suite.

â€¢ [SLOW TEST:30.742 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":179,"skipped":2895,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:19:07.820: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7509
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7509
I0830 17:19:08.038754      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-7509, replica count: 2
Aug 30 17:19:11.089: INFO: Creating new exec pod
I0830 17:19:11.089228      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 17:19:14.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-7509 execpodhsdxw -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 30 17:19:14.657: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 30 17:19:14.657: INFO: stdout: ""
Aug 30 17:19:14.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-7509 execpodhsdxw -- /bin/sh -x -c nc -zv -t -w 2 10.240.23.168 80'
Aug 30 17:19:15.221: INFO: stderr: "+ nc -zv -t -w 2 10.240.23.168 80\nConnection to 10.240.23.168 80 port [tcp/http] succeeded!\n"
Aug 30 17:19:15.221: INFO: stdout: ""
Aug 30 17:19:15.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-7509 execpodhsdxw -- /bin/sh -x -c nc -zv -t -w 2 46.101.153.64 30694'
Aug 30 17:19:15.748: INFO: stderr: "+ nc -zv -t -w 2 46.101.153.64 30694\nConnection to 46.101.153.64 30694 port [tcp/30694] succeeded!\n"
Aug 30 17:19:15.748: INFO: stdout: ""
Aug 30 17:19:15.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-7509 execpodhsdxw -- /bin/sh -x -c nc -zv -t -w 2 46.101.135.210 30694'
Aug 30 17:19:16.306: INFO: stderr: "+ nc -zv -t -w 2 46.101.135.210 30694\nConnection to 46.101.135.210 30694 port [tcp/30694] succeeded!\n"
Aug 30 17:19:16.306: INFO: stdout: ""
Aug 30 17:19:16.306: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:19:16.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7509" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:8.535 seconds]
[sig-network] Services
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":180,"skipped":2897,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:19:16.354: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-72
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:19:21.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-72" for this suite.

â€¢ [SLOW TEST:5.516 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":181,"skipped":2898,"failed":0}
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:19:21.871: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7569.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7569.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 17:19:32.772: INFO: DNS probes using dns-7569/dns-test-2416c6ae-f8d1-4eb8-bb34-1b604d707dcf succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:19:32.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7569" for this suite.

â€¢ [SLOW TEST:10.933 seconds]
[sig-network] DNS
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":182,"skipped":2898,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:19:32.805: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:19:33.271: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 17:19:35.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404773, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404773, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404773, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404773, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:19:38.306: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:19:38.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2355" for this suite.
STEP: Destroying namespace "webhook-2355-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.109 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":183,"skipped":2913,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:19:38.916: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-766
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Aug 30 17:19:39.096: INFO: Found 0 stateful pods, waiting for 3
Aug 30 17:19:49.104: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:19:49.104: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:19:49.104: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 30 17:19:49.148: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 30 17:19:59.194: INFO: Updating stateful set ss2
Aug 30 17:19:59.206: INFO: Waiting for Pod statefulset-766/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Aug 30 17:20:09.314: INFO: Found 2 stateful pods, waiting for 3
Aug 30 17:20:19.323: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:20:19.323: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:20:19.323: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 30 17:20:19.358: INFO: Updating stateful set ss2
Aug 30 17:20:19.371: INFO: Waiting for Pod statefulset-766/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 30 17:20:29.388: INFO: Waiting for Pod statefulset-766/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 30 17:20:39.406: INFO: Updating stateful set ss2
Aug 30 17:20:39.418: INFO: Waiting for StatefulSet statefulset-766/ss2 to complete update
Aug 30 17:20:39.418: INFO: Waiting for Pod statefulset-766/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 30 17:20:49.432: INFO: Deleting all statefulset in ns statefulset-766
Aug 30 17:20:49.437: INFO: Scaling statefulset ss2 to 0
Aug 30 17:20:59.460: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 17:20:59.467: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:20:59.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-766" for this suite.

â€¢ [SLOW TEST:80.588 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":184,"skipped":2913,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:20:59.507: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:20:59.695: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf1468dd-22bd-419f-8159-f41d23ae3c63" in namespace "projected-4304" to be "success or failure"
Aug 30 17:20:59.699: INFO: Pod "downwardapi-volume-bf1468dd-22bd-419f-8159-f41d23ae3c63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.289494ms
Aug 30 17:21:01.705: INFO: Pod "downwardapi-volume-bf1468dd-22bd-419f-8159-f41d23ae3c63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010243467s
STEP: Saw pod success
Aug 30 17:21:01.705: INFO: Pod "downwardapi-volume-bf1468dd-22bd-419f-8159-f41d23ae3c63" satisfied condition "success or failure"
Aug 30 17:21:01.710: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-bf1468dd-22bd-419f-8159-f41d23ae3c63 container client-container: <nil>
STEP: delete the pod
Aug 30 17:21:01.778: INFO: Waiting for pod downwardapi-volume-bf1468dd-22bd-419f-8159-f41d23ae3c63 to disappear
Aug 30 17:21:01.782: INFO: Pod downwardapi-volume-bf1468dd-22bd-419f-8159-f41d23ae3c63 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:01.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4304" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":2948,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:01.800: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5789
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:21:01.962: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:02.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5789" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":186,"skipped":2951,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:02.539: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9014
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-49819cd2-3c1a-4c2f-9897-143652697abd
STEP: Creating a pod to test consume secrets
Aug 30 17:21:02.722: INFO: Waiting up to 5m0s for pod "pod-secrets-7b8e2088-f11c-445f-8f31-9f26bf43108a" in namespace "secrets-9014" to be "success or failure"
Aug 30 17:21:02.727: INFO: Pod "pod-secrets-7b8e2088-f11c-445f-8f31-9f26bf43108a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.049148ms
Aug 30 17:21:04.733: INFO: Pod "pod-secrets-7b8e2088-f11c-445f-8f31-9f26bf43108a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011013358s
STEP: Saw pod success
Aug 30 17:21:04.733: INFO: Pod "pod-secrets-7b8e2088-f11c-445f-8f31-9f26bf43108a" satisfied condition "success or failure"
Aug 30 17:21:04.740: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-secrets-7b8e2088-f11c-445f-8f31-9f26bf43108a container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 17:21:04.809: INFO: Waiting for pod pod-secrets-7b8e2088-f11c-445f-8f31-9f26bf43108a to disappear
Aug 30 17:21:04.815: INFO: Pod pod-secrets-7b8e2088-f11c-445f-8f31-9f26bf43108a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:04.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9014" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":187,"skipped":3002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:04.832: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-759
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:21:05.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 version'
Aug 30 17:21:05.083: INFO: stderr: ""
Aug 30 17:21:05.083: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.9\", GitCommit:\"4fb7ed12476d57b8437ada90b4f93b17ffaeed99\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:18:16Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.9\", GitCommit:\"4fb7ed12476d57b8437ada90b4f93b17ffaeed99\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:10:45Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:05.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-759" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":188,"skipped":3035,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:05.100: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:21:05.475: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 17:21:07.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404865, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404865, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404865, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734404865, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:21:10.513: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 30 17:21:12.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 attach --namespace=webhook-8903 to-be-attached-pod -i -c=container1'
Aug 30 17:21:12.803: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:12.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8903" for this suite.
STEP: Destroying namespace "webhook-8903-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.802 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":189,"skipped":3048,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:12.902: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Aug 30 17:21:13.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 cluster-info'
Aug 30 17:21:13.129: INFO: stderr: ""
Aug 30 17:21:13.129: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\x1b[0;32mkube-dns\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns-tcp/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:13.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5628" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":190,"skipped":3068,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:13.146: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-e423e462-ca93-48d7-9ee5-b17bc303a653
STEP: Creating a pod to test consume configMaps
Aug 30 17:21:13.324: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c123d2c0-21e2-46a8-a726-c1e2e763b978" in namespace "projected-6994" to be "success or failure"
Aug 30 17:21:13.330: INFO: Pod "pod-projected-configmaps-c123d2c0-21e2-46a8-a726-c1e2e763b978": Phase="Pending", Reason="", readiness=false. Elapsed: 5.171141ms
Aug 30 17:21:15.335: INFO: Pod "pod-projected-configmaps-c123d2c0-21e2-46a8-a726-c1e2e763b978": Phase="Running", Reason="", readiness=true. Elapsed: 2.010466664s
Aug 30 17:21:17.341: INFO: Pod "pod-projected-configmaps-c123d2c0-21e2-46a8-a726-c1e2e763b978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016386575s
STEP: Saw pod success
Aug 30 17:21:17.341: INFO: Pod "pod-projected-configmaps-c123d2c0-21e2-46a8-a726-c1e2e763b978" satisfied condition "success or failure"
Aug 30 17:21:17.345: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-projected-configmaps-c123d2c0-21e2-46a8-a726-c1e2e763b978 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 17:21:17.414: INFO: Waiting for pod pod-projected-configmaps-c123d2c0-21e2-46a8-a726-c1e2e763b978 to disappear
Aug 30 17:21:17.419: INFO: Pod pod-projected-configmaps-c123d2c0-21e2-46a8-a726-c1e2e763b978 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:17.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6994" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":191,"skipped":3073,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:17.436: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2605
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2605
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2605
I0830 17:21:17.667426      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-2605, replica count: 2
Aug 30 17:21:20.717: INFO: Creating new exec pod
I0830 17:21:20.717750      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 17:21:25.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-2605 execpodcsgvc -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 30 17:21:26.313: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 30 17:21:26.313: INFO: stdout: ""
Aug 30 17:21:26.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-2605 execpodcsgvc -- /bin/sh -x -c nc -zv -t -w 2 10.240.26.73 80'
Aug 30 17:21:26.852: INFO: stderr: "+ nc -zv -t -w 2 10.240.26.73 80\nConnection to 10.240.26.73 80 port [tcp/http] succeeded!\n"
Aug 30 17:21:26.852: INFO: stdout: ""
Aug 30 17:21:26.852: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:26.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2605" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:9.459 seconds]
[sig-network] Services
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":192,"skipped":3094,"failed":0}
S
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:26.895: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5910
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5910 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5910;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5910 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5910;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5910.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5910.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5910.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5910.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5910.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5910.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5910.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5910.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5910.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5910.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5910.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5910.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5910.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 109.21.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.21.109_udp@PTR;check="$$(dig +tcp +noall +answer +search 109.21.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.21.109_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5910 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5910;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5910 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5910;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5910.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5910.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5910.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5910.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5910.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5910.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5910.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5910.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5910.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5910.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5910.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5910.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5910.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 109.21.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.21.109_udp@PTR;check="$$(dig +tcp +noall +answer +search 109.21.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.21.109_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 17:21:29.229: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.264: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.275: INFO: Unable to read wheezy_udp@dns-test-service.dns-5910 from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.320: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5910 from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.330: INFO: Unable to read wheezy_udp@dns-test-service.dns-5910.svc from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.339: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5910.svc from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.349: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5910.svc from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.359: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5910.svc from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.885: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.897: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.906: INFO: Unable to read jessie_udp@dns-test-service.dns-5910 from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.915: INFO: Unable to read jessie_tcp@dns-test-service.dns-5910 from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.923: INFO: Unable to read jessie_udp@dns-test-service.dns-5910.svc from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.933: INFO: Unable to read jessie_tcp@dns-test-service.dns-5910.svc from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.951: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5910.svc from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:29.961: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5910.svc from pod dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7: the server could not find the requested resource (get pods dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7)
Aug 30 17:21:30.399: INFO: Lookups using dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5910 wheezy_tcp@dns-test-service.dns-5910 wheezy_udp@dns-test-service.dns-5910.svc wheezy_tcp@dns-test-service.dns-5910.svc wheezy_udp@_http._tcp.dns-test-service.dns-5910.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5910.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5910 jessie_tcp@dns-test-service.dns-5910 jessie_udp@dns-test-service.dns-5910.svc jessie_tcp@dns-test-service.dns-5910.svc jessie_udp@_http._tcp.dns-test-service.dns-5910.svc jessie_tcp@_http._tcp.dns-test-service.dns-5910.svc]

Aug 30 17:21:37.712: INFO: DNS probes using dns-5910/dns-test-7e453f47-16a6-40f0-86ba-a4deee87dfc7 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:37.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5910" for this suite.

â€¢ [SLOW TEST:10.905 seconds]
[sig-network] DNS
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":193,"skipped":3095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:37.802: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:21:37.983: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06aa5a72-978f-434c-894d-313964f80ca3" in namespace "projected-7216" to be "success or failure"
Aug 30 17:21:37.988: INFO: Pod "downwardapi-volume-06aa5a72-978f-434c-894d-313964f80ca3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.534811ms
Aug 30 17:21:39.994: INFO: Pod "downwardapi-volume-06aa5a72-978f-434c-894d-313964f80ca3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010293749s
Aug 30 17:21:41.999: INFO: Pod "downwardapi-volume-06aa5a72-978f-434c-894d-313964f80ca3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01588502s
STEP: Saw pod success
Aug 30 17:21:41.999: INFO: Pod "downwardapi-volume-06aa5a72-978f-434c-894d-313964f80ca3" satisfied condition "success or failure"
Aug 30 17:21:42.004: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-06aa5a72-978f-434c-894d-313964f80ca3 container client-container: <nil>
STEP: delete the pod
Aug 30 17:21:42.072: INFO: Waiting for pod downwardapi-volume-06aa5a72-978f-434c-894d-313964f80ca3 to disappear
Aug 30 17:21:42.076: INFO: Pod downwardapi-volume-06aa5a72-978f-434c-894d-313964f80ca3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:42.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7216" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":3117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:42.095: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:21:42.252: INFO: Creating ReplicaSet my-hostname-basic-7f79c213-99b1-4acc-b58c-55a8454da939
Aug 30 17:21:42.264: INFO: Pod name my-hostname-basic-7f79c213-99b1-4acc-b58c-55a8454da939: Found 0 pods out of 1
Aug 30 17:21:47.272: INFO: Pod name my-hostname-basic-7f79c213-99b1-4acc-b58c-55a8454da939: Found 1 pods out of 1
Aug 30 17:21:47.272: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7f79c213-99b1-4acc-b58c-55a8454da939" is running
Aug 30 17:21:47.277: INFO: Pod "my-hostname-basic-7f79c213-99b1-4acc-b58c-55a8454da939-vfb42" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-30 17:21:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-30 17:21:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-30 17:21:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-30 17:21:42 +0000 UTC Reason: Message:}])
Aug 30 17:21:47.277: INFO: Trying to dial the pod
Aug 30 17:21:52.385: INFO: Controller my-hostname-basic-7f79c213-99b1-4acc-b58c-55a8454da939: Got expected result from replica 1 [my-hostname-basic-7f79c213-99b1-4acc-b58c-55a8454da939-vfb42]: "my-hostname-basic-7f79c213-99b1-4acc-b58c-55a8454da939-vfb42", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:52.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4164" for this suite.

â€¢ [SLOW TEST:10.306 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":195,"skipped":3143,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:52.401: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 30 17:21:52.572: INFO: Waiting up to 5m0s for pod "pod-df88dd45-03a6-406a-ad93-0d33dcf2b5f4" in namespace "emptydir-1667" to be "success or failure"
Aug 30 17:21:52.578: INFO: Pod "pod-df88dd45-03a6-406a-ad93-0d33dcf2b5f4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.708736ms
Aug 30 17:21:54.584: INFO: Pod "pod-df88dd45-03a6-406a-ad93-0d33dcf2b5f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011099403s
STEP: Saw pod success
Aug 30 17:21:54.584: INFO: Pod "pod-df88dd45-03a6-406a-ad93-0d33dcf2b5f4" satisfied condition "success or failure"
Aug 30 17:21:54.588: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-df88dd45-03a6-406a-ad93-0d33dcf2b5f4 container test-container: <nil>
STEP: delete the pod
Aug 30 17:21:54.657: INFO: Waiting for pod pod-df88dd45-03a6-406a-ad93-0d33dcf2b5f4 to disappear
Aug 30 17:21:54.662: INFO: Pod pod-df88dd45-03a6-406a-ad93-0d33dcf2b5f4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:54.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1667" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":196,"skipped":3146,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:54.677: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7323
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:21:55.212: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:21:58.239: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:21:58.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7323" for this suite.
STEP: Destroying namespace "webhook-7323-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":197,"skipped":3155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:21:58.618: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Aug 30 17:21:58.798: INFO: Waiting up to 5m0s for pod "downward-api-b442d4c5-f0ae-4f09-8593-0930ce32cee6" in namespace "downward-api-4492" to be "success or failure"
Aug 30 17:21:58.810: INFO: Pod "downward-api-b442d4c5-f0ae-4f09-8593-0930ce32cee6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.614199ms
Aug 30 17:22:00.816: INFO: Pod "downward-api-b442d4c5-f0ae-4f09-8593-0930ce32cee6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018269028s
Aug 30 17:22:02.822: INFO: Pod "downward-api-b442d4c5-f0ae-4f09-8593-0930ce32cee6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023758077s
STEP: Saw pod success
Aug 30 17:22:02.822: INFO: Pod "downward-api-b442d4c5-f0ae-4f09-8593-0930ce32cee6" satisfied condition "success or failure"
Aug 30 17:22:02.827: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downward-api-b442d4c5-f0ae-4f09-8593-0930ce32cee6 container dapi-container: <nil>
STEP: delete the pod
Aug 30 17:22:02.899: INFO: Waiting for pod downward-api-b442d4c5-f0ae-4f09-8593-0930ce32cee6 to disappear
Aug 30 17:22:02.904: INFO: Pod downward-api-b442d4c5-f0ae-4f09-8593-0930ce32cee6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:22:02.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4492" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":198,"skipped":3183,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:22:02.918: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8223
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 30 17:22:03.091: INFO: Waiting up to 5m0s for pod "pod-a88ef8f2-fe21-4192-8c65-b798abf50f1e" in namespace "emptydir-8223" to be "success or failure"
Aug 30 17:22:03.103: INFO: Pod "pod-a88ef8f2-fe21-4192-8c65-b798abf50f1e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.892351ms
Aug 30 17:22:05.109: INFO: Pod "pod-a88ef8f2-fe21-4192-8c65-b798abf50f1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017407432s
STEP: Saw pod success
Aug 30 17:22:05.109: INFO: Pod "pod-a88ef8f2-fe21-4192-8c65-b798abf50f1e" satisfied condition "success or failure"
Aug 30 17:22:05.114: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-a88ef8f2-fe21-4192-8c65-b798abf50f1e container test-container: <nil>
STEP: delete the pod
Aug 30 17:22:05.180: INFO: Waiting for pod pod-a88ef8f2-fe21-4192-8c65-b798abf50f1e to disappear
Aug 30 17:22:05.184: INFO: Pod pod-a88ef8f2-fe21-4192-8c65-b798abf50f1e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:22:05.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8223" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":199,"skipped":3184,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:22:05.206: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9495, will wait for the garbage collector to delete the pods
Aug 30 17:22:09.450: INFO: Deleting Job.batch foo took: 11.430463ms
Aug 30 17:22:09.950: INFO: Terminating Job.batch foo pods took: 500.322356ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:22:54.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9495" for this suite.

â€¢ [SLOW TEST:49.165 seconds]
[sig-apps] Job
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":200,"skipped":3190,"failed":0}
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:22:54.371: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:22:54.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-1432'
Aug 30 17:22:54.751: INFO: stderr: ""
Aug 30 17:22:54.751: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Aug 30 17:22:54.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-1432'
Aug 30 17:22:54.983: INFO: stderr: ""
Aug 30 17:22:54.983: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Aug 30 17:22:55.988: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 17:22:55.988: INFO: Found 0 / 1
Aug 30 17:22:56.989: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 17:22:56.989: INFO: Found 1 / 1
Aug 30 17:22:56.989: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 30 17:22:56.995: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 17:22:56.995: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 30 17:22:56.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 describe pod agnhost-master-78g9p --namespace=kubectl-1432'
Aug 30 17:22:57.079: INFO: stderr: ""
Aug 30 17:22:57.079: INFO: stdout: "Name:         agnhost-master-78g9p\nNamespace:    kubectl-1432\nPriority:     0\nNode:         adoring-wozniak-54dcfd79fc-948mf/46.101.135.210\nStart Time:   Sun, 30 Aug 2020 17:22:54 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.25.0.209/32\nStatus:       Running\nIP:           172.25.0.209\nIPs:\n  IP:           172.25.0.209\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://a726f34e47631cb65ddcee9cc9da17c8bd42bf48c83d16a2d2fddb11e369b046\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 30 Aug 2020 17:22:55 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ndhbh (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-ndhbh:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-ndhbh\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                       Message\n  ----    ------     ----       ----                                       -------\n  Normal  Scheduled  <unknown>  default-scheduler                          Successfully assigned kubectl-1432/agnhost-master-78g9p to adoring-wozniak-54dcfd79fc-948mf\n  Normal  Pulled     2s         kubelet, adoring-wozniak-54dcfd79fc-948mf  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    2s         kubelet, adoring-wozniak-54dcfd79fc-948mf  Created container agnhost-master\n  Normal  Started    1s         kubelet, adoring-wozniak-54dcfd79fc-948mf  Started container agnhost-master\n"
Aug 30 17:22:57.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 describe rc agnhost-master --namespace=kubectl-1432'
Aug 30 17:22:57.168: INFO: stderr: ""
Aug 30 17:22:57.168: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-1432\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-78g9p\n"
Aug 30 17:22:57.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 describe service agnhost-master --namespace=kubectl-1432'
Aug 30 17:22:57.248: INFO: stderr: ""
Aug 30 17:22:57.248: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-1432\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.240.18.223\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.0.209:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 30 17:22:57.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 describe node adoring-wozniak-54dcfd79fc-6rshr'
Aug 30 17:22:57.362: INFO: stderr: ""
Aug 30 17:22:57.362: INFO: stdout: "Name:               adoring-wozniak-54dcfd79fc-6rshr\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=adoring-wozniak-54dcfd79fc-6rshr\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=40ce4e72-1841-4057-a4ec-16381070e5bf\n                    system/cluster=x2pxtrxdh8\n                    system/project=kr6n49b5rm\n                    x-kubernetes.io/distribution=ubuntu\nAnnotations:        cluster.k8s.io/machine: kube-system/adoring-wozniak-54dcfd79fc-6rshr\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"b2:d8:4e:ac:b9:a7\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 46.101.153.64\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 30 Aug 2020 16:28:14 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  adoring-wozniak-54dcfd79fc-6rshr\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 30 Aug 2020 17:22:55 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sun, 30 Aug 2020 17:21:06 +0000   Sun, 30 Aug 2020 16:28:14 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sun, 30 Aug 2020 17:21:06 +0000   Sun, 30 Aug 2020 16:28:14 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sun, 30 Aug 2020 17:21:06 +0000   Sun, 30 Aug 2020 16:28:14 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sun, 30 Aug 2020 17:21:06 +0000   Sun, 30 Aug 2020 16:28:34 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  46.101.153.64\n  Hostname:    adoring-wozniak-54dcfd79fc-6rshr\nCapacity:\n  cpu:                4\n  ephemeral-storage:  50633164Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8167684Ki\n  pods:               110\nAllocatable:\n  cpu:                3800m\n  ephemeral-storage:  44516040218\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7860484Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 810db956215e4e3795031a26bde87471\n  System UUID:                810DB956-215E-4E37-9503-1A26BDE87471\n  Boot ID:                    6faecf09-95fe-4ae9-9349-7ebfef62a596\n  Kernel Version:             4.15.0-112-generic\n  OS Image:                   Ubuntu 18.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.9.9\n  Kubelet Version:            v1.17.9\n  Kube-Proxy Version:         v1.17.9\nPodCIDR:                      172.25.1.0/24\nPodCIDRs:                     172.25.1.0/24\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-54glj                                                250m (6%)     0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                 coredns-54457d966b-6cw7g                                   50m (1%)      100m (2%)   32Mi (0%)        64Mi (0%)      4m51s\n  kube-system                 coredns-54457d966b-fbnz4                                   50m (1%)      100m (2%)   32Mi (0%)        64Mi (0%)      56m\n  kube-system                 kube-proxy-wxdxv                                           75m (1%)      250m (6%)   50Mi (0%)        250Mi (3%)     54m\n  kube-system                 logrotate-57bmz                                            75m (1%)      250m (6%)   50Mi (0%)        250Mi (3%)     54m\n  kube-system                 node-local-dns-cvxnf                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                 openvpn-client-78d595f58b-vpgdr                            30m (0%)      200m (5%)   30Mi (0%)        82Mi (1%)      4m51s\n  kube-system                 user-ssh-keys-agent-xwrzj                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  kubernetes-dashboard        dashboard-metrics-scraper-59bfc65dc9-252k4                 50m (1%)      100m (2%)   32Mi (0%)        64Mi (0%)      4m51s\n  kubernetes-dashboard        dashboard-metrics-scraper-59bfc65dc9-t82kj                 50m (1%)      100m (2%)   32Mi (0%)        64Mi (0%)      4m51s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  sonobuoy                    sonobuoy-e2e-job-cf49606f646f4c8a                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-nd5nc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                630m (16%)  1100m (28%)\n  memory             258Mi (3%)  838Mi (10%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                 Age                From                                          Message\n  ----    ------                 ----               ----                                          -------\n  Normal  NodeHasNoDiskPressure  54m (x8 over 54m)  kubelet, adoring-wozniak-54dcfd79fc-6rshr     Node adoring-wozniak-54dcfd79fc-6rshr status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID   54m (x8 over 54m)  kubelet, adoring-wozniak-54dcfd79fc-6rshr     Node adoring-wozniak-54dcfd79fc-6rshr status is now: NodeHasSufficientPID\n  Normal  Starting               54m                kube-proxy, adoring-wozniak-54dcfd79fc-6rshr  Starting kube-proxy.\n"
Aug 30 17:22:57.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 describe namespace kubectl-1432'
Aug 30 17:22:57.444: INFO: stderr: ""
Aug 30 17:22:57.444: INFO: stdout: "Name:         kubectl-1432\nLabels:       e2e-framework=kubectl\n              e2e-run=eda2017a-4ba1-4abb-bd48-e853704860b3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:22:57.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1432" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":201,"skipped":3190,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:22:57.461: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9559
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Aug 30 17:22:57.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-9559'
Aug 30 17:22:57.797: INFO: stderr: ""
Aug 30 17:22:57.797: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 30 17:22:57.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9559'
Aug 30 17:22:57.885: INFO: stderr: ""
Aug 30 17:22:57.885: INFO: stdout: "update-demo-nautilus-6wf9z update-demo-nautilus-hwwwv "
Aug 30 17:22:57.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-6wf9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9559'
Aug 30 17:22:57.968: INFO: stderr: ""
Aug 30 17:22:57.968: INFO: stdout: ""
Aug 30 17:22:57.968: INFO: update-demo-nautilus-6wf9z is created but not running
Aug 30 17:23:02.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9559'
Aug 30 17:23:03.037: INFO: stderr: ""
Aug 30 17:23:03.037: INFO: stdout: "update-demo-nautilus-6wf9z update-demo-nautilus-hwwwv "
Aug 30 17:23:03.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-6wf9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9559'
Aug 30 17:23:03.107: INFO: stderr: ""
Aug 30 17:23:03.107: INFO: stdout: "true"
Aug 30 17:23:03.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-6wf9z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9559'
Aug 30 17:23:03.174: INFO: stderr: ""
Aug 30 17:23:03.174: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 30 17:23:03.174: INFO: validating pod update-demo-nautilus-6wf9z
Aug 30 17:23:03.306: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 17:23:03.306: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 17:23:03.306: INFO: update-demo-nautilus-6wf9z is verified up and running
Aug 30 17:23:03.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-hwwwv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9559'
Aug 30 17:23:03.371: INFO: stderr: ""
Aug 30 17:23:03.371: INFO: stdout: "true"
Aug 30 17:23:03.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-hwwwv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9559'
Aug 30 17:23:03.432: INFO: stderr: ""
Aug 30 17:23:03.432: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 30 17:23:03.432: INFO: validating pod update-demo-nautilus-hwwwv
Aug 30 17:23:03.564: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 17:23:03.565: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 17:23:03.565: INFO: update-demo-nautilus-hwwwv is verified up and running
STEP: using delete to clean up resources
Aug 30 17:23:03.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete --grace-period=0 --force -f - --namespace=kubectl-9559'
Aug 30 17:23:03.634: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 17:23:03.634: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 30 17:23:03.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9559'
Aug 30 17:23:03.701: INFO: stderr: "No resources found in kubectl-9559 namespace.\n"
Aug 30 17:23:03.701: INFO: stdout: ""
Aug 30 17:23:03.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -l name=update-demo --namespace=kubectl-9559 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 17:23:03.765: INFO: stderr: ""
Aug 30 17:23:03.765: INFO: stdout: "update-demo-nautilus-6wf9z\nupdate-demo-nautilus-hwwwv\n"
Aug 30 17:23:04.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9559'
Aug 30 17:23:04.352: INFO: stderr: "No resources found in kubectl-9559 namespace.\n"
Aug 30 17:23:04.352: INFO: stdout: ""
Aug 30 17:23:04.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -l name=update-demo --namespace=kubectl-9559 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 17:23:04.440: INFO: stderr: ""
Aug 30 17:23:04.440: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:23:04.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9559" for this suite.

â€¢ [SLOW TEST:6.995 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":202,"skipped":3194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:23:04.458: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6074
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:23:04.623: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 30 17:23:07.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-6074 create -f -'
Aug 30 17:23:07.892: INFO: stderr: ""
Aug 30 17:23:07.892: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 30 17:23:07.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-6074 delete e2e-test-crd-publish-openapi-8313-crds test-cr'
Aug 30 17:23:08.022: INFO: stderr: ""
Aug 30 17:23:08.022: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 30 17:23:08.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-6074 apply -f -'
Aug 30 17:23:08.287: INFO: stderr: ""
Aug 30 17:23:08.287: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 30 17:23:08.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=crd-publish-openapi-6074 delete e2e-test-crd-publish-openapi-8313-crds test-cr'
Aug 30 17:23:08.390: INFO: stderr: ""
Aug 30 17:23:08.390: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 30 17:23:08.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 explain e2e-test-crd-publish-openapi-8313-crds'
Aug 30 17:23:08.627: INFO: stderr: ""
Aug 30 17:23:08.627: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8313-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:23:11.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6074" for this suite.

â€¢ [SLOW TEST:7.536 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":203,"skipped":3217,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:23:11.994: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 30 17:23:12.215: INFO: Number of nodes with available pods: 0
Aug 30 17:23:12.215: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 17:23:13.227: INFO: Number of nodes with available pods: 0
Aug 30 17:23:13.227: INFO: Node adoring-wozniak-54dcfd79fc-6rshr is running more than one daemon pod
Aug 30 17:23:14.228: INFO: Number of nodes with available pods: 2
Aug 30 17:23:14.228: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 30 17:23:14.260: INFO: Number of nodes with available pods: 1
Aug 30 17:23:14.260: INFO: Node adoring-wozniak-54dcfd79fc-948mf is running more than one daemon pod
Aug 30 17:23:15.272: INFO: Number of nodes with available pods: 1
Aug 30 17:23:15.272: INFO: Node adoring-wozniak-54dcfd79fc-948mf is running more than one daemon pod
Aug 30 17:23:16.273: INFO: Number of nodes with available pods: 2
Aug 30 17:23:16.273: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6474, will wait for the garbage collector to delete the pods
Aug 30 17:23:16.348: INFO: Deleting DaemonSet.extensions daemon-set took: 11.940516ms
Aug 30 17:23:16.449: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.207982ms
Aug 30 17:23:20.355: INFO: Number of nodes with available pods: 0
Aug 30 17:23:20.355: INFO: Number of running nodes: 0, number of available pods: 0
Aug 30 17:23:20.359: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6474/daemonsets","resourceVersion":"24006"},"items":null}

Aug 30 17:23:20.364: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6474/pods","resourceVersion":"24006"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:23:20.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6474" for this suite.

â€¢ [SLOW TEST:8.401 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":204,"skipped":3234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:23:20.399: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3907
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-585f7b15-60d2-44aa-a880-b52a7d890a5b
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-585f7b15-60d2-44aa-a880-b52a7d890a5b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:23:24.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3907" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":205,"skipped":3257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:23:24.758: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5886
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-8h6x
STEP: Creating a pod to test atomic-volume-subpath
Aug 30 17:23:24.942: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8h6x" in namespace "subpath-5886" to be "success or failure"
Aug 30 17:23:24.946: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.233639ms
Aug 30 17:23:26.963: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Running", Reason="", readiness=true. Elapsed: 2.020940213s
Aug 30 17:23:28.968: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Running", Reason="", readiness=true. Elapsed: 4.026443797s
Aug 30 17:23:30.975: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Running", Reason="", readiness=true. Elapsed: 6.032983762s
Aug 30 17:23:32.981: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Running", Reason="", readiness=true. Elapsed: 8.039441782s
Aug 30 17:23:34.987: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Running", Reason="", readiness=true. Elapsed: 10.045363864s
Aug 30 17:23:36.993: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Running", Reason="", readiness=true. Elapsed: 12.050679787s
Aug 30 17:23:38.999: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Running", Reason="", readiness=true. Elapsed: 14.056718488s
Aug 30 17:23:41.004: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Running", Reason="", readiness=true. Elapsed: 16.062280821s
Aug 30 17:23:43.010: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Running", Reason="", readiness=true. Elapsed: 18.068534495s
Aug 30 17:23:45.017: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Running", Reason="", readiness=true. Elapsed: 20.074740255s
Aug 30 17:23:47.024: INFO: Pod "pod-subpath-test-secret-8h6x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.081983529s
STEP: Saw pod success
Aug 30 17:23:47.024: INFO: Pod "pod-subpath-test-secret-8h6x" satisfied condition "success or failure"
Aug 30 17:23:47.029: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-subpath-test-secret-8h6x container test-container-subpath-secret-8h6x: <nil>
STEP: delete the pod
Aug 30 17:23:47.106: INFO: Waiting for pod pod-subpath-test-secret-8h6x to disappear
Aug 30 17:23:47.111: INFO: Pod pod-subpath-test-secret-8h6x no longer exists
STEP: Deleting pod pod-subpath-test-secret-8h6x
Aug 30 17:23:47.111: INFO: Deleting pod "pod-subpath-test-secret-8h6x" in namespace "subpath-5886"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:23:47.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5886" for this suite.

â€¢ [SLOW TEST:22.376 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":206,"skipped":3288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:23:47.135: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4214
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:23:47.641: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:23:50.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:23:50.677: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7635-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:23:52.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4214" for this suite.
STEP: Destroying namespace "webhook-4214-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.169 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":207,"skipped":3318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:23:52.313: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4896
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4896
STEP: creating replication controller externalsvc in namespace services-4896
I0830 17:23:52.575499      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-4896, replica count: 2
I0830 17:23:55.625977      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 30 17:23:55.657: INFO: Creating new exec pod
Aug 30 17:23:57.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=services-4896 execpod4945t -- /bin/sh -x -c nslookup clusterip-service'
Aug 30 17:23:58.216: INFO: stderr: "+ nslookup clusterip-service\n"
Aug 30 17:23:58.216: INFO: stdout: "Server:\t\t10.240.16.10\nAddress:\t10.240.16.10#53\n\nclusterip-service.services-4896.svc.cluster.local\tcanonical name = externalsvc.services-4896.svc.cluster.local.\nName:\texternalsvc.services-4896.svc.cluster.local\nAddress: 10.240.27.141\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4896, will wait for the garbage collector to delete the pods
Aug 30 17:23:58.282: INFO: Deleting ReplicationController externalsvc took: 11.30533ms
Aug 30 17:23:58.382: INFO: Terminating ReplicationController externalsvc pods took: 100.171299ms
Aug 30 17:24:10.009: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:24:10.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4896" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:17.736 seconds]
[sig-network] Services
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":208,"skipped":3386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:24:10.049: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-9872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Aug 30 17:24:10.231: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 17:25:10.254: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:25:10.259: INFO: Starting informer...
STEP: Starting pod...
Aug 30 17:25:10.480: INFO: Pod is running on adoring-wozniak-54dcfd79fc-948mf. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 30 17:25:10.500: INFO: Pod wasn't evicted. Proceeding
Aug 30 17:25:10.500: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 30 17:26:25.526: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:26:25.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9872" for this suite.

â€¢ [SLOW TEST:135.494 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":209,"skipped":3414,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:26:25.545: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0830 17:26:35.748159      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 30 17:26:35.748: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:26:35.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4368" for this suite.

â€¢ [SLOW TEST:10.218 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":210,"skipped":3430,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:26:35.763: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 30 17:26:37.960: INFO: &Pod{ObjectMeta:{send-events-227d055c-ea1a-41eb-8cd8-39d9673a5cb5  events-5257 /api/v1/namespaces/events-5257/pods/send-events-227d055c-ea1a-41eb-8cd8-39d9673a5cb5 52e73ac7-b400-42e4-adb3-e5cbfc62ef96 25119 0 2020-08-30 17:26:35 +0000 UTC <nil> <nil> map[name:foo time:929334961] map[cni.projectcalico.org/podIP:172.25.0.221/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mpw6z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mpw6z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mpw6z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:adoring-wozniak-54dcfd79fc-948mf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:26:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:26:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:26:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-30 17:26:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:46.101.135.210,PodIP:172.25.0.221,StartTime:2020-08-30 17:26:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-30 17:26:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://f25ee0d7d6cd223046af7f19dc786125de6785681b0597143abcb7f5b439ad4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Aug 30 17:26:39.969: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 30 17:26:41.976: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:26:41.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5257" for this suite.

â€¢ [SLOW TEST:6.236 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":211,"skipped":3451,"failed":0}
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:26:41.999: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Aug 30 17:26:42.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-5183'
Aug 30 17:26:42.396: INFO: stderr: ""
Aug 30 17:26:42.396: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Aug 30 17:26:43.402: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 17:26:43.402: INFO: Found 0 / 1
Aug 30 17:26:44.403: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 17:26:44.403: INFO: Found 1 / 1
Aug 30 17:26:44.403: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 30 17:26:44.408: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 17:26:44.408: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 30 17:26:44.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 patch pod agnhost-master-lm769 --namespace=kubectl-5183 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 30 17:26:44.485: INFO: stderr: ""
Aug 30 17:26:44.485: INFO: stdout: "pod/agnhost-master-lm769 patched\n"
STEP: checking annotations
Aug 30 17:26:44.490: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 17:26:44.490: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:26:44.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5183" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":212,"skipped":3451,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:26:44.504: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Aug 30 17:26:44.666: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 17:26:44.683: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 17:26:44.687: INFO: 
Logging pods the kubelet thinks is on node adoring-wozniak-54dcfd79fc-6rshr before test
Aug 30 17:26:44.753: INFO: kube-proxy-wxdxv from kube-system started at 2020-08-30 16:28:14 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.753: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 17:26:44.753: INFO: node-local-dns-cvxnf from kube-system started at 2020-08-30 16:28:34 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.753: INFO: 	Container node-cache ready: true, restart count 0
Aug 30 17:26:44.753: INFO: logrotate-57bmz from kube-system started at 2020-08-30 16:28:34 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.753: INFO: 	Container logrotate ready: true, restart count 0
Aug 30 17:26:44.753: INFO: sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-nd5nc from sonobuoy started at 2020-08-30 16:29:24 +0000 UTC (2 container statuses recorded)
Aug 30 17:26:44.753: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 17:26:44.753: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 17:26:44.753: INFO: coredns-54457d966b-fbnz4 from kube-system started at 2020-08-30 16:28:39 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.753: INFO: 	Container coredns ready: true, restart count 0
Aug 30 17:26:44.753: INFO: sonobuoy-e2e-job-cf49606f646f4c8a from sonobuoy started at 2020-08-30 16:29:23 +0000 UTC (2 container statuses recorded)
Aug 30 17:26:44.753: INFO: 	Container e2e ready: true, restart count 0
Aug 30 17:26:44.753: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 17:26:44.753: INFO: dashboard-metrics-scraper-59bfc65dc9-252k4 from kubernetes-dashboard started at 2020-08-30 17:18:06 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.754: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 30 17:26:44.754: INFO: canal-54glj from kube-system started at 2020-08-30 16:28:14 +0000 UTC (2 container statuses recorded)
Aug 30 17:26:44.754: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 17:26:44.754: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 30 17:26:44.754: INFO: user-ssh-keys-agent-xwrzj from kube-system started at 2020-08-30 16:28:14 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.754: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Aug 30 17:26:44.754: INFO: coredns-54457d966b-6cw7g from kube-system started at 2020-08-30 17:18:06 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.754: INFO: 	Container coredns ready: true, restart count 0
Aug 30 17:26:44.754: INFO: sonobuoy from sonobuoy started at 2020-08-30 16:29:18 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.754: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 17:26:44.754: INFO: openvpn-client-78d595f58b-vpgdr from kube-system started at 2020-08-30 17:18:06 +0000 UTC (2 container statuses recorded)
Aug 30 17:26:44.754: INFO: 	Container dnat-controller ready: true, restart count 0
Aug 30 17:26:44.754: INFO: 	Container openvpn-client ready: true, restart count 0
Aug 30 17:26:44.754: INFO: dashboard-metrics-scraper-59bfc65dc9-t82kj from kubernetes-dashboard started at 2020-08-30 17:18:06 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.754: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 30 17:26:44.754: INFO: 
Logging pods the kubelet thinks is on node adoring-wozniak-54dcfd79fc-948mf before test
Aug 30 17:26:44.837: INFO: canal-lg5hn from kube-system started at 2020-08-30 16:28:09 +0000 UTC (2 container statuses recorded)
Aug 30 17:26:44.837: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 17:26:44.837: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 30 17:26:44.837: INFO: agnhost-master-lm769 from kubectl-5183 started at 2020-08-30 17:26:42 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.837: INFO: 	Container agnhost-master ready: true, restart count 0
Aug 30 17:26:44.837: INFO: sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-cdm8v from sonobuoy started at 2020-08-30 16:29:23 +0000 UTC (2 container statuses recorded)
Aug 30 17:26:44.837: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 17:26:44.837: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 17:26:44.837: INFO: node-local-dns-s2mgx from kube-system started at 2020-08-30 17:25:10 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.837: INFO: 	Container node-cache ready: true, restart count 0
Aug 30 17:26:44.837: INFO: logrotate-bnfcr from kube-system started at 2020-08-30 17:25:24 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.837: INFO: 	Container logrotate ready: true, restart count 0
Aug 30 17:26:44.837: INFO: kube-proxy-v88gx from kube-system started at 2020-08-30 16:28:09 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.837: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 17:26:44.837: INFO: user-ssh-keys-agent-vkbs9 from kube-system started at 2020-08-30 16:28:09 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.837: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Aug 30 17:26:44.837: INFO: send-events-227d055c-ea1a-41eb-8cd8-39d9673a5cb5 from events-5257 started at 2020-08-30 17:26:35 +0000 UTC (1 container statuses recorded)
Aug 30 17:26:44.837: INFO: 	Container p ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node adoring-wozniak-54dcfd79fc-6rshr
STEP: verifying the node has the label node adoring-wozniak-54dcfd79fc-948mf
Aug 30 17:26:44.885: INFO: Pod send-events-227d055c-ea1a-41eb-8cd8-39d9673a5cb5 requesting resource cpu=0m on Node adoring-wozniak-54dcfd79fc-948mf
Aug 30 17:26:44.885: INFO: Pod canal-54glj requesting resource cpu=250m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod canal-lg5hn requesting resource cpu=250m on Node adoring-wozniak-54dcfd79fc-948mf
Aug 30 17:26:44.885: INFO: Pod coredns-54457d966b-6cw7g requesting resource cpu=50m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod coredns-54457d966b-fbnz4 requesting resource cpu=50m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod kube-proxy-v88gx requesting resource cpu=75m on Node adoring-wozniak-54dcfd79fc-948mf
Aug 30 17:26:44.885: INFO: Pod kube-proxy-wxdxv requesting resource cpu=75m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod logrotate-57bmz requesting resource cpu=75m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod logrotate-bnfcr requesting resource cpu=75m on Node adoring-wozniak-54dcfd79fc-948mf
Aug 30 17:26:44.885: INFO: Pod node-local-dns-cvxnf requesting resource cpu=0m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod node-local-dns-s2mgx requesting resource cpu=0m on Node adoring-wozniak-54dcfd79fc-948mf
Aug 30 17:26:44.885: INFO: Pod openvpn-client-78d595f58b-vpgdr requesting resource cpu=30m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod user-ssh-keys-agent-vkbs9 requesting resource cpu=0m on Node adoring-wozniak-54dcfd79fc-948mf
Aug 30 17:26:44.885: INFO: Pod user-ssh-keys-agent-xwrzj requesting resource cpu=0m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod agnhost-master-lm769 requesting resource cpu=0m on Node adoring-wozniak-54dcfd79fc-948mf
Aug 30 17:26:44.885: INFO: Pod dashboard-metrics-scraper-59bfc65dc9-252k4 requesting resource cpu=50m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod dashboard-metrics-scraper-59bfc65dc9-t82kj requesting resource cpu=50m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod sonobuoy requesting resource cpu=0m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod sonobuoy-e2e-job-cf49606f646f4c8a requesting resource cpu=0m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.885: INFO: Pod sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-cdm8v requesting resource cpu=0m on Node adoring-wozniak-54dcfd79fc-948mf
Aug 30 17:26:44.885: INFO: Pod sonobuoy-systemd-logs-daemon-set-fc27dc07d16945d5-nd5nc requesting resource cpu=0m on Node adoring-wozniak-54dcfd79fc-6rshr
STEP: Starting Pods to consume most of the cluster CPU.
Aug 30 17:26:44.885: INFO: Creating a pod which consumes cpu=2219m on Node adoring-wozniak-54dcfd79fc-6rshr
Aug 30 17:26:44.895: INFO: Creating a pod which consumes cpu=2380m on Node adoring-wozniak-54dcfd79fc-948mf
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05156bb3-b84b-4a11-a3c8-f689a3a69a08.16301bc5fd892c6c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4333/filler-pod-05156bb3-b84b-4a11-a3c8-f689a3a69a08 to adoring-wozniak-54dcfd79fc-6rshr]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05156bb3-b84b-4a11-a3c8-f689a3a69a08.16301bc63b56ba57], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05156bb3-b84b-4a11-a3c8-f689a3a69a08.16301bc63faad98d], Reason = [Created], Message = [Created container filler-pod-05156bb3-b84b-4a11-a3c8-f689a3a69a08]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05156bb3-b84b-4a11-a3c8-f689a3a69a08.16301bc64a2e9fc1], Reason = [Started], Message = [Started container filler-pod-05156bb3-b84b-4a11-a3c8-f689a3a69a08]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d7f8a65-b4c2-4299-ad5b-a838b5c282fa.16301bc5fdf4940d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4333/filler-pod-0d7f8a65-b4c2-4299-ad5b-a838b5c282fa to adoring-wozniak-54dcfd79fc-948mf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d7f8a65-b4c2-4299-ad5b-a838b5c282fa.16301bc636154c81], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d7f8a65-b4c2-4299-ad5b-a838b5c282fa.16301bc639e04a7f], Reason = [Created], Message = [Created container filler-pod-0d7f8a65-b4c2-4299-ad5b-a838b5c282fa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d7f8a65-b4c2-4299-ad5b-a838b5c282fa.16301bc6434fa5bb], Reason = [Started], Message = [Started container filler-pod-0d7f8a65-b4c2-4299-ad5b-a838b5c282fa]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16301bc6772d5d6f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16301bc6780f5291], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node adoring-wozniak-54dcfd79fc-6rshr
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node adoring-wozniak-54dcfd79fc-948mf
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:26:48.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4333" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":213,"skipped":3457,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:26:48.017: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8626
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Aug 30 17:26:48.208: INFO: Found 0 stateful pods, waiting for 3
Aug 30 17:26:58.215: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:26:58.215: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:26:58.215: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:26:58.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8626 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 17:26:58.767: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 17:26:58.767: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 17:26:58.767: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 30 17:27:08.809: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 30 17:27:18.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8626 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:27:19.362: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 17:27:19.362: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 17:27:19.362: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Aug 30 17:27:39.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8626 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 17:27:39.980: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 17:27:39.980: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 17:27:39.980: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 17:27:50.022: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 30 17:28:00.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8626 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:28:00.577: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 17:28:00.577: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 17:28:00.577: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 17:28:00.606: INFO: Waiting for StatefulSet statefulset-8626/ss2 to complete update
Aug 30 17:28:00.606: INFO: Waiting for Pod statefulset-8626/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 30 17:28:00.606: INFO: Waiting for Pod statefulset-8626/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 30 17:28:00.606: INFO: Waiting for Pod statefulset-8626/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 30 17:28:10.618: INFO: Waiting for StatefulSet statefulset-8626/ss2 to complete update
Aug 30 17:28:10.618: INFO: Waiting for Pod statefulset-8626/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 30 17:28:20.617: INFO: Waiting for StatefulSet statefulset-8626/ss2 to complete update
Aug 30 17:28:20.617: INFO: Waiting for Pod statefulset-8626/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 30 17:28:30.618: INFO: Deleting all statefulset in ns statefulset-8626
Aug 30 17:28:30.622: INFO: Scaling statefulset ss2 to 0
Aug 30 17:28:40.644: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 17:28:40.650: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:28:40.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8626" for this suite.

â€¢ [SLOW TEST:112.680 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":214,"skipped":3468,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:28:40.697: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Aug 30 17:28:40.858: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:28:44.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-144" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":215,"skipped":3488,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:28:44.849: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:28:45.030: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd46d762-cdcf-4874-a76f-bd5314b04e8d" in namespace "downward-api-7207" to be "success or failure"
Aug 30 17:28:45.037: INFO: Pod "downwardapi-volume-fd46d762-cdcf-4874-a76f-bd5314b04e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110258ms
Aug 30 17:28:47.042: INFO: Pod "downwardapi-volume-fd46d762-cdcf-4874-a76f-bd5314b04e8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011982126s
STEP: Saw pod success
Aug 30 17:28:47.042: INFO: Pod "downwardapi-volume-fd46d762-cdcf-4874-a76f-bd5314b04e8d" satisfied condition "success or failure"
Aug 30 17:28:47.048: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-fd46d762-cdcf-4874-a76f-bd5314b04e8d container client-container: <nil>
STEP: delete the pod
Aug 30 17:28:47.120: INFO: Waiting for pod downwardapi-volume-fd46d762-cdcf-4874-a76f-bd5314b04e8d to disappear
Aug 30 17:28:47.124: INFO: Pod downwardapi-volume-fd46d762-cdcf-4874-a76f-bd5314b04e8d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:28:47.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7207" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":216,"skipped":3492,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:28:47.142: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Aug 30 17:28:47.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 --namespace=kubectl-8842 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 30 17:28:50.006: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 30 17:28:50.006: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:28:52.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8842" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":217,"skipped":3513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:28:52.034: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:29:03.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9570" for this suite.

â€¢ [SLOW TEST:11.242 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":218,"skipped":3542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:29:03.276: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4906
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 30 17:29:03.835: INFO: Pod name wrapped-volume-race-c4fa8e8e-ab69-4cb6-bb58-48920bf0a2d6: Found 0 pods out of 5
Aug 30 17:29:08.844: INFO: Pod name wrapped-volume-race-c4fa8e8e-ab69-4cb6-bb58-48920bf0a2d6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c4fa8e8e-ab69-4cb6-bb58-48920bf0a2d6 in namespace emptydir-wrapper-4906, will wait for the garbage collector to delete the pods
Aug 30 17:29:18.948: INFO: Deleting ReplicationController wrapped-volume-race-c4fa8e8e-ab69-4cb6-bb58-48920bf0a2d6 took: 13.863546ms
Aug 30 17:29:19.448: INFO: Terminating ReplicationController wrapped-volume-race-c4fa8e8e-ab69-4cb6-bb58-48920bf0a2d6 pods took: 500.205623ms
STEP: Creating RC which spawns configmap-volume pods
Aug 30 17:29:29.975: INFO: Pod name wrapped-volume-race-d492aa4c-7589-4850-bb3e-4bc4191c098f: Found 0 pods out of 5
Aug 30 17:29:34.993: INFO: Pod name wrapped-volume-race-d492aa4c-7589-4850-bb3e-4bc4191c098f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d492aa4c-7589-4850-bb3e-4bc4191c098f in namespace emptydir-wrapper-4906, will wait for the garbage collector to delete the pods
Aug 30 17:29:45.096: INFO: Deleting ReplicationController wrapped-volume-race-d492aa4c-7589-4850-bb3e-4bc4191c098f took: 13.417828ms
Aug 30 17:29:45.596: INFO: Terminating ReplicationController wrapped-volume-race-d492aa4c-7589-4850-bb3e-4bc4191c098f pods took: 500.163726ms
STEP: Creating RC which spawns configmap-volume pods
Aug 30 17:30:04.321: INFO: Pod name wrapped-volume-race-ab147d24-b88e-45a3-974b-8b5f31125cd3: Found 0 pods out of 5
Aug 30 17:30:09.333: INFO: Pod name wrapped-volume-race-ab147d24-b88e-45a3-974b-8b5f31125cd3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ab147d24-b88e-45a3-974b-8b5f31125cd3 in namespace emptydir-wrapper-4906, will wait for the garbage collector to delete the pods
Aug 30 17:30:19.439: INFO: Deleting ReplicationController wrapped-volume-race-ab147d24-b88e-45a3-974b-8b5f31125cd3 took: 15.173264ms
Aug 30 17:30:19.939: INFO: Terminating ReplicationController wrapped-volume-race-ab147d24-b88e-45a3-974b-8b5f31125cd3 pods took: 500.337345ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:30:35.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4906" for this suite.

â€¢ [SLOW TEST:92.534 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":219,"skipped":3575,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:30:35.811: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:30:36.355: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 17:30:38.370: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405436, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405436, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405436, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405436, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:30:41.396: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:30:41.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9547" for this suite.
STEP: Destroying namespace "webhook-9547-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.057 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":220,"skipped":3576,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:30:41.868: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:30:42.238: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 17:30:44.262: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405442, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405442, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405442, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405442, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:30:47.288: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 30 17:30:47.476: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:30:47.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9343" for this suite.
STEP: Destroying namespace "webhook-9343-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.817 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":221,"skipped":3594,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:30:47.689: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9024
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-846658d2-62c0-4dbd-9ba8-2838542a3d15
STEP: Creating a pod to test consume secrets
Aug 30 17:30:47.891: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-104ef37b-e543-4946-a4c9-51ef09d6adca" in namespace "projected-9024" to be "success or failure"
Aug 30 17:30:47.896: INFO: Pod "pod-projected-secrets-104ef37b-e543-4946-a4c9-51ef09d6adca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.410914ms
Aug 30 17:30:49.902: INFO: Pod "pod-projected-secrets-104ef37b-e543-4946-a4c9-51ef09d6adca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011388434s
STEP: Saw pod success
Aug 30 17:30:49.902: INFO: Pod "pod-projected-secrets-104ef37b-e543-4946-a4c9-51ef09d6adca" satisfied condition "success or failure"
Aug 30 17:30:49.907: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-projected-secrets-104ef37b-e543-4946-a4c9-51ef09d6adca container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 30 17:30:50.016: INFO: Waiting for pod pod-projected-secrets-104ef37b-e543-4946-a4c9-51ef09d6adca to disappear
Aug 30 17:30:50.021: INFO: Pod pod-projected-secrets-104ef37b-e543-4946-a4c9-51ef09d6adca no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:30:50.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9024" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":222,"skipped":3619,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:30:50.040: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7776
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:30:50.900: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 17:30:52.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405450, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405450, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405450, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405450, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:30:55.936: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:30:55.942: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3188-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:30:57.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7776" for this suite.
STEP: Destroying namespace "webhook-7776-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.364 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":223,"skipped":3622,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:30:57.405: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7672
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-593f6803-a3c9-431f-9329-95f18086aaa4
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:30:57.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7672" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":224,"skipped":3627,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:30:57.589: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-708fe94d-32b6-4c04-923a-9acc8ae9f1e8
STEP: Creating a pod to test consume secrets
Aug 30 17:30:57.767: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a6dfb05d-279d-45a8-b198-3757ba596b39" in namespace "projected-1153" to be "success or failure"
Aug 30 17:30:57.772: INFO: Pod "pod-projected-secrets-a6dfb05d-279d-45a8-b198-3757ba596b39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.472381ms
Aug 30 17:30:59.778: INFO: Pod "pod-projected-secrets-a6dfb05d-279d-45a8-b198-3757ba596b39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010426382s
STEP: Saw pod success
Aug 30 17:30:59.778: INFO: Pod "pod-projected-secrets-a6dfb05d-279d-45a8-b198-3757ba596b39" satisfied condition "success or failure"
Aug 30 17:30:59.784: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-projected-secrets-a6dfb05d-279d-45a8-b198-3757ba596b39 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 30 17:30:59.852: INFO: Waiting for pod pod-projected-secrets-a6dfb05d-279d-45a8-b198-3757ba596b39 to disappear
Aug 30 17:30:59.860: INFO: Pod pod-projected-secrets-a6dfb05d-279d-45a8-b198-3757ba596b39 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:30:59.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1153" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":225,"skipped":3636,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:30:59.875: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 30 17:31:00.050: INFO: Waiting up to 5m0s for pod "pod-6efff8f3-ba97-4487-8958-81320d2e4e21" in namespace "emptydir-1506" to be "success or failure"
Aug 30 17:31:00.059: INFO: Pod "pod-6efff8f3-ba97-4487-8958-81320d2e4e21": Phase="Pending", Reason="", readiness=false. Elapsed: 8.813133ms
Aug 30 17:31:02.074: INFO: Pod "pod-6efff8f3-ba97-4487-8958-81320d2e4e21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024239199s
STEP: Saw pod success
Aug 30 17:31:02.074: INFO: Pod "pod-6efff8f3-ba97-4487-8958-81320d2e4e21" satisfied condition "success or failure"
Aug 30 17:31:02.082: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-6efff8f3-ba97-4487-8958-81320d2e4e21 container test-container: <nil>
STEP: delete the pod
Aug 30 17:31:02.166: INFO: Waiting for pod pod-6efff8f3-ba97-4487-8958-81320d2e4e21 to disappear
Aug 30 17:31:02.176: INFO: Pod pod-6efff8f3-ba97-4487-8958-81320d2e4e21 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:31:02.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1506" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":226,"skipped":3654,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:31:02.196: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 30 17:31:06.943: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2493 pod-service-account-1ff6b08c-d972-4a7d-a92c-6cef1506c747 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 30 17:31:07.493: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2493 pod-service-account-1ff6b08c-d972-4a7d-a92c-6cef1506c747 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 30 17:31:08.064: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2493 pod-service-account-1ff6b08c-d972-4a7d-a92c-6cef1506c747 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:31:08.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2493" for this suite.

â€¢ [SLOW TEST:6.481 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":227,"skipped":3670,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:31:08.677: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:31:08.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4757" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":228,"skipped":3683,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:31:08.897: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:31:09.064: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 30 17:31:11.119: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:31:11.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4947" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":229,"skipped":3691,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:31:11.141: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-dde0efd5-0c0f-4946-be10-fec9ec20f9bf
STEP: Creating a pod to test consume configMaps
Aug 30 17:31:11.318: INFO: Waiting up to 5m0s for pod "pod-configmaps-f901b73a-9ec3-405f-9106-b7fb78fe8e04" in namespace "configmap-2587" to be "success or failure"
Aug 30 17:31:11.322: INFO: Pod "pod-configmaps-f901b73a-9ec3-405f-9106-b7fb78fe8e04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.331573ms
Aug 30 17:31:13.329: INFO: Pod "pod-configmaps-f901b73a-9ec3-405f-9106-b7fb78fe8e04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010574837s
STEP: Saw pod success
Aug 30 17:31:13.329: INFO: Pod "pod-configmaps-f901b73a-9ec3-405f-9106-b7fb78fe8e04" satisfied condition "success or failure"
Aug 30 17:31:13.334: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-configmaps-f901b73a-9ec3-405f-9106-b7fb78fe8e04 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 17:31:13.402: INFO: Waiting for pod pod-configmaps-f901b73a-9ec3-405f-9106-b7fb78fe8e04 to disappear
Aug 30 17:31:13.407: INFO: Pod pod-configmaps-f901b73a-9ec3-405f-9106-b7fb78fe8e04 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:31:13.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2587" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":230,"skipped":3697,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:31:13.426: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8156
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0830 17:31:19.635251      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 30 17:31:19.635: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:31:19.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8156" for this suite.

â€¢ [SLOW TEST:6.223 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":231,"skipped":3699,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:31:19.649: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-gxhd
STEP: Creating a pod to test atomic-volume-subpath
Aug 30 17:31:19.836: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gxhd" in namespace "subpath-5691" to be "success or failure"
Aug 30 17:31:19.845: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.927303ms
Aug 30 17:31:21.855: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018780932s
Aug 30 17:31:23.861: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024870698s
Aug 30 17:31:25.867: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Running", Reason="", readiness=true. Elapsed: 6.030985585s
Aug 30 17:31:27.873: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Running", Reason="", readiness=true. Elapsed: 8.036884194s
Aug 30 17:31:29.881: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Running", Reason="", readiness=true. Elapsed: 10.044861084s
Aug 30 17:31:31.887: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Running", Reason="", readiness=true. Elapsed: 12.050756921s
Aug 30 17:31:33.893: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Running", Reason="", readiness=true. Elapsed: 14.056471882s
Aug 30 17:31:35.899: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Running", Reason="", readiness=true. Elapsed: 16.062489451s
Aug 30 17:31:37.904: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Running", Reason="", readiness=true. Elapsed: 18.068172415s
Aug 30 17:31:39.910: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Running", Reason="", readiness=true. Elapsed: 20.073839055s
Aug 30 17:31:41.916: INFO: Pod "pod-subpath-test-configmap-gxhd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.079966687s
STEP: Saw pod success
Aug 30 17:31:41.916: INFO: Pod "pod-subpath-test-configmap-gxhd" satisfied condition "success or failure"
Aug 30 17:31:41.921: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-subpath-test-configmap-gxhd container test-container-subpath-configmap-gxhd: <nil>
STEP: delete the pod
Aug 30 17:31:41.988: INFO: Waiting for pod pod-subpath-test-configmap-gxhd to disappear
Aug 30 17:31:41.992: INFO: Pod pod-subpath-test-configmap-gxhd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gxhd
Aug 30 17:31:41.992: INFO: Deleting pod "pod-subpath-test-configmap-gxhd" in namespace "subpath-5691"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:31:41.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5691" for this suite.

â€¢ [SLOW TEST:22.362 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":232,"skipped":3712,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:31:42.011: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2675
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-q92q
STEP: Creating a pod to test atomic-volume-subpath
Aug 30 17:31:42.203: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-q92q" in namespace "subpath-2675" to be "success or failure"
Aug 30 17:31:42.207: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.418265ms
Aug 30 17:31:44.213: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 2.010213245s
Aug 30 17:31:46.219: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 4.016132822s
Aug 30 17:31:48.224: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 6.021197058s
Aug 30 17:31:50.230: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 8.026919432s
Aug 30 17:31:52.236: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 10.032997155s
Aug 30 17:31:54.242: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 12.03924511s
Aug 30 17:31:56.247: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 14.044575306s
Aug 30 17:31:58.253: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 16.050515043s
Aug 30 17:32:00.259: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 18.055941643s
Aug 30 17:32:02.264: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 20.061725201s
Aug 30 17:32:04.271: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Running", Reason="", readiness=true. Elapsed: 22.068031518s
Aug 30 17:32:06.277: INFO: Pod "pod-subpath-test-projected-q92q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.074033167s
STEP: Saw pod success
Aug 30 17:32:06.277: INFO: Pod "pod-subpath-test-projected-q92q" satisfied condition "success or failure"
Aug 30 17:32:06.282: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-subpath-test-projected-q92q container test-container-subpath-projected-q92q: <nil>
STEP: delete the pod
Aug 30 17:32:06.362: INFO: Waiting for pod pod-subpath-test-projected-q92q to disappear
Aug 30 17:32:06.369: INFO: Pod pod-subpath-test-projected-q92q no longer exists
STEP: Deleting pod pod-subpath-test-projected-q92q
Aug 30 17:32:06.369: INFO: Deleting pod "pod-subpath-test-projected-q92q" in namespace "subpath-2675"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:32:06.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2675" for this suite.

â€¢ [SLOW TEST:24.378 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":233,"skipped":3720,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:32:06.390: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Aug 30 17:32:06.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 api-versions'
Aug 30 17:32:06.615: INFO: stderr: ""
Aug 30 17:32:06.615: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:32:06.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7245" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":234,"skipped":3737,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:32:06.637: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:32:22.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1210" for this suite.

â€¢ [SLOW TEST:16.327 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":235,"skipped":3751,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:32:22.964: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1671
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 30 17:32:23.152: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-a cc8f8e00-9ef4-4c0b-bc07-e6c2b722ecc9 28801 0 2020-08-30 17:32:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 30 17:32:23.152: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-a cc8f8e00-9ef4-4c0b-bc07-e6c2b722ecc9 28801 0 2020-08-30 17:32:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 30 17:32:33.168: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-a cc8f8e00-9ef4-4c0b-bc07-e6c2b722ecc9 28858 0 2020-08-30 17:32:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 30 17:32:33.169: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-a cc8f8e00-9ef4-4c0b-bc07-e6c2b722ecc9 28858 0 2020-08-30 17:32:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 30 17:32:43.182: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-a cc8f8e00-9ef4-4c0b-bc07-e6c2b722ecc9 28893 0 2020-08-30 17:32:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 30 17:32:43.182: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-a cc8f8e00-9ef4-4c0b-bc07-e6c2b722ecc9 28893 0 2020-08-30 17:32:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 30 17:32:53.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-a cc8f8e00-9ef4-4c0b-bc07-e6c2b722ecc9 28928 0 2020-08-30 17:32:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 30 17:32:53.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-a cc8f8e00-9ef4-4c0b-bc07-e6c2b722ecc9 28928 0 2020-08-30 17:32:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 30 17:33:03.207: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-b 79745b80-4982-4d44-bdef-0915385b460e 28963 0 2020-08-30 17:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 30 17:33:03.207: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-b 79745b80-4982-4d44-bdef-0915385b460e 28963 0 2020-08-30 17:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 30 17:33:13.220: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-b 79745b80-4982-4d44-bdef-0915385b460e 28996 0 2020-08-30 17:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 30 17:33:13.220: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1671 /api/v1/namespaces/watch-1671/configmaps/e2e-watch-test-configmap-b 79745b80-4982-4d44-bdef-0915385b460e 28996 0 2020-08-30 17:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:33:23.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1671" for this suite.

â€¢ [SLOW TEST:60.273 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":236,"skipped":3753,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:33:23.238: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:33:47.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-119" for this suite.

â€¢ [SLOW TEST:24.517 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":237,"skipped":3764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:33:47.756: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Aug 30 17:33:47.934: INFO: Waiting up to 5m0s for pod "var-expansion-2acf63f0-f207-4ef8-a083-32c0e43353fe" in namespace "var-expansion-8096" to be "success or failure"
Aug 30 17:33:47.939: INFO: Pod "var-expansion-2acf63f0-f207-4ef8-a083-32c0e43353fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.91574ms
Aug 30 17:33:49.945: INFO: Pod "var-expansion-2acf63f0-f207-4ef8-a083-32c0e43353fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009985604s
STEP: Saw pod success
Aug 30 17:33:49.945: INFO: Pod "var-expansion-2acf63f0-f207-4ef8-a083-32c0e43353fe" satisfied condition "success or failure"
Aug 30 17:33:49.950: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod var-expansion-2acf63f0-f207-4ef8-a083-32c0e43353fe container dapi-container: <nil>
STEP: delete the pod
Aug 30 17:33:50.021: INFO: Waiting for pod var-expansion-2acf63f0-f207-4ef8-a083-32c0e43353fe to disappear
Aug 30 17:33:50.025: INFO: Pod var-expansion-2acf63f0-f207-4ef8-a083-32c0e43353fe no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:33:50.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8096" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":238,"skipped":3789,"failed":0}

------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:33:50.041: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:34:10.234: INFO: Container started at 2020-08-30 17:33:51 +0000 UTC, pod became ready at 2020-08-30 17:34:08 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:34:10.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9340" for this suite.

â€¢ [SLOW TEST:20.210 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":239,"skipped":3789,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:34:10.251: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:34:10.436: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b964c188-06eb-4761-aa6a-2421f0cac2cf" in namespace "projected-8939" to be "success or failure"
Aug 30 17:34:10.444: INFO: Pod "downwardapi-volume-b964c188-06eb-4761-aa6a-2421f0cac2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.236189ms
Aug 30 17:34:12.451: INFO: Pod "downwardapi-volume-b964c188-06eb-4761-aa6a-2421f0cac2cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015133659s
STEP: Saw pod success
Aug 30 17:34:12.451: INFO: Pod "downwardapi-volume-b964c188-06eb-4761-aa6a-2421f0cac2cf" satisfied condition "success or failure"
Aug 30 17:34:12.456: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-b964c188-06eb-4761-aa6a-2421f0cac2cf container client-container: <nil>
STEP: delete the pod
Aug 30 17:34:12.531: INFO: Waiting for pod downwardapi-volume-b964c188-06eb-4761-aa6a-2421f0cac2cf to disappear
Aug 30 17:34:12.535: INFO: Pod downwardapi-volume-b964c188-06eb-4761-aa6a-2421f0cac2cf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:34:12.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8939" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":240,"skipped":3790,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:34:12.549: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Aug 30 17:34:15.278: INFO: Successfully updated pod "annotationupdatefe1e7538-38e4-45ef-bf83-6a52ab850f60"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:34:17.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4779" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":241,"skipped":3808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:34:17.325: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1206
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-8b9e6561-2bca-43e7-956b-e8b1072527a8
STEP: Creating secret with name s-test-opt-upd-e763d665-4ec6-411c-8eeb-2f60e173e133
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8b9e6561-2bca-43e7-956b-e8b1072527a8
STEP: Updating secret s-test-opt-upd-e763d665-4ec6-411c-8eeb-2f60e173e133
STEP: Creating secret with name s-test-opt-create-a2014a50-6be1-47b8-8844-0a266b75a54e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:35:34.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1206" for this suite.

â€¢ [SLOW TEST:77.367 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":242,"skipped":3875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:35:34.693: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5835
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:35:34.881: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de4f28f4-8193-47ea-b5d8-b7c5fc5edcbd" in namespace "downward-api-5835" to be "success or failure"
Aug 30 17:35:34.895: INFO: Pod "downwardapi-volume-de4f28f4-8193-47ea-b5d8-b7c5fc5edcbd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.022138ms
Aug 30 17:35:36.901: INFO: Pod "downwardapi-volume-de4f28f4-8193-47ea-b5d8-b7c5fc5edcbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020276556s
STEP: Saw pod success
Aug 30 17:35:36.901: INFO: Pod "downwardapi-volume-de4f28f4-8193-47ea-b5d8-b7c5fc5edcbd" satisfied condition "success or failure"
Aug 30 17:35:36.905: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-de4f28f4-8193-47ea-b5d8-b7c5fc5edcbd container client-container: <nil>
STEP: delete the pod
Aug 30 17:35:36.974: INFO: Waiting for pod downwardapi-volume-de4f28f4-8193-47ea-b5d8-b7c5fc5edcbd to disappear
Aug 30 17:35:36.979: INFO: Pod downwardapi-volume-de4f28f4-8193-47ea-b5d8-b7c5fc5edcbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:35:36.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5835" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":243,"skipped":3905,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:35:37.001: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 30 17:35:41.695: INFO: Successfully updated pod "adopt-release-kf9tr"
STEP: Checking that the Job readopts the Pod
Aug 30 17:35:41.695: INFO: Waiting up to 15m0s for pod "adopt-release-kf9tr" in namespace "job-3698" to be "adopted"
Aug 30 17:35:41.699: INFO: Pod "adopt-release-kf9tr": Phase="Running", Reason="", readiness=true. Elapsed: 4.269078ms
Aug 30 17:35:43.705: INFO: Pod "adopt-release-kf9tr": Phase="Running", Reason="", readiness=true. Elapsed: 2.009736213s
Aug 30 17:35:43.705: INFO: Pod "adopt-release-kf9tr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 30 17:35:44.219: INFO: Successfully updated pod "adopt-release-kf9tr"
STEP: Checking that the Job releases the Pod
Aug 30 17:35:44.219: INFO: Waiting up to 15m0s for pod "adopt-release-kf9tr" in namespace "job-3698" to be "released"
Aug 30 17:35:44.223: INFO: Pod "adopt-release-kf9tr": Phase="Running", Reason="", readiness=true. Elapsed: 4.526787ms
Aug 30 17:35:46.229: INFO: Pod "adopt-release-kf9tr": Phase="Running", Reason="", readiness=true. Elapsed: 2.010599196s
Aug 30 17:35:46.230: INFO: Pod "adopt-release-kf9tr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:35:46.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3698" for this suite.

â€¢ [SLOW TEST:9.243 seconds]
[sig-apps] Job
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":244,"skipped":3918,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:35:46.245: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4640
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:35:46.447: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"0fd81173-035d-4f31-8d95-ece8eaeead2d", Controller:(*bool)(0xc004d14176), BlockOwnerDeletion:(*bool)(0xc004d14177)}}
Aug 30 17:35:46.456: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"895a778d-af2c-414f-9eb0-100ed980d8a4", Controller:(*bool)(0xc004e57bc6), BlockOwnerDeletion:(*bool)(0xc004e57bc7)}}
Aug 30 17:35:46.464: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"05718575-3730-4738-8b7c-8ebc032a58f9", Controller:(*bool)(0xc004d14336), BlockOwnerDeletion:(*bool)(0xc004d14337)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:35:51.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4640" for this suite.

â€¢ [SLOW TEST:5.267 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":245,"skipped":3957,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:35:51.512: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1918
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:35:53.740: INFO: Waiting up to 5m0s for pod "client-envvars-240c4bf3-28a0-44a8-be0d-77992470667e" in namespace "pods-1918" to be "success or failure"
Aug 30 17:35:53.748: INFO: Pod "client-envvars-240c4bf3-28a0-44a8-be0d-77992470667e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.411245ms
Aug 30 17:35:55.754: INFO: Pod "client-envvars-240c4bf3-28a0-44a8-be0d-77992470667e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013395056s
STEP: Saw pod success
Aug 30 17:35:55.754: INFO: Pod "client-envvars-240c4bf3-28a0-44a8-be0d-77992470667e" satisfied condition "success or failure"
Aug 30 17:35:55.759: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod client-envvars-240c4bf3-28a0-44a8-be0d-77992470667e container env3cont: <nil>
STEP: delete the pod
Aug 30 17:35:55.797: INFO: Waiting for pod client-envvars-240c4bf3-28a0-44a8-be0d-77992470667e to disappear
Aug 30 17:35:55.802: INFO: Pod client-envvars-240c4bf3-28a0-44a8-be0d-77992470667e no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:35:55.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1918" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":246,"skipped":3966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:35:55.822: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-3588
STEP: Creating secret with name secret-test-fe7ff3ac-d3d7-4105-9ac6-4e5b0212473a
STEP: Creating a pod to test consume secrets
Aug 30 17:35:56.180: INFO: Waiting up to 5m0s for pod "pod-secrets-8df46728-804f-45f7-b65e-5a6ba7e16195" in namespace "secrets-8349" to be "success or failure"
Aug 30 17:35:56.187: INFO: Pod "pod-secrets-8df46728-804f-45f7-b65e-5a6ba7e16195": Phase="Pending", Reason="", readiness=false. Elapsed: 7.792178ms
Aug 30 17:35:58.193: INFO: Pod "pod-secrets-8df46728-804f-45f7-b65e-5a6ba7e16195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013364769s
STEP: Saw pod success
Aug 30 17:35:58.193: INFO: Pod "pod-secrets-8df46728-804f-45f7-b65e-5a6ba7e16195" satisfied condition "success or failure"
Aug 30 17:35:58.198: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-secrets-8df46728-804f-45f7-b65e-5a6ba7e16195 container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 17:35:58.277: INFO: Waiting for pod pod-secrets-8df46728-804f-45f7-b65e-5a6ba7e16195 to disappear
Aug 30 17:35:58.281: INFO: Pod pod-secrets-8df46728-804f-45f7-b65e-5a6ba7e16195 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:35:58.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8349" for this suite.
STEP: Destroying namespace "secret-namespace-3588" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":247,"skipped":3995,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:35:58.307: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1681
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 30 17:35:58.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2218'
Aug 30 17:35:58.702: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 30 17:35:58.702: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
Aug 30 17:35:58.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete jobs e2e-test-httpd-job --namespace=kubectl-2218'
Aug 30 17:35:58.791: INFO: stderr: ""
Aug 30 17:35:58.791: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:35:58.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2218" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":248,"skipped":4006,"failed":0}

------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:35:58.810: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-634
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7928
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5106
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:36:05.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-634" for this suite.
STEP: Destroying namespace "nsdeletetest-7928" for this suite.
Aug 30 17:36:05.351: INFO: Namespace nsdeletetest-7928 was already deleted
STEP: Destroying namespace "nsdeletetest-5106" for this suite.

â€¢ [SLOW TEST:6.551 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":249,"skipped":4006,"failed":0}
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:36:05.360: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-6406
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6406 to expose endpoints map[]
Aug 30 17:36:05.555: INFO: Get endpoints failed (5.694606ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Aug 30 17:36:06.570: INFO: successfully validated that service endpoint-test2 in namespace services-6406 exposes endpoints map[] (1.021149707s elapsed)
STEP: Creating pod pod1 in namespace services-6406
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6406 to expose endpoints map[pod1:[80]]
Aug 30 17:36:08.623: INFO: successfully validated that service endpoint-test2 in namespace services-6406 exposes endpoints map[pod1:[80]] (2.033486003s elapsed)
STEP: Creating pod pod2 in namespace services-6406
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6406 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 30 17:36:10.683: INFO: successfully validated that service endpoint-test2 in namespace services-6406 exposes endpoints map[pod1:[80] pod2:[80]] (2.051096297s elapsed)
STEP: Deleting pod pod1 in namespace services-6406
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6406 to expose endpoints map[pod2:[80]]
Aug 30 17:36:11.725: INFO: successfully validated that service endpoint-test2 in namespace services-6406 exposes endpoints map[pod2:[80]] (1.027520825s elapsed)
STEP: Deleting pod pod2 in namespace services-6406
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6406 to expose endpoints map[]
Aug 30 17:36:12.747: INFO: successfully validated that service endpoint-test2 in namespace services-6406 exposes endpoints map[] (1.009907265s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:36:12.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6406" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:7.436 seconds]
[sig-network] Services
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":250,"skipped":4008,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:36:12.797: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5045
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Aug 30 17:36:12.971: INFO: Waiting up to 5m0s for pod "downward-api-91c9c019-1063-4428-be9c-ec96d8acb532" in namespace "downward-api-5045" to be "success or failure"
Aug 30 17:36:12.979: INFO: Pod "downward-api-91c9c019-1063-4428-be9c-ec96d8acb532": Phase="Pending", Reason="", readiness=false. Elapsed: 8.186921ms
Aug 30 17:36:14.985: INFO: Pod "downward-api-91c9c019-1063-4428-be9c-ec96d8acb532": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014521683s
STEP: Saw pod success
Aug 30 17:36:14.985: INFO: Pod "downward-api-91c9c019-1063-4428-be9c-ec96d8acb532" satisfied condition "success or failure"
Aug 30 17:36:14.990: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downward-api-91c9c019-1063-4428-be9c-ec96d8acb532 container dapi-container: <nil>
STEP: delete the pod
Aug 30 17:36:15.060: INFO: Waiting for pod downward-api-91c9c019-1063-4428-be9c-ec96d8acb532 to disappear
Aug 30 17:36:15.065: INFO: Pod downward-api-91c9c019-1063-4428-be9c-ec96d8acb532 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:36:15.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5045" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":251,"skipped":4028,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:36:15.082: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 30 17:36:15.591: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 17:36:17.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405775, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405775, loc:(*time.Location)(0x7925260)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405775, loc:(*time.Location)(0x7925260)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734405775, loc:(*time.Location)(0x7925260)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 17:36:20.626: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:36:20.632: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-872-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:36:21.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3466" for this suite.
STEP: Destroying namespace "webhook-3466-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.989 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":252,"skipped":4029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:36:22.072: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Aug 30 17:36:22.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-2573'
Aug 30 17:36:22.603: INFO: stderr: ""
Aug 30 17:36:22.603: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 30 17:36:22.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2573'
Aug 30 17:36:22.671: INFO: stderr: ""
Aug 30 17:36:22.671: INFO: stdout: "update-demo-nautilus-qx7q8 update-demo-nautilus-z5zjr "
Aug 30 17:36:22.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-qx7q8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:22.738: INFO: stderr: ""
Aug 30 17:36:22.738: INFO: stdout: ""
Aug 30 17:36:22.738: INFO: update-demo-nautilus-qx7q8 is created but not running
Aug 30 17:36:27.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2573'
Aug 30 17:36:27.803: INFO: stderr: ""
Aug 30 17:36:27.803: INFO: stdout: "update-demo-nautilus-qx7q8 update-demo-nautilus-z5zjr "
Aug 30 17:36:27.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-qx7q8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:27.864: INFO: stderr: ""
Aug 30 17:36:27.864: INFO: stdout: "true"
Aug 30 17:36:27.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-qx7q8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:27.937: INFO: stderr: ""
Aug 30 17:36:27.937: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 30 17:36:27.937: INFO: validating pod update-demo-nautilus-qx7q8
Aug 30 17:36:28.072: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 17:36:28.072: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 17:36:28.072: INFO: update-demo-nautilus-qx7q8 is verified up and running
Aug 30 17:36:28.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-z5zjr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:28.145: INFO: stderr: ""
Aug 30 17:36:28.145: INFO: stdout: "true"
Aug 30 17:36:28.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-z5zjr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:28.211: INFO: stderr: ""
Aug 30 17:36:28.211: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 30 17:36:28.211: INFO: validating pod update-demo-nautilus-z5zjr
Aug 30 17:36:28.344: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 17:36:28.344: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 17:36:28.344: INFO: update-demo-nautilus-z5zjr is verified up and running
STEP: scaling down the replication controller
Aug 30 17:36:28.346: INFO: scanned /root for discovery docs: <nil>
Aug 30 17:36:28.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2573'
Aug 30 17:36:29.440: INFO: stderr: ""
Aug 30 17:36:29.440: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 30 17:36:29.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2573'
Aug 30 17:36:29.511: INFO: stderr: ""
Aug 30 17:36:29.511: INFO: stdout: "update-demo-nautilus-qx7q8 update-demo-nautilus-z5zjr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 30 17:36:34.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2573'
Aug 30 17:36:34.587: INFO: stderr: ""
Aug 30 17:36:34.587: INFO: stdout: "update-demo-nautilus-qx7q8 "
Aug 30 17:36:34.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-qx7q8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:34.654: INFO: stderr: ""
Aug 30 17:36:34.654: INFO: stdout: "true"
Aug 30 17:36:34.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-qx7q8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:34.719: INFO: stderr: ""
Aug 30 17:36:34.719: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 30 17:36:34.719: INFO: validating pod update-demo-nautilus-qx7q8
Aug 30 17:36:34.745: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 17:36:34.745: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 17:36:34.745: INFO: update-demo-nautilus-qx7q8 is verified up and running
STEP: scaling up the replication controller
Aug 30 17:36:34.747: INFO: scanned /root for discovery docs: <nil>
Aug 30 17:36:34.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2573'
Aug 30 17:36:35.837: INFO: stderr: ""
Aug 30 17:36:35.837: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 30 17:36:35.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2573'
Aug 30 17:36:35.941: INFO: stderr: ""
Aug 30 17:36:35.941: INFO: stdout: "update-demo-nautilus-kgczl update-demo-nautilus-qx7q8 "
Aug 30 17:36:35.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-kgczl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:36.038: INFO: stderr: ""
Aug 30 17:36:36.038: INFO: stdout: ""
Aug 30 17:36:36.038: INFO: update-demo-nautilus-kgczl is created but not running
Aug 30 17:36:41.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2573'
Aug 30 17:36:41.104: INFO: stderr: ""
Aug 30 17:36:41.104: INFO: stdout: "update-demo-nautilus-kgczl update-demo-nautilus-qx7q8 "
Aug 30 17:36:41.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-kgczl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:41.165: INFO: stderr: ""
Aug 30 17:36:41.165: INFO: stdout: "true"
Aug 30 17:36:41.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-kgczl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:41.235: INFO: stderr: ""
Aug 30 17:36:41.235: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 30 17:36:41.235: INFO: validating pod update-demo-nautilus-kgczl
Aug 30 17:36:41.368: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 17:36:41.368: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 17:36:41.368: INFO: update-demo-nautilus-kgczl is verified up and running
Aug 30 17:36:41.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-qx7q8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:41.436: INFO: stderr: ""
Aug 30 17:36:41.436: INFO: stdout: "true"
Aug 30 17:36:41.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods update-demo-nautilus-qx7q8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2573'
Aug 30 17:36:41.499: INFO: stderr: ""
Aug 30 17:36:41.499: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 30 17:36:41.499: INFO: validating pod update-demo-nautilus-qx7q8
Aug 30 17:36:41.508: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 17:36:41.508: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 17:36:41.508: INFO: update-demo-nautilus-qx7q8 is verified up and running
STEP: using delete to clean up resources
Aug 30 17:36:41.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete --grace-period=0 --force -f - --namespace=kubectl-2573'
Aug 30 17:36:41.597: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 17:36:41.597: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 30 17:36:41.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2573'
Aug 30 17:36:41.666: INFO: stderr: "No resources found in kubectl-2573 namespace.\n"
Aug 30 17:36:41.666: INFO: stdout: ""
Aug 30 17:36:41.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -l name=update-demo --namespace=kubectl-2573 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 17:36:41.728: INFO: stderr: ""
Aug 30 17:36:41.728: INFO: stdout: "update-demo-nautilus-kgczl\nupdate-demo-nautilus-qx7q8\n"
Aug 30 17:36:42.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2573'
Aug 30 17:36:42.333: INFO: stderr: "No resources found in kubectl-2573 namespace.\n"
Aug 30 17:36:42.333: INFO: stdout: ""
Aug 30 17:36:42.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -l name=update-demo --namespace=kubectl-2573 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 17:36:42.425: INFO: stderr: ""
Aug 30 17:36:42.425: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:36:42.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2573" for this suite.

â€¢ [SLOW TEST:20.373 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":253,"skipped":4055,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:36:42.446: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Aug 30 17:36:42.608: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Aug 30 17:36:42.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-8966'
Aug 30 17:36:42.834: INFO: stderr: ""
Aug 30 17:36:42.834: INFO: stdout: "service/agnhost-slave created\n"
Aug 30 17:36:42.835: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Aug 30 17:36:42.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-8966'
Aug 30 17:36:43.006: INFO: stderr: ""
Aug 30 17:36:43.006: INFO: stdout: "service/agnhost-master created\n"
Aug 30 17:36:43.006: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 30 17:36:43.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-8966'
Aug 30 17:36:43.243: INFO: stderr: ""
Aug 30 17:36:43.243: INFO: stdout: "service/frontend created\n"
Aug 30 17:36:43.243: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 30 17:36:43.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-8966'
Aug 30 17:36:43.432: INFO: stderr: ""
Aug 30 17:36:43.433: INFO: stdout: "deployment.apps/frontend created\n"
Aug 30 17:36:43.433: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 30 17:36:43.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-8966'
Aug 30 17:36:43.684: INFO: stderr: ""
Aug 30 17:36:43.684: INFO: stdout: "deployment.apps/agnhost-master created\n"
Aug 30 17:36:43.684: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 30 17:36:43.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 create -f - --namespace=kubectl-8966'
Aug 30 17:36:43.833: INFO: stderr: ""
Aug 30 17:36:43.833: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Aug 30 17:36:43.833: INFO: Waiting for all frontend pods to be Running.
Aug 30 17:36:48.884: INFO: Waiting for frontend to serve content.
Aug 30 17:36:48.993: INFO: Trying to add a new entry to the guestbook.
Aug 30 17:36:49.125: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 30 17:36:49.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete --grace-period=0 --force -f - --namespace=kubectl-8966'
Aug 30 17:36:49.353: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 17:36:49.353: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 30 17:36:49.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete --grace-period=0 --force -f - --namespace=kubectl-8966'
Aug 30 17:36:49.458: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 17:36:49.458: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 30 17:36:49.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete --grace-period=0 --force -f - --namespace=kubectl-8966'
Aug 30 17:36:49.554: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 17:36:49.554: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 30 17:36:49.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete --grace-period=0 --force -f - --namespace=kubectl-8966'
Aug 30 17:36:49.630: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 17:36:49.630: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 30 17:36:49.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete --grace-period=0 --force -f - --namespace=kubectl-8966'
Aug 30 17:36:49.729: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 17:36:49.729: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 30 17:36:49.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete --grace-period=0 --force -f - --namespace=kubectl-8966'
Aug 30 17:36:49.834: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 17:36:49.834: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:36:49.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8966" for this suite.

â€¢ [SLOW TEST:7.406 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:380
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":254,"skipped":4056,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:36:49.852: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 30 17:37:00.128: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0830 17:37:00.128645      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:37:00.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6535" for this suite.

â€¢ [SLOW TEST:10.291 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":255,"skipped":4071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:37:00.144: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6718
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 30 17:37:05.372: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:37:06.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6718" for this suite.

â€¢ [SLOW TEST:6.266 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":256,"skipped":4097,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:37:06.414: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9606
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-72709657-5fc7-48a9-892f-b29de0edb2fd
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-72709657-5fc7-48a9-892f-b29de0edb2fd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:37:10.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9606" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":257,"skipped":4136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:37:10.798: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8346
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:37:16.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8346" for this suite.

â€¢ [SLOW TEST:5.243 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":258,"skipped":4170,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:37:16.042: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1503
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 30 17:37:16.222: INFO: Waiting up to 5m0s for pod "pod-c7d76681-90c7-41fc-a8e2-55d5d4726881" in namespace "emptydir-1503" to be "success or failure"
Aug 30 17:37:16.228: INFO: Pod "pod-c7d76681-90c7-41fc-a8e2-55d5d4726881": Phase="Pending", Reason="", readiness=false. Elapsed: 6.546889ms
Aug 30 17:37:18.234: INFO: Pod "pod-c7d76681-90c7-41fc-a8e2-55d5d4726881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012313754s
STEP: Saw pod success
Aug 30 17:37:18.234: INFO: Pod "pod-c7d76681-90c7-41fc-a8e2-55d5d4726881" satisfied condition "success or failure"
Aug 30 17:37:18.240: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-c7d76681-90c7-41fc-a8e2-55d5d4726881 container test-container: <nil>
STEP: delete the pod
Aug 30 17:37:18.330: INFO: Waiting for pod pod-c7d76681-90c7-41fc-a8e2-55d5d4726881 to disappear
Aug 30 17:37:18.334: INFO: Pod pod-c7d76681-90c7-41fc-a8e2-55d5d4726881 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:37:18.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1503" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":259,"skipped":4171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:37:18.349: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-369df46f-3838-4e5f-9b78-a35fb9745d25
STEP: Creating a pod to test consume secrets
Aug 30 17:37:18.538: INFO: Waiting up to 5m0s for pod "pod-secrets-015e69ef-bfa4-4c17-8bc3-3b7ada9b01f3" in namespace "secrets-3225" to be "success or failure"
Aug 30 17:37:18.546: INFO: Pod "pod-secrets-015e69ef-bfa4-4c17-8bc3-3b7ada9b01f3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.602152ms
Aug 30 17:37:20.552: INFO: Pod "pod-secrets-015e69ef-bfa4-4c17-8bc3-3b7ada9b01f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013566666s
STEP: Saw pod success
Aug 30 17:37:20.552: INFO: Pod "pod-secrets-015e69ef-bfa4-4c17-8bc3-3b7ada9b01f3" satisfied condition "success or failure"
Aug 30 17:37:20.557: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-secrets-015e69ef-bfa4-4c17-8bc3-3b7ada9b01f3 container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 17:37:20.590: INFO: Waiting for pod pod-secrets-015e69ef-bfa4-4c17-8bc3-3b7ada9b01f3 to disappear
Aug 30 17:37:20.595: INFO: Pod pod-secrets-015e69ef-bfa4-4c17-8bc3-3b7ada9b01f3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:37:20.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3225" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":260,"skipped":4200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:37:20.614: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:37:20.793: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-60482ec4-0546-42b4-91a7-6847253f7ec7" in namespace "security-context-test-5446" to be "success or failure"
Aug 30 17:37:20.798: INFO: Pod "busybox-readonly-false-60482ec4-0546-42b4-91a7-6847253f7ec7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.161755ms
Aug 30 17:37:22.803: INFO: Pod "busybox-readonly-false-60482ec4-0546-42b4-91a7-6847253f7ec7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010312737s
Aug 30 17:37:24.809: INFO: Pod "busybox-readonly-false-60482ec4-0546-42b4-91a7-6847253f7ec7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01588593s
Aug 30 17:37:24.809: INFO: Pod "busybox-readonly-false-60482ec4-0546-42b4-91a7-6847253f7ec7" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:37:24.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5446" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":261,"skipped":4225,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:37:24.825: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-0fef218d-7043-4b5b-b18f-26a2561b97e3
STEP: Creating a pod to test consume configMaps
Aug 30 17:37:25.000: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ca31bb2-cc5a-4714-ac93-67e4498e841d" in namespace "configmap-5311" to be "success or failure"
Aug 30 17:37:25.007: INFO: Pod "pod-configmaps-7ca31bb2-cc5a-4714-ac93-67e4498e841d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.137561ms
Aug 30 17:37:27.013: INFO: Pod "pod-configmaps-7ca31bb2-cc5a-4714-ac93-67e4498e841d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013182925s
STEP: Saw pod success
Aug 30 17:37:27.013: INFO: Pod "pod-configmaps-7ca31bb2-cc5a-4714-ac93-67e4498e841d" satisfied condition "success or failure"
Aug 30 17:37:27.018: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-configmaps-7ca31bb2-cc5a-4714-ac93-67e4498e841d container configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 17:37:27.094: INFO: Waiting for pod pod-configmaps-7ca31bb2-cc5a-4714-ac93-67e4498e841d to disappear
Aug 30 17:37:27.100: INFO: Pod pod-configmaps-7ca31bb2-cc5a-4714-ac93-67e4498e841d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:37:27.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5311" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":262,"skipped":4232,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:37:27.118: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1489
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 30 17:37:27.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1921'
Aug 30 17:37:27.375: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 30 17:37:27.375: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1495
Aug 30 17:37:29.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1921'
Aug 30 17:37:29.467: INFO: stderr: ""
Aug 30 17:37:29.467: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:37:29.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1921" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":263,"skipped":4235,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:37:29.487: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-3983
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:37:29.669: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Creating first CR 
Aug 30 17:37:30.299: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-30T17:37:30Z generation:1 name:name1 resourceVersion:31564 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1acf2841-9429-43d2-b5a0-d7ece4423026] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 30 17:37:40.309: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-30T17:37:40Z generation:1 name:name2 resourceVersion:31631 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:0c57ab91-98d6-40f9-adec-66e3d0a78f78] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 30 17:37:50.318: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-30T17:37:30Z generation:2 name:name1 resourceVersion:31670 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1acf2841-9429-43d2-b5a0-d7ece4423026] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 30 17:38:00.329: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-30T17:37:40Z generation:2 name:name2 resourceVersion:31707 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:0c57ab91-98d6-40f9-adec-66e3d0a78f78] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 30 17:38:10.342: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-30T17:37:30Z generation:2 name:name1 resourceVersion:31742 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1acf2841-9429-43d2-b5a0-d7ece4423026] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 30 17:38:20.355: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-30T17:37:40Z generation:2 name:name2 resourceVersion:31777 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:0c57ab91-98d6-40f9-adec-66e3d0a78f78] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:38:30.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3983" for this suite.

â€¢ [SLOW TEST:61.405 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":264,"skipped":4252,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:38:30.892: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:38:39.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6171" for this suite.

â€¢ [SLOW TEST:8.204 seconds]
[sig-apps] Job
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":265,"skipped":4277,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:38:39.098: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:38:39.271: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-35c87b39-92ea-4597-bc68-92203237e640" in namespace "security-context-test-8501" to be "success or failure"
Aug 30 17:38:39.278: INFO: Pod "alpine-nnp-false-35c87b39-92ea-4597-bc68-92203237e640": Phase="Pending", Reason="", readiness=false. Elapsed: 6.470966ms
Aug 30 17:38:41.284: INFO: Pod "alpine-nnp-false-35c87b39-92ea-4597-bc68-92203237e640": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012415597s
Aug 30 17:38:43.290: INFO: Pod "alpine-nnp-false-35c87b39-92ea-4597-bc68-92203237e640": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018962621s
Aug 30 17:38:43.291: INFO: Pod "alpine-nnp-false-35c87b39-92ea-4597-bc68-92203237e640" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:38:43.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8501" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":266,"skipped":4281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:38:43.400: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1609
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-6cc38b18-7471-41f6-9765-12ddb0b8e6b6
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:38:45.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1609" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":267,"skipped":4322,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:38:45.730: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8598
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8598
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8598
Aug 30 17:38:45.911: INFO: Found 0 stateful pods, waiting for 1
Aug 30 17:38:55.920: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 30 17:38:55.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8598 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 17:38:56.468: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 17:38:56.468: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 17:38:56.468: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 17:38:56.476: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 30 17:39:06.485: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 17:39:06.485: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 17:39:06.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999771s
Aug 30 17:39:07.527: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994253541s
Aug 30 17:39:08.543: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987658623s
Aug 30 17:39:09.549: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.972045034s
Aug 30 17:39:10.555: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.966007102s
Aug 30 17:39:11.560: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.960124494s
Aug 30 17:39:12.566: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.954608542s
Aug 30 17:39:13.573: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.948536464s
Aug 30 17:39:14.579: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.942007675s
Aug 30 17:39:15.585: INFO: Verifying statefulset ss doesn't scale past 1 for another 936.046112ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8598
Aug 30 17:39:16.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8598 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:39:17.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 17:39:17.135: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 17:39:17.135: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 17:39:17.140: INFO: Found 1 stateful pods, waiting for 3
Aug 30 17:39:27.147: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:39:27.147: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 17:39:27.147: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 30 17:39:27.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8598 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 17:39:27.689: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 17:39:27.689: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 17:39:27.689: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 17:39:27.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8598 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 17:39:28.272: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 17:39:28.272: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 17:39:28.272: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 17:39:28.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8598 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 17:39:28.846: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 17:39:28.846: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 17:39:28.846: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 17:39:28.846: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 17:39:28.851: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 30 17:39:38.865: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 17:39:38.865: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 17:39:38.865: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 17:39:38.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999809s
Aug 30 17:39:39.893: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992858964s
Aug 30 17:39:40.900: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986062848s
Aug 30 17:39:41.906: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979501863s
Aug 30 17:39:42.912: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972627621s
Aug 30 17:39:43.919: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967248185s
Aug 30 17:39:44.925: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960562097s
Aug 30 17:39:45.931: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.954152895s
Aug 30 17:39:46.937: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.948441166s
Aug 30 17:39:47.944: INFO: Verifying statefulset ss doesn't scale past 3 for another 941.642874ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8598
Aug 30 17:39:48.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8598 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:39:49.523: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 17:39:49.523: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 17:39:49.523: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 17:39:49.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8598 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:39:50.070: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 17:39:50.070: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 17:39:50.070: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 17:39:50.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 exec --namespace=statefulset-8598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 17:39:50.621: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 17:39:50.621: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 17:39:50.621: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 17:39:50.621: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 30 17:40:10.647: INFO: Deleting all statefulset in ns statefulset-8598
Aug 30 17:40:10.652: INFO: Scaling statefulset ss to 0
Aug 30 17:40:10.666: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 17:40:10.670: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:40:10.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8598" for this suite.

â€¢ [SLOW TEST:84.975 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":268,"skipped":4328,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:40:10.707: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4076.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4076.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4076.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4076.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 17:40:13.126: INFO: DNS probes using dns-test-faf4e455-ff42-4394-affa-751eafbe802c succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4076.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4076.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4076.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4076.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 17:40:15.313: INFO: File wheezy_udp@dns-test-service-3.dns-4076.svc.cluster.local from pod  dns-4076/dns-test-771385ad-1462-40b3-b5e5-4f57f49672bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 30 17:40:15.402: INFO: File jessie_udp@dns-test-service-3.dns-4076.svc.cluster.local from pod  dns-4076/dns-test-771385ad-1462-40b3-b5e5-4f57f49672bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 30 17:40:15.402: INFO: Lookups using dns-4076/dns-test-771385ad-1462-40b3-b5e5-4f57f49672bc failed for: [wheezy_udp@dns-test-service-3.dns-4076.svc.cluster.local jessie_udp@dns-test-service-3.dns-4076.svc.cluster.local]

Aug 30 17:40:20.499: INFO: DNS probes using dns-test-771385ad-1462-40b3-b5e5-4f57f49672bc succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4076.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4076.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4076.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4076.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 17:40:22.791: INFO: DNS probes using dns-test-68d45c06-4090-4960-a0ae-9e2a8ad4c0f7 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:40:22.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4076" for this suite.

â€¢ [SLOW TEST:12.140 seconds]
[sig-network] DNS
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":269,"skipped":4345,"failed":0}
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:40:22.848: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1197
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:40:23.021: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1197
I0830 17:40:23.035532      23 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1197, replica count: 1
I0830 17:40:24.085900      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 17:40:25.086162      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 17:40:25.201: INFO: Created: latency-svc-mtcsf
Aug 30 17:40:25.207: INFO: Got endpoints: latency-svc-mtcsf [21.027577ms]
Aug 30 17:40:25.220: INFO: Created: latency-svc-sfp86
Aug 30 17:40:25.227: INFO: Got endpoints: latency-svc-sfp86 [19.747092ms]
Aug 30 17:40:25.229: INFO: Created: latency-svc-wb9wk
Aug 30 17:40:25.235: INFO: Got endpoints: latency-svc-wb9wk [27.371439ms]
Aug 30 17:40:25.235: INFO: Created: latency-svc-z2jqv
Aug 30 17:40:25.241: INFO: Got endpoints: latency-svc-z2jqv [32.864282ms]
Aug 30 17:40:25.242: INFO: Created: latency-svc-fzptm
Aug 30 17:40:25.247: INFO: Got endpoints: latency-svc-fzptm [38.99426ms]
Aug 30 17:40:25.250: INFO: Created: latency-svc-xwt6d
Aug 30 17:40:25.254: INFO: Got endpoints: latency-svc-xwt6d [45.109874ms]
Aug 30 17:40:25.263: INFO: Created: latency-svc-57ls2
Aug 30 17:40:25.265: INFO: Got endpoints: latency-svc-57ls2 [56.536062ms]
Aug 30 17:40:25.268: INFO: Created: latency-svc-mqnzh
Aug 30 17:40:25.269: INFO: Got endpoints: latency-svc-mqnzh [59.951364ms]
Aug 30 17:40:25.272: INFO: Created: latency-svc-jm8bc
Aug 30 17:40:25.278: INFO: Got endpoints: latency-svc-jm8bc [68.91451ms]
Aug 30 17:40:25.284: INFO: Created: latency-svc-cknnk
Aug 30 17:40:25.290: INFO: Got endpoints: latency-svc-cknnk [80.728323ms]
Aug 30 17:40:25.290: INFO: Created: latency-svc-6z86n
Aug 30 17:40:25.295: INFO: Created: latency-svc-8hzmb
Aug 30 17:40:25.297: INFO: Got endpoints: latency-svc-6z86n [87.855098ms]
Aug 30 17:40:25.302: INFO: Got endpoints: latency-svc-8hzmb [92.146731ms]
Aug 30 17:40:25.306: INFO: Created: latency-svc-4jphk
Aug 30 17:40:25.314: INFO: Got endpoints: latency-svc-4jphk [104.801016ms]
Aug 30 17:40:25.315: INFO: Created: latency-svc-9c8fj
Aug 30 17:40:25.320: INFO: Got endpoints: latency-svc-9c8fj [111.190115ms]
Aug 30 17:40:25.321: INFO: Created: latency-svc-zbmth
Aug 30 17:40:25.325: INFO: Got endpoints: latency-svc-zbmth [115.842441ms]
Aug 30 17:40:25.328: INFO: Created: latency-svc-v8xml
Aug 30 17:40:25.334: INFO: Got endpoints: latency-svc-v8xml [125.143967ms]
Aug 30 17:40:25.336: INFO: Created: latency-svc-tnzqm
Aug 30 17:40:25.342: INFO: Created: latency-svc-mclht
Aug 30 17:40:25.342: INFO: Got endpoints: latency-svc-tnzqm [115.39452ms]
Aug 30 17:40:25.348: INFO: Got endpoints: latency-svc-mclht [112.566162ms]
Aug 30 17:40:25.353: INFO: Created: latency-svc-lfgsm
Aug 30 17:40:25.355: INFO: Created: latency-svc-bbdhl
Aug 30 17:40:25.356: INFO: Got endpoints: latency-svc-lfgsm [115.218468ms]
Aug 30 17:40:25.362: INFO: Got endpoints: latency-svc-bbdhl [114.388375ms]
Aug 30 17:40:25.364: INFO: Created: latency-svc-xtmxj
Aug 30 17:40:25.370: INFO: Got endpoints: latency-svc-xtmxj [116.696427ms]
Aug 30 17:40:25.371: INFO: Created: latency-svc-bthd6
Aug 30 17:40:25.378: INFO: Got endpoints: latency-svc-bthd6 [112.429517ms]
Aug 30 17:40:25.384: INFO: Created: latency-svc-2br4g
Aug 30 17:40:25.390: INFO: Got endpoints: latency-svc-2br4g [121.471705ms]
Aug 30 17:40:25.394: INFO: Created: latency-svc-h6s9q
Aug 30 17:40:25.402: INFO: Created: latency-svc-vtpst
Aug 30 17:40:25.402: INFO: Got endpoints: latency-svc-h6s9q [123.994493ms]
Aug 30 17:40:25.410: INFO: Created: latency-svc-brjr8
Aug 30 17:40:25.410: INFO: Got endpoints: latency-svc-vtpst [120.110218ms]
Aug 30 17:40:25.415: INFO: Got endpoints: latency-svc-brjr8 [118.111443ms]
Aug 30 17:40:25.419: INFO: Created: latency-svc-v5k8f
Aug 30 17:40:25.426: INFO: Got endpoints: latency-svc-v5k8f [124.344694ms]
Aug 30 17:40:25.431: INFO: Created: latency-svc-cbdrq
Aug 30 17:40:25.437: INFO: Got endpoints: latency-svc-cbdrq [122.689519ms]
Aug 30 17:40:25.444: INFO: Created: latency-svc-gmpwd
Aug 30 17:40:25.452: INFO: Got endpoints: latency-svc-gmpwd [131.640602ms]
Aug 30 17:40:25.456: INFO: Created: latency-svc-pcgps
Aug 30 17:40:25.462: INFO: Got endpoints: latency-svc-pcgps [136.218889ms]
Aug 30 17:40:25.465: INFO: Created: latency-svc-k9fwk
Aug 30 17:40:25.474: INFO: Created: latency-svc-x5c5d
Aug 30 17:40:25.474: INFO: Got endpoints: latency-svc-k9fwk [140.739548ms]
Aug 30 17:40:25.480: INFO: Got endpoints: latency-svc-x5c5d [137.277606ms]
Aug 30 17:40:25.481: INFO: Created: latency-svc-js5dv
Aug 30 17:40:25.488: INFO: Got endpoints: latency-svc-js5dv [140.030187ms]
Aug 30 17:40:25.490: INFO: Created: latency-svc-dfk59
Aug 30 17:40:25.495: INFO: Got endpoints: latency-svc-dfk59 [138.72036ms]
Aug 30 17:40:25.499: INFO: Created: latency-svc-4qxgq
Aug 30 17:40:25.504: INFO: Got endpoints: latency-svc-4qxgq [142.367773ms]
Aug 30 17:40:25.508: INFO: Created: latency-svc-7v2bd
Aug 30 17:40:25.516: INFO: Got endpoints: latency-svc-7v2bd [145.804483ms]
Aug 30 17:40:25.518: INFO: Created: latency-svc-fksd5
Aug 30 17:40:25.526: INFO: Created: latency-svc-q4cnm
Aug 30 17:40:25.533: INFO: Created: latency-svc-q27d6
Aug 30 17:40:25.542: INFO: Created: latency-svc-dggbp
Aug 30 17:40:25.550: INFO: Created: latency-svc-phqlt
Aug 30 17:40:25.556: INFO: Created: latency-svc-cgrl4
Aug 30 17:40:25.558: INFO: Got endpoints: latency-svc-fksd5 [180.239938ms]
Aug 30 17:40:25.566: INFO: Created: latency-svc-z44ms
Aug 30 17:40:25.579: INFO: Created: latency-svc-n64v6
Aug 30 17:40:25.591: INFO: Created: latency-svc-pt9g4
Aug 30 17:40:25.600: INFO: Created: latency-svc-btkv9
Aug 30 17:40:25.606: INFO: Got endpoints: latency-svc-q4cnm [216.12899ms]
Aug 30 17:40:25.607: INFO: Created: latency-svc-p7wmx
Aug 30 17:40:25.616: INFO: Created: latency-svc-9qcgx
Aug 30 17:40:25.625: INFO: Created: latency-svc-mrl8t
Aug 30 17:40:25.630: INFO: Created: latency-svc-dfcnj
Aug 30 17:40:25.642: INFO: Created: latency-svc-qzklq
Aug 30 17:40:25.651: INFO: Created: latency-svc-dkvbz
Aug 30 17:40:25.656: INFO: Created: latency-svc-zmqvh
Aug 30 17:40:25.657: INFO: Got endpoints: latency-svc-q27d6 [255.051452ms]
Aug 30 17:40:25.671: INFO: Created: latency-svc-69rrm
Aug 30 17:40:25.708: INFO: Got endpoints: latency-svc-dggbp [297.633383ms]
Aug 30 17:40:25.725: INFO: Created: latency-svc-5v7m8
Aug 30 17:40:25.757: INFO: Got endpoints: latency-svc-phqlt [342.137437ms]
Aug 30 17:40:25.775: INFO: Created: latency-svc-26xw6
Aug 30 17:40:25.808: INFO: Got endpoints: latency-svc-cgrl4 [382.163721ms]
Aug 30 17:40:25.823: INFO: Created: latency-svc-2gzpg
Aug 30 17:40:25.858: INFO: Got endpoints: latency-svc-z44ms [420.88148ms]
Aug 30 17:40:25.880: INFO: Created: latency-svc-8g4sx
Aug 30 17:40:25.907: INFO: Got endpoints: latency-svc-n64v6 [455.264055ms]
Aug 30 17:40:25.922: INFO: Created: latency-svc-zqtcn
Aug 30 17:40:25.960: INFO: Got endpoints: latency-svc-pt9g4 [498.679ms]
Aug 30 17:40:25.979: INFO: Created: latency-svc-jpswr
Aug 30 17:40:26.014: INFO: Got endpoints: latency-svc-btkv9 [539.593052ms]
Aug 30 17:40:26.031: INFO: Created: latency-svc-w2bh8
Aug 30 17:40:26.057: INFO: Got endpoints: latency-svc-p7wmx [577.231985ms]
Aug 30 17:40:26.071: INFO: Created: latency-svc-cdh8m
Aug 30 17:40:26.107: INFO: Got endpoints: latency-svc-9qcgx [618.651758ms]
Aug 30 17:40:26.120: INFO: Created: latency-svc-jdg7f
Aug 30 17:40:26.158: INFO: Got endpoints: latency-svc-mrl8t [662.216785ms]
Aug 30 17:40:26.174: INFO: Created: latency-svc-d8f7n
Aug 30 17:40:26.207: INFO: Got endpoints: latency-svc-dfcnj [703.188085ms]
Aug 30 17:40:26.220: INFO: Created: latency-svc-wgr88
Aug 30 17:40:26.258: INFO: Got endpoints: latency-svc-qzklq [741.808567ms]
Aug 30 17:40:26.274: INFO: Created: latency-svc-zwn69
Aug 30 17:40:26.307: INFO: Got endpoints: latency-svc-dkvbz [748.918449ms]
Aug 30 17:40:26.321: INFO: Created: latency-svc-pm98t
Aug 30 17:40:26.357: INFO: Got endpoints: latency-svc-zmqvh [750.355651ms]
Aug 30 17:40:26.370: INFO: Created: latency-svc-6f8b2
Aug 30 17:40:26.409: INFO: Got endpoints: latency-svc-69rrm [751.780504ms]
Aug 30 17:40:26.422: INFO: Created: latency-svc-xdp56
Aug 30 17:40:26.457: INFO: Got endpoints: latency-svc-5v7m8 [749.808028ms]
Aug 30 17:40:26.472: INFO: Created: latency-svc-t8mg6
Aug 30 17:40:26.508: INFO: Got endpoints: latency-svc-26xw6 [750.848902ms]
Aug 30 17:40:26.520: INFO: Created: latency-svc-l2lqk
Aug 30 17:40:26.557: INFO: Got endpoints: latency-svc-2gzpg [748.923618ms]
Aug 30 17:40:26.572: INFO: Created: latency-svc-5hhfh
Aug 30 17:40:26.608: INFO: Got endpoints: latency-svc-8g4sx [750.013375ms]
Aug 30 17:40:26.621: INFO: Created: latency-svc-68lxv
Aug 30 17:40:26.658: INFO: Got endpoints: latency-svc-zqtcn [750.353553ms]
Aug 30 17:40:26.671: INFO: Created: latency-svc-6chlq
Aug 30 17:40:26.707: INFO: Got endpoints: latency-svc-jpswr [746.741792ms]
Aug 30 17:40:26.721: INFO: Created: latency-svc-2d7pz
Aug 30 17:40:26.758: INFO: Got endpoints: latency-svc-w2bh8 [743.699187ms]
Aug 30 17:40:26.771: INFO: Created: latency-svc-fmzj5
Aug 30 17:40:26.808: INFO: Got endpoints: latency-svc-cdh8m [751.340517ms]
Aug 30 17:40:26.822: INFO: Created: latency-svc-qpmtb
Aug 30 17:40:26.862: INFO: Got endpoints: latency-svc-jdg7f [755.155608ms]
Aug 30 17:40:26.877: INFO: Created: latency-svc-2bxff
Aug 30 17:40:26.908: INFO: Got endpoints: latency-svc-d8f7n [750.1407ms]
Aug 30 17:40:26.923: INFO: Created: latency-svc-qgrqp
Aug 30 17:40:26.959: INFO: Got endpoints: latency-svc-wgr88 [751.392303ms]
Aug 30 17:40:26.974: INFO: Created: latency-svc-xgxc6
Aug 30 17:40:27.016: INFO: Got endpoints: latency-svc-zwn69 [758.094973ms]
Aug 30 17:40:27.032: INFO: Created: latency-svc-jsbqr
Aug 30 17:40:27.058: INFO: Got endpoints: latency-svc-pm98t [750.507079ms]
Aug 30 17:40:27.076: INFO: Created: latency-svc-gcdgz
Aug 30 17:40:27.107: INFO: Got endpoints: latency-svc-6f8b2 [750.519771ms]
Aug 30 17:40:27.120: INFO: Created: latency-svc-6rlqj
Aug 30 17:40:27.157: INFO: Got endpoints: latency-svc-xdp56 [748.374491ms]
Aug 30 17:40:27.172: INFO: Created: latency-svc-tnpch
Aug 30 17:40:27.208: INFO: Got endpoints: latency-svc-t8mg6 [750.625412ms]
Aug 30 17:40:27.224: INFO: Created: latency-svc-vrq9b
Aug 30 17:40:27.259: INFO: Got endpoints: latency-svc-l2lqk [751.191465ms]
Aug 30 17:40:27.274: INFO: Created: latency-svc-f5dqn
Aug 30 17:40:27.311: INFO: Got endpoints: latency-svc-5hhfh [753.593291ms]
Aug 30 17:40:27.326: INFO: Created: latency-svc-8flb9
Aug 30 17:40:27.357: INFO: Got endpoints: latency-svc-68lxv [749.095614ms]
Aug 30 17:40:27.371: INFO: Created: latency-svc-2ndv8
Aug 30 17:40:27.410: INFO: Got endpoints: latency-svc-6chlq [751.791899ms]
Aug 30 17:40:27.422: INFO: Created: latency-svc-wpnkz
Aug 30 17:40:27.458: INFO: Got endpoints: latency-svc-2d7pz [750.373818ms]
Aug 30 17:40:27.472: INFO: Created: latency-svc-lmt5b
Aug 30 17:40:27.507: INFO: Got endpoints: latency-svc-fmzj5 [749.044054ms]
Aug 30 17:40:27.522: INFO: Created: latency-svc-stblj
Aug 30 17:40:27.557: INFO: Got endpoints: latency-svc-qpmtb [748.828823ms]
Aug 30 17:40:27.569: INFO: Created: latency-svc-ktgmp
Aug 30 17:40:27.608: INFO: Got endpoints: latency-svc-2bxff [746.104044ms]
Aug 30 17:40:27.623: INFO: Created: latency-svc-ztskb
Aug 30 17:40:27.657: INFO: Got endpoints: latency-svc-qgrqp [749.533985ms]
Aug 30 17:40:27.681: INFO: Created: latency-svc-rjdkj
Aug 30 17:40:27.707: INFO: Got endpoints: latency-svc-xgxc6 [748.491534ms]
Aug 30 17:40:27.721: INFO: Created: latency-svc-vvft2
Aug 30 17:40:27.757: INFO: Got endpoints: latency-svc-jsbqr [740.597973ms]
Aug 30 17:40:27.772: INFO: Created: latency-svc-qx2jf
Aug 30 17:40:27.807: INFO: Got endpoints: latency-svc-gcdgz [749.026397ms]
Aug 30 17:40:27.820: INFO: Created: latency-svc-6z5qg
Aug 30 17:40:27.858: INFO: Got endpoints: latency-svc-6rlqj [750.037082ms]
Aug 30 17:40:27.872: INFO: Created: latency-svc-hjzhl
Aug 30 17:40:27.908: INFO: Got endpoints: latency-svc-tnpch [751.162759ms]
Aug 30 17:40:27.925: INFO: Created: latency-svc-7lcz6
Aug 30 17:40:27.958: INFO: Got endpoints: latency-svc-vrq9b [749.509216ms]
Aug 30 17:40:27.973: INFO: Created: latency-svc-474tq
Aug 30 17:40:28.014: INFO: Got endpoints: latency-svc-f5dqn [754.438236ms]
Aug 30 17:40:28.026: INFO: Created: latency-svc-4msw7
Aug 30 17:40:28.057: INFO: Got endpoints: latency-svc-8flb9 [746.617233ms]
Aug 30 17:40:28.074: INFO: Created: latency-svc-pgk7h
Aug 30 17:40:28.107: INFO: Got endpoints: latency-svc-2ndv8 [750.552746ms]
Aug 30 17:40:28.120: INFO: Created: latency-svc-2fj6k
Aug 30 17:40:28.157: INFO: Got endpoints: latency-svc-wpnkz [746.906536ms]
Aug 30 17:40:28.174: INFO: Created: latency-svc-s62m6
Aug 30 17:40:28.206: INFO: Got endpoints: latency-svc-lmt5b [748.893136ms]
Aug 30 17:40:28.220: INFO: Created: latency-svc-qs98c
Aug 30 17:40:28.258: INFO: Got endpoints: latency-svc-stblj [750.811925ms]
Aug 30 17:40:28.272: INFO: Created: latency-svc-hjxmk
Aug 30 17:40:28.307: INFO: Got endpoints: latency-svc-ktgmp [749.902398ms]
Aug 30 17:40:28.321: INFO: Created: latency-svc-q488f
Aug 30 17:40:28.357: INFO: Got endpoints: latency-svc-ztskb [748.258222ms]
Aug 30 17:40:28.372: INFO: Created: latency-svc-vn9hv
Aug 30 17:40:28.408: INFO: Got endpoints: latency-svc-rjdkj [750.481405ms]
Aug 30 17:40:28.421: INFO: Created: latency-svc-6fdbx
Aug 30 17:40:28.457: INFO: Got endpoints: latency-svc-vvft2 [749.275855ms]
Aug 30 17:40:28.475: INFO: Created: latency-svc-lm2v7
Aug 30 17:40:28.511: INFO: Got endpoints: latency-svc-qx2jf [754.211249ms]
Aug 30 17:40:28.525: INFO: Created: latency-svc-cqtqc
Aug 30 17:40:28.556: INFO: Got endpoints: latency-svc-6z5qg [748.977457ms]
Aug 30 17:40:28.570: INFO: Created: latency-svc-s8rw9
Aug 30 17:40:28.608: INFO: Got endpoints: latency-svc-hjzhl [750.163781ms]
Aug 30 17:40:28.621: INFO: Created: latency-svc-w8xtm
Aug 30 17:40:28.658: INFO: Got endpoints: latency-svc-7lcz6 [749.715529ms]
Aug 30 17:40:28.673: INFO: Created: latency-svc-d92xt
Aug 30 17:40:28.708: INFO: Got endpoints: latency-svc-474tq [750.096147ms]
Aug 30 17:40:28.721: INFO: Created: latency-svc-wjpmj
Aug 30 17:40:28.757: INFO: Got endpoints: latency-svc-4msw7 [743.59917ms]
Aug 30 17:40:28.772: INFO: Created: latency-svc-hdwp7
Aug 30 17:40:28.807: INFO: Got endpoints: latency-svc-pgk7h [749.957765ms]
Aug 30 17:40:28.822: INFO: Created: latency-svc-d5hsn
Aug 30 17:40:28.857: INFO: Got endpoints: latency-svc-2fj6k [749.780399ms]
Aug 30 17:40:28.871: INFO: Created: latency-svc-95xdz
Aug 30 17:40:28.908: INFO: Got endpoints: latency-svc-s62m6 [751.568163ms]
Aug 30 17:40:28.924: INFO: Created: latency-svc-qbjzc
Aug 30 17:40:28.957: INFO: Got endpoints: latency-svc-qs98c [750.935842ms]
Aug 30 17:40:28.970: INFO: Created: latency-svc-9qwpg
Aug 30 17:40:29.007: INFO: Got endpoints: latency-svc-hjxmk [748.45108ms]
Aug 30 17:40:29.019: INFO: Created: latency-svc-mpv2r
Aug 30 17:40:29.058: INFO: Got endpoints: latency-svc-q488f [750.959362ms]
Aug 30 17:40:29.076: INFO: Created: latency-svc-c27lx
Aug 30 17:40:29.112: INFO: Got endpoints: latency-svc-vn9hv [754.824827ms]
Aug 30 17:40:29.128: INFO: Created: latency-svc-rrdnh
Aug 30 17:40:29.169: INFO: Got endpoints: latency-svc-6fdbx [760.736085ms]
Aug 30 17:40:29.194: INFO: Created: latency-svc-q4kbf
Aug 30 17:40:29.207: INFO: Got endpoints: latency-svc-lm2v7 [750.785818ms]
Aug 30 17:40:29.220: INFO: Created: latency-svc-cvqzh
Aug 30 17:40:29.259: INFO: Got endpoints: latency-svc-cqtqc [747.415829ms]
Aug 30 17:40:29.274: INFO: Created: latency-svc-q6hjh
Aug 30 17:40:29.307: INFO: Got endpoints: latency-svc-s8rw9 [751.456671ms]
Aug 30 17:40:29.321: INFO: Created: latency-svc-kg8c2
Aug 30 17:40:29.359: INFO: Got endpoints: latency-svc-w8xtm [750.82407ms]
Aug 30 17:40:29.375: INFO: Created: latency-svc-gsbkb
Aug 30 17:40:29.407: INFO: Got endpoints: latency-svc-d92xt [749.336362ms]
Aug 30 17:40:29.422: INFO: Created: latency-svc-ldc7g
Aug 30 17:40:29.458: INFO: Got endpoints: latency-svc-wjpmj [750.219094ms]
Aug 30 17:40:29.473: INFO: Created: latency-svc-mxpmh
Aug 30 17:40:29.508: INFO: Got endpoints: latency-svc-hdwp7 [750.501449ms]
Aug 30 17:40:29.521: INFO: Created: latency-svc-pltwv
Aug 30 17:40:29.559: INFO: Got endpoints: latency-svc-d5hsn [751.393725ms]
Aug 30 17:40:29.572: INFO: Created: latency-svc-74wbm
Aug 30 17:40:29.608: INFO: Got endpoints: latency-svc-95xdz [750.009318ms]
Aug 30 17:40:29.622: INFO: Created: latency-svc-wlgj4
Aug 30 17:40:29.657: INFO: Got endpoints: latency-svc-qbjzc [749.184806ms]
Aug 30 17:40:29.670: INFO: Created: latency-svc-kbcvr
Aug 30 17:40:29.711: INFO: Got endpoints: latency-svc-9qwpg [753.198298ms]
Aug 30 17:40:29.725: INFO: Created: latency-svc-2kbxz
Aug 30 17:40:29.759: INFO: Got endpoints: latency-svc-mpv2r [752.380471ms]
Aug 30 17:40:29.778: INFO: Created: latency-svc-s7bjh
Aug 30 17:40:29.808: INFO: Got endpoints: latency-svc-c27lx [749.376372ms]
Aug 30 17:40:29.828: INFO: Created: latency-svc-s28hx
Aug 30 17:40:29.858: INFO: Got endpoints: latency-svc-rrdnh [746.743075ms]
Aug 30 17:40:29.873: INFO: Created: latency-svc-5mql7
Aug 30 17:40:29.907: INFO: Got endpoints: latency-svc-q4kbf [738.457575ms]
Aug 30 17:40:29.921: INFO: Created: latency-svc-bjzbv
Aug 30 17:40:29.962: INFO: Got endpoints: latency-svc-cvqzh [754.036872ms]
Aug 30 17:40:29.975: INFO: Created: latency-svc-rv52m
Aug 30 17:40:30.007: INFO: Got endpoints: latency-svc-q6hjh [748.334898ms]
Aug 30 17:40:30.022: INFO: Created: latency-svc-w6rmt
Aug 30 17:40:30.058: INFO: Got endpoints: latency-svc-kg8c2 [750.714656ms]
Aug 30 17:40:30.072: INFO: Created: latency-svc-sf9b2
Aug 30 17:40:30.107: INFO: Got endpoints: latency-svc-gsbkb [748.668967ms]
Aug 30 17:40:30.122: INFO: Created: latency-svc-wn7z6
Aug 30 17:40:30.157: INFO: Got endpoints: latency-svc-ldc7g [749.785173ms]
Aug 30 17:40:30.173: INFO: Created: latency-svc-54qd4
Aug 30 17:40:30.208: INFO: Got endpoints: latency-svc-mxpmh [749.402901ms]
Aug 30 17:40:30.221: INFO: Created: latency-svc-tkj68
Aug 30 17:40:30.259: INFO: Got endpoints: latency-svc-pltwv [750.735277ms]
Aug 30 17:40:30.274: INFO: Created: latency-svc-zsffq
Aug 30 17:40:30.308: INFO: Got endpoints: latency-svc-74wbm [749.12238ms]
Aug 30 17:40:30.321: INFO: Created: latency-svc-nhbjk
Aug 30 17:40:30.360: INFO: Got endpoints: latency-svc-wlgj4 [752.201013ms]
Aug 30 17:40:30.375: INFO: Created: latency-svc-vrd58
Aug 30 17:40:30.408: INFO: Got endpoints: latency-svc-kbcvr [750.758051ms]
Aug 30 17:40:30.423: INFO: Created: latency-svc-fbbqw
Aug 30 17:40:30.457: INFO: Got endpoints: latency-svc-2kbxz [746.344645ms]
Aug 30 17:40:30.472: INFO: Created: latency-svc-l8krb
Aug 30 17:40:30.507: INFO: Got endpoints: latency-svc-s7bjh [747.733484ms]
Aug 30 17:40:30.520: INFO: Created: latency-svc-svxrl
Aug 30 17:40:30.559: INFO: Got endpoints: latency-svc-s28hx [751.770098ms]
Aug 30 17:40:30.571: INFO: Created: latency-svc-pw5lt
Aug 30 17:40:30.607: INFO: Got endpoints: latency-svc-5mql7 [749.006624ms]
Aug 30 17:40:30.621: INFO: Created: latency-svc-r8ldv
Aug 30 17:40:30.659: INFO: Got endpoints: latency-svc-bjzbv [751.450032ms]
Aug 30 17:40:30.673: INFO: Created: latency-svc-9cvwl
Aug 30 17:40:30.708: INFO: Got endpoints: latency-svc-rv52m [746.23213ms]
Aug 30 17:40:30.723: INFO: Created: latency-svc-8jmbf
Aug 30 17:40:30.759: INFO: Got endpoints: latency-svc-w6rmt [751.444774ms]
Aug 30 17:40:30.776: INFO: Created: latency-svc-f5tbf
Aug 30 17:40:30.808: INFO: Got endpoints: latency-svc-sf9b2 [749.820774ms]
Aug 30 17:40:30.822: INFO: Created: latency-svc-trrhn
Aug 30 17:40:30.860: INFO: Got endpoints: latency-svc-wn7z6 [752.208863ms]
Aug 30 17:40:30.876: INFO: Created: latency-svc-gvjjr
Aug 30 17:40:30.907: INFO: Got endpoints: latency-svc-54qd4 [749.897046ms]
Aug 30 17:40:30.926: INFO: Created: latency-svc-fwfgp
Aug 30 17:40:30.958: INFO: Got endpoints: latency-svc-tkj68 [750.032985ms]
Aug 30 17:40:30.972: INFO: Created: latency-svc-cv8q4
Aug 30 17:40:31.013: INFO: Got endpoints: latency-svc-zsffq [753.942733ms]
Aug 30 17:40:31.026: INFO: Created: latency-svc-6jp95
Aug 30 17:40:31.060: INFO: Got endpoints: latency-svc-nhbjk [751.581256ms]
Aug 30 17:40:31.074: INFO: Created: latency-svc-z6w94
Aug 30 17:40:31.108: INFO: Got endpoints: latency-svc-vrd58 [747.721953ms]
Aug 30 17:40:31.122: INFO: Created: latency-svc-4qmh5
Aug 30 17:40:31.159: INFO: Got endpoints: latency-svc-fbbqw [750.188758ms]
Aug 30 17:40:31.174: INFO: Created: latency-svc-nhwpg
Aug 30 17:40:31.207: INFO: Got endpoints: latency-svc-l8krb [750.398426ms]
Aug 30 17:40:31.221: INFO: Created: latency-svc-7zbp4
Aug 30 17:40:31.258: INFO: Got endpoints: latency-svc-svxrl [751.345062ms]
Aug 30 17:40:31.272: INFO: Created: latency-svc-6q2jv
Aug 30 17:40:31.313: INFO: Got endpoints: latency-svc-pw5lt [753.327947ms]
Aug 30 17:40:31.349: INFO: Created: latency-svc-zhrr7
Aug 30 17:40:31.362: INFO: Got endpoints: latency-svc-r8ldv [755.054112ms]
Aug 30 17:40:31.386: INFO: Created: latency-svc-5rfdk
Aug 30 17:40:31.407: INFO: Got endpoints: latency-svc-9cvwl [748.469815ms]
Aug 30 17:40:31.422: INFO: Created: latency-svc-krskz
Aug 30 17:40:31.459: INFO: Got endpoints: latency-svc-8jmbf [750.816982ms]
Aug 30 17:40:31.473: INFO: Created: latency-svc-stcfq
Aug 30 17:40:31.507: INFO: Got endpoints: latency-svc-f5tbf [748.355561ms]
Aug 30 17:40:31.521: INFO: Created: latency-svc-h92cs
Aug 30 17:40:31.558: INFO: Got endpoints: latency-svc-trrhn [750.053366ms]
Aug 30 17:40:31.573: INFO: Created: latency-svc-plcm6
Aug 30 17:40:31.607: INFO: Got endpoints: latency-svc-gvjjr [747.846015ms]
Aug 30 17:40:31.621: INFO: Created: latency-svc-gbhxp
Aug 30 17:40:31.657: INFO: Got endpoints: latency-svc-fwfgp [750.228857ms]
Aug 30 17:40:31.671: INFO: Created: latency-svc-vf54h
Aug 30 17:40:31.710: INFO: Got endpoints: latency-svc-cv8q4 [751.908599ms]
Aug 30 17:40:31.723: INFO: Created: latency-svc-b8xm2
Aug 30 17:40:31.763: INFO: Got endpoints: latency-svc-6jp95 [749.954908ms]
Aug 30 17:40:31.777: INFO: Created: latency-svc-hw5vk
Aug 30 17:40:31.808: INFO: Got endpoints: latency-svc-z6w94 [747.929809ms]
Aug 30 17:40:31.824: INFO: Created: latency-svc-wgfdm
Aug 30 17:40:31.858: INFO: Got endpoints: latency-svc-4qmh5 [750.773634ms]
Aug 30 17:40:31.873: INFO: Created: latency-svc-svcgd
Aug 30 17:40:31.909: INFO: Got endpoints: latency-svc-nhwpg [749.965167ms]
Aug 30 17:40:31.923: INFO: Created: latency-svc-4vp7g
Aug 30 17:40:31.958: INFO: Got endpoints: latency-svc-7zbp4 [750.36664ms]
Aug 30 17:40:31.974: INFO: Created: latency-svc-2rcdp
Aug 30 17:40:32.017: INFO: Got endpoints: latency-svc-6q2jv [758.455567ms]
Aug 30 17:40:32.032: INFO: Created: latency-svc-b2lxg
Aug 30 17:40:32.057: INFO: Got endpoints: latency-svc-zhrr7 [744.587165ms]
Aug 30 17:40:32.077: INFO: Created: latency-svc-ghkxx
Aug 30 17:40:32.108: INFO: Got endpoints: latency-svc-5rfdk [745.373197ms]
Aug 30 17:40:32.125: INFO: Created: latency-svc-4464s
Aug 30 17:40:32.159: INFO: Got endpoints: latency-svc-krskz [751.960873ms]
Aug 30 17:40:32.173: INFO: Created: latency-svc-wn4fj
Aug 30 17:40:32.209: INFO: Got endpoints: latency-svc-stcfq [750.143944ms]
Aug 30 17:40:32.228: INFO: Created: latency-svc-glmcc
Aug 30 17:40:32.257: INFO: Got endpoints: latency-svc-h92cs [749.938407ms]
Aug 30 17:40:32.272: INFO: Created: latency-svc-f9xfq
Aug 30 17:40:32.307: INFO: Got endpoints: latency-svc-plcm6 [749.151887ms]
Aug 30 17:40:32.322: INFO: Created: latency-svc-6cj7x
Aug 30 17:40:32.360: INFO: Got endpoints: latency-svc-gbhxp [752.067327ms]
Aug 30 17:40:32.373: INFO: Created: latency-svc-rcn9b
Aug 30 17:40:32.407: INFO: Got endpoints: latency-svc-vf54h [749.711859ms]
Aug 30 17:40:32.422: INFO: Created: latency-svc-mwnwc
Aug 30 17:40:32.459: INFO: Got endpoints: latency-svc-b8xm2 [748.931835ms]
Aug 30 17:40:32.472: INFO: Created: latency-svc-wtrfc
Aug 30 17:40:32.506: INFO: Got endpoints: latency-svc-hw5vk [743.195028ms]
Aug 30 17:40:32.522: INFO: Created: latency-svc-dvs9f
Aug 30 17:40:32.558: INFO: Got endpoints: latency-svc-wgfdm [750.2298ms]
Aug 30 17:40:32.571: INFO: Created: latency-svc-zl7jm
Aug 30 17:40:32.608: INFO: Got endpoints: latency-svc-svcgd [749.413394ms]
Aug 30 17:40:32.622: INFO: Created: latency-svc-95tkg
Aug 30 17:40:32.657: INFO: Got endpoints: latency-svc-4vp7g [748.807499ms]
Aug 30 17:40:32.671: INFO: Created: latency-svc-7jm9t
Aug 30 17:40:32.708: INFO: Got endpoints: latency-svc-2rcdp [749.75062ms]
Aug 30 17:40:32.722: INFO: Created: latency-svc-cdkgx
Aug 30 17:40:32.760: INFO: Got endpoints: latency-svc-b2lxg [743.402254ms]
Aug 30 17:40:32.773: INFO: Created: latency-svc-ndnpd
Aug 30 17:40:32.807: INFO: Got endpoints: latency-svc-ghkxx [749.829467ms]
Aug 30 17:40:32.822: INFO: Created: latency-svc-bcvn9
Aug 30 17:40:32.858: INFO: Got endpoints: latency-svc-4464s [749.665511ms]
Aug 30 17:40:32.874: INFO: Created: latency-svc-xk49v
Aug 30 17:40:32.908: INFO: Got endpoints: latency-svc-wn4fj [748.620725ms]
Aug 30 17:40:32.924: INFO: Created: latency-svc-m267r
Aug 30 17:40:32.958: INFO: Got endpoints: latency-svc-glmcc [748.983877ms]
Aug 30 17:40:32.973: INFO: Created: latency-svc-xfl2r
Aug 30 17:40:33.010: INFO: Got endpoints: latency-svc-f9xfq [752.876983ms]
Aug 30 17:40:33.023: INFO: Created: latency-svc-vpdzx
Aug 30 17:40:33.058: INFO: Got endpoints: latency-svc-6cj7x [750.94493ms]
Aug 30 17:40:33.108: INFO: Got endpoints: latency-svc-rcn9b [748.509201ms]
Aug 30 17:40:33.158: INFO: Got endpoints: latency-svc-mwnwc [750.30676ms]
Aug 30 17:40:33.209: INFO: Got endpoints: latency-svc-wtrfc [749.840268ms]
Aug 30 17:40:33.258: INFO: Got endpoints: latency-svc-dvs9f [751.532256ms]
Aug 30 17:40:33.307: INFO: Got endpoints: latency-svc-zl7jm [749.509709ms]
Aug 30 17:40:33.358: INFO: Got endpoints: latency-svc-95tkg [749.700255ms]
Aug 30 17:40:33.408: INFO: Got endpoints: latency-svc-7jm9t [750.275171ms]
Aug 30 17:40:33.458: INFO: Got endpoints: latency-svc-cdkgx [750.530946ms]
Aug 30 17:40:33.508: INFO: Got endpoints: latency-svc-ndnpd [747.745505ms]
Aug 30 17:40:33.559: INFO: Got endpoints: latency-svc-bcvn9 [751.921701ms]
Aug 30 17:40:33.608: INFO: Got endpoints: latency-svc-xk49v [750.447661ms]
Aug 30 17:40:33.658: INFO: Got endpoints: latency-svc-m267r [750.019235ms]
Aug 30 17:40:33.708: INFO: Got endpoints: latency-svc-xfl2r [749.935921ms]
Aug 30 17:40:33.758: INFO: Got endpoints: latency-svc-vpdzx [748.105029ms]
Aug 30 17:40:33.758: INFO: Latencies: [19.747092ms 27.371439ms 32.864282ms 38.99426ms 45.109874ms 56.536062ms 59.951364ms 68.91451ms 80.728323ms 87.855098ms 92.146731ms 104.801016ms 111.190115ms 112.429517ms 112.566162ms 114.388375ms 115.218468ms 115.39452ms 115.842441ms 116.696427ms 118.111443ms 120.110218ms 121.471705ms 122.689519ms 123.994493ms 124.344694ms 125.143967ms 131.640602ms 136.218889ms 137.277606ms 138.72036ms 140.030187ms 140.739548ms 142.367773ms 145.804483ms 180.239938ms 216.12899ms 255.051452ms 297.633383ms 342.137437ms 382.163721ms 420.88148ms 455.264055ms 498.679ms 539.593052ms 577.231985ms 618.651758ms 662.216785ms 703.188085ms 738.457575ms 740.597973ms 741.808567ms 743.195028ms 743.402254ms 743.59917ms 743.699187ms 744.587165ms 745.373197ms 746.104044ms 746.23213ms 746.344645ms 746.617233ms 746.741792ms 746.743075ms 746.906536ms 747.415829ms 747.721953ms 747.733484ms 747.745505ms 747.846015ms 747.929809ms 748.105029ms 748.258222ms 748.334898ms 748.355561ms 748.374491ms 748.45108ms 748.469815ms 748.491534ms 748.509201ms 748.620725ms 748.668967ms 748.807499ms 748.828823ms 748.893136ms 748.918449ms 748.923618ms 748.931835ms 748.977457ms 748.983877ms 749.006624ms 749.026397ms 749.044054ms 749.095614ms 749.12238ms 749.151887ms 749.184806ms 749.275855ms 749.336362ms 749.376372ms 749.402901ms 749.413394ms 749.509216ms 749.509709ms 749.533985ms 749.665511ms 749.700255ms 749.711859ms 749.715529ms 749.75062ms 749.780399ms 749.785173ms 749.808028ms 749.820774ms 749.829467ms 749.840268ms 749.897046ms 749.902398ms 749.935921ms 749.938407ms 749.954908ms 749.957765ms 749.965167ms 750.009318ms 750.013375ms 750.019235ms 750.032985ms 750.037082ms 750.053366ms 750.096147ms 750.1407ms 750.143944ms 750.163781ms 750.188758ms 750.219094ms 750.228857ms 750.2298ms 750.275171ms 750.30676ms 750.353553ms 750.355651ms 750.36664ms 750.373818ms 750.398426ms 750.447661ms 750.481405ms 750.501449ms 750.507079ms 750.519771ms 750.530946ms 750.552746ms 750.625412ms 750.714656ms 750.735277ms 750.758051ms 750.773634ms 750.785818ms 750.811925ms 750.816982ms 750.82407ms 750.848902ms 750.935842ms 750.94493ms 750.959362ms 751.162759ms 751.191465ms 751.340517ms 751.345062ms 751.392303ms 751.393725ms 751.444774ms 751.450032ms 751.456671ms 751.532256ms 751.568163ms 751.581256ms 751.770098ms 751.780504ms 751.791899ms 751.908599ms 751.921701ms 751.960873ms 752.067327ms 752.201013ms 752.208863ms 752.380471ms 752.876983ms 753.198298ms 753.327947ms 753.593291ms 753.942733ms 754.036872ms 754.211249ms 754.438236ms 754.824827ms 755.054112ms 755.155608ms 758.094973ms 758.455567ms 760.736085ms]
Aug 30 17:40:33.759: INFO: 50 %ile: 749.402901ms
Aug 30 17:40:33.759: INFO: 90 %ile: 751.921701ms
Aug 30 17:40:33.759: INFO: 99 %ile: 758.455567ms
Aug 30 17:40:33.759: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:40:33.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1197" for this suite.

â€¢ [SLOW TEST:10.927 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":270,"skipped":4351,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:40:33.776: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6685
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 30 17:40:33.955: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 30 17:40:38.969: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:40:39.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6685" for this suite.

â€¢ [SLOW TEST:6.241 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":271,"skipped":4352,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:40:40.017: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1585
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 30 17:40:40.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2938'
Aug 30 17:40:40.271: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 30 17:40:40.271: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Aug 30 17:40:40.285: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Aug 30 17:40:40.291: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Aug 30 17:40:40.303: INFO: scanned /root for discovery docs: <nil>
Aug 30 17:40:40.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2938'
Aug 30 17:40:56.151: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 30 17:40:56.151: INFO: stdout: "Created e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91\nScaling up e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Aug 30 17:40:56.151: INFO: stdout: "Created e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91\nScaling up e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Aug 30 17:40:56.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-2938'
Aug 30 17:40:56.224: INFO: stderr: ""
Aug 30 17:40:56.224: INFO: stdout: "e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91-dcp2d "
Aug 30 17:40:56.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91-dcp2d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2938'
Aug 30 17:40:56.290: INFO: stderr: ""
Aug 30 17:40:56.290: INFO: stdout: "true"
Aug 30 17:40:56.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 get pods e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91-dcp2d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2938'
Aug 30 17:40:56.352: INFO: stderr: ""
Aug 30 17:40:56.352: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Aug 30 17:40:56.352: INFO: e2e-test-httpd-rc-b0ac9a6d7e4559c2ed5f4aab59e66e91-dcp2d is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
Aug 30 17:40:56.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-962893509 delete rc e2e-test-httpd-rc --namespace=kubectl-2938'
Aug 30 17:40:56.434: INFO: stderr: ""
Aug 30 17:40:56.434: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:40:56.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2938" for this suite.

â€¢ [SLOW TEST:16.440 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1580
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":272,"skipped":4372,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:40:56.457: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:41:13.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7823" for this suite.

â€¢ [SLOW TEST:17.247 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":273,"skipped":4377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:41:13.705: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:42:13.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9194" for this suite.

â€¢ [SLOW TEST:60.202 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":274,"skipped":4408,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:42:13.907: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4579
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 30 17:42:14.086: INFO: Waiting up to 5m0s for pod "pod-82cea4cc-255e-44fe-8e6a-8ab320a026c7" in namespace "emptydir-4579" to be "success or failure"
Aug 30 17:42:14.093: INFO: Pod "pod-82cea4cc-255e-44fe-8e6a-8ab320a026c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.689636ms
Aug 30 17:42:16.099: INFO: Pod "pod-82cea4cc-255e-44fe-8e6a-8ab320a026c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012748754s
Aug 30 17:42:18.105: INFO: Pod "pod-82cea4cc-255e-44fe-8e6a-8ab320a026c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018817136s
STEP: Saw pod success
Aug 30 17:42:18.105: INFO: Pod "pod-82cea4cc-255e-44fe-8e6a-8ab320a026c7" satisfied condition "success or failure"
Aug 30 17:42:18.111: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-82cea4cc-255e-44fe-8e6a-8ab320a026c7 container test-container: <nil>
STEP: delete the pod
Aug 30 17:42:18.182: INFO: Waiting for pod pod-82cea4cc-255e-44fe-8e6a-8ab320a026c7 to disappear
Aug 30 17:42:18.186: INFO: Pod pod-82cea4cc-255e-44fe-8e6a-8ab320a026c7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:42:18.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4579" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":275,"skipped":4411,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:42:18.203: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 30 17:42:18.376: INFO: Waiting up to 5m0s for pod "pod-af5693d2-8657-4599-b376-aa92cd67e162" in namespace "emptydir-8645" to be "success or failure"
Aug 30 17:42:18.383: INFO: Pod "pod-af5693d2-8657-4599-b376-aa92cd67e162": Phase="Pending", Reason="", readiness=false. Elapsed: 6.364121ms
Aug 30 17:42:20.390: INFO: Pod "pod-af5693d2-8657-4599-b376-aa92cd67e162": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013512984s
Aug 30 17:42:22.397: INFO: Pod "pod-af5693d2-8657-4599-b376-aa92cd67e162": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020056611s
STEP: Saw pod success
Aug 30 17:42:22.397: INFO: Pod "pod-af5693d2-8657-4599-b376-aa92cd67e162" satisfied condition "success or failure"
Aug 30 17:42:22.402: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod pod-af5693d2-8657-4599-b376-aa92cd67e162 container test-container: <nil>
STEP: delete the pod
Aug 30 17:42:22.481: INFO: Waiting for pod pod-af5693d2-8657-4599-b376-aa92cd67e162 to disappear
Aug 30 17:42:22.487: INFO: Pod pod-af5693d2-8657-4599-b376-aa92cd67e162 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:42:22.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8645" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":276,"skipped":4425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:42:22.503: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7290
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:42:22.690: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3453ee5-0c22-4ef3-b80c-0b8eaf920918" in namespace "downward-api-7290" to be "success or failure"
Aug 30 17:42:22.698: INFO: Pod "downwardapi-volume-d3453ee5-0c22-4ef3-b80c-0b8eaf920918": Phase="Pending", Reason="", readiness=false. Elapsed: 7.857731ms
Aug 30 17:42:24.704: INFO: Pod "downwardapi-volume-d3453ee5-0c22-4ef3-b80c-0b8eaf920918": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013330565s
STEP: Saw pod success
Aug 30 17:42:24.704: INFO: Pod "downwardapi-volume-d3453ee5-0c22-4ef3-b80c-0b8eaf920918" satisfied condition "success or failure"
Aug 30 17:42:24.709: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-d3453ee5-0c22-4ef3-b80c-0b8eaf920918 container client-container: <nil>
STEP: delete the pod
Aug 30 17:42:24.777: INFO: Waiting for pod downwardapi-volume-d3453ee5-0c22-4ef3-b80c-0b8eaf920918 to disappear
Aug 30 17:42:24.781: INFO: Pod downwardapi-volume-d3453ee5-0c22-4ef3-b80c-0b8eaf920918 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:42:24.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7290" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":277,"skipped":4475,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:42:24.797: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1069
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 30 17:42:24.966: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4ad67ea-7193-4bcc-84d3-04a3a3142c4b" in namespace "downward-api-1069" to be "success or failure"
Aug 30 17:42:24.971: INFO: Pod "downwardapi-volume-e4ad67ea-7193-4bcc-84d3-04a3a3142c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.932212ms
Aug 30 17:42:26.977: INFO: Pod "downwardapi-volume-e4ad67ea-7193-4bcc-84d3-04a3a3142c4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010977872s
STEP: Saw pod success
Aug 30 17:42:26.977: INFO: Pod "downwardapi-volume-e4ad67ea-7193-4bcc-84d3-04a3a3142c4b" satisfied condition "success or failure"
Aug 30 17:42:26.981: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod downwardapi-volume-e4ad67ea-7193-4bcc-84d3-04a3a3142c4b container client-container: <nil>
STEP: delete the pod
Aug 30 17:42:27.052: INFO: Waiting for pod downwardapi-volume-e4ad67ea-7193-4bcc-84d3-04a3a3142c4b to disappear
Aug 30 17:42:27.056: INFO: Pod downwardapi-volume-e4ad67ea-7193-4bcc-84d3-04a3a3142c4b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:42:27.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1069" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":278,"skipped":4490,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:42:27.071: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4906
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Aug 30 17:42:27.242: INFO: Waiting up to 5m0s for pod "client-containers-5989e026-75f5-4f8e-8d77-73348f32dd78" in namespace "containers-4906" to be "success or failure"
Aug 30 17:42:27.248: INFO: Pod "client-containers-5989e026-75f5-4f8e-8d77-73348f32dd78": Phase="Pending", Reason="", readiness=false. Elapsed: 5.553687ms
Aug 30 17:42:29.254: INFO: Pod "client-containers-5989e026-75f5-4f8e-8d77-73348f32dd78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011449889s
STEP: Saw pod success
Aug 30 17:42:29.254: INFO: Pod "client-containers-5989e026-75f5-4f8e-8d77-73348f32dd78" satisfied condition "success or failure"
Aug 30 17:42:29.258: INFO: Trying to get logs from node adoring-wozniak-54dcfd79fc-948mf pod client-containers-5989e026-75f5-4f8e-8d77-73348f32dd78 container test-container: <nil>
STEP: delete the pod
Aug 30 17:42:29.329: INFO: Waiting for pod client-containers-5989e026-75f5-4f8e-8d77-73348f32dd78 to disappear
Aug 30 17:42:29.334: INFO: Pod client-containers-5989e026-75f5-4f8e-8d77-73348f32dd78 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:42:29.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4906" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":279,"skipped":4492,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 30 17:42:29.351: INFO: >>> kubeConfig: /tmp/kubeconfig-962893509
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-90
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 30 17:42:29.531: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-2893db3e-2fcf-416f-888a-6284d1c29a21" in namespace "security-context-test-90" to be "success or failure"
Aug 30 17:42:29.539: INFO: Pod "busybox-privileged-false-2893db3e-2fcf-416f-888a-6284d1c29a21": Phase="Pending", Reason="", readiness=false. Elapsed: 7.692098ms
Aug 30 17:42:31.545: INFO: Pod "busybox-privileged-false-2893db3e-2fcf-416f-888a-6284d1c29a21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01369878s
Aug 30 17:42:31.545: INFO: Pod "busybox-privileged-false-2893db3e-2fcf-416f-888a-6284d1c29a21" satisfied condition "success or failure"
Aug 30 17:42:31.599: INFO: Got logs for pod "busybox-privileged-false-2893db3e-2fcf-416f-888a-6284d1c29a21": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.9-rc.0.37+d1c2f63bd4fc89/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 30 17:42:31.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-90" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":280,"skipped":4524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSAug 30 17:42:31.620: INFO: Running AfterSuite actions on all nodes
Aug 30 17:42:31.620: INFO: Running AfterSuite actions on node 1
Aug 30 17:42:31.620: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4563,"failed":0}

Ran 280 of 4843 Specs in 4370.080 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4563 Skipped
PASS

Ginkgo ran 1 suite in 1h12m51.174094841s
Test Suite Passed
