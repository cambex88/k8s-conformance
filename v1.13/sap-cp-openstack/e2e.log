Conformance test: not doing test setup.
I0220 20:53:51.164866   32391 e2e.go:224] Starting e2e run "a0182cec-3551-11e9-8ceb-aaeae242ec87" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1550696030 - Will randomize all specs
Will run 201 of 2161 specs

Feb 20 20:53:51.361: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 20:53:51.364: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 20 20:53:51.383: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 20 20:53:51.416: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 20 20:53:51.416: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 20 20:53:51.416: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 20 20:53:51.439: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 20 20:53:51.439: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 20 20:53:51.439: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 20 20:53:51.439: INFO: e2e test version: v1.13.3
Feb 20 20:53:51.441: INFO: kube-apiserver version: v1.13.3
SSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 20:53:51.441: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename hostpath
Feb 20 20:53:51.561: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 20 20:53:51.577: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-mwdg2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 20 20:53:51.696: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-mwdg2" to be "success or failure"
Feb 20 20:53:51.699: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.077297ms
Feb 20 20:53:53.704: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00796714s
Feb 20 20:53:55.710: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014343461s
Feb 20 20:53:57.715: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019607986s
Feb 20 20:53:59.719: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023639061s
Feb 20 20:54:01.725: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029420912s
Feb 20 20:54:03.730: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034776691s
Feb 20 20:54:05.735: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039490174s
Feb 20 20:54:07.740: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 16.044532128s
Feb 20 20:54:09.745: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 18.049674016s
Feb 20 20:54:11.751: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 20.055263671s
Feb 20 20:54:13.756: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 22.060341708s
Feb 20 20:54:15.767: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 24.071123883s
Feb 20 20:54:17.772: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 26.076393675s
Feb 20 20:54:19.777: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 28.081380862s
Feb 20 20:54:21.783: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 30.087882727s
Feb 20 20:54:23.790: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 32.094644086s
Feb 20 20:54:25.799: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 34.103929215s
Feb 20 20:54:27.805: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 36.109182419s
Feb 20 20:54:29.809: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 38.113931826s
Feb 20 20:54:31.815: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 40.118958671s
Feb 20 20:54:33.822: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 42.126678887s
Feb 20 20:54:35.827: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 44.131902634s
Feb 20 20:54:37.834: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 46.138278874s
Feb 20 20:54:39.838: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 48.142750505s
STEP: Saw pod success
Feb 20 20:54:39.838: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 20 20:54:39.842: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 20 20:54:40.004: INFO: Waiting for pod pod-host-path-test to disappear
Feb 20 20:54:40.015: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 20:54:40.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-mwdg2" for this suite.
Feb 20 20:54:46.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 20:54:46.080: INFO: namespace: e2e-tests-hostpath-mwdg2, resource: bindings, ignored listing per whitelist
Feb 20 20:54:46.202: INFO: namespace e2e-tests-hostpath-mwdg2 deletion completed in 6.182749067s

â€¢ [SLOW TEST:54.761 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 20:54:46.203: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-vdd4k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 20 20:54:46.493: INFO: Waiting up to 5m0s for pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-vdd4k" to be "success or failure"
Feb 20 20:54:46.499: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.50574ms
Feb 20 20:54:48.504: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011180485s
Feb 20 20:54:50.509: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015954621s
Feb 20 20:54:52.514: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021014723s
Feb 20 20:54:54.519: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025906912s
Feb 20 20:54:56.524: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030722215s
Feb 20 20:54:58.528: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035678403s
Feb 20 20:55:00.534: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 14.041198575s
Feb 20 20:55:02.539: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 16.046158599s
Feb 20 20:55:04.544: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 18.051016168s
Feb 20 20:55:06.549: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 20.055815269s
Feb 20 20:55:08.553: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 22.060485857s
Feb 20 20:55:10.558: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 24.06513202s
Feb 20 20:55:12.565: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 26.072530826s
Feb 20 20:55:14.570: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 28.077518819s
Feb 20 20:55:16.575: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 30.082447464s
Feb 20 20:55:18.581: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 32.088379515s
Feb 20 20:55:20.585: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 34.092160812s
Feb 20 20:55:22.590: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 36.09705691s
Feb 20 20:55:24.594: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 38.101452465s
Feb 20 20:55:26.605: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 40.112562463s
Feb 20 20:55:28.611: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 42.117898783s
Feb 20 20:55:30.615: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 44.122069938s
Feb 20 20:55:32.623: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 46.130112938s
Feb 20 20:55:34.629: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 48.13605153s
STEP: Saw pod success
Feb 20 20:55:34.629: INFO: Pod "pod-c166794d-3551-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 20:55:34.632: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-c166794d-3551-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 20:55:34.659: INFO: Waiting for pod pod-c166794d-3551-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 20:55:34.662: INFO: Pod pod-c166794d-3551-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 20:55:34.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vdd4k" for this suite.
Feb 20 20:55:40.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 20:55:40.832: INFO: namespace: e2e-tests-emptydir-vdd4k, resource: bindings, ignored listing per whitelist
Feb 20 20:55:40.901: INFO: namespace e2e-tests-emptydir-vdd4k deletion completed in 6.234883328s

â€¢ [SLOW TEST:54.699 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 20:55:40.901: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rmj9v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 20:55:41.174: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-rmj9v'
Feb 20 20:55:41.346: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 20 20:55:41.346: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Feb 20 20:55:41.351: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-rmj9v'
Feb 20 20:55:41.465: INFO: stderr: ""
Feb 20 20:55:41.466: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 20:55:41.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rmj9v" for this suite.
Feb 20 20:56:03.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 20:56:03.559: INFO: namespace: e2e-tests-kubectl-rmj9v, resource: bindings, ignored listing per whitelist
Feb 20 20:56:03.650: INFO: namespace e2e-tests-kubectl-rmj9v deletion completed in 22.177095003s

â€¢ [SLOW TEST:22.749 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 20:56:03.650: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-lvcq8
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 20 20:56:03.912: INFO: Waiting up to 5m0s for pod "pod-ef8bedce-3551-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-lvcq8" to be "success or failure"
Feb 20 20:56:03.916: INFO: Pod "pod-ef8bedce-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818578ms
Feb 20 20:56:05.922: INFO: Pod "pod-ef8bedce-3551-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009557648s
STEP: Saw pod success
Feb 20 20:56:05.922: INFO: Pod "pod-ef8bedce-3551-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 20:56:05.926: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-ef8bedce-3551-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 20:56:05.947: INFO: Waiting for pod pod-ef8bedce-3551-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 20:56:05.950: INFO: Pod pod-ef8bedce-3551-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 20:56:05.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lvcq8" for this suite.
Feb 20 20:56:11.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 20:56:12.160: INFO: namespace: e2e-tests-emptydir-lvcq8, resource: bindings, ignored listing per whitelist
Feb 20 20:56:12.236: INFO: namespace e2e-tests-emptydir-lvcq8 deletion completed in 6.282733446s

â€¢ [SLOW TEST:8.586 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 20:56:12.237: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-zts86
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 20:56:12.501: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f4aa0d49-3551-11e9-91b0-3e11857925bf", Controller:(*bool)(0xc001a95346), BlockOwnerDeletion:(*bool)(0xc001a95347)}}
Feb 20 20:56:12.508: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f4a87b62-3551-11e9-91b0-3e11857925bf", Controller:(*bool)(0xc001088fb6), BlockOwnerDeletion:(*bool)(0xc001088fb7)}}
Feb 20 20:56:12.514: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f4a95111-3551-11e9-91b0-3e11857925bf", Controller:(*bool)(0xc000c2da5a), BlockOwnerDeletion:(*bool)(0xc000c2da5b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 20:56:17.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-zts86" for this suite.
Feb 20 20:56:23.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 20:56:23.702: INFO: namespace: e2e-tests-gc-zts86, resource: bindings, ignored listing per whitelist
Feb 20 20:56:23.713: INFO: namespace e2e-tests-gc-zts86 deletion completed in 6.181258792s

â€¢ [SLOW TEST:11.476 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 20:56:23.713: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-kcmbv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-fb83cbff-3551-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 20:56:23.996: INFO: Waiting up to 5m0s for pod "pod-secrets-fb8471a0-3551-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-secrets-kcmbv" to be "success or failure"
Feb 20 20:56:24.000: INFO: Pod "pod-secrets-fb8471a0-3551-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.721995ms
Feb 20 20:56:26.008: INFO: Pod "pod-secrets-fb8471a0-3551-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01168344s
STEP: Saw pod success
Feb 20 20:56:26.008: INFO: Pod "pod-secrets-fb8471a0-3551-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 20:56:26.011: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-secrets-fb8471a0-3551-11e9-8ceb-aaeae242ec87 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 20:56:26.031: INFO: Waiting for pod pod-secrets-fb8471a0-3551-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 20:56:26.034: INFO: Pod pod-secrets-fb8471a0-3551-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 20:56:26.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kcmbv" for this suite.
Feb 20 20:56:32.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 20:56:32.093: INFO: namespace: e2e-tests-secrets-kcmbv, resource: bindings, ignored listing per whitelist
Feb 20 20:56:32.190: INFO: namespace e2e-tests-secrets-kcmbv deletion completed in 6.151985106s

â€¢ [SLOW TEST:8.477 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 20:56:32.190: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-k8mq6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 20 20:58:00.493: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 20:58:00.498: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 20:58:02.498: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 20:58:02.503: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 20:58:04.498: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 20:58:04.503: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 20:58:06.499: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 20:58:06.504: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 20:58:08.498: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 20:58:08.506: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 20:58:10.498: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 20:58:10.504: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 20:58:10.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-k8mq6" for this suite.
Feb 20 20:58:32.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 20:58:32.639: INFO: namespace: e2e-tests-container-lifecycle-hook-k8mq6, resource: bindings, ignored listing per whitelist
Feb 20 20:58:32.715: INFO: namespace e2e-tests-container-lifecycle-hook-k8mq6 deletion completed in 22.195361535s

â€¢ [SLOW TEST:120.525 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 20:58:32.715: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bwrcj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-486776a1-3552-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 20:58:33.000: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4868c5b6-3552-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-bwrcj" to be "success or failure"
Feb 20 20:58:33.010: INFO: Pod "pod-projected-secrets-4868c5b6-3552-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 9.736964ms
Feb 20 20:58:35.017: INFO: Pod "pod-projected-secrets-4868c5b6-3552-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016931195s
STEP: Saw pod success
Feb 20 20:58:35.017: INFO: Pod "pod-projected-secrets-4868c5b6-3552-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 20:58:35.021: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-secrets-4868c5b6-3552-11e9-8ceb-aaeae242ec87 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 20:58:35.041: INFO: Waiting for pod pod-projected-secrets-4868c5b6-3552-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 20:58:35.044: INFO: Pod pod-projected-secrets-4868c5b6-3552-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 20:58:35.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bwrcj" for this suite.
Feb 20 20:58:41.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 20:58:41.143: INFO: namespace: e2e-tests-projected-bwrcj, resource: bindings, ignored listing per whitelist
Feb 20 20:58:41.211: INFO: namespace e2e-tests-projected-bwrcj deletion completed in 6.163333963s

â€¢ [SLOW TEST:8.496 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 20:58:41.211: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-8k9vc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0220 20:58:51.518025   32391 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 20:58:51.518: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 20:58:51.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-8k9vc" for this suite.
Feb 20 20:58:57.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 20:58:57.590: INFO: namespace: e2e-tests-gc-8k9vc, resource: bindings, ignored listing per whitelist
Feb 20 20:58:57.729: INFO: namespace e2e-tests-gc-8k9vc deletion completed in 6.207121644s

â€¢ [SLOW TEST:16.518 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 20:58:57.729: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-7cltb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 20:58:57.964: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:00:00.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-7cltb" for this suite.
Feb 20 21:00:42.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:00:42.201: INFO: namespace: e2e-tests-pods-7cltb, resource: bindings, ignored listing per whitelist
Feb 20 21:00:42.337: INFO: namespace e2e-tests-pods-7cltb deletion completed in 42.22135524s

â€¢ [SLOW TEST:104.608 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:00:42.338: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-54hng
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 21:00:42.688: INFO: (0) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 95.175039ms)
Feb 20 21:00:42.694: INFO: (1) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.868113ms)
Feb 20 21:00:42.700: INFO: (2) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.738652ms)
Feb 20 21:00:42.706: INFO: (3) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.736285ms)
Feb 20 21:00:42.712: INFO: (4) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.002156ms)
Feb 20 21:00:42.719: INFO: (5) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.206384ms)
Feb 20 21:00:42.724: INFO: (6) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.593794ms)
Feb 20 21:00:42.729: INFO: (7) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.986699ms)
Feb 20 21:00:42.734: INFO: (8) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.984739ms)
Feb 20 21:00:42.738: INFO: (9) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.645256ms)
Feb 20 21:00:42.744: INFO: (10) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.373505ms)
Feb 20 21:00:42.750: INFO: (11) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.409559ms)
Feb 20 21:00:42.757: INFO: (12) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.717926ms)
Feb 20 21:00:42.761: INFO: (13) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.6152ms)
Feb 20 21:00:42.766: INFO: (14) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.916123ms)
Feb 20 21:00:42.772: INFO: (15) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.223417ms)
Feb 20 21:00:42.777: INFO: (16) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.557422ms)
Feb 20 21:00:42.784: INFO: (17) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.007769ms)
Feb 20 21:00:42.790: INFO: (18) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.211924ms)
Feb 20 21:00:42.796: INFO: (19) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.584934ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:00:42.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-54hng" for this suite.
Feb 20 21:00:48.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:00:49.020: INFO: namespace: e2e-tests-proxy-54hng, resource: bindings, ignored listing per whitelist
Feb 20 21:00:49.044: INFO: namespace e2e-tests-proxy-54hng deletion completed in 6.241096288s

â€¢ [SLOW TEST:6.707 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:00:49.045: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-lb9m7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 20 21:00:49.327: INFO: Waiting up to 5m0s for pod "var-expansion-99aac9f9-3552-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-var-expansion-lb9m7" to be "success or failure"
Feb 20 21:00:49.333: INFO: Pod "var-expansion-99aac9f9-3552-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.360375ms
Feb 20 21:00:51.339: INFO: Pod "var-expansion-99aac9f9-3552-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011577063s
STEP: Saw pod success
Feb 20 21:00:51.339: INFO: Pod "var-expansion-99aac9f9-3552-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:00:51.343: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod var-expansion-99aac9f9-3552-11e9-8ceb-aaeae242ec87 container dapi-container: <nil>
STEP: delete the pod
Feb 20 21:00:51.364: INFO: Waiting for pod var-expansion-99aac9f9-3552-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:00:51.367: INFO: Pod var-expansion-99aac9f9-3552-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:00:51.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-lb9m7" for this suite.
Feb 20 21:00:57.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:00:57.542: INFO: namespace: e2e-tests-var-expansion-lb9m7, resource: bindings, ignored listing per whitelist
Feb 20 21:00:57.580: INFO: namespace e2e-tests-var-expansion-lb9m7 deletion completed in 6.207723513s

â€¢ [SLOW TEST:8.536 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:00:57.580: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-m74cq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 21:00:57.855: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-m74cq'
Feb 20 21:00:58.011: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 20 21:00:58.011: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 20 21:00:58.021: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-kvxh6]
Feb 20 21:00:58.021: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-kvxh6" in namespace "e2e-tests-kubectl-m74cq" to be "running and ready"
Feb 20 21:00:58.024: INFO: Pod "e2e-test-nginx-rc-kvxh6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.559633ms
Feb 20 21:01:00.032: INFO: Pod "e2e-test-nginx-rc-kvxh6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011198189s
Feb 20 21:01:00.032: INFO: Pod "e2e-test-nginx-rc-kvxh6" satisfied condition "running and ready"
Feb 20 21:01:00.032: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-kvxh6]
Feb 20 21:01:00.032: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-m74cq'
Feb 20 21:01:00.200: INFO: stderr: ""
Feb 20 21:01:00.200: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Feb 20 21:01:00.201: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-m74cq'
Feb 20 21:01:00.354: INFO: stderr: ""
Feb 20 21:01:00.354: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:01:00.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-m74cq" for this suite.
Feb 20 21:01:22.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:01:22.560: INFO: namespace: e2e-tests-kubectl-m74cq, resource: bindings, ignored listing per whitelist
Feb 20 21:01:22.613: INFO: namespace e2e-tests-kubectl-m74cq deletion completed in 22.253374287s

â€¢ [SLOW TEST:25.033 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:01:22.613: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-7qqpj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 21:01:22.860: INFO: Waiting up to 5m0s for pod "downward-api-ada75c52-3552-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-7qqpj" to be "success or failure"
Feb 20 21:01:22.864: INFO: Pod "downward-api-ada75c52-3552-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.126369ms
Feb 20 21:01:24.869: INFO: Pod "downward-api-ada75c52-3552-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008908082s
STEP: Saw pod success
Feb 20 21:01:24.869: INFO: Pod "downward-api-ada75c52-3552-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:01:24.873: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downward-api-ada75c52-3552-11e9-8ceb-aaeae242ec87 container dapi-container: <nil>
STEP: delete the pod
Feb 20 21:01:24.893: INFO: Waiting for pod downward-api-ada75c52-3552-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:01:24.899: INFO: Pod downward-api-ada75c52-3552-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:01:24.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7qqpj" for this suite.
Feb 20 21:01:30.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:01:30.982: INFO: namespace: e2e-tests-downward-api-7qqpj, resource: bindings, ignored listing per whitelist
Feb 20 21:01:31.068: INFO: namespace e2e-tests-downward-api-7qqpj deletion completed in 6.164459345s

â€¢ [SLOW TEST:8.454 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:01:31.068: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-j5k8d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-j5k8d
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 20 21:01:31.297: INFO: Found 0 stateful pods, waiting for 3
Feb 20 21:01:41.304: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 21:01:41.304: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 21:01:41.304: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 21:01:41.317: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-j5k8d ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 21:01:41.829: INFO: stderr: ""
Feb 20 21:01:41.829: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 21:01:41.829: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 20 21:01:51.866: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 20 21:02:01.889: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-j5k8d ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 21:02:02.414: INFO: stderr: ""
Feb 20 21:02:02.414: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 21:02:02.414: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 21:02:12.441: INFO: Waiting for StatefulSet e2e-tests-statefulset-j5k8d/ss2 to complete update
Feb 20 21:02:12.441: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:02:12.441: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:02:22.450: INFO: Waiting for StatefulSet e2e-tests-statefulset-j5k8d/ss2 to complete update
Feb 20 21:02:22.450: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:02:22.450: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:02:32.450: INFO: Waiting for StatefulSet e2e-tests-statefulset-j5k8d/ss2 to complete update
Feb 20 21:02:32.450: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:02:32.450: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:02:42.452: INFO: Waiting for StatefulSet e2e-tests-statefulset-j5k8d/ss2 to complete update
Feb 20 21:02:42.452: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:02:42.452: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:02:52.450: INFO: Waiting for StatefulSet e2e-tests-statefulset-j5k8d/ss2 to complete update
Feb 20 21:02:52.450: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:03:02.458: INFO: Waiting for StatefulSet e2e-tests-statefulset-j5k8d/ss2 to complete update
Feb 20 21:03:02.458: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:03:12.453: INFO: Waiting for StatefulSet e2e-tests-statefulset-j5k8d/ss2 to complete update
Feb 20 21:03:12.453: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:03:22.450: INFO: Waiting for StatefulSet e2e-tests-statefulset-j5k8d/ss2 to complete update
Feb 20 21:03:22.450: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:03:32.450: INFO: Waiting for StatefulSet e2e-tests-statefulset-j5k8d/ss2 to complete update
STEP: Rolling back to a previous revision
Feb 20 21:03:42.453: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-j5k8d ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 21:03:43.116: INFO: stderr: ""
Feb 20 21:03:43.116: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 21:03:43.116: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 21:03:53.153: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 20 21:04:03.179: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-j5k8d ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 21:04:03.731: INFO: stderr: ""
Feb 20 21:04:03.731: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 21:04:03.731: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 21:04:23.761: INFO: Waiting for StatefulSet e2e-tests-statefulset-j5k8d/ss2 to complete update
Feb 20 21:04:23.761: INFO: Waiting for Pod e2e-tests-statefulset-j5k8d/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 21:04:33.770: INFO: Deleting all statefulset in ns e2e-tests-statefulset-j5k8d
Feb 20 21:04:33.774: INFO: Scaling statefulset ss2 to 0
Feb 20 21:04:53.792: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 21:04:53.796: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:04:53.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-j5k8d" for this suite.
Feb 20 21:04:59.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:04:59.928: INFO: namespace: e2e-tests-statefulset-j5k8d, resource: bindings, ignored listing per whitelist
Feb 20 21:05:00.018: INFO: namespace e2e-tests-statefulset-j5k8d deletion completed in 6.203565217s

â€¢ [SLOW TEST:208.950 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:05:00.019: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-hnp2j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:05:00.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-hnp2j" for this suite.
Feb 20 21:05:22.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:05:22.536: INFO: namespace: e2e-tests-kubelet-test-hnp2j, resource: bindings, ignored listing per whitelist
Feb 20 21:05:22.544: INFO: namespace e2e-tests-kubelet-test-hnp2j deletion completed in 22.201146538s

â€¢ [SLOW TEST:22.525 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:05:22.544: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-8rf2s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:05:24.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-8rf2s" for this suite.
Feb 20 21:06:16.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:06:16.943: INFO: namespace: e2e-tests-kubelet-test-8rf2s, resource: bindings, ignored listing per whitelist
Feb 20 21:06:17.031: INFO: namespace e2e-tests-kubelet-test-8rf2s deletion completed in 52.170623321s

â€¢ [SLOW TEST:54.487 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:06:17.031: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qqwtv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 21:06:17.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d232855-3553-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-qqwtv" to be "success or failure"
Feb 20 21:06:17.280: INFO: Pod "downwardapi-volume-5d232855-3553-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962229ms
Feb 20 21:06:19.286: INFO: Pod "downwardapi-volume-5d232855-3553-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01099885s
STEP: Saw pod success
Feb 20 21:06:19.286: INFO: Pod "downwardapi-volume-5d232855-3553-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:06:19.289: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-5d232855-3553-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 21:06:19.313: INFO: Waiting for pod downwardapi-volume-5d232855-3553-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:06:19.318: INFO: Pod downwardapi-volume-5d232855-3553-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:06:19.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qqwtv" for this suite.
Feb 20 21:06:25.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:06:25.493: INFO: namespace: e2e-tests-projected-qqwtv, resource: bindings, ignored listing per whitelist
Feb 20 21:06:25.549: INFO: namespace e2e-tests-projected-qqwtv deletion completed in 6.225760521s

â€¢ [SLOW TEST:8.518 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:06:25.549: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-2gt2g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-6238ce7c-3553-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 21:06:25.807: INFO: Waiting up to 5m0s for pod "pod-secrets-623977fb-3553-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-secrets-2gt2g" to be "success or failure"
Feb 20 21:06:25.812: INFO: Pod "pod-secrets-623977fb-3553-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.376639ms
Feb 20 21:06:27.817: INFO: Pod "pod-secrets-623977fb-3553-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010428846s
STEP: Saw pod success
Feb 20 21:06:27.817: INFO: Pod "pod-secrets-623977fb-3553-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:06:27.821: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-secrets-623977fb-3553-11e9-8ceb-aaeae242ec87 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 21:06:27.842: INFO: Waiting for pod pod-secrets-623977fb-3553-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:06:27.845: INFO: Pod pod-secrets-623977fb-3553-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:06:27.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2gt2g" for this suite.
Feb 20 21:06:33.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:06:33.946: INFO: namespace: e2e-tests-secrets-2gt2g, resource: bindings, ignored listing per whitelist
Feb 20 21:06:34.052: INFO: namespace e2e-tests-secrets-2gt2g deletion completed in 6.202518398s

â€¢ [SLOW TEST:8.503 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:06:34.052: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-ntk6w
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-6748291f-3553-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 21:06:34.296: INFO: Waiting up to 5m0s for pod "pod-secrets-6748cead-3553-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-secrets-ntk6w" to be "success or failure"
Feb 20 21:06:34.299: INFO: Pod "pod-secrets-6748cead-3553-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.478771ms
Feb 20 21:06:36.304: INFO: Pod "pod-secrets-6748cead-3553-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008533201s
STEP: Saw pod success
Feb 20 21:06:36.304: INFO: Pod "pod-secrets-6748cead-3553-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:06:36.308: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-secrets-6748cead-3553-11e9-8ceb-aaeae242ec87 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 21:06:36.328: INFO: Waiting for pod pod-secrets-6748cead-3553-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:06:36.332: INFO: Pod pod-secrets-6748cead-3553-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:06:36.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-ntk6w" for this suite.
Feb 20 21:06:42.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:06:42.502: INFO: namespace: e2e-tests-secrets-ntk6w, resource: bindings, ignored listing per whitelist
Feb 20 21:06:42.526: INFO: namespace e2e-tests-secrets-ntk6w deletion completed in 6.189413551s

â€¢ [SLOW TEST:8.474 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:06:42.526: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-9bhlg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 20 21:06:42.781: INFO: Waiting up to 5m0s for pod "pod-6c57509f-3553-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-9bhlg" to be "success or failure"
Feb 20 21:06:42.787: INFO: Pod "pod-6c57509f-3553-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.886054ms
Feb 20 21:06:44.791: INFO: Pod "pod-6c57509f-3553-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010450608s
STEP: Saw pod success
Feb 20 21:06:44.791: INFO: Pod "pod-6c57509f-3553-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:06:44.795: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-6c57509f-3553-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:06:44.828: INFO: Waiting for pod pod-6c57509f-3553-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:06:44.833: INFO: Pod pod-6c57509f-3553-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:06:44.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9bhlg" for this suite.
Feb 20 21:06:50.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:06:50.957: INFO: namespace: e2e-tests-emptydir-9bhlg, resource: bindings, ignored listing per whitelist
Feb 20 21:06:51.049: INFO: namespace e2e-tests-emptydir-9bhlg deletion completed in 6.211037181s

â€¢ [SLOW TEST:8.523 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:06:51.049: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-pprhm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 21:06:51.293: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 20 21:06:56.299: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 20 21:06:56.299: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 21:06:56.321: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-pprhm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pprhm/deployments/test-cleanup-deployment,UID:74687dc2-3553-11e9-91b0-3e11857925bf,ResourceVersion:5311,Generation:1,CreationTimestamp:2019-02-20 21:06:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Feb 20 21:06:56.325: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Feb 20 21:06:56.325: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 20 21:06:56.325: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-pprhm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-pprhm/replicasets/test-cleanup-controller,UID:716a8e63-3553-11e9-91b0-3e11857925bf,ResourceVersion:5312,Generation:1,CreationTimestamp:2019-02-20 21:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 74687dc2-3553-11e9-91b0-3e11857925bf 0xc00091c9df 0xc00091c9f0}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 20 21:06:56.331: INFO: Pod "test-cleanup-controller-szx6x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-szx6x,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-pprhm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-pprhm/pods/test-cleanup-controller-szx6x,UID:716bf910-3553-11e9-91b0-3e11857925bf,ResourceVersion:5302,Generation:0,CreationTimestamp:2019-02-20 21:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.25/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 716a8e63-3553-11e9-91b0-3e11857925bf 0xc00091d12f 0xc00091d150}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jxsp5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jxsp5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jxsp5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00091d1b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00091d1d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:06:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:06:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:06:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:06:51 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:100.96.1.25,StartTime:2019-02-20 21:06:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 21:06:52 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://843e9f74285b344a5b884c16a20a711f8ef72763c84bb4e1fe762e05e3ca7428}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:06:56.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-pprhm" for this suite.
Feb 20 21:07:02.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:07:02.516: INFO: namespace: e2e-tests-deployment-pprhm, resource: bindings, ignored listing per whitelist
Feb 20 21:07:02.539: INFO: namespace e2e-tests-deployment-pprhm deletion completed in 6.203800576s

â€¢ [SLOW TEST:11.490 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:07:02.539: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-79xj9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 21:07:05.328: INFO: Successfully updated pod "labelsupdate7844cf1e-3553-11e9-8ceb-aaeae242ec87"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:07:09.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-79xj9" for this suite.
Feb 20 21:07:25.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:07:25.483: INFO: namespace: e2e-tests-downward-api-79xj9, resource: bindings, ignored listing per whitelist
Feb 20 21:07:25.554: INFO: namespace e2e-tests-downward-api-79xj9 deletion completed in 16.183440653s

â€¢ [SLOW TEST:23.015 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:07:25.554: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-275gk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Feb 20 21:07:25.772: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-275gk'
Feb 20 21:07:26.612: INFO: stderr: ""
Feb 20 21:07:26.612: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 20 21:07:27.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:27.617: INFO: Found 0 / 1
Feb 20 21:07:28.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:28.617: INFO: Found 0 / 1
Feb 20 21:07:29.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:29.617: INFO: Found 0 / 1
Feb 20 21:07:30.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:30.617: INFO: Found 0 / 1
Feb 20 21:07:31.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:31.617: INFO: Found 0 / 1
Feb 20 21:07:32.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:32.619: INFO: Found 0 / 1
Feb 20 21:07:33.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:33.617: INFO: Found 0 / 1
Feb 20 21:07:34.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:34.620: INFO: Found 0 / 1
Feb 20 21:07:35.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:35.617: INFO: Found 0 / 1
Feb 20 21:07:36.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:36.619: INFO: Found 0 / 1
Feb 20 21:07:37.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:37.618: INFO: Found 0 / 1
Feb 20 21:07:38.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:38.617: INFO: Found 0 / 1
Feb 20 21:07:39.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:39.619: INFO: Found 0 / 1
Feb 20 21:07:40.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:40.618: INFO: Found 0 / 1
Feb 20 21:07:41.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:41.618: INFO: Found 0 / 1
Feb 20 21:07:42.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:42.617: INFO: Found 0 / 1
Feb 20 21:07:43.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:43.618: INFO: Found 0 / 1
Feb 20 21:07:44.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:44.618: INFO: Found 0 / 1
Feb 20 21:07:45.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:45.618: INFO: Found 0 / 1
Feb 20 21:07:46.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:46.619: INFO: Found 0 / 1
Feb 20 21:07:47.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:47.618: INFO: Found 0 / 1
Feb 20 21:07:48.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:48.619: INFO: Found 0 / 1
Feb 20 21:07:49.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:49.618: INFO: Found 0 / 1
Feb 20 21:07:50.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:50.617: INFO: Found 0 / 1
Feb 20 21:07:51.620: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:51.620: INFO: Found 0 / 1
Feb 20 21:07:52.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:52.618: INFO: Found 0 / 1
Feb 20 21:07:53.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:53.618: INFO: Found 0 / 1
Feb 20 21:07:54.620: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:54.620: INFO: Found 0 / 1
Feb 20 21:07:55.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:55.617: INFO: Found 0 / 1
Feb 20 21:07:56.621: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:56.621: INFO: Found 0 / 1
Feb 20 21:07:57.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:57.618: INFO: Found 0 / 1
Feb 20 21:07:58.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:58.619: INFO: Found 0 / 1
Feb 20 21:07:59.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:07:59.617: INFO: Found 0 / 1
Feb 20 21:08:00.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:00.618: INFO: Found 0 / 1
Feb 20 21:08:01.620: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:01.620: INFO: Found 0 / 1
Feb 20 21:08:02.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:02.618: INFO: Found 0 / 1
Feb 20 21:08:03.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:03.619: INFO: Found 0 / 1
Feb 20 21:08:04.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:04.618: INFO: Found 0 / 1
Feb 20 21:08:05.624: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:05.624: INFO: Found 0 / 1
Feb 20 21:08:06.621: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:06.621: INFO: Found 0 / 1
Feb 20 21:08:07.622: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:07.623: INFO: Found 0 / 1
Feb 20 21:08:08.624: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:08.624: INFO: Found 0 / 1
Feb 20 21:08:09.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:09.617: INFO: Found 0 / 1
Feb 20 21:08:10.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:10.618: INFO: Found 0 / 1
Feb 20 21:08:11.620: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:11.620: INFO: Found 0 / 1
Feb 20 21:08:12.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:12.619: INFO: Found 0 / 1
Feb 20 21:08:13.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:13.619: INFO: Found 0 / 1
Feb 20 21:08:14.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:14.618: INFO: Found 0 / 1
Feb 20 21:08:15.619: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:15.619: INFO: Found 0 / 1
Feb 20 21:08:16.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:16.618: INFO: Found 0 / 1
Feb 20 21:08:17.620: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:17.620: INFO: Found 0 / 1
Feb 20 21:08:18.620: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:18.620: INFO: Found 0 / 1
Feb 20 21:08:19.618: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:19.618: INFO: Found 0 / 1
Feb 20 21:08:20.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:20.617: INFO: Found 1 / 1
Feb 20 21:08:20.617: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 21:08:20.622: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:08:20.622: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 20 21:08:20.622: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs redis-master-w2psc redis-master --namespace=e2e-tests-kubectl-275gk'
Feb 20 21:08:20.730: INFO: stderr: ""
Feb 20 21:08:20.730: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 20 Feb 21:08:19.633 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Feb 21:08:19.633 # Server started, Redis version 3.2.12\n1:M 20 Feb 21:08:19.633 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Feb 21:08:19.633 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 20 21:08:20.730: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-w2psc redis-master --namespace=e2e-tests-kubectl-275gk --tail=1'
Feb 20 21:08:20.846: INFO: stderr: ""
Feb 20 21:08:20.847: INFO: stdout: "1:M 20 Feb 21:08:19.633 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 20 21:08:20.847: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-w2psc redis-master --namespace=e2e-tests-kubectl-275gk --limit-bytes=1'
Feb 20 21:08:20.978: INFO: stderr: ""
Feb 20 21:08:20.978: INFO: stdout: " "
STEP: exposing timestamps
Feb 20 21:08:20.978: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-w2psc redis-master --namespace=e2e-tests-kubectl-275gk --tail=1 --timestamps'
Feb 20 21:08:21.117: INFO: stderr: ""
Feb 20 21:08:21.117: INFO: stdout: "2019-02-20T21:08:19.633602032Z 1:M 20 Feb 21:08:19.633 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 20 21:08:23.617: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-w2psc redis-master --namespace=e2e-tests-kubectl-275gk --since=1s'
Feb 20 21:08:23.761: INFO: stderr: ""
Feb 20 21:08:23.761: INFO: stdout: ""
Feb 20 21:08:23.762: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-w2psc redis-master --namespace=e2e-tests-kubectl-275gk --since=24h'
Feb 20 21:08:24.075: INFO: stderr: ""
Feb 20 21:08:24.075: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 20 Feb 21:08:19.633 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Feb 21:08:19.633 # Server started, Redis version 3.2.12\n1:M 20 Feb 21:08:19.633 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Feb 21:08:19.633 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Feb 20 21:08:24.075: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-275gk'
Feb 20 21:08:24.374: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 21:08:24.374: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 20 21:08:24.374: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-275gk'
Feb 20 21:08:24.644: INFO: stderr: "No resources found.\n"
Feb 20 21:08:24.644: INFO: stdout: ""
Feb 20 21:08:24.644: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=nginx --namespace=e2e-tests-kubectl-275gk -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 21:08:24.841: INFO: stderr: ""
Feb 20 21:08:24.841: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:08:24.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-275gk" for this suite.
Feb 20 21:08:46.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:08:46.933: INFO: namespace: e2e-tests-kubectl-275gk, resource: bindings, ignored listing per whitelist
Feb 20 21:08:47.057: INFO: namespace e2e-tests-kubectl-275gk deletion completed in 22.211184016s

â€¢ [SLOW TEST:81.503 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:08:47.057: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-6gntt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 20 21:08:47.825: INFO: Waiting up to 5m0s for pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-bsbh8" in namespace "e2e-tests-svcaccounts-6gntt" to be "success or failure"
Feb 20 21:08:47.828: INFO: Pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-bsbh8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.054399ms
Feb 20 21:08:49.838: INFO: Pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-bsbh8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012865282s
STEP: Saw pod success
Feb 20 21:08:49.838: INFO: Pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-bsbh8" satisfied condition "success or failure"
Feb 20 21:08:49.842: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-bsbh8 container token-test: <nil>
STEP: delete the pod
Feb 20 21:08:49.860: INFO: Waiting for pod pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-bsbh8 to disappear
Feb 20 21:08:49.863: INFO: Pod pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-bsbh8 no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 20 21:08:49.868: INFO: Waiting up to 5m0s for pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-jxdpw" in namespace "e2e-tests-svcaccounts-6gntt" to be "success or failure"
Feb 20 21:08:49.875: INFO: Pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-jxdpw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.764599ms
Feb 20 21:08:51.882: INFO: Pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-jxdpw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0145011s
STEP: Saw pod success
Feb 20 21:08:51.883: INFO: Pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-jxdpw" satisfied condition "success or failure"
Feb 20 21:08:51.886: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-jxdpw container root-ca-test: <nil>
STEP: delete the pod
Feb 20 21:08:51.910: INFO: Waiting for pod pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-jxdpw to disappear
Feb 20 21:08:51.913: INFO: Pod pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-jxdpw no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 20 21:08:51.917: INFO: Waiting up to 5m0s for pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-ln9vc" in namespace "e2e-tests-svcaccounts-6gntt" to be "success or failure"
Feb 20 21:08:51.921: INFO: Pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-ln9vc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490021ms
Feb 20 21:08:53.925: INFO: Pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-ln9vc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007882832s
STEP: Saw pod success
Feb 20 21:08:53.925: INFO: Pod "pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-ln9vc" satisfied condition "success or failure"
Feb 20 21:08:53.929: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-ln9vc container namespace-test: <nil>
STEP: delete the pod
Feb 20 21:08:53.958: INFO: Waiting for pod pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-ln9vc to disappear
Feb 20 21:08:53.962: INFO: Pod pod-service-account-b6dfc74b-3553-11e9-8ceb-aaeae242ec87-ln9vc no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:08:53.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-6gntt" for this suite.
Feb 20 21:09:00.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:09:00.210: INFO: namespace: e2e-tests-svcaccounts-6gntt, resource: bindings, ignored listing per whitelist
Feb 20 21:09:00.284: INFO: namespace e2e-tests-svcaccounts-6gntt deletion completed in 6.311713177s

â€¢ [SLOW TEST:13.228 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:09:00.285: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-tjr94
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-gtc4
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 21:09:00.531: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gtc4" in namespace "e2e-tests-subpath-tjr94" to be "success or failure"
Feb 20 21:09:00.537: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07445ms
Feb 20 21:09:02.566: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034985122s
Feb 20 21:09:04.571: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Running", Reason="", readiness=false. Elapsed: 4.040196593s
Feb 20 21:09:06.577: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Running", Reason="", readiness=false. Elapsed: 6.045944786s
Feb 20 21:09:08.582: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Running", Reason="", readiness=false. Elapsed: 8.051161344s
Feb 20 21:09:10.587: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Running", Reason="", readiness=false. Elapsed: 10.056237087s
Feb 20 21:09:12.592: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Running", Reason="", readiness=false. Elapsed: 12.061532236s
Feb 20 21:09:14.600: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Running", Reason="", readiness=false. Elapsed: 14.068936631s
Feb 20 21:09:16.606: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Running", Reason="", readiness=false. Elapsed: 16.074974799s
Feb 20 21:09:18.611: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Running", Reason="", readiness=false. Elapsed: 18.080252811s
Feb 20 21:09:20.616: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Running", Reason="", readiness=false. Elapsed: 20.084911131s
Feb 20 21:09:22.621: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Running", Reason="", readiness=false. Elapsed: 22.090181401s
Feb 20 21:09:24.628: INFO: Pod "pod-subpath-test-downwardapi-gtc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.097224862s
STEP: Saw pod success
Feb 20 21:09:24.628: INFO: Pod "pod-subpath-test-downwardapi-gtc4" satisfied condition "success or failure"
Feb 20 21:09:24.631: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-subpath-test-downwardapi-gtc4 container test-container-subpath-downwardapi-gtc4: <nil>
STEP: delete the pod
Feb 20 21:09:24.661: INFO: Waiting for pod pod-subpath-test-downwardapi-gtc4 to disappear
Feb 20 21:09:24.665: INFO: Pod pod-subpath-test-downwardapi-gtc4 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-gtc4
Feb 20 21:09:24.665: INFO: Deleting pod "pod-subpath-test-downwardapi-gtc4" in namespace "e2e-tests-subpath-tjr94"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:09:24.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-tjr94" for this suite.
Feb 20 21:09:30.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:09:30.717: INFO: namespace: e2e-tests-subpath-tjr94, resource: bindings, ignored listing per whitelist
Feb 20 21:09:30.850: INFO: namespace e2e-tests-subpath-tjr94 deletion completed in 6.178482644s

â€¢ [SLOW TEST:30.565 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:09:30.850: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-2v5mn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 21:09:31.212: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:09:34.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-2v5mn" for this suite.
Feb 20 21:09:40.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:09:40.926: INFO: namespace: e2e-tests-init-container-2v5mn, resource: bindings, ignored listing per whitelist
Feb 20 21:09:41.074: INFO: namespace e2e-tests-init-container-2v5mn deletion completed in 6.173321927s

â€¢ [SLOW TEST:10.224 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:09:41.075: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-5prqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 21:09:41.456: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:09:42.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-5prqg" for this suite.
Feb 20 21:09:48.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:09:48.694: INFO: namespace: e2e-tests-custom-resource-definition-5prqg, resource: bindings, ignored listing per whitelist
Feb 20 21:09:48.869: INFO: namespace e2e-tests-custom-resource-definition-5prqg deletion completed in 6.299028555s

â€¢ [SLOW TEST:7.794 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:09:48.869: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mfk9q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-db69635a-3553-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 21:09:49.133: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-db6a8efb-3553-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-mfk9q" to be "success or failure"
Feb 20 21:09:49.138: INFO: Pod "pod-projected-secrets-db6a8efb-3553-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912602ms
Feb 20 21:09:51.143: INFO: Pod "pod-projected-secrets-db6a8efb-3553-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009690688s
STEP: Saw pod success
Feb 20 21:09:51.143: INFO: Pod "pod-projected-secrets-db6a8efb-3553-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:09:51.146: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-secrets-db6a8efb-3553-11e9-8ceb-aaeae242ec87 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 21:09:51.167: INFO: Waiting for pod pod-projected-secrets-db6a8efb-3553-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:09:51.170: INFO: Pod pod-projected-secrets-db6a8efb-3553-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:09:51.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mfk9q" for this suite.
Feb 20 21:09:57.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:09:57.248: INFO: namespace: e2e-tests-projected-mfk9q, resource: bindings, ignored listing per whitelist
Feb 20 21:09:57.330: INFO: namespace e2e-tests-projected-mfk9q deletion completed in 6.15469245s

â€¢ [SLOW TEST:8.460 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:09:57.330: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-vw458
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-e072e59c-3553-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 21:09:57.580: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0737afc-3553-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-configmap-vw458" to be "success or failure"
Feb 20 21:09:57.585: INFO: Pod "pod-configmaps-e0737afc-3553-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.823687ms
Feb 20 21:09:59.590: INFO: Pod "pod-configmaps-e0737afc-3553-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009756891s
STEP: Saw pod success
Feb 20 21:09:59.590: INFO: Pod "pod-configmaps-e0737afc-3553-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:09:59.594: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-configmaps-e0737afc-3553-11e9-8ceb-aaeae242ec87 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 21:09:59.612: INFO: Waiting for pod pod-configmaps-e0737afc-3553-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:09:59.616: INFO: Pod pod-configmaps-e0737afc-3553-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:09:59.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vw458" for this suite.
Feb 20 21:10:05.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:10:05.696: INFO: namespace: e2e-tests-configmap-vw458, resource: bindings, ignored listing per whitelist
Feb 20 21:10:05.807: INFO: namespace e2e-tests-configmap-vw458 deletion completed in 6.186036107s

â€¢ [SLOW TEST:8.477 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:10:05.807: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-cwm29
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-mprxl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-kt6vt
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:10:12.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-cwm29" for this suite.
Feb 20 21:10:18.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:10:18.530: INFO: namespace: e2e-tests-namespaces-cwm29, resource: bindings, ignored listing per whitelist
Feb 20 21:10:18.586: INFO: namespace e2e-tests-namespaces-cwm29 deletion completed in 6.227306432s
STEP: Destroying namespace "e2e-tests-nsdeletetest-mprxl" for this suite.
Feb 20 21:10:18.589: INFO: Namespace e2e-tests-nsdeletetest-mprxl was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-kt6vt" for this suite.
Feb 20 21:10:24.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:10:24.887: INFO: namespace: e2e-tests-nsdeletetest-kt6vt, resource: bindings, ignored listing per whitelist
Feb 20 21:10:24.977: INFO: namespace e2e-tests-nsdeletetest-kt6vt deletion completed in 6.387862797s

â€¢ [SLOW TEST:19.170 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:10:24.978: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-fp68f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 20 21:10:25.281: INFO: Waiting up to 5m0s for pod "pod-f0f65680-3553-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-fp68f" to be "success or failure"
Feb 20 21:10:25.286: INFO: Pod "pod-f0f65680-3553-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.267401ms
Feb 20 21:10:27.291: INFO: Pod "pod-f0f65680-3553-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010242799s
STEP: Saw pod success
Feb 20 21:10:27.291: INFO: Pod "pod-f0f65680-3553-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:10:27.294: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-f0f65680-3553-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:10:27.317: INFO: Waiting for pod pod-f0f65680-3553-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:10:27.320: INFO: Pod pod-f0f65680-3553-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:10:27.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-fp68f" for this suite.
Feb 20 21:10:33.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:10:33.495: INFO: namespace: e2e-tests-emptydir-fp68f, resource: bindings, ignored listing per whitelist
Feb 20 21:10:33.523: INFO: namespace e2e-tests-emptydir-fp68f deletion completed in 6.198818922s

â€¢ [SLOW TEST:8.545 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:10:33.523: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-fnmzb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 21:10:33.779: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:10:36.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-fnmzb" for this suite.
Feb 20 21:10:42.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:10:42.653: INFO: namespace: e2e-tests-init-container-fnmzb, resource: bindings, ignored listing per whitelist
Feb 20 21:10:42.717: INFO: namespace e2e-tests-init-container-fnmzb deletion completed in 6.226888816s

â€¢ [SLOW TEST:9.194 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:10:42.718: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-ngjd7
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-fb871776-3553-11e9-8ceb-aaeae242ec87
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-fb871776-3553-11e9-8ceb-aaeae242ec87
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:10:47.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ngjd7" for this suite.
Feb 20 21:11:09.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:11:09.327: INFO: namespace: e2e-tests-configmap-ngjd7, resource: bindings, ignored listing per whitelist
Feb 20 21:11:09.332: INFO: namespace e2e-tests-configmap-ngjd7 deletion completed in 22.180784257s

â€¢ [SLOW TEST:26.614 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:11:09.332: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-2r456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 21:11:09.572: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:11:12.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-2r456" for this suite.
Feb 20 21:11:34.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:11:35.004: INFO: namespace: e2e-tests-init-container-2r456, resource: bindings, ignored listing per whitelist
Feb 20 21:11:35.024: INFO: namespace e2e-tests-init-container-2r456 deletion completed in 22.189075529s

â€¢ [SLOW TEST:25.692 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:11:35.025: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-k8c55
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 20 21:11:35.294: INFO: Number of nodes with available pods: 0
Feb 20 21:11:35.294: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:36.305: INFO: Number of nodes with available pods: 0
Feb 20 21:11:36.305: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:37.307: INFO: Number of nodes with available pods: 0
Feb 20 21:11:37.307: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:38.305: INFO: Number of nodes with available pods: 0
Feb 20 21:11:38.305: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:39.302: INFO: Number of nodes with available pods: 0
Feb 20 21:11:39.302: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:40.305: INFO: Number of nodes with available pods: 0
Feb 20 21:11:40.305: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:41.303: INFO: Number of nodes with available pods: 0
Feb 20 21:11:41.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:42.304: INFO: Number of nodes with available pods: 0
Feb 20 21:11:42.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:43.309: INFO: Number of nodes with available pods: 0
Feb 20 21:11:43.309: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:44.303: INFO: Number of nodes with available pods: 0
Feb 20 21:11:44.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:45.306: INFO: Number of nodes with available pods: 0
Feb 20 21:11:45.306: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:46.303: INFO: Number of nodes with available pods: 0
Feb 20 21:11:46.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:47.303: INFO: Number of nodes with available pods: 0
Feb 20 21:11:47.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:48.315: INFO: Number of nodes with available pods: 0
Feb 20 21:11:48.315: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:49.304: INFO: Number of nodes with available pods: 0
Feb 20 21:11:49.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:50.303: INFO: Number of nodes with available pods: 0
Feb 20 21:11:50.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:51.303: INFO: Number of nodes with available pods: 0
Feb 20 21:11:51.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:52.304: INFO: Number of nodes with available pods: 0
Feb 20 21:11:52.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:53.302: INFO: Number of nodes with available pods: 0
Feb 20 21:11:53.302: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:54.306: INFO: Number of nodes with available pods: 0
Feb 20 21:11:54.306: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:55.303: INFO: Number of nodes with available pods: 0
Feb 20 21:11:55.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:56.304: INFO: Number of nodes with available pods: 0
Feb 20 21:11:56.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:57.304: INFO: Number of nodes with available pods: 0
Feb 20 21:11:57.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:58.303: INFO: Number of nodes with available pods: 0
Feb 20 21:11:58.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:11:59.303: INFO: Number of nodes with available pods: 0
Feb 20 21:11:59.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:00.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:00.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:01.305: INFO: Number of nodes with available pods: 0
Feb 20 21:12:01.305: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:02.304: INFO: Number of nodes with available pods: 0
Feb 20 21:12:02.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:03.304: INFO: Number of nodes with available pods: 0
Feb 20 21:12:03.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:04.305: INFO: Number of nodes with available pods: 0
Feb 20 21:12:04.305: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:05.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:05.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:06.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:06.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:07.304: INFO: Number of nodes with available pods: 0
Feb 20 21:12:07.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:08.304: INFO: Number of nodes with available pods: 0
Feb 20 21:12:08.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:09.312: INFO: Number of nodes with available pods: 0
Feb 20 21:12:09.312: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:10.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:10.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:11.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:11.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:12.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:12.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:13.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:13.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:14.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:14.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:15.304: INFO: Number of nodes with available pods: 0
Feb 20 21:12:15.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:16.306: INFO: Number of nodes with available pods: 0
Feb 20 21:12:16.306: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:17.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:17.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:18.304: INFO: Number of nodes with available pods: 0
Feb 20 21:12:18.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:19.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:19.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:20.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:20.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:21.303: INFO: Number of nodes with available pods: 0
Feb 20 21:12:21.303: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:22.304: INFO: Number of nodes with available pods: 0
Feb 20 21:12:22.304: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:23.306: INFO: Number of nodes with available pods: 1
Feb 20 21:12:23.306: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:12:24.304: INFO: Number of nodes with available pods: 2
Feb 20 21:12:24.304: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 20 21:12:24.323: INFO: Number of nodes with available pods: 1
Feb 20 21:12:24.323: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 21:12:25.334: INFO: Number of nodes with available pods: 2
Feb 20 21:12:25.334: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-k8c55, will wait for the garbage collector to delete the pods
Feb 20 21:12:25.400: INFO: Deleting DaemonSet.extensions daemon-set took: 6.924913ms
Feb 20 21:12:25.500: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.188835ms
Feb 20 21:13:10.305: INFO: Number of nodes with available pods: 0
Feb 20 21:13:10.305: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 21:13:10.310: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-k8c55/daemonsets","resourceVersion":"6417"},"items":null}

Feb 20 21:13:10.315: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-k8c55/pods","resourceVersion":"6417"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:13:10.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-k8c55" for this suite.
Feb 20 21:13:16.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:13:16.355: INFO: namespace: e2e-tests-daemonsets-k8c55, resource: bindings, ignored listing per whitelist
Feb 20 21:13:16.552: INFO: namespace e2e-tests-daemonsets-k8c55 deletion completed in 6.223819527s

â€¢ [SLOW TEST:101.527 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:13:16.552: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-svxgr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 21:13:16.802: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57325ce3-3554-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-svxgr" to be "success or failure"
Feb 20 21:13:16.805: INFO: Pod "downwardapi-volume-57325ce3-3554-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.070786ms
Feb 20 21:13:18.811: INFO: Pod "downwardapi-volume-57325ce3-3554-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008883438s
STEP: Saw pod success
Feb 20 21:13:18.811: INFO: Pod "downwardapi-volume-57325ce3-3554-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:13:18.823: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-57325ce3-3554-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 21:13:18.847: INFO: Waiting for pod downwardapi-volume-57325ce3-3554-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:13:18.852: INFO: Pod downwardapi-volume-57325ce3-3554-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:13:18.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-svxgr" for this suite.
Feb 20 21:13:24.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:13:24.970: INFO: namespace: e2e-tests-projected-svxgr, resource: bindings, ignored listing per whitelist
Feb 20 21:13:25.064: INFO: namespace e2e-tests-projected-svxgr deletion completed in 6.207673318s

â€¢ [SLOW TEST:8.513 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:13:25.065: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-cxvlz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 21:13:25.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c46b707-3554-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-cxvlz" to be "success or failure"
Feb 20 21:13:25.332: INFO: Pod "downwardapi-volume-5c46b707-3554-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.972395ms
Feb 20 21:13:27.337: INFO: Pod "downwardapi-volume-5c46b707-3554-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009988371s
STEP: Saw pod success
Feb 20 21:13:27.337: INFO: Pod "downwardapi-volume-5c46b707-3554-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:13:27.342: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-5c46b707-3554-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 21:13:27.364: INFO: Waiting for pod downwardapi-volume-5c46b707-3554-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:13:27.368: INFO: Pod downwardapi-volume-5c46b707-3554-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:13:27.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cxvlz" for this suite.
Feb 20 21:13:33.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:13:33.465: INFO: namespace: e2e-tests-downward-api-cxvlz, resource: bindings, ignored listing per whitelist
Feb 20 21:13:33.624: INFO: namespace e2e-tests-downward-api-cxvlz deletion completed in 6.251781413s

â€¢ [SLOW TEST:8.560 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:13:33.624: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-jmclw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 20 21:13:36.441: INFO: Successfully updated pod "pod-update-6164b58d-3554-11e9-8ceb-aaeae242ec87"
STEP: verifying the updated pod is in kubernetes
Feb 20 21:13:36.465: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:13:36.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-jmclw" for this suite.
Feb 20 21:13:58.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:13:58.680: INFO: namespace: e2e-tests-pods-jmclw, resource: bindings, ignored listing per whitelist
Feb 20 21:13:58.732: INFO: namespace e2e-tests-pods-jmclw deletion completed in 22.262964537s

â€¢ [SLOW TEST:25.107 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:13:58.732: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-xzhb9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 20 21:13:59.033: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 21:13:59.058: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 21:13:59.064: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs before test
Feb 20 21:13:59.091: INFO: metrics-server-7594c945c8-pndpd from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container metrics-server ready: true, restart count 0
Feb 20 21:13:59.091: INFO: coredns-67df79bbdd-6dv5v from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container coredns ready: true, restart count 0
Feb 20 21:13:59.091: INFO: blackbox-exporter-d6c46f9fc-jt4nc from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 20 21:13:59.091: INFO: addons-nginx-ingress-controller-d74bfff57-wsvnp from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 20 21:13:59.091: INFO: addons-kubernetes-dashboard-6579b646c5-ttbst from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 20 21:13:59.091: INFO: calico-node-9f88m from kube-system started at 2019-02-20 20:36:04 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 21:13:59.091: INFO: kube-proxy-slfj9 from kube-system started at 2019-02-20 20:36:04 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 21:13:59.091: INFO: node-exporter-zfb8s from kube-system started at 2019-02-20 20:36:04 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 21:13:59.091: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-h9ds6 from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 20 21:13:59.091: INFO: vpn-shoot-67f9c4c5ff-z2l8j from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 20 21:13:59.091: INFO: addons-kube-lego-69bbdc96b6-7c626 from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.091: INFO: 	Container kube-lego ready: true, restart count 0
Feb 20 21:13:59.091: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq before test
Feb 20 21:13:59.133: INFO: calico-node-wdqxm from kube-system started at 2019-02-20 20:36:26 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.133: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 21:13:59.133: INFO: node-exporter-kqq7b from kube-system started at 2019-02-20 20:36:26 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.133: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 21:13:59.133: INFO: kube-proxy-pp9s2 from kube-system started at 2019-02-20 20:36:26 +0000 UTC (1 container statuses recorded)
Feb 20 21:13:59.133: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-71a4485b-3554-11e9-8ceb-aaeae242ec87 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-71a4485b-3554-11e9-8ceb-aaeae242ec87 off the node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq
STEP: verifying the node doesn't have the label kubernetes.io/e2e-71a4485b-3554-11e9-8ceb-aaeae242ec87
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:14:03.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-xzhb9" for this suite.
Feb 20 21:14:23.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:14:23.248: INFO: namespace: e2e-tests-sched-pred-xzhb9, resource: bindings, ignored listing per whitelist
Feb 20 21:14:23.457: INFO: namespace e2e-tests-sched-pred-xzhb9 deletion completed in 20.244299386s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:24.725 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:14:23.458: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-czlgk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0220 21:14:24.750267   32391 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 21:14:24.750: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:14:24.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-czlgk" for this suite.
Feb 20 21:14:30.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:14:30.907: INFO: namespace: e2e-tests-gc-czlgk, resource: bindings, ignored listing per whitelist
Feb 20 21:14:30.942: INFO: namespace e2e-tests-gc-czlgk deletion completed in 6.188622909s

â€¢ [SLOW TEST:7.485 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:14:30.943: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6hrr4
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-83899802-3554-11e9-8ceb-aaeae242ec87
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-83899802-3554-11e9-8ceb-aaeae242ec87
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:14:35.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6hrr4" for this suite.
Feb 20 21:14:57.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:14:57.382: INFO: namespace: e2e-tests-projected-6hrr4, resource: bindings, ignored listing per whitelist
Feb 20 21:14:57.517: INFO: namespace e2e-tests-projected-6hrr4 deletion completed in 22.198606994s

â€¢ [SLOW TEST:26.574 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:14:57.517: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-gkw2b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-935f4e73-3554-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 21:14:57.766: INFO: Waiting up to 5m0s for pod "pod-configmaps-9360249d-3554-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-configmap-gkw2b" to be "success or failure"
Feb 20 21:14:57.770: INFO: Pod "pod-configmaps-9360249d-3554-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069261ms
Feb 20 21:14:59.775: INFO: Pod "pod-configmaps-9360249d-3554-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008877258s
STEP: Saw pod success
Feb 20 21:14:59.775: INFO: Pod "pod-configmaps-9360249d-3554-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:14:59.779: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-configmaps-9360249d-3554-11e9-8ceb-aaeae242ec87 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 21:14:59.801: INFO: Waiting for pod pod-configmaps-9360249d-3554-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:14:59.804: INFO: Pod pod-configmaps-9360249d-3554-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:14:59.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gkw2b" for this suite.
Feb 20 21:15:05.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:15:06.049: INFO: namespace: e2e-tests-configmap-gkw2b, resource: bindings, ignored listing per whitelist
Feb 20 21:15:06.054: INFO: namespace e2e-tests-configmap-gkw2b deletion completed in 6.24633405s

â€¢ [SLOW TEST:8.537 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:15:06.054: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-rsvl5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 21:15:06.348: INFO: Creating deployment "nginx-deployment"
Feb 20 21:15:06.354: INFO: Waiting for observed generation 1
Feb 20 21:15:08.367: INFO: Waiting for all required pods to come up
Feb 20 21:15:08.376: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 20 21:15:10.388: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 20 21:15:10.398: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 20 21:15:10.409: INFO: Updating deployment nginx-deployment
Feb 20 21:15:10.409: INFO: Waiting for observed generation 2
Feb 20 21:15:12.418: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 20 21:15:12.424: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 20 21:15:12.428: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 20 21:15:12.441: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 20 21:15:12.441: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 20 21:15:12.445: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 20 21:15:12.452: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 20 21:15:12.452: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 20 21:15:12.463: INFO: Updating deployment nginx-deployment
Feb 20 21:15:12.463: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 20 21:15:12.484: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 20 21:15:12.491: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 21:15:14.511: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rsvl5/deployments/nginx-deployment,UID:987f2a7a-3554-11e9-91b0-3e11857925bf,ResourceVersion:7009,Generation:3,CreationTimestamp:2019-02-20 21:15:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-02-20 21:15:12 +0000 UTC 2019-02-20 21:15:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-20 21:15:12 +0000 UTC 2019-02-20 21:15:06 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 20 21:15:14.515: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rsvl5/replicasets/nginx-deployment-65bbdb5f8,UID:9aeb3634-3554-11e9-91b0-3e11857925bf,ResourceVersion:6997,Generation:3,CreationTimestamp:2019-02-20 21:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 987f2a7a-3554-11e9-91b0-3e11857925bf 0xc002dbe7f7 0xc002dbe7f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 21:15:14.515: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 20 21:15:14.515: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rsvl5/replicasets/nginx-deployment-555b55d965,UID:9880e64d-3554-11e9-91b0-3e11857925bf,ResourceVersion:7008,Generation:3,CreationTimestamp:2019-02-20 21:15:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 987f2a7a-3554-11e9-91b0-3e11857925bf 0xc002dbe737 0xc002dbe738}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 20 21:15:14.525: INFO: Pod "nginx-deployment-555b55d965-44sfc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-44sfc,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-44sfc,UID:9c2eac41-3554-11e9-91b0-3e11857925bf,ResourceVersion:7021,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc001779b60 0xc001779b61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001779bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001779be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.525: INFO: Pod "nginx-deployment-555b55d965-4nm4t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4nm4t,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-4nm4t,UID:9c29123a-3554-11e9-91b0-3e11857925bf,ResourceVersion:7020,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc001779d10 0xc001779d11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001779d70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001779d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.525: INFO: Pod "nginx-deployment-555b55d965-4wdns" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4wdns,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-4wdns,UID:9c2e9311-3554-11e9-91b0-3e11857925bf,ResourceVersion:7029,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.22/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc001779e90 0xc001779e91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001779ef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001779f10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.525: INFO: Pod "nginx-deployment-555b55d965-5wwzw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5wwzw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-5wwzw,UID:9c28fab0-3554-11e9-91b0-3e11857925bf,ResourceVersion:7031,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.23/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e10000 0xc002e10001}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e10060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e10080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.525: INFO: Pod "nginx-deployment-555b55d965-7qcfm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-7qcfm,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-7qcfm,UID:98849cb6-3554-11e9-91b0-3e11857925bf,ResourceVersion:6895,Generation:0,CreationTimestamp:2019-02-20 21:15:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.52/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e10150 0xc002e10151}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e102f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e10310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:100.96.1.52,StartTime:2019-02-20 21:15:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 21:15:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://03525e1072287e729c086c8e305a6219df89acda2965810f0d98962e0703e2d9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.525: INFO: Pod "nginx-deployment-555b55d965-89jvd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-89jvd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-89jvd,UID:9885c88d-3554-11e9-91b0-3e11857925bf,ResourceVersion:6886,Generation:0,CreationTimestamp:2019-02-20 21:15:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.15/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e103f0 0xc002e103f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e10450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e10470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.96.0.15,StartTime:2019-02-20 21:15:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 21:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://65aa3de2d72f4779e677d04b28fbb5511349b3e7751f979799208e03c3f4b08a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.526: INFO: Pod "nginx-deployment-555b55d965-8ftqw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8ftqw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-8ftqw,UID:9c28ffd7-3554-11e9-91b0-3e11857925bf,ResourceVersion:7015,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e10530 0xc002e10531}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e10590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e105b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.526: INFO: Pod "nginx-deployment-555b55d965-9kvvx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9kvvx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-9kvvx,UID:9c325f4d-3554-11e9-91b0-3e11857925bf,ResourceVersion:7038,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.29/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e10670 0xc002e10671}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e106d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e106f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.526: INFO: Pod "nginx-deployment-555b55d965-9rmpc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9rmpc,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-9rmpc,UID:98828ecc-3554-11e9-91b0-3e11857925bf,ResourceVersion:6883,Generation:0,CreationTimestamp:2019-02-20 21:15:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.49/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e107b0 0xc002e107b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e10810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e10830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:100.96.1.49,StartTime:2019-02-20 21:15:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 21:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://08d1a29118c3e23e501d954792da14755d522af8163448e81effa1cb365c9147}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.526: INFO: Pod "nginx-deployment-555b55d965-ct8gl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ct8gl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-ct8gl,UID:9885e522-3554-11e9-91b0-3e11857925bf,ResourceVersion:6899,Generation:0,CreationTimestamp:2019-02-20 21:15:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.54/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e10900 0xc002e10901}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e10970} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e10990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:100.96.1.54,StartTime:2019-02-20 21:15:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 21:15:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://2af40c7c751b024b0eb0593fba38c7276926f0aa87160f98ac2ae88284aef77d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.526: INFO: Pod "nginx-deployment-555b55d965-f24cx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-f24cx,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-f24cx,UID:9c2ea3f8-3554-11e9-91b0-3e11857925bf,ResourceVersion:7022,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e10a50 0xc002e10a51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e10ab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e10ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.526: INFO: Pod "nginx-deployment-555b55d965-gt2ss" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gt2ss,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-gt2ss,UID:9c26b51e-3554-11e9-91b0-3e11857925bf,ResourceVersion:7028,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.59/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e10ba0 0xc002e10ba1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e10c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e10c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.526: INFO: Pod "nginx-deployment-555b55d965-ls6x7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ls6x7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-ls6x7,UID:9c291dfa-3554-11e9-91b0-3e11857925bf,ResourceVersion:7035,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.26/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e10ce0 0xc002e10ce1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e10d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e10d80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.527: INFO: Pod "nginx-deployment-555b55d965-nlnk7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-nlnk7,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-nlnk7,UID:9c279de3-3554-11e9-91b0-3e11857925bf,ResourceVersion:7026,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.58/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e10e60 0xc002e10e61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e10ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e10ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.527: INFO: Pod "nginx-deployment-555b55d965-nxh4t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-nxh4t,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-nxh4t,UID:9c2a28af-3554-11e9-91b0-3e11857925bf,ResourceVersion:7039,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.28/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e10fa0 0xc002e10fa1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e11000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e11020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.527: INFO: Pod "nginx-deployment-555b55d965-qntwf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qntwf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-qntwf,UID:9884a5f2-3554-11e9-91b0-3e11857925bf,ResourceVersion:6889,Generation:0,CreationTimestamp:2019-02-20 21:15:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.18/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e110f0 0xc002e110f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e11170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e11190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.96.0.18,StartTime:2019-02-20 21:15:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 21:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://abdec8c266a7b96f3cd9bdbef9a61d8b297f3ac60ff2d87aa8546f402bb38c1c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.527: INFO: Pod "nginx-deployment-555b55d965-rq5mw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rq5mw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-rq5mw,UID:9883b5fa-3554-11e9-91b0-3e11857925bf,ResourceVersion:6892,Generation:0,CreationTimestamp:2019-02-20 21:15:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.50/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e11290 0xc002e11291}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e11300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e11320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:100.96.1.50,StartTime:2019-02-20 21:15:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 21:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://062b68ad91253ecec1e41c4fb4f78b5e57d21a1ca7a08fd0ac833d7bfbb13583}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.527: INFO: Pod "nginx-deployment-555b55d965-skhpp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-skhpp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-skhpp,UID:9c27ae20-3554-11e9-91b0-3e11857925bf,ResourceVersion:7032,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.24/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e113f0 0xc002e113f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e11470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e114c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.527: INFO: Pod "nginx-deployment-555b55d965-t7mpr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-t7mpr,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-t7mpr,UID:98837d45-3554-11e9-91b0-3e11857925bf,ResourceVersion:6875,Generation:0,CreationTimestamp:2019-02-20 21:15:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.17/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e11580 0xc002e11581}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e115e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e11600}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.96.0.17,StartTime:2019-02-20 21:15:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 21:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://3767fe3afa661ecf34cd06a8613d55b5265033056e4a55d8d75e786ba64d4b84}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.527: INFO: Pod "nginx-deployment-555b55d965-xp9v5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-xp9v5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-555b55d965-xp9v5,UID:988499a6-3554-11e9-91b0-3e11857925bf,ResourceVersion:6878,Generation:0,CreationTimestamp:2019-02-20 21:15:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.16/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 9880e64d-3554-11e9-91b0-3e11857925bf 0xc002e11ea0 0xc002e11ea1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e11f00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e11f20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:06 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.96.0.16,StartTime:2019-02-20 21:15:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 21:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://1ca6263d08f9b65fdca77fa381aab639524c56292d9ed190e7bec4b8b513881a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.528: INFO: Pod "nginx-deployment-65bbdb5f8-49nqj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-49nqj,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-49nqj,UID:9c28e4fe-3554-11e9-91b0-3e11857925bf,ResourceVersion:7019,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e92110 0xc002e92111}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e92260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e92280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.528: INFO: Pod "nginx-deployment-65bbdb5f8-6l2mr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6l2mr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-6l2mr,UID:9c28b479-3554-11e9-91b0-3e11857925bf,ResourceVersion:7030,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.60/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e92660 0xc002e92661}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e926d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e926f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.528: INFO: Pod "nginx-deployment-65bbdb5f8-8jrf2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-8jrf2,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-8jrf2,UID:9af0bcd3-3554-11e9-91b0-3e11857925bf,ResourceVersion:6949,Generation:0,CreationTimestamp:2019-02-20 21:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.57/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e927f0 0xc002e927f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e92a00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e92a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.528: INFO: Pod "nginx-deployment-65bbdb5f8-94mhc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-94mhc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-94mhc,UID:9c27d0a9-3554-11e9-91b0-3e11857925bf,ResourceVersion:7034,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.61/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e92af0 0xc002e92af1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e92bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e92be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.528: INFO: Pod "nginx-deployment-65bbdb5f8-98vg9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-98vg9,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-98vg9,UID:9af13d31-3554-11e9-91b0-3e11857925bf,ResourceVersion:6945,Generation:0,CreationTimestamp:2019-02-20 21:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.20/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e92cb0 0xc002e92cb1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e92d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e92d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.528: INFO: Pod "nginx-deployment-65bbdb5f8-dm6md" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dm6md,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-dm6md,UID:9aebcb1c-3554-11e9-91b0-3e11857925bf,ResourceVersion:6944,Generation:0,CreationTimestamp:2019-02-20 21:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.19/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e92e10 0xc002e92e11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e92e80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e92ea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.528: INFO: Pod "nginx-deployment-65bbdb5f8-g6s9d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-g6s9d,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-g6s9d,UID:9aec6b5e-3554-11e9-91b0-3e11857925bf,ResourceVersion:6946,Generation:0,CreationTimestamp:2019-02-20 21:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.55/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e92f70 0xc002e92f71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e92fe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e93000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.528: INFO: Pod "nginx-deployment-65bbdb5f8-mj62l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-mj62l,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-mj62l,UID:9aec71f4-3554-11e9-91b0-3e11857925bf,ResourceVersion:6947,Generation:0,CreationTimestamp:2019-02-20 21:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.56/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e930d0 0xc002e930d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e93140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e93160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:10 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.529: INFO: Pod "nginx-deployment-65bbdb5f8-p5859" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-p5859,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-p5859,UID:9c27265e-3554-11e9-91b0-3e11857925bf,ResourceVersion:7025,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.21/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e93230 0xc002e93231}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e932a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e932c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.529: INFO: Pod "nginx-deployment-65bbdb5f8-rsmvs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rsmvs,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-rsmvs,UID:9c29fbeb-3554-11e9-91b0-3e11857925bf,ResourceVersion:7036,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.62/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e93390 0xc002e93391}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e93400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e93420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.529: INFO: Pod "nginx-deployment-65bbdb5f8-vjfwn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-vjfwn,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-vjfwn,UID:9c28d986-3554-11e9-91b0-3e11857925bf,ResourceVersion:7033,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.25/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e934f0 0xc002e934f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e93560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e93580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.529: INFO: Pod "nginx-deployment-65bbdb5f8-vmths" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-vmths,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-vmths,UID:9c27d899-3554-11e9-91b0-3e11857925bf,ResourceVersion:7037,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.27/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e93650 0xc002e93651}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e936c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e936e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 21:15:14.529: INFO: Pod "nginx-deployment-65bbdb5f8-xwqfp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-xwqfp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-rsvl5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rsvl5/pods/nginx-deployment-65bbdb5f8-xwqfp,UID:9c28dbbd-3554-11e9-91b0-3e11857925bf,ResourceVersion:7013,Generation:0,CreationTimestamp:2019-02-20 21:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 9aeb3634-3554-11e9-91b0-3e11857925bf 0xc002e937a0 0xc002e937a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rxr86 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rxr86,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rxr86 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e93810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e93830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 21:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:15:14.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-rsvl5" for this suite.
Feb 20 21:15:20.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:15:20.635: INFO: namespace: e2e-tests-deployment-rsvl5, resource: bindings, ignored listing per whitelist
Feb 20 21:15:20.756: INFO: namespace e2e-tests-deployment-rsvl5 deletion completed in 6.207123531s

â€¢ [SLOW TEST:14.702 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:15:20.756: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-j8m4g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-j8m4g
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 21:15:21.046: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 21:18:29.138: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.70:8080/dial?request=hostName&protocol=http&host=100.96.1.69&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-j8m4g PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 21:18:29.138: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 21:18:29.596: INFO: Waiting for endpoints: map[]
Feb 20 21:18:29.601: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.70:8080/dial?request=hostName&protocol=http&host=100.96.0.30&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-j8m4g PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 21:18:29.601: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 21:18:30.143: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:18:30.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-j8m4g" for this suite.
Feb 20 21:18:52.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:18:52.375: INFO: namespace: e2e-tests-pod-network-test-j8m4g, resource: bindings, ignored listing per whitelist
Feb 20 21:18:52.387: INFO: namespace e2e-tests-pod-network-test-j8m4g deletion completed in 22.23851898s

â€¢ [SLOW TEST:211.631 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:18:52.388: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-db87v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 21:18:52.657: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f61abe1-3555-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-db87v" to be "success or failure"
Feb 20 21:18:52.660: INFO: Pod "downwardapi-volume-1f61abe1-3555-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.299857ms
Feb 20 21:18:54.666: INFO: Pod "downwardapi-volume-1f61abe1-3555-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008923958s
STEP: Saw pod success
Feb 20 21:18:54.666: INFO: Pod "downwardapi-volume-1f61abe1-3555-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:18:54.670: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-1f61abe1-3555-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 21:18:54.697: INFO: Waiting for pod downwardapi-volume-1f61abe1-3555-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:18:54.701: INFO: Pod downwardapi-volume-1f61abe1-3555-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:18:54.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-db87v" for this suite.
Feb 20 21:19:00.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:19:00.858: INFO: namespace: e2e-tests-projected-db87v, resource: bindings, ignored listing per whitelist
Feb 20 21:19:00.930: INFO: namespace e2e-tests-projected-db87v deletion completed in 6.207727723s

â€¢ [SLOW TEST:8.543 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:19:00.931: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-5kjjg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0220 21:19:11.246302   32391 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 21:19:11.246: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:19:11.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-5kjjg" for this suite.
Feb 20 21:19:17.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:19:17.357: INFO: namespace: e2e-tests-gc-5kjjg, resource: bindings, ignored listing per whitelist
Feb 20 21:19:17.526: INFO: namespace e2e-tests-gc-5kjjg deletion completed in 6.276110663s

â€¢ [SLOW TEST:16.595 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:19:17.526: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-977mp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 20 21:19:17.801: INFO: Waiting up to 5m0s for pod "pod-2e5e494d-3555-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-977mp" to be "success or failure"
Feb 20 21:19:17.810: INFO: Pod "pod-2e5e494d-3555-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 8.968569ms
Feb 20 21:19:19.815: INFO: Pod "pod-2e5e494d-3555-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013995699s
Feb 20 21:19:21.820: INFO: Pod "pod-2e5e494d-3555-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018614922s
STEP: Saw pod success
Feb 20 21:19:21.820: INFO: Pod "pod-2e5e494d-3555-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:19:21.823: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-2e5e494d-3555-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:19:21.842: INFO: Waiting for pod pod-2e5e494d-3555-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:19:21.845: INFO: Pod pod-2e5e494d-3555-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:19:21.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-977mp" for this suite.
Feb 20 21:19:27.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:19:27.975: INFO: namespace: e2e-tests-emptydir-977mp, resource: bindings, ignored listing per whitelist
Feb 20 21:19:28.100: INFO: namespace e2e-tests-emptydir-977mp deletion completed in 6.250590234s

â€¢ [SLOW TEST:10.574 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:19:28.100: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-r5kkk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-r5kkk
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-r5kkk to expose endpoints map[]
Feb 20 21:19:28.407: INFO: Get endpoints failed (3.984367ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 20 21:19:29.412: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-r5kkk exposes endpoints map[] (1.008837098s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-r5kkk
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-r5kkk to expose endpoints map[pod1:[100]]
Feb 20 21:19:31.446: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-r5kkk exposes endpoints map[pod1:[100]] (2.025306278s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-r5kkk
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-r5kkk to expose endpoints map[pod1:[100] pod2:[101]]
Feb 20 21:19:35.523: INFO: Unexpected endpoints: found map[354bfbdc-3555-11e9-91b0-3e11857925bf:[100]], expected map[pod1:[100] pod2:[101]] (4.070877331s elapsed, will retry)
Feb 20 21:19:40.585: INFO: Unexpected endpoints: found map[354bfbdc-3555-11e9-91b0-3e11857925bf:[100]], expected map[pod1:[100] pod2:[101]] (9.133596276s elapsed, will retry)
Feb 20 21:19:45.658: INFO: Unexpected endpoints: found map[354bfbdc-3555-11e9-91b0-3e11857925bf:[100]], expected map[pod1:[100] pod2:[101]] (14.205834215s elapsed, will retry)
Feb 20 21:19:50.726: INFO: Unexpected endpoints: found map[354bfbdc-3555-11e9-91b0-3e11857925bf:[100]], expected map[pod1:[100] pod2:[101]] (19.274063003s elapsed, will retry)
Feb 20 21:19:55.804: INFO: Unexpected endpoints: found map[354bfbdc-3555-11e9-91b0-3e11857925bf:[100]], expected map[pod1:[100] pod2:[101]] (24.352558042s elapsed, will retry)
Feb 20 21:20:00.869: INFO: Unexpected endpoints: found map[354bfbdc-3555-11e9-91b0-3e11857925bf:[100]], expected map[pod1:[100] pod2:[101]] (29.417167676s elapsed, will retry)
Feb 20 21:20:05.954: INFO: Unexpected endpoints: found map[354bfbdc-3555-11e9-91b0-3e11857925bf:[100]], expected map[pod1:[100] pod2:[101]] (34.502448468s elapsed, will retry)
Feb 20 21:20:11.034: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-r5kkk exposes endpoints map[pod1:[100] pod2:[101]] (39.581984182s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-r5kkk
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-r5kkk to expose endpoints map[pod2:[101]]
Feb 20 21:20:11.047: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-r5kkk exposes endpoints map[pod2:[101]] (8.434089ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-r5kkk
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-r5kkk to expose endpoints map[]
Feb 20 21:20:11.057: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-r5kkk exposes endpoints map[] (4.416502ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:20:11.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-r5kkk" for this suite.
Feb 20 21:20:17.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:20:17.274: INFO: namespace: e2e-tests-services-r5kkk, resource: bindings, ignored listing per whitelist
Feb 20 21:20:17.364: INFO: namespace e2e-tests-services-r5kkk deletion completed in 6.266310492s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:49.264 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:20:17.364: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6z6ck
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-520ae41f-3555-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 21:20:17.656: INFO: Waiting up to 5m0s for pod "pod-secrets-520ba267-3555-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-secrets-6z6ck" to be "success or failure"
Feb 20 21:20:17.660: INFO: Pod "pod-secrets-520ba267-3555-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.659281ms
Feb 20 21:20:19.666: INFO: Pod "pod-secrets-520ba267-3555-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00939489s
STEP: Saw pod success
Feb 20 21:20:19.666: INFO: Pod "pod-secrets-520ba267-3555-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:20:19.671: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-secrets-520ba267-3555-11e9-8ceb-aaeae242ec87 container secret-env-test: <nil>
STEP: delete the pod
Feb 20 21:20:19.692: INFO: Waiting for pod pod-secrets-520ba267-3555-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:20:19.699: INFO: Pod pod-secrets-520ba267-3555-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:20:19.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6z6ck" for this suite.
Feb 20 21:20:25.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:20:25.955: INFO: namespace: e2e-tests-secrets-6z6ck, resource: bindings, ignored listing per whitelist
Feb 20 21:20:26.078: INFO: namespace e2e-tests-secrets-6z6ck deletion completed in 6.373495132s

â€¢ [SLOW TEST:8.714 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:20:26.078: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-c6mql
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-573de3c1-3555-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 21:20:26.385: INFO: Waiting up to 5m0s for pod "pod-configmaps-573e8806-3555-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-configmap-c6mql" to be "success or failure"
Feb 20 21:20:26.389: INFO: Pod "pod-configmaps-573e8806-3555-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.77806ms
Feb 20 21:20:28.396: INFO: Pod "pod-configmaps-573e8806-3555-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009995415s
STEP: Saw pod success
Feb 20 21:20:28.396: INFO: Pod "pod-configmaps-573e8806-3555-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:20:28.400: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-configmaps-573e8806-3555-11e9-8ceb-aaeae242ec87 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 21:20:28.429: INFO: Waiting for pod pod-configmaps-573e8806-3555-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:20:28.434: INFO: Pod pod-configmaps-573e8806-3555-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:20:28.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-c6mql" for this suite.
Feb 20 21:20:34.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:20:34.621: INFO: namespace: e2e-tests-configmap-c6mql, resource: bindings, ignored listing per whitelist
Feb 20 21:20:34.684: INFO: namespace e2e-tests-configmap-c6mql deletion completed in 6.240191302s

â€¢ [SLOW TEST:8.606 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:20:34.684: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-rf8tp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-qxc2
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 21:20:34.967: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qxc2" in namespace "e2e-tests-subpath-rf8tp" to be "success or failure"
Feb 20 21:20:34.971: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.869534ms
Feb 20 21:20:36.977: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010060935s
Feb 20 21:20:38.983: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Running", Reason="", readiness=false. Elapsed: 4.016264814s
Feb 20 21:20:40.988: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Running", Reason="", readiness=false. Elapsed: 6.020983103s
Feb 20 21:20:42.993: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Running", Reason="", readiness=false. Elapsed: 8.026538384s
Feb 20 21:20:45.000: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Running", Reason="", readiness=false. Elapsed: 10.033265827s
Feb 20 21:20:47.005: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Running", Reason="", readiness=false. Elapsed: 12.038383226s
Feb 20 21:20:49.010: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Running", Reason="", readiness=false. Elapsed: 14.043213546s
Feb 20 21:20:51.018: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Running", Reason="", readiness=false. Elapsed: 16.051600861s
Feb 20 21:20:53.023: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Running", Reason="", readiness=false. Elapsed: 18.056726583s
Feb 20 21:20:55.030: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Running", Reason="", readiness=false. Elapsed: 20.06309524s
Feb 20 21:20:57.035: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Running", Reason="", readiness=false. Elapsed: 22.06870168s
Feb 20 21:20:59.043: INFO: Pod "pod-subpath-test-configmap-qxc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.075872775s
STEP: Saw pod success
Feb 20 21:20:59.043: INFO: Pod "pod-subpath-test-configmap-qxc2" satisfied condition "success or failure"
Feb 20 21:20:59.047: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-subpath-test-configmap-qxc2 container test-container-subpath-configmap-qxc2: <nil>
STEP: delete the pod
Feb 20 21:20:59.078: INFO: Waiting for pod pod-subpath-test-configmap-qxc2 to disappear
Feb 20 21:20:59.084: INFO: Pod pod-subpath-test-configmap-qxc2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qxc2
Feb 20 21:20:59.084: INFO: Deleting pod "pod-subpath-test-configmap-qxc2" in namespace "e2e-tests-subpath-rf8tp"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:20:59.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-rf8tp" for this suite.
Feb 20 21:21:05.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:21:05.172: INFO: namespace: e2e-tests-subpath-rf8tp, resource: bindings, ignored listing per whitelist
Feb 20 21:21:05.292: INFO: namespace e2e-tests-subpath-rf8tp deletion completed in 6.197778831s

â€¢ [SLOW TEST:30.608 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:21:05.292: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mk2b8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-6e99734f-3555-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 21:21:05.565: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6e9a139d-3555-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-mk2b8" to be "success or failure"
Feb 20 21:21:05.569: INFO: Pod "pod-projected-secrets-6e9a139d-3555-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281882ms
Feb 20 21:21:07.573: INFO: Pod "pod-projected-secrets-6e9a139d-3555-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007825618s
STEP: Saw pod success
Feb 20 21:21:07.573: INFO: Pod "pod-projected-secrets-6e9a139d-3555-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:21:07.577: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-secrets-6e9a139d-3555-11e9-8ceb-aaeae242ec87 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 21:21:07.601: INFO: Waiting for pod pod-projected-secrets-6e9a139d-3555-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:21:07.605: INFO: Pod pod-projected-secrets-6e9a139d-3555-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:21:07.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mk2b8" for this suite.
Feb 20 21:21:13.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:21:13.811: INFO: namespace: e2e-tests-projected-mk2b8, resource: bindings, ignored listing per whitelist
Feb 20 21:21:13.890: INFO: namespace e2e-tests-projected-mk2b8 deletion completed in 6.280491564s

â€¢ [SLOW TEST:8.598 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:21:13.890: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-v8bbf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 21:21:14.191: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73be0d39-3555-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-v8bbf" to be "success or failure"
Feb 20 21:21:14.194: INFO: Pod "downwardapi-volume-73be0d39-3555-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.948339ms
Feb 20 21:21:16.203: INFO: Pod "downwardapi-volume-73be0d39-3555-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011805275s
STEP: Saw pod success
Feb 20 21:21:16.203: INFO: Pod "downwardapi-volume-73be0d39-3555-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:21:16.208: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-73be0d39-3555-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 21:21:16.231: INFO: Waiting for pod downwardapi-volume-73be0d39-3555-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:21:16.235: INFO: Pod downwardapi-volume-73be0d39-3555-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:21:16.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-v8bbf" for this suite.
Feb 20 21:21:22.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:21:22.520: INFO: namespace: e2e-tests-downward-api-v8bbf, resource: bindings, ignored listing per whitelist
Feb 20 21:21:22.520: INFO: namespace e2e-tests-downward-api-v8bbf deletion completed in 6.280805313s

â€¢ [SLOW TEST:8.630 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:21:22.520: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-vtb2k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-vtb2k
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 21:21:22.759: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 21:21:40.860: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.96.0.36 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vtb2k PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 21:21:40.860: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 21:21:42.333: INFO: Found all expected endpoints: [netserver-0]
Feb 20 21:21:42.338: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.96.1.85 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-vtb2k PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 21:21:42.338: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 21:21:43.699: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:21:43.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-vtb2k" for this suite.
Feb 20 21:22:05.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:22:05.814: INFO: namespace: e2e-tests-pod-network-test-vtb2k, resource: bindings, ignored listing per whitelist
Feb 20 21:22:05.933: INFO: namespace e2e-tests-pod-network-test-vtb2k deletion completed in 22.229322377s

â€¢ [SLOW TEST:43.413 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:22:05.933: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-cdvwb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-cdvwb
Feb 20 21:22:08.231: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-cdvwb
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 21:22:08.235: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:26:08.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cdvwb" for this suite.
Feb 20 21:26:15.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:26:15.059: INFO: namespace: e2e-tests-container-probe-cdvwb, resource: bindings, ignored listing per whitelist
Feb 20 21:26:15.209: INFO: namespace e2e-tests-container-probe-cdvwb deletion completed in 6.218536939s

â€¢ [SLOW TEST:249.275 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:26:15.209: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rbvx5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 20 21:26:15.471: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml --namespace=e2e-tests-kubectl-rbvx5 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 20 21:26:18.445: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 20 21:26:18.445: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:26:20.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rbvx5" for this suite.
Feb 20 21:26:34.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:26:34.549: INFO: namespace: e2e-tests-kubectl-rbvx5, resource: bindings, ignored listing per whitelist
Feb 20 21:26:34.649: INFO: namespace e2e-tests-kubectl-rbvx5 deletion completed in 14.189656934s

â€¢ [SLOW TEST:19.440 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:26:34.649: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4vcfn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 21:26:34.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32eeeb70-3556-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-4vcfn" to be "success or failure"
Feb 20 21:26:34.959: INFO: Pod "downwardapi-volume-32eeeb70-3556-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.503769ms
Feb 20 21:26:36.966: INFO: Pod "downwardapi-volume-32eeeb70-3556-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011440192s
STEP: Saw pod success
Feb 20 21:26:36.966: INFO: Pod "downwardapi-volume-32eeeb70-3556-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:26:36.970: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-32eeeb70-3556-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 21:26:36.995: INFO: Waiting for pod downwardapi-volume-32eeeb70-3556-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:26:36.998: INFO: Pod downwardapi-volume-32eeeb70-3556-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:26:36.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4vcfn" for this suite.
Feb 20 21:26:43.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:26:43.140: INFO: namespace: e2e-tests-downward-api-4vcfn, resource: bindings, ignored listing per whitelist
Feb 20 21:26:43.247: INFO: namespace e2e-tests-downward-api-4vcfn deletion completed in 6.244881689s

â€¢ [SLOW TEST:8.598 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:26:43.247: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-nmqwx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-nmqwx
Feb 20 21:27:31.521: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-nmqwx
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 21:27:31.524: INFO: Initial restart count of pod liveness-http is 0
Feb 20 21:27:45.567: INFO: Restart count of pod e2e-tests-container-probe-nmqwx/liveness-http is now 1 (14.042096639s elapsed)
Feb 20 21:28:05.625: INFO: Restart count of pod e2e-tests-container-probe-nmqwx/liveness-http is now 2 (34.100247565s elapsed)
Feb 20 21:28:27.682: INFO: Restart count of pod e2e-tests-container-probe-nmqwx/liveness-http is now 3 (56.157483629s elapsed)
Feb 20 21:28:47.736: INFO: Restart count of pod e2e-tests-container-probe-nmqwx/liveness-http is now 4 (1m16.21108986s elapsed)
Feb 20 21:29:49.915: INFO: Restart count of pod e2e-tests-container-probe-nmqwx/liveness-http is now 5 (2m18.390988008s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:29:49.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-nmqwx" for this suite.
Feb 20 21:29:55.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:29:56.082: INFO: namespace: e2e-tests-container-probe-nmqwx, resource: bindings, ignored listing per whitelist
Feb 20 21:29:56.196: INFO: namespace e2e-tests-container-probe-nmqwx deletion completed in 6.267563605s

â€¢ [SLOW TEST:192.949 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:29:56.197: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-gsmdf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-ab074af0-3556-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 21:29:56.447: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab07eedf-3556-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-configmap-gsmdf" to be "success or failure"
Feb 20 21:29:56.451: INFO: Pod "pod-configmaps-ab07eedf-3556-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.903516ms
Feb 20 21:29:58.458: INFO: Pod "pod-configmaps-ab07eedf-3556-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0100972s
STEP: Saw pod success
Feb 20 21:29:58.458: INFO: Pod "pod-configmaps-ab07eedf-3556-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:29:58.463: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-configmaps-ab07eedf-3556-11e9-8ceb-aaeae242ec87 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 21:29:58.490: INFO: Waiting for pod pod-configmaps-ab07eedf-3556-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:29:58.493: INFO: Pod pod-configmaps-ab07eedf-3556-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:29:58.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gsmdf" for this suite.
Feb 20 21:30:04.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:30:04.682: INFO: namespace: e2e-tests-configmap-gsmdf, resource: bindings, ignored listing per whitelist
Feb 20 21:30:04.699: INFO: namespace e2e-tests-configmap-gsmdf deletion completed in 6.2009851s

â€¢ [SLOW TEST:8.503 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:30:04.700: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-qr779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 21:30:04.950: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 20 21:30:04.962: INFO: Number of nodes with available pods: 0
Feb 20 21:30:04.962: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:30:05.972: INFO: Number of nodes with available pods: 0
Feb 20 21:30:05.972: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:30:06.971: INFO: Number of nodes with available pods: 2
Feb 20 21:30:06.971: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 20 21:30:07.004: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:07.004: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:08.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:08.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:09.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:09.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:10.064: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:10.064: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:11.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:11.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:12.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:12.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:13.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:13.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:14.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:14.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:15.064: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:15.064: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:16.064: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:16.064: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:17.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:17.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:18.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:18.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:19.066: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:19.066: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:20.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:20.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:21.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:21.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:22.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:22.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:23.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:23.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:24.065: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:24.065: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:25.064: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:25.064: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:26.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:26.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:27.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:27.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:28.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:28.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:29.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:29.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:30.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:30.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:31.066: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:31.066: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:32.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:32.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:33.064: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:33.064: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:34.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:34.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:35.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:35.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:36.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:36.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:37.065: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:37.065: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:38.065: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:38.065: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:39.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:39.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:40.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:40.063: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:40.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:41.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:41.062: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:41.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:42.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:42.062: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:42.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:43.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:43.063: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:43.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:44.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:44.062: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:44.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:45.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:45.063: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:45.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:46.064: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:46.064: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:46.064: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:47.063: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:47.063: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:47.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:48.065: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:48.065: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:48.065: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:49.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:49.062: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:49.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:50.062: INFO: Wrong image for pod: daemon-set-hdj44. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:50.063: INFO: Pod daemon-set-hdj44 is not available
Feb 20 21:30:50.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:51.062: INFO: Pod daemon-set-2jw4s is not available
Feb 20 21:30:51.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:52.067: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:53.061: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:54.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:55.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:56.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:57.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:58.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:30:59.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:00.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:01.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:02.070: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:03.064: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:04.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:05.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:06.066: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:07.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:08.067: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:09.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:10.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:11.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:12.066: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:13.064: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:14.065: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:15.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:16.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:17.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:18.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:19.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:20.065: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:21.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:22.066: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:23.068: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:23.068: INFO: Pod daemon-set-rq4mv is not available
Feb 20 21:31:24.066: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:24.066: INFO: Pod daemon-set-rq4mv is not available
Feb 20 21:31:25.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:25.062: INFO: Pod daemon-set-rq4mv is not available
Feb 20 21:31:26.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:26.063: INFO: Pod daemon-set-rq4mv is not available
Feb 20 21:31:27.063: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:27.063: INFO: Pod daemon-set-rq4mv is not available
Feb 20 21:31:28.062: INFO: Wrong image for pod: daemon-set-rq4mv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 21:31:28.062: INFO: Pod daemon-set-rq4mv is not available
Feb 20 21:31:29.061: INFO: Pod daemon-set-twc86 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 20 21:31:29.075: INFO: Number of nodes with available pods: 1
Feb 20 21:31:29.075: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:30.085: INFO: Number of nodes with available pods: 1
Feb 20 21:31:30.085: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:31.084: INFO: Number of nodes with available pods: 1
Feb 20 21:31:31.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:32.085: INFO: Number of nodes with available pods: 1
Feb 20 21:31:32.085: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:33.086: INFO: Number of nodes with available pods: 1
Feb 20 21:31:33.086: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:34.085: INFO: Number of nodes with available pods: 1
Feb 20 21:31:34.085: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:35.084: INFO: Number of nodes with available pods: 1
Feb 20 21:31:35.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:36.083: INFO: Number of nodes with available pods: 1
Feb 20 21:31:36.083: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:37.084: INFO: Number of nodes with available pods: 1
Feb 20 21:31:37.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:38.088: INFO: Number of nodes with available pods: 1
Feb 20 21:31:38.088: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:39.084: INFO: Number of nodes with available pods: 1
Feb 20 21:31:39.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:40.084: INFO: Number of nodes with available pods: 1
Feb 20 21:31:40.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:41.084: INFO: Number of nodes with available pods: 1
Feb 20 21:31:41.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:42.086: INFO: Number of nodes with available pods: 1
Feb 20 21:31:42.086: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:43.084: INFO: Number of nodes with available pods: 1
Feb 20 21:31:43.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:44.083: INFO: Number of nodes with available pods: 1
Feb 20 21:31:44.083: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:45.087: INFO: Number of nodes with available pods: 1
Feb 20 21:31:45.087: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:46.086: INFO: Number of nodes with available pods: 1
Feb 20 21:31:46.086: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:47.084: INFO: Number of nodes with available pods: 1
Feb 20 21:31:47.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:48.085: INFO: Number of nodes with available pods: 1
Feb 20 21:31:48.085: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:49.090: INFO: Number of nodes with available pods: 1
Feb 20 21:31:49.090: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:50.083: INFO: Number of nodes with available pods: 1
Feb 20 21:31:50.083: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:51.085: INFO: Number of nodes with available pods: 1
Feb 20 21:31:51.085: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:52.085: INFO: Number of nodes with available pods: 1
Feb 20 21:31:52.085: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:53.088: INFO: Number of nodes with available pods: 1
Feb 20 21:31:53.088: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:54.085: INFO: Number of nodes with available pods: 1
Feb 20 21:31:54.085: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:55.084: INFO: Number of nodes with available pods: 1
Feb 20 21:31:55.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:56.085: INFO: Number of nodes with available pods: 1
Feb 20 21:31:56.085: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:57.085: INFO: Number of nodes with available pods: 1
Feb 20 21:31:57.086: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:58.087: INFO: Number of nodes with available pods: 1
Feb 20 21:31:58.087: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:31:59.083: INFO: Number of nodes with available pods: 1
Feb 20 21:31:59.083: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:00.090: INFO: Number of nodes with available pods: 1
Feb 20 21:32:00.090: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:01.086: INFO: Number of nodes with available pods: 1
Feb 20 21:32:01.086: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:02.083: INFO: Number of nodes with available pods: 1
Feb 20 21:32:02.083: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:03.084: INFO: Number of nodes with available pods: 1
Feb 20 21:32:03.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:04.084: INFO: Number of nodes with available pods: 1
Feb 20 21:32:04.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:05.085: INFO: Number of nodes with available pods: 1
Feb 20 21:32:05.085: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:06.086: INFO: Number of nodes with available pods: 1
Feb 20 21:32:06.086: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:07.085: INFO: Number of nodes with available pods: 1
Feb 20 21:32:07.085: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:08.087: INFO: Number of nodes with available pods: 1
Feb 20 21:32:08.087: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:09.086: INFO: Number of nodes with available pods: 1
Feb 20 21:32:09.086: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:10.084: INFO: Number of nodes with available pods: 1
Feb 20 21:32:10.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:11.086: INFO: Number of nodes with available pods: 1
Feb 20 21:32:11.086: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:12.087: INFO: Number of nodes with available pods: 1
Feb 20 21:32:12.087: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:13.084: INFO: Number of nodes with available pods: 1
Feb 20 21:32:13.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:14.088: INFO: Number of nodes with available pods: 1
Feb 20 21:32:14.088: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:15.084: INFO: Number of nodes with available pods: 1
Feb 20 21:32:15.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:16.087: INFO: Number of nodes with available pods: 1
Feb 20 21:32:16.087: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:17.084: INFO: Number of nodes with available pods: 1
Feb 20 21:32:17.084: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:32:18.085: INFO: Number of nodes with available pods: 2
Feb 20 21:32:18.085: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-qr779, will wait for the garbage collector to delete the pods
Feb 20 21:32:18.164: INFO: Deleting DaemonSet.extensions daemon-set took: 7.157984ms
Feb 20 21:32:18.265: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.807934ms
Feb 20 21:32:21.169: INFO: Number of nodes with available pods: 0
Feb 20 21:32:21.169: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 21:32:21.172: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-qr779/daemonsets","resourceVersion":"9696"},"items":null}

Feb 20 21:32:21.175: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-qr779/pods","resourceVersion":"9696"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:32:21.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-qr779" for this suite.
Feb 20 21:32:27.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:32:27.247: INFO: namespace: e2e-tests-daemonsets-qr779, resource: bindings, ignored listing per whitelist
Feb 20 21:32:27.393: INFO: namespace e2e-tests-daemonsets-qr779 deletion completed in 6.19905939s

â€¢ [SLOW TEST:142.693 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:32:27.393: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-plrvq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-plrvq/secret-test-052497fa-3557-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 21:32:27.635: INFO: Waiting up to 5m0s for pod "pod-configmaps-052563a4-3557-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-secrets-plrvq" to be "success or failure"
Feb 20 21:32:27.641: INFO: Pod "pod-configmaps-052563a4-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.338644ms
Feb 20 21:32:29.647: INFO: Pod "pod-configmaps-052563a4-3557-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011065113s
STEP: Saw pod success
Feb 20 21:32:29.647: INFO: Pod "pod-configmaps-052563a4-3557-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:32:29.650: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-configmaps-052563a4-3557-11e9-8ceb-aaeae242ec87 container env-test: <nil>
STEP: delete the pod
Feb 20 21:32:29.671: INFO: Waiting for pod pod-configmaps-052563a4-3557-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:32:29.674: INFO: Pod pod-configmaps-052563a4-3557-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:32:29.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-plrvq" for this suite.
Feb 20 21:32:35.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:32:35.950: INFO: namespace: e2e-tests-secrets-plrvq, resource: bindings, ignored listing per whitelist
Feb 20 21:32:35.991: INFO: namespace e2e-tests-secrets-plrvq deletion completed in 6.312843367s

â€¢ [SLOW TEST:8.598 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:32:35.991: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lblw7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 20 21:32:36.310: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:36.750: INFO: stderr: ""
Feb 20 21:32:36.751: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 21:32:36.751: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:36.850: INFO: stderr: ""
Feb 20 21:32:36.850: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:32:36.850: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:36.965: INFO: stderr: ""
Feb 20 21:32:36.965: INFO: stdout: ""
Feb 20 21:32:36.965: INFO: update-demo-nautilus-5sjlt is created but not running
Feb 20 21:32:41.965: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:42.064: INFO: stderr: ""
Feb 20 21:32:42.064: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:32:42.064: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:42.170: INFO: stderr: ""
Feb 20 21:32:42.170: INFO: stdout: ""
Feb 20 21:32:42.170: INFO: update-demo-nautilus-5sjlt is created but not running
Feb 20 21:32:47.170: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:47.509: INFO: stderr: ""
Feb 20 21:32:47.509: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:32:47.510: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:47.602: INFO: stderr: ""
Feb 20 21:32:47.602: INFO: stdout: ""
Feb 20 21:32:47.602: INFO: update-demo-nautilus-5sjlt is created but not running
Feb 20 21:32:52.602: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:52.702: INFO: stderr: ""
Feb 20 21:32:52.702: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:32:52.702: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:52.802: INFO: stderr: ""
Feb 20 21:32:52.802: INFO: stdout: ""
Feb 20 21:32:52.802: INFO: update-demo-nautilus-5sjlt is created but not running
Feb 20 21:32:57.802: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:57.978: INFO: stderr: ""
Feb 20 21:32:57.978: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:32:57.978: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:32:58.124: INFO: stderr: ""
Feb 20 21:32:58.125: INFO: stdout: ""
Feb 20 21:32:58.125: INFO: update-demo-nautilus-5sjlt is created but not running
Feb 20 21:33:03.125: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:03.261: INFO: stderr: ""
Feb 20 21:33:03.261: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:33:03.261: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:03.360: INFO: stderr: ""
Feb 20 21:33:03.360: INFO: stdout: ""
Feb 20 21:33:03.360: INFO: update-demo-nautilus-5sjlt is created but not running
Feb 20 21:33:08.361: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:08.477: INFO: stderr: ""
Feb 20 21:33:08.477: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:33:08.477: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:08.566: INFO: stderr: ""
Feb 20 21:33:08.566: INFO: stdout: ""
Feb 20 21:33:08.566: INFO: update-demo-nautilus-5sjlt is created but not running
Feb 20 21:33:13.567: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:13.675: INFO: stderr: ""
Feb 20 21:33:13.675: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:33:13.675: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:13.782: INFO: stderr: ""
Feb 20 21:33:13.782: INFO: stdout: ""
Feb 20 21:33:13.782: INFO: update-demo-nautilus-5sjlt is created but not running
Feb 20 21:33:18.783: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:18.937: INFO: stderr: ""
Feb 20 21:33:18.937: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:33:18.937: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:19.057: INFO: stderr: ""
Feb 20 21:33:19.057: INFO: stdout: ""
Feb 20 21:33:19.057: INFO: update-demo-nautilus-5sjlt is created but not running
Feb 20 21:33:24.057: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:24.169: INFO: stderr: ""
Feb 20 21:33:24.169: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:33:24.169: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:24.265: INFO: stderr: ""
Feb 20 21:33:24.265: INFO: stdout: ""
Feb 20 21:33:24.265: INFO: update-demo-nautilus-5sjlt is created but not running
Feb 20 21:33:29.265: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:29.386: INFO: stderr: ""
Feb 20 21:33:29.386: INFO: stdout: "update-demo-nautilus-5sjlt update-demo-nautilus-hp49j "
Feb 20 21:33:29.386: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:29.504: INFO: stderr: ""
Feb 20 21:33:29.505: INFO: stdout: "true"
Feb 20 21:33:29.505: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5sjlt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:29.629: INFO: stderr: ""
Feb 20 21:33:29.629: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 21:33:29.629: INFO: validating pod update-demo-nautilus-5sjlt
Feb 20 21:33:29.733: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 21:33:29.733: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 21:33:29.733: INFO: update-demo-nautilus-5sjlt is verified up and running
Feb 20 21:33:29.733: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-hp49j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:29.834: INFO: stderr: ""
Feb 20 21:33:29.834: INFO: stdout: "true"
Feb 20 21:33:29.834: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-hp49j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:30.007: INFO: stderr: ""
Feb 20 21:33:30.007: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 21:33:30.007: INFO: validating pod update-demo-nautilus-hp49j
Feb 20 21:33:30.098: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 21:33:30.099: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 21:33:30.099: INFO: update-demo-nautilus-hp49j is verified up and running
STEP: using delete to clean up resources
Feb 20 21:33:30.099: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:30.314: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 21:33:30.314: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 20 21:33:30.314: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-lblw7'
Feb 20 21:33:30.509: INFO: stderr: "No resources found.\n"
Feb 20 21:33:30.509: INFO: stdout: ""
Feb 20 21:33:30.509: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-lblw7 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 21:33:30.702: INFO: stderr: ""
Feb 20 21:33:30.702: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:33:30.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lblw7" for this suite.
Feb 20 21:33:52.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:33:53.002: INFO: namespace: e2e-tests-kubectl-lblw7, resource: bindings, ignored listing per whitelist
Feb 20 21:33:53.129: INFO: namespace e2e-tests-kubectl-lblw7 deletion completed in 22.415682224s

â€¢ [SLOW TEST:77.137 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:33:53.129: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-r8kfk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 21:33:53.519: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 20 21:33:53.533: INFO: Number of nodes with available pods: 0
Feb 20 21:33:53.533: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 20 21:33:53.555: INFO: Number of nodes with available pods: 0
Feb 20 21:33:53.555: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:33:54.561: INFO: Number of nodes with available pods: 0
Feb 20 21:33:54.561: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:33:55.561: INFO: Number of nodes with available pods: 1
Feb 20 21:33:55.561: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 20 21:33:55.628: INFO: Number of nodes with available pods: 0
Feb 20 21:33:55.628: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 20 21:33:55.643: INFO: Number of nodes with available pods: 0
Feb 20 21:33:55.643: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:33:56.650: INFO: Number of nodes with available pods: 0
Feb 20 21:33:56.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:33:57.652: INFO: Number of nodes with available pods: 0
Feb 20 21:33:57.652: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:33:58.650: INFO: Number of nodes with available pods: 0
Feb 20 21:33:58.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:33:59.648: INFO: Number of nodes with available pods: 0
Feb 20 21:33:59.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:00.651: INFO: Number of nodes with available pods: 0
Feb 20 21:34:00.651: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:01.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:01.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:02.649: INFO: Number of nodes with available pods: 0
Feb 20 21:34:02.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:03.648: INFO: Number of nodes with available pods: 0
Feb 20 21:34:03.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:04.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:04.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:05.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:05.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:06.653: INFO: Number of nodes with available pods: 0
Feb 20 21:34:06.653: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:07.652: INFO: Number of nodes with available pods: 0
Feb 20 21:34:07.652: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:08.655: INFO: Number of nodes with available pods: 0
Feb 20 21:34:08.655: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:09.653: INFO: Number of nodes with available pods: 0
Feb 20 21:34:09.653: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:10.653: INFO: Number of nodes with available pods: 0
Feb 20 21:34:10.653: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:11.651: INFO: Number of nodes with available pods: 0
Feb 20 21:34:11.651: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:12.652: INFO: Number of nodes with available pods: 0
Feb 20 21:34:12.652: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:13.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:13.651: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:14.649: INFO: Number of nodes with available pods: 0
Feb 20 21:34:14.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:15.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:15.651: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:16.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:16.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:17.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:17.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:18.649: INFO: Number of nodes with available pods: 0
Feb 20 21:34:18.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:19.648: INFO: Number of nodes with available pods: 0
Feb 20 21:34:19.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:20.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:20.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:21.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:21.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:22.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:22.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:23.648: INFO: Number of nodes with available pods: 0
Feb 20 21:34:23.648: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:24.649: INFO: Number of nodes with available pods: 0
Feb 20 21:34:24.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:25.649: INFO: Number of nodes with available pods: 0
Feb 20 21:34:25.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:26.648: INFO: Number of nodes with available pods: 0
Feb 20 21:34:26.648: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:27.648: INFO: Number of nodes with available pods: 0
Feb 20 21:34:27.648: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:28.648: INFO: Number of nodes with available pods: 0
Feb 20 21:34:28.648: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:29.648: INFO: Number of nodes with available pods: 0
Feb 20 21:34:29.648: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:30.648: INFO: Number of nodes with available pods: 0
Feb 20 21:34:30.648: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:31.648: INFO: Number of nodes with available pods: 0
Feb 20 21:34:31.648: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:32.651: INFO: Number of nodes with available pods: 0
Feb 20 21:34:32.651: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:33.649: INFO: Number of nodes with available pods: 0
Feb 20 21:34:33.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:34.653: INFO: Number of nodes with available pods: 0
Feb 20 21:34:34.653: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:35.648: INFO: Number of nodes with available pods: 0
Feb 20 21:34:35.648: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:36.650: INFO: Number of nodes with available pods: 0
Feb 20 21:34:36.650: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:37.649: INFO: Number of nodes with available pods: 0
Feb 20 21:34:37.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:38.649: INFO: Number of nodes with available pods: 0
Feb 20 21:34:38.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:39.649: INFO: Number of nodes with available pods: 0
Feb 20 21:34:39.649: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 21:34:40.650: INFO: Number of nodes with available pods: 1
Feb 20 21:34:40.650: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-r8kfk, will wait for the garbage collector to delete the pods
Feb 20 21:34:40.720: INFO: Deleting DaemonSet.extensions daemon-set took: 6.862367ms
Feb 20 21:34:40.820: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.456548ms
Feb 20 21:35:18.424: INFO: Number of nodes with available pods: 0
Feb 20 21:35:18.424: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 21:35:18.428: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-r8kfk/daemonsets","resourceVersion":"10147"},"items":null}

Feb 20 21:35:18.431: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-r8kfk/pods","resourceVersion":"10147"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:35:18.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-r8kfk" for this suite.
Feb 20 21:35:24.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:35:24.488: INFO: namespace: e2e-tests-daemonsets-r8kfk, resource: bindings, ignored listing per whitelist
Feb 20 21:35:24.710: INFO: namespace e2e-tests-daemonsets-r8kfk deletion completed in 6.258352784s

â€¢ [SLOW TEST:91.581 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:35:24.710: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-hdzvk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-6ee95211-3557-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 21:35:25.084: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ee9fb9e-3557-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-configmap-hdzvk" to be "success or failure"
Feb 20 21:35:25.088: INFO: Pod "pod-configmaps-6ee9fb9e-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.675317ms
Feb 20 21:35:27.092: INFO: Pod "pod-configmaps-6ee9fb9e-3557-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007901033s
STEP: Saw pod success
Feb 20 21:35:27.092: INFO: Pod "pod-configmaps-6ee9fb9e-3557-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:35:27.095: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-configmaps-6ee9fb9e-3557-11e9-8ceb-aaeae242ec87 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 21:35:27.114: INFO: Waiting for pod pod-configmaps-6ee9fb9e-3557-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:35:27.119: INFO: Pod pod-configmaps-6ee9fb9e-3557-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:35:27.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hdzvk" for this suite.
Feb 20 21:35:33.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:35:33.325: INFO: namespace: e2e-tests-configmap-hdzvk, resource: bindings, ignored listing per whitelist
Feb 20 21:35:33.336: INFO: namespace e2e-tests-configmap-hdzvk deletion completed in 6.212320257s

â€¢ [SLOW TEST:8.626 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:35:33.336: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-rkkgt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 20 21:36:12.622: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:36:12.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-rkkgt" for this suite.
Feb 20 21:36:34.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:36:34.699: INFO: namespace: e2e-tests-replicaset-rkkgt, resource: bindings, ignored listing per whitelist
Feb 20 21:36:34.800: INFO: namespace e2e-tests-replicaset-rkkgt deletion completed in 22.156252146s

â€¢ [SLOW TEST:61.464 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:36:34.800: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-vj7jb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 20 21:36:35.162: INFO: Waiting up to 5m0s for pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-containers-vj7jb" to be "success or failure"
Feb 20 21:36:35.167: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.116373ms
Feb 20 21:36:37.172: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010138175s
Feb 20 21:36:39.178: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015794969s
Feb 20 21:36:41.184: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022151503s
Feb 20 21:36:43.189: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026967889s
Feb 20 21:36:45.195: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033195102s
Feb 20 21:36:47.201: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038611737s
Feb 20 21:36:49.205: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 14.043348097s
Feb 20 21:36:51.211: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 16.048998825s
Feb 20 21:36:53.217: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 18.054858305s
Feb 20 21:36:55.225: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 20.063092905s
Feb 20 21:36:57.233: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 22.070785345s
Feb 20 21:36:59.239: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 24.076703615s
Feb 20 21:37:01.244: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 26.082039388s
Feb 20 21:37:03.250: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 28.087884465s
Feb 20 21:37:05.255: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 30.092814378s
Feb 20 21:37:07.259: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 32.097411387s
Feb 20 21:37:09.264: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 34.101999183s
Feb 20 21:37:11.268: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 36.106279788s
Feb 20 21:37:13.273: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 38.111563729s
Feb 20 21:37:15.278: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 40.116197758s
Feb 20 21:37:17.283: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 42.121341307s
Feb 20 21:37:19.291: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 44.129142093s
Feb 20 21:37:21.303: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 46.1409876s
Feb 20 21:37:23.308: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 48.145839255s
STEP: Saw pod success
Feb 20 21:37:23.308: INFO: Pod "client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:37:23.311: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:37:23.337: INFO: Waiting for pod client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:37:23.341: INFO: Pod client-containers-98af2aa8-3557-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:37:23.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-vj7jb" for this suite.
Feb 20 21:37:29.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:37:29.480: INFO: namespace: e2e-tests-containers-vj7jb, resource: bindings, ignored listing per whitelist
Feb 20 21:37:29.538: INFO: namespace e2e-tests-containers-vj7jb deletion completed in 6.192876281s

â€¢ [SLOW TEST:54.738 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:37:29.538: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-5fts8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:37:29.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5fts8" for this suite.
Feb 20 21:37:51.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:37:51.819: INFO: namespace: e2e-tests-pods-5fts8, resource: bindings, ignored listing per whitelist
Feb 20 21:37:51.958: INFO: namespace e2e-tests-pods-5fts8 deletion completed in 22.183735206s

â€¢ [SLOW TEST:22.420 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:37:51.958: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-m7hjk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 20 21:37:52.188: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-m7hjk'
Feb 20 21:37:53.020: INFO: stderr: ""
Feb 20 21:37:53.020: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 20 21:37:54.026: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:37:54.026: INFO: Found 0 / 1
Feb 20 21:37:55.027: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:37:55.027: INFO: Found 1 / 1
Feb 20 21:37:55.027: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 20 21:37:55.031: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:37:55.031: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 21:37:55.031: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml patch pod redis-master-lhlnc --namespace=e2e-tests-kubectl-m7hjk -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 20 21:37:55.125: INFO: stderr: ""
Feb 20 21:37:55.125: INFO: stdout: "pod/redis-master-lhlnc patched\n"
STEP: checking annotations
Feb 20 21:37:55.134: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 21:37:55.134: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:37:55.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-m7hjk" for this suite.
Feb 20 21:38:17.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:38:17.175: INFO: namespace: e2e-tests-kubectl-m7hjk, resource: bindings, ignored listing per whitelist
Feb 20 21:38:17.329: INFO: namespace e2e-tests-kubectl-m7hjk deletion completed in 22.189314446s

â€¢ [SLOW TEST:25.370 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:38:17.329: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-6zq66
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 20 21:38:17.561: INFO: Waiting up to 5m0s for pod "client-containers-d5b7fdca-3557-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-containers-6zq66" to be "success or failure"
Feb 20 21:38:17.564: INFO: Pod "client-containers-d5b7fdca-3557-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.229ms
Feb 20 21:38:19.569: INFO: Pod "client-containers-d5b7fdca-3557-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008093155s
STEP: Saw pod success
Feb 20 21:38:19.569: INFO: Pod "client-containers-d5b7fdca-3557-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:38:19.573: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod client-containers-d5b7fdca-3557-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:38:19.594: INFO: Waiting for pod client-containers-d5b7fdca-3557-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:38:19.598: INFO: Pod client-containers-d5b7fdca-3557-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:38:19.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-6zq66" for this suite.
Feb 20 21:38:25.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:38:25.807: INFO: namespace: e2e-tests-containers-6zq66, resource: bindings, ignored listing per whitelist
Feb 20 21:38:25.807: INFO: namespace e2e-tests-containers-6zq66 deletion completed in 6.200705645s

â€¢ [SLOW TEST:8.478 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:38:25.808: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-bw6lc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 20 21:38:28.124: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-dacdd50f-3557-11e9-8ceb-aaeae242ec87", GenerateName:"", Namespace:"e2e-tests-pods-bw6lc", SelfLink:"/api/v1/namespaces/e2e-tests-pods-bw6lc/pods/pod-submit-remove-dacdd50f-3557-11e9-8ceb-aaeae242ec87", UID:"dacf74fe-3557-11e9-91b0-3e11857925bf", ResourceVersion:"10666", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686295506, loc:(*time.Location)(0x791ea40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"86024478"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.103/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-76m5x", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002344c80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-76m5x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001843c38), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000a54840), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001843c70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001843c90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001843c98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001843c9c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686295506, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686295507, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686295507, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686295506, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.17", PodIP:"100.96.1.103", StartTime:(*v1.Time)(0xc001ce5420), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001ce5440), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://6d92e714bc8e04ee978b90fda79a8a543af43736f178d90ba833b45048ddb6d2"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:38:40.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bw6lc" for this suite.
Feb 20 21:38:46.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:38:46.304: INFO: namespace: e2e-tests-pods-bw6lc, resource: bindings, ignored listing per whitelist
Feb 20 21:38:46.455: INFO: namespace e2e-tests-pods-bw6lc deletion completed in 6.241433334s

â€¢ [SLOW TEST:20.648 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:38:46.455: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-95q9q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-95q9q
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-95q9q
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-95q9q
Feb 20 21:38:46.740: INFO: Found 0 stateful pods, waiting for 1
Feb 20 21:38:56.746: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 20 21:38:56.751: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-95q9q ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 21:38:57.266: INFO: stderr: ""
Feb 20 21:38:57.266: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 21:38:57.266: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 21:38:57.270: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 20 21:39:07.276: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 21:39:07.276: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 21:39:07.293: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:07.293: INFO: ss-0  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:38:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:38:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:38:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:38:46 +0000 UTC  }]
Feb 20 21:39:07.293: INFO: 
Feb 20 21:39:07.293: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 20 21:39:08.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99549418s
Feb 20 21:39:09.307: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988623845s
Feb 20 21:39:10.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980767782s
Feb 20 21:39:11.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974627874s
Feb 20 21:39:12.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968756406s
Feb 20 21:39:13.340: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960201212s
Feb 20 21:39:14.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.948843688s
Feb 20 21:39:15.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.943427396s
Feb 20 21:39:16.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 935.569506ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-95q9q
Feb 20 21:39:17.364: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-95q9q ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 21:39:17.856: INFO: stderr: ""
Feb 20 21:39:17.857: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 21:39:17.857: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 21:39:17.857: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-95q9q ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 21:39:18.415: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 20 21:39:18.415: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 21:39:18.415: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 21:39:18.415: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-95q9q ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 21:39:19.022: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 20 21:39:19.022: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 21:39:19.022: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 21:39:19.028: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 21:39:19.028: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 21:39:19.028: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 20 21:39:19.032: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-95q9q ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 21:39:19.608: INFO: stderr: ""
Feb 20 21:39:19.608: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 21:39:19.608: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 21:39:19.608: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-95q9q ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 21:39:20.080: INFO: stderr: ""
Feb 20 21:39:20.081: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 21:39:20.081: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 21:39:20.081: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-95q9q ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 21:39:20.652: INFO: stderr: ""
Feb 20 21:39:20.652: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 21:39:20.652: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 21:39:20.652: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 21:39:20.656: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 20 21:39:30.665: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 21:39:30.665: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 21:39:30.665: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 21:39:30.675: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:30.675: INFO: ss-0  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:38:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:38:46 +0000 UTC  }]
Feb 20 21:39:30.675: INFO: ss-1  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:30.675: INFO: ss-2  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:30.675: INFO: 
Feb 20 21:39:30.675: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 21:39:31.681: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:31.681: INFO: ss-0  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:38:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:38:46 +0000 UTC  }]
Feb 20 21:39:31.681: INFO: ss-1  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:31.681: INFO: ss-2  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:31.681: INFO: 
Feb 20 21:39:31.681: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 21:39:32.687: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:32.687: INFO: ss-0  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:38:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:38:46 +0000 UTC  }]
Feb 20 21:39:32.687: INFO: ss-1  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:32.687: INFO: ss-2  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:32.687: INFO: 
Feb 20 21:39:32.687: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 21:39:33.693: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:33.693: INFO: ss-2  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:33.693: INFO: 
Feb 20 21:39:33.693: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 20 21:39:34.698: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:34.698: INFO: ss-2  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:34.698: INFO: 
Feb 20 21:39:34.698: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 20 21:39:35.704: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:35.705: INFO: ss-2  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:35.705: INFO: 
Feb 20 21:39:35.705: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 20 21:39:36.710: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:36.710: INFO: ss-2  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:36.710: INFO: 
Feb 20 21:39:36.710: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 20 21:39:37.716: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:37.716: INFO: ss-2  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:37.716: INFO: 
Feb 20 21:39:37.716: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 20 21:39:38.721: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:38.721: INFO: ss-2  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:38.721: INFO: 
Feb 20 21:39:38.721: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 20 21:39:39.726: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 20 21:39:39.726: INFO: ss-2  shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:39:07 +0000 UTC  }]
Feb 20 21:39:39.727: INFO: 
Feb 20 21:39:39.727: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-95q9q
Feb 20 21:39:40.732: INFO: Scaling statefulset ss to 0
Feb 20 21:39:40.744: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 21:39:40.748: INFO: Deleting all statefulset in ns e2e-tests-statefulset-95q9q
Feb 20 21:39:40.752: INFO: Scaling statefulset ss to 0
Feb 20 21:39:40.771: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 21:39:40.781: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:39:40.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-95q9q" for this suite.
Feb 20 21:39:46.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:39:46.969: INFO: namespace: e2e-tests-statefulset-95q9q, resource: bindings, ignored listing per whitelist
Feb 20 21:39:46.973: INFO: namespace e2e-tests-statefulset-95q9q deletion completed in 6.171680147s

â€¢ [SLOW TEST:60.517 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:39:46.973: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-8mmd9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-8mmd9
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 20 21:39:47.261: INFO: Found 0 stateful pods, waiting for 3
Feb 20 21:39:57.267: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 21:39:57.267: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 21:39:57.267: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 20 21:39:57.297: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 20 21:40:07.335: INFO: Updating stateful set ss2
Feb 20 21:40:07.342: INFO: Waiting for Pod e2e-tests-statefulset-8mmd9/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:40:17.358: INFO: Waiting for Pod e2e-tests-statefulset-8mmd9/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 20 21:40:27.407: INFO: Found 2 stateful pods, waiting for 3
Feb 20 21:40:37.412: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 21:40:37.412: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 21:40:37.412: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 20 21:40:37.438: INFO: Updating stateful set ss2
Feb 20 21:40:37.446: INFO: Waiting for Pod e2e-tests-statefulset-8mmd9/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:40:47.458: INFO: Waiting for Pod e2e-tests-statefulset-8mmd9/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 21:40:57.477: INFO: Updating stateful set ss2
Feb 20 21:40:57.485: INFO: Waiting for StatefulSet e2e-tests-statefulset-8mmd9/ss2 to complete update
Feb 20 21:40:57.485: INFO: Waiting for Pod e2e-tests-statefulset-8mmd9/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 21:41:07.495: INFO: Deleting all statefulset in ns e2e-tests-statefulset-8mmd9
Feb 20 21:41:07.499: INFO: Scaling statefulset ss2 to 0
Feb 20 21:41:37.517: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 21:41:37.521: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:41:37.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-8mmd9" for this suite.
Feb 20 21:41:43.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:41:43.668: INFO: namespace: e2e-tests-statefulset-8mmd9, resource: bindings, ignored listing per whitelist
Feb 20 21:41:43.793: INFO: namespace e2e-tests-statefulset-8mmd9 deletion completed in 6.250248342s

â€¢ [SLOW TEST:116.820 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:41:43.793: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-7jvpq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:41:46.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-7jvpq" for this suite.
Feb 20 21:42:24.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:42:24.138: INFO: namespace: e2e-tests-kubelet-test-7jvpq, resource: bindings, ignored listing per whitelist
Feb 20 21:42:24.306: INFO: namespace e2e-tests-kubelet-test-7jvpq deletion completed in 38.219584886s

â€¢ [SLOW TEST:40.513 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:42:24.306: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-bdkj8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 20 21:42:24.559: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-a,UID:68f1ca45-3558-11e9-91b0-3e11857925bf,ResourceVersion:11409,Generation:0,CreationTimestamp:2019-02-20 21:42:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 21:42:24.559: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-a,UID:68f1ca45-3558-11e9-91b0-3e11857925bf,ResourceVersion:11409,Generation:0,CreationTimestamp:2019-02-20 21:42:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 20 21:42:34.573: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-a,UID:68f1ca45-3558-11e9-91b0-3e11857925bf,ResourceVersion:11431,Generation:0,CreationTimestamp:2019-02-20 21:42:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 20 21:42:34.573: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-a,UID:68f1ca45-3558-11e9-91b0-3e11857925bf,ResourceVersion:11431,Generation:0,CreationTimestamp:2019-02-20 21:42:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 20 21:42:44.583: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-a,UID:68f1ca45-3558-11e9-91b0-3e11857925bf,ResourceVersion:11451,Generation:0,CreationTimestamp:2019-02-20 21:42:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 21:42:44.584: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-a,UID:68f1ca45-3558-11e9-91b0-3e11857925bf,ResourceVersion:11451,Generation:0,CreationTimestamp:2019-02-20 21:42:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 20 21:42:54.591: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-a,UID:68f1ca45-3558-11e9-91b0-3e11857925bf,ResourceVersion:11471,Generation:0,CreationTimestamp:2019-02-20 21:42:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 21:42:54.591: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-a,UID:68f1ca45-3558-11e9-91b0-3e11857925bf,ResourceVersion:11471,Generation:0,CreationTimestamp:2019-02-20 21:42:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 20 21:43:04.607: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-b,UID:80cee893-3558-11e9-91b0-3e11857925bf,ResourceVersion:11492,Generation:0,CreationTimestamp:2019-02-20 21:43:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 21:43:04.607: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-b,UID:80cee893-3558-11e9-91b0-3e11857925bf,ResourceVersion:11492,Generation:0,CreationTimestamp:2019-02-20 21:43:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 20 21:43:14.614: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-b,UID:80cee893-3558-11e9-91b0-3e11857925bf,ResourceVersion:11512,Generation:0,CreationTimestamp:2019-02-20 21:43:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 21:43:14.614: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-bdkj8,SelfLink:/api/v1/namespaces/e2e-tests-watch-bdkj8/configmaps/e2e-watch-test-configmap-b,UID:80cee893-3558-11e9-91b0-3e11857925bf,ResourceVersion:11512,Generation:0,CreationTimestamp:2019-02-20 21:43:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:43:24.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-bdkj8" for this suite.
Feb 20 21:43:30.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:43:30.676: INFO: namespace: e2e-tests-watch-bdkj8, resource: bindings, ignored listing per whitelist
Feb 20 21:43:30.834: INFO: namespace e2e-tests-watch-bdkj8 deletion completed in 6.212930406s

â€¢ [SLOW TEST:66.528 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:43:30.835: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-wbm4m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 20 21:43:31.104: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix760801220/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:43:31.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wbm4m" for this suite.
Feb 20 21:43:37.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:43:37.243: INFO: namespace: e2e-tests-kubectl-wbm4m, resource: bindings, ignored listing per whitelist
Feb 20 21:43:37.399: INFO: namespace e2e-tests-kubectl-wbm4m deletion completed in 6.193555535s

â€¢ [SLOW TEST:6.564 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:43:37.399: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-rccn8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 20 21:43:37.642: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 21:43:37.652: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 21:43:37.656: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs before test
Feb 20 21:43:37.671: INFO: metrics-server-7594c945c8-pndpd from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container metrics-server ready: true, restart count 0
Feb 20 21:43:37.671: INFO: coredns-67df79bbdd-6dv5v from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container coredns ready: true, restart count 0
Feb 20 21:43:37.671: INFO: blackbox-exporter-d6c46f9fc-jt4nc from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 20 21:43:37.671: INFO: addons-nginx-ingress-controller-d74bfff57-wsvnp from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 20 21:43:37.671: INFO: calico-node-9f88m from kube-system started at 2019-02-20 20:36:04 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 21:43:37.671: INFO: kube-proxy-slfj9 from kube-system started at 2019-02-20 20:36:04 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 21:43:37.671: INFO: node-exporter-zfb8s from kube-system started at 2019-02-20 20:36:04 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 21:43:37.671: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-h9ds6 from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 20 21:43:37.671: INFO: vpn-shoot-67f9c4c5ff-z2l8j from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 20 21:43:37.671: INFO: addons-kube-lego-69bbdc96b6-7c626 from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container kube-lego ready: true, restart count 0
Feb 20 21:43:37.671: INFO: addons-kubernetes-dashboard-6579b646c5-ttbst from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.671: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 20 21:43:37.671: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq before test
Feb 20 21:43:37.718: INFO: node-exporter-kqq7b from kube-system started at 2019-02-20 20:36:26 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.718: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 21:43:37.718: INFO: calico-node-wdqxm from kube-system started at 2019-02-20 20:36:26 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.718: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 21:43:37.718: INFO: kube-proxy-pp9s2 from kube-system started at 2019-02-20 20:36:26 +0000 UTC (1 container statuses recorded)
Feb 20 21:43:37.718: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
STEP: verifying the node has the label node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq
Feb 20 21:43:37.756: INFO: Pod addons-kube-lego-69bbdc96b6-7c626 requesting resource cpu=20m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
Feb 20 21:43:37.756: INFO: Pod addons-kubernetes-dashboard-6579b646c5-ttbst requesting resource cpu=50m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
Feb 20 21:43:37.756: INFO: Pod addons-nginx-ingress-controller-d74bfff57-wsvnp requesting resource cpu=100m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
Feb 20 21:43:37.756: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-h9ds6 requesting resource cpu=0m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
Feb 20 21:43:37.756: INFO: Pod blackbox-exporter-d6c46f9fc-jt4nc requesting resource cpu=5m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
Feb 20 21:43:37.756: INFO: Pod calico-node-9f88m requesting resource cpu=100m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
Feb 20 21:43:37.756: INFO: Pod calico-node-wdqxm requesting resource cpu=100m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq
Feb 20 21:43:37.756: INFO: Pod coredns-67df79bbdd-6dv5v requesting resource cpu=50m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
Feb 20 21:43:37.756: INFO: Pod kube-proxy-pp9s2 requesting resource cpu=20m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq
Feb 20 21:43:37.756: INFO: Pod kube-proxy-slfj9 requesting resource cpu=20m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
Feb 20 21:43:37.756: INFO: Pod metrics-server-7594c945c8-pndpd requesting resource cpu=20m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
Feb 20 21:43:37.756: INFO: Pod node-exporter-kqq7b requesting resource cpu=5m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq
Feb 20 21:43:37.756: INFO: Pod node-exporter-zfb8s requesting resource cpu=5m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
Feb 20 21:43:37.756: INFO: Pod vpn-shoot-67f9c4c5ff-z2l8j requesting resource cpu=50m on Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9492f89d-3558-11e9-8ceb-aaeae242ec87.1585308a6b8adf5d], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-rccn8/filler-pod-9492f89d-3558-11e9-8ceb-aaeae242ec87 to shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9492f89d-3558-11e9-8ceb-aaeae242ec87.1585308a97a1bd3b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9492f89d-3558-11e9-8ceb-aaeae242ec87.1585308a9a7ebcf0], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9492f89d-3558-11e9-8ceb-aaeae242ec87.1585308aa2b19e51], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94942523-3558-11e9-8ceb-aaeae242ec87.1585308a6bd009e8], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-rccn8/filler-pod-94942523-3558-11e9-8ceb-aaeae242ec87 to shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94942523-3558-11e9-8ceb-aaeae242ec87.1585308a96837bda], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94942523-3558-11e9-8ceb-aaeae242ec87.1585308a99db99a4], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94942523-3558-11e9-8ceb-aaeae242ec87.1585308aa0689edc], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1585308ae4729db7], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:43:40.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-rccn8" for this suite.
Feb 20 21:43:46.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:43:46.976: INFO: namespace: e2e-tests-sched-pred-rccn8, resource: bindings, ignored listing per whitelist
Feb 20 21:43:47.036: INFO: namespace e2e-tests-sched-pred-rccn8 deletion completed in 6.201747644s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:9.637 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:43:47.036: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rqh8z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-9a3e6e40-3558-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 21:43:47.280: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a3f23b0-3558-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-rqh8z" to be "success or failure"
Feb 20 21:43:47.287: INFO: Pod "pod-projected-configmaps-9a3f23b0-3558-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 7.092994ms
Feb 20 21:43:49.292: INFO: Pod "pod-projected-configmaps-9a3f23b0-3558-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011869646s
STEP: Saw pod success
Feb 20 21:43:49.292: INFO: Pod "pod-projected-configmaps-9a3f23b0-3558-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:43:49.296: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-configmaps-9a3f23b0-3558-11e9-8ceb-aaeae242ec87 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 21:43:49.321: INFO: Waiting for pod pod-projected-configmaps-9a3f23b0-3558-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:43:49.326: INFO: Pod pod-projected-configmaps-9a3f23b0-3558-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:43:49.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rqh8z" for this suite.
Feb 20 21:43:55.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:43:55.422: INFO: namespace: e2e-tests-projected-rqh8z, resource: bindings, ignored listing per whitelist
Feb 20 21:43:55.518: INFO: namespace e2e-tests-projected-rqh8z deletion completed in 6.185154952s

â€¢ [SLOW TEST:8.482 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:43:55.518: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-7wxn7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 20 21:43:55.739: INFO: Waiting up to 5m0s for pod "pod-9f49b37e-3558-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-7wxn7" to be "success or failure"
Feb 20 21:43:55.743: INFO: Pod "pod-9f49b37e-3558-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.721143ms
Feb 20 21:43:57.750: INFO: Pod "pod-9f49b37e-3558-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010969444s
STEP: Saw pod success
Feb 20 21:43:57.750: INFO: Pod "pod-9f49b37e-3558-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:43:57.753: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-9f49b37e-3558-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:43:57.774: INFO: Waiting for pod pod-9f49b37e-3558-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:43:57.778: INFO: Pod pod-9f49b37e-3558-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:43:57.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7wxn7" for this suite.
Feb 20 21:44:03.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:44:03.936: INFO: namespace: e2e-tests-emptydir-7wxn7, resource: bindings, ignored listing per whitelist
Feb 20 21:44:04.008: INFO: namespace e2e-tests-emptydir-7wxn7 deletion completed in 6.224815686s

â€¢ [SLOW TEST:8.489 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:44:04.008: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-2schg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0220 21:44:44.286937   32391 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 21:44:44.286: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:44:44.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2schg" for this suite.
Feb 20 21:44:50.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:44:50.509: INFO: namespace: e2e-tests-gc-2schg, resource: bindings, ignored listing per whitelist
Feb 20 21:44:50.540: INFO: namespace e2e-tests-gc-2schg deletion completed in 6.236731268s

â€¢ [SLOW TEST:46.533 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:44:50.541: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-5n2kp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:44:50.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-5n2kp" for this suite.
Feb 20 21:44:56.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:44:56.930: INFO: namespace: e2e-tests-services-5n2kp, resource: bindings, ignored listing per whitelist
Feb 20 21:44:57.018: INFO: namespace e2e-tests-services-5n2kp deletion completed in 6.198681253s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:6.477 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:44:57.018: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-xz7rp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-pwsb
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 21:44:57.254: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pwsb" in namespace "e2e-tests-subpath-xz7rp" to be "success or failure"
Feb 20 21:44:57.260: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.810248ms
Feb 20 21:44:59.264: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010245691s
Feb 20 21:45:01.269: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Running", Reason="", readiness=false. Elapsed: 4.014951149s
Feb 20 21:45:03.274: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Running", Reason="", readiness=false. Elapsed: 6.020055193s
Feb 20 21:45:05.281: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Running", Reason="", readiness=false. Elapsed: 8.026963978s
Feb 20 21:45:07.287: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Running", Reason="", readiness=false. Elapsed: 10.033191402s
Feb 20 21:45:09.293: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Running", Reason="", readiness=false. Elapsed: 12.039341618s
Feb 20 21:45:11.298: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Running", Reason="", readiness=false. Elapsed: 14.044329794s
Feb 20 21:45:13.303: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Running", Reason="", readiness=false. Elapsed: 16.049311872s
Feb 20 21:45:15.309: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Running", Reason="", readiness=false. Elapsed: 18.055072912s
Feb 20 21:45:17.314: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Running", Reason="", readiness=false. Elapsed: 20.06011433s
Feb 20 21:45:19.320: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Running", Reason="", readiness=false. Elapsed: 22.06573293s
Feb 20 21:45:21.338: INFO: Pod "pod-subpath-test-secret-pwsb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.083819413s
STEP: Saw pod success
Feb 20 21:45:21.338: INFO: Pod "pod-subpath-test-secret-pwsb" satisfied condition "success or failure"
Feb 20 21:45:21.343: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-subpath-test-secret-pwsb container test-container-subpath-secret-pwsb: <nil>
STEP: delete the pod
Feb 20 21:45:21.368: INFO: Waiting for pod pod-subpath-test-secret-pwsb to disappear
Feb 20 21:45:21.372: INFO: Pod pod-subpath-test-secret-pwsb no longer exists
STEP: Deleting pod pod-subpath-test-secret-pwsb
Feb 20 21:45:21.372: INFO: Deleting pod "pod-subpath-test-secret-pwsb" in namespace "e2e-tests-subpath-xz7rp"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:45:21.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-xz7rp" for this suite.
Feb 20 21:45:27.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:45:27.468: INFO: namespace: e2e-tests-subpath-xz7rp, resource: bindings, ignored listing per whitelist
Feb 20 21:45:27.602: INFO: namespace e2e-tests-subpath-xz7rp deletion completed in 6.221793839s

â€¢ [SLOW TEST:30.584 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:45:27.602: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-8scq4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 20 21:45:29.918: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-d637d036-3558-11e9-8ceb-aaeae242ec87,GenerateName:,Namespace:e2e-tests-events-8scq4,SelfLink:/api/v1/namespaces/e2e-tests-events-8scq4/pods/send-events-d637d036-3558-11e9-8ceb-aaeae242ec87,UID:d638e35a-3558-11e9-91b0-3e11857925bf,ResourceVersion:12022,Generation:0,CreationTimestamp:2019-02-20 21:45:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 888701940,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.123/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-k5xzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k5xzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-k5xzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00170eb10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00170eb30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:45:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:45:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:45:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:45:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:100.96.1.123,StartTime:2019-02-20 21:45:27 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-20 21:45:28 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://93d89d0e4e9573a354ef523aa481f7db0944d65ea66ff817549853e3328b5973}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 20 21:45:31.926: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 20 21:45:33.933: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:45:33.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-8scq4" for this suite.
Feb 20 21:46:13.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:46:14.175: INFO: namespace: e2e-tests-events-8scq4, resource: bindings, ignored listing per whitelist
Feb 20 21:46:14.214: INFO: namespace e2e-tests-events-8scq4 deletion completed in 40.273116263s

â€¢ [SLOW TEST:46.613 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:46:14.215: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-vftn8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 21:46:16.509: INFO: Waiting up to 5m0s for pod "client-envvars-f331ef01-3558-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-pods-vftn8" to be "success or failure"
Feb 20 21:46:16.514: INFO: Pod "client-envvars-f331ef01-3558-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.306748ms
Feb 20 21:46:18.518: INFO: Pod "client-envvars-f331ef01-3558-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008793004s
STEP: Saw pod success
Feb 20 21:46:18.518: INFO: Pod "client-envvars-f331ef01-3558-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:46:18.522: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod client-envvars-f331ef01-3558-11e9-8ceb-aaeae242ec87 container env3cont: <nil>
STEP: delete the pod
Feb 20 21:46:18.550: INFO: Waiting for pod client-envvars-f331ef01-3558-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:46:18.554: INFO: Pod client-envvars-f331ef01-3558-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:46:18.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vftn8" for this suite.
Feb 20 21:47:02.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:47:02.775: INFO: namespace: e2e-tests-pods-vftn8, resource: bindings, ignored listing per whitelist
Feb 20 21:47:02.861: INFO: namespace e2e-tests-pods-vftn8 deletion completed in 44.302125821s

â€¢ [SLOW TEST:48.646 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:47:02.861: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-flfbj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:47:32.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-flfbj" for this suite.
Feb 20 21:47:54.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:47:54.368: INFO: namespace: e2e-tests-replication-controller-flfbj, resource: bindings, ignored listing per whitelist
Feb 20 21:47:54.392: INFO: namespace e2e-tests-replication-controller-flfbj deletion completed in 22.219596015s

â€¢ [SLOW TEST:51.531 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:47:54.392: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-w7htv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 21:47:54.635: INFO: (0) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.335721ms)
Feb 20 21:47:54.678: INFO: (1) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 43.198623ms)
Feb 20 21:47:54.685: INFO: (2) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.832886ms)
Feb 20 21:47:54.697: INFO: (3) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 11.46699ms)
Feb 20 21:47:54.705: INFO: (4) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.236873ms)
Feb 20 21:47:54.712: INFO: (5) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.208567ms)
Feb 20 21:47:54.721: INFO: (6) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.196663ms)
Feb 20 21:47:54.729: INFO: (7) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.960108ms)
Feb 20 21:47:54.736: INFO: (8) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.447623ms)
Feb 20 21:47:54.744: INFO: (9) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.190885ms)
Feb 20 21:47:54.753: INFO: (10) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.058488ms)
Feb 20 21:47:54.763: INFO: (11) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.224505ms)
Feb 20 21:47:54.769: INFO: (12) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.524915ms)
Feb 20 21:47:54.777: INFO: (13) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.357082ms)
Feb 20 21:47:54.784: INFO: (14) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.484915ms)
Feb 20 21:47:54.790: INFO: (15) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.962871ms)
Feb 20 21:47:54.795: INFO: (16) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.427977ms)
Feb 20 21:47:54.801: INFO: (17) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.622415ms)
Feb 20 21:47:54.808: INFO: (18) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.895514ms)
Feb 20 21:47:54.815: INFO: (19) /api/v1/nodes/shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.705625ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:47:54.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-w7htv" for this suite.
Feb 20 21:48:00.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:48:00.895: INFO: namespace: e2e-tests-proxy-w7htv, resource: bindings, ignored listing per whitelist
Feb 20 21:48:01.180: INFO: namespace e2e-tests-proxy-w7htv deletion completed in 6.360949942s

â€¢ [SLOW TEST:6.789 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:48:01.181: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qc59q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-31c416f4-3559-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 21:48:01.505: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-31c66c79-3559-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-qc59q" to be "success or failure"
Feb 20 21:48:01.514: INFO: Pod "pod-projected-secrets-31c66c79-3559-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 9.452431ms
Feb 20 21:48:03.522: INFO: Pod "pod-projected-secrets-31c66c79-3559-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016815343s
STEP: Saw pod success
Feb 20 21:48:03.522: INFO: Pod "pod-projected-secrets-31c66c79-3559-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:48:03.526: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-secrets-31c66c79-3559-11e9-8ceb-aaeae242ec87 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 21:48:03.550: INFO: Waiting for pod pod-projected-secrets-31c66c79-3559-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:48:03.554: INFO: Pod pod-projected-secrets-31c66c79-3559-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:48:03.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qc59q" for this suite.
Feb 20 21:48:09.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:48:09.641: INFO: namespace: e2e-tests-projected-qc59q, resource: bindings, ignored listing per whitelist
Feb 20 21:48:09.809: INFO: namespace e2e-tests-projected-qc59q deletion completed in 6.250031231s

â€¢ [SLOW TEST:8.628 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:48:09.809: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-t9sqb
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 20 21:48:10.063: INFO: Waiting up to 5m0s for pod "pod-36e0ab4a-3559-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-t9sqb" to be "success or failure"
Feb 20 21:48:10.067: INFO: Pod "pod-36e0ab4a-3559-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.393582ms
Feb 20 21:48:12.072: INFO: Pod "pod-36e0ab4a-3559-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009886982s
STEP: Saw pod success
Feb 20 21:48:12.073: INFO: Pod "pod-36e0ab4a-3559-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:48:12.076: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-36e0ab4a-3559-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:48:12.113: INFO: Waiting for pod pod-36e0ab4a-3559-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:48:12.117: INFO: Pod pod-36e0ab4a-3559-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:48:12.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-t9sqb" for this suite.
Feb 20 21:48:18.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:48:18.157: INFO: namespace: e2e-tests-emptydir-t9sqb, resource: bindings, ignored listing per whitelist
Feb 20 21:48:18.368: INFO: namespace e2e-tests-emptydir-t9sqb deletion completed in 6.245429521s

â€¢ [SLOW TEST:8.559 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:48:18.369: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-h94b6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 20 21:48:18.665: INFO: Waiting up to 5m0s for pod "pod-3c007b9e-3559-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-h94b6" to be "success or failure"
Feb 20 21:48:18.675: INFO: Pod "pod-3c007b9e-3559-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 10.060172ms
Feb 20 21:48:20.682: INFO: Pod "pod-3c007b9e-3559-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016782981s
STEP: Saw pod success
Feb 20 21:48:20.682: INFO: Pod "pod-3c007b9e-3559-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:48:20.685: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-3c007b9e-3559-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:48:20.707: INFO: Waiting for pod pod-3c007b9e-3559-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:48:20.712: INFO: Pod pod-3c007b9e-3559-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:48:20.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-h94b6" for this suite.
Feb 20 21:48:26.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:48:26.825: INFO: namespace: e2e-tests-emptydir-h94b6, resource: bindings, ignored listing per whitelist
Feb 20 21:48:26.913: INFO: namespace e2e-tests-emptydir-h94b6 deletion completed in 6.196999258s

â€¢ [SLOW TEST:8.544 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:48:26.913: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-qmd5z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 20 21:48:27.238: INFO: Waiting up to 5m0s for pod "var-expansion-411cb1cd-3559-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-var-expansion-qmd5z" to be "success or failure"
Feb 20 21:48:27.245: INFO: Pod "var-expansion-411cb1cd-3559-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.956212ms
Feb 20 21:48:29.253: INFO: Pod "var-expansion-411cb1cd-3559-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015237772s
STEP: Saw pod success
Feb 20 21:48:29.253: INFO: Pod "var-expansion-411cb1cd-3559-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:48:29.257: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod var-expansion-411cb1cd-3559-11e9-8ceb-aaeae242ec87 container dapi-container: <nil>
STEP: delete the pod
Feb 20 21:48:29.286: INFO: Waiting for pod var-expansion-411cb1cd-3559-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:48:29.291: INFO: Pod var-expansion-411cb1cd-3559-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:48:29.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-qmd5z" for this suite.
Feb 20 21:48:35.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:48:35.450: INFO: namespace: e2e-tests-var-expansion-qmd5z, resource: bindings, ignored listing per whitelist
Feb 20 21:48:35.453: INFO: namespace e2e-tests-var-expansion-qmd5z deletion completed in 6.154355788s

â€¢ [SLOW TEST:8.540 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:48:35.454: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-6jfhf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 20 21:48:39.759: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:39.762: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:48:41.767: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:41.772: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:48:43.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:43.768: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:48:45.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:45.769: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:48:47.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:47.769: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:48:49.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:49.769: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:48:51.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:51.773: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:48:53.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:53.768: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:48:55.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:55.768: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:48:57.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:57.768: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:48:59.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:48:59.769: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 21:49:01.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 21:49:01.768: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:49:01.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-6jfhf" for this suite.
Feb 20 21:49:23.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:49:23.853: INFO: namespace: e2e-tests-container-lifecycle-hook-6jfhf, resource: bindings, ignored listing per whitelist
Feb 20 21:49:24.015: INFO: namespace e2e-tests-container-lifecycle-hook-6jfhf deletion completed in 22.242951448s

â€¢ [SLOW TEST:48.561 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:49:24.015: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-jhckl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 20 21:49:24.416: INFO: Waiting up to 5m0s for pod "pod-6331ddf5-3559-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-jhckl" to be "success or failure"
Feb 20 21:49:24.420: INFO: Pod "pod-6331ddf5-3559-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.971017ms
Feb 20 21:49:26.428: INFO: Pod "pod-6331ddf5-3559-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01204311s
STEP: Saw pod success
Feb 20 21:49:26.428: INFO: Pod "pod-6331ddf5-3559-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:49:26.432: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-6331ddf5-3559-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:49:26.451: INFO: Waiting for pod pod-6331ddf5-3559-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:49:26.455: INFO: Pod pod-6331ddf5-3559-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:49:26.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jhckl" for this suite.
Feb 20 21:49:32.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:49:32.583: INFO: namespace: e2e-tests-emptydir-jhckl, resource: bindings, ignored listing per whitelist
Feb 20 21:49:32.623: INFO: namespace e2e-tests-emptydir-jhckl deletion completed in 6.162868762s

â€¢ [SLOW TEST:8.608 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:49:32.623: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-snhcj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-683cc629-3559-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 21:49:32.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-683dfe9c-3559-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-snhcj" to be "success or failure"
Feb 20 21:49:32.889: INFO: Pod "pod-projected-configmaps-683dfe9c-3559-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.883756ms
Feb 20 21:49:34.894: INFO: Pod "pod-projected-configmaps-683dfe9c-3559-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009107043s
STEP: Saw pod success
Feb 20 21:49:34.894: INFO: Pod "pod-projected-configmaps-683dfe9c-3559-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:49:34.898: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-configmaps-683dfe9c-3559-11e9-8ceb-aaeae242ec87 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 21:49:34.919: INFO: Waiting for pod pod-projected-configmaps-683dfe9c-3559-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:49:34.922: INFO: Pod pod-projected-configmaps-683dfe9c-3559-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:49:34.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-snhcj" for this suite.
Feb 20 21:49:40.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:49:41.057: INFO: namespace: e2e-tests-projected-snhcj, resource: bindings, ignored listing per whitelist
Feb 20 21:49:41.175: INFO: namespace e2e-tests-projected-snhcj deletion completed in 6.24890625s

â€¢ [SLOW TEST:8.552 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:49:41.175: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-s9sjx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-6d58dce7-3559-11e9-8ceb-aaeae242ec87
STEP: Creating secret with name secret-projected-all-test-volume-6d58dccf-3559-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 20 21:49:41.470: INFO: Waiting up to 5m0s for pod "projected-volume-6d58dc96-3559-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-s9sjx" to be "success or failure"
Feb 20 21:49:41.473: INFO: Pod "projected-volume-6d58dc96-3559-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.203866ms
Feb 20 21:49:43.485: INFO: Pod "projected-volume-6d58dc96-3559-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015024706s
STEP: Saw pod success
Feb 20 21:49:43.485: INFO: Pod "projected-volume-6d58dc96-3559-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:49:43.489: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod projected-volume-6d58dc96-3559-11e9-8ceb-aaeae242ec87 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 20 21:49:43.513: INFO: Waiting for pod projected-volume-6d58dc96-3559-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:49:43.516: INFO: Pod projected-volume-6d58dc96-3559-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:49:43.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s9sjx" for this suite.
Feb 20 21:49:49.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:49:49.741: INFO: namespace: e2e-tests-projected-s9sjx, resource: bindings, ignored listing per whitelist
Feb 20 21:49:49.762: INFO: namespace e2e-tests-projected-s9sjx deletion completed in 6.242154036s

â€¢ [SLOW TEST:8.587 seconds]
[sig-storage] Projected combined
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:49:49.763: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-jdzzv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 21:49:50.045: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 20 21:49:55.051: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 20 21:49:55.051: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 20 21:49:57.055: INFO: Creating deployment "test-rollover-deployment"
Feb 20 21:49:57.065: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 20 21:49:59.074: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 20 21:49:59.082: INFO: Ensure that both replica sets have 1 created replica
Feb 20 21:49:59.092: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 20 21:49:59.100: INFO: Updating deployment test-rollover-deployment
Feb 20 21:49:59.100: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 20 21:50:01.115: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 20 21:50:01.122: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 20 21:50:01.153: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 21:50:01.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296200, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 21:50:03.165: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 21:50:03.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296200, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 21:50:05.167: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 21:50:05.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296200, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 21:50:07.162: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 21:50:07.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296200, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 21:50:09.162: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 21:50:09.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296200, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686296197, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 21:50:11.164: INFO: 
Feb 20 21:50:11.164: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 21:50:11.177: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-jdzzv,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jdzzv/deployments/test-rollover-deployment,UID:76a7cc1d-3559-11e9-91b0-3e11857925bf,ResourceVersion:12869,Generation:2,CreationTimestamp:2019-02-20 21:49:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-20 21:49:57 +0000 UTC 2019-02-20 21:49:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-20 21:50:10 +0000 UTC 2019-02-20 21:49:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 21:50:11.183: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-jdzzv,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jdzzv/replicasets/test-rollover-deployment-6b7f9d6597,UID:77dfda27-3559-11e9-91b0-3e11857925bf,ResourceVersion:12861,Generation:2,CreationTimestamp:2019-02-20 21:49:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 76a7cc1d-3559-11e9-91b0-3e11857925bf 0xc000c91127 0xc000c91128}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 20 21:50:11.183: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 20 21:50:11.183: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-jdzzv,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jdzzv/replicasets/test-rollover-controller,UID:727886e3-3559-11e9-91b0-3e11857925bf,ResourceVersion:12868,Generation:2,CreationTimestamp:2019-02-20 21:49:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 76a7cc1d-3559-11e9-91b0-3e11857925bf 0xc000c90e77 0xc000c90e78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 21:50:11.183: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-jdzzv,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jdzzv/replicasets/test-rollover-deployment-6586df867b,UID:76aa00c4-3559-11e9-91b0-3e11857925bf,ResourceVersion:12826,Generation:2,CreationTimestamp:2019-02-20 21:49:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 76a7cc1d-3559-11e9-91b0-3e11857925bf 0xc000c90f87 0xc000c90f88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 21:50:11.187: INFO: Pod "test-rollover-deployment-6b7f9d6597-jgnwl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-jgnwl,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-jdzzv,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jdzzv/pods/test-rollover-deployment-6b7f9d6597-jgnwl,UID:77e2eb50-3559-11e9-91b0-3e11857925bf,ResourceVersion:12839,Generation:0,CreationTimestamp:2019-02-20 21:49:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.138/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 77dfda27-3559-11e9-91b0-3e11857925bf 0xc001cc8337 0xc001cc8338}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-w5hcm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w5hcm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-w5hcm true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001cc8460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001cc8480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:49:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:50:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:50:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 21:49:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:100.96.1.138,StartTime:2019-02-20 21:49:59 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-20 21:49:59 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://e98f8b687af58e5a9fba8d2e60508512a2333c8bec7106ddeabe6665f17e7aeb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:50:11.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-jdzzv" for this suite.
Feb 20 21:50:17.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:50:17.257: INFO: namespace: e2e-tests-deployment-jdzzv, resource: bindings, ignored listing per whitelist
Feb 20 21:50:17.423: INFO: namespace e2e-tests-deployment-jdzzv deletion completed in 6.230678183s

â€¢ [SLOW TEST:27.660 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:50:17.423: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-tfm5z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-82efd5f2-3559-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 21:50:17.675: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-82f09d19-3559-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-tfm5z" to be "success or failure"
Feb 20 21:50:17.679: INFO: Pod "pod-projected-configmaps-82f09d19-3559-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.283041ms
Feb 20 21:50:19.687: INFO: Pod "pod-projected-configmaps-82f09d19-3559-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012081897s
STEP: Saw pod success
Feb 20 21:50:19.687: INFO: Pod "pod-projected-configmaps-82f09d19-3559-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:50:19.690: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-configmaps-82f09d19-3559-11e9-8ceb-aaeae242ec87 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 21:50:19.713: INFO: Waiting for pod pod-projected-configmaps-82f09d19-3559-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:50:19.717: INFO: Pod pod-projected-configmaps-82f09d19-3559-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:50:19.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tfm5z" for this suite.
Feb 20 21:50:25.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:50:26.026: INFO: namespace: e2e-tests-projected-tfm5z, resource: bindings, ignored listing per whitelist
Feb 20 21:50:26.063: INFO: namespace e2e-tests-projected-tfm5z deletion completed in 6.341590569s

â€¢ [SLOW TEST:8.640 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:50:26.063: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-pkl5v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-pkl5v
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-pkl5v
STEP: Deleting pre-stop pod
Feb 20 21:51:23.447: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:51:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-pkl5v" for this suite.
Feb 20 21:52:01.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:52:01.519: INFO: namespace: e2e-tests-prestop-pkl5v, resource: bindings, ignored listing per whitelist
Feb 20 21:52:01.621: INFO: namespace e2e-tests-prestop-pkl5v deletion completed in 38.154294255s

â€¢ [SLOW TEST:95.558 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:52:01.621: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-kprnp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0220 21:52:32.394614   32391 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 21:52:32.394: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:52:32.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-kprnp" for this suite.
Feb 20 21:52:38.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:52:38.448: INFO: namespace: e2e-tests-gc-kprnp, resource: bindings, ignored listing per whitelist
Feb 20 21:52:38.572: INFO: namespace e2e-tests-gc-kprnp deletion completed in 6.174535277s

â€¢ [SLOW TEST:36.952 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:52:38.573: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-s5cbj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 20 21:52:38.839: INFO: Waiting up to 5m0s for pod "client-containers-d71498ca-3559-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-containers-s5cbj" to be "success or failure"
Feb 20 21:52:38.844: INFO: Pod "client-containers-d71498ca-3559-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.43701ms
Feb 20 21:52:40.849: INFO: Pod "client-containers-d71498ca-3559-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010588827s
STEP: Saw pod success
Feb 20 21:52:40.849: INFO: Pod "client-containers-d71498ca-3559-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:52:40.853: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod client-containers-d71498ca-3559-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:52:40.870: INFO: Waiting for pod client-containers-d71498ca-3559-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:52:40.873: INFO: Pod client-containers-d71498ca-3559-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:52:40.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-s5cbj" for this suite.
Feb 20 21:52:46.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:52:46.990: INFO: namespace: e2e-tests-containers-s5cbj, resource: bindings, ignored listing per whitelist
Feb 20 21:52:47.114: INFO: namespace e2e-tests-containers-s5cbj deletion completed in 6.237565342s

â€¢ [SLOW TEST:8.542 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:52:47.115: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-49vrj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-49vrj
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 21:52:47.458: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 21:53:03.542: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.144:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-49vrj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 21:53:03.542: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 21:53:04.027: INFO: Found all expected endpoints: [netserver-0]
Feb 20 21:53:04.031: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.51:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-49vrj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 21:53:04.032: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 21:53:04.481: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:53:04.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-49vrj" for this suite.
Feb 20 21:53:26.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:53:26.544: INFO: namespace: e2e-tests-pod-network-test-49vrj, resource: bindings, ignored listing per whitelist
Feb 20 21:53:26.721: INFO: namespace e2e-tests-pod-network-test-49vrj deletion completed in 22.233372387s

â€¢ [SLOW TEST:39.606 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:53:26.721: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-d6dwx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0220 21:53:33.000810   32391 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 21:53:33.000: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:53:33.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-d6dwx" for this suite.
Feb 20 21:53:39.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:53:39.045: INFO: namespace: e2e-tests-gc-d6dwx, resource: bindings, ignored listing per whitelist
Feb 20 21:53:39.195: INFO: namespace e2e-tests-gc-d6dwx deletion completed in 6.188239451s

â€¢ [SLOW TEST:12.474 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:53:39.195: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-278kk
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-fb34add4-3559-11e9-8ceb-aaeae242ec87
STEP: Creating secret with name s-test-opt-upd-fb34ae1f-3559-11e9-8ceb-aaeae242ec87
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-fb34add4-3559-11e9-8ceb-aaeae242ec87
STEP: Updating secret s-test-opt-upd-fb34ae1f-3559-11e9-8ceb-aaeae242ec87
STEP: Creating secret with name s-test-opt-create-fb34ae40-3559-11e9-8ceb-aaeae242ec87
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:53:45.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-278kk" for this suite.
Feb 20 21:54:07.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:54:08.101: INFO: namespace: e2e-tests-secrets-278kk, resource: bindings, ignored listing per whitelist
Feb 20 21:54:08.131: INFO: namespace e2e-tests-secrets-278kk deletion completed in 22.17402822s

â€¢ [SLOW TEST:28.936 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:54:08.132: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-r8q7q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 21:54:08.379: INFO: Waiting up to 5m0s for pod "downward-api-0c72caf8-355a-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-r8q7q" to be "success or failure"
Feb 20 21:54:08.382: INFO: Pod "downward-api-0c72caf8-355a-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580551ms
Feb 20 21:54:10.386: INFO: Pod "downward-api-0c72caf8-355a-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006346357s
STEP: Saw pod success
Feb 20 21:54:10.386: INFO: Pod "downward-api-0c72caf8-355a-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:54:10.389: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downward-api-0c72caf8-355a-11e9-8ceb-aaeae242ec87 container dapi-container: <nil>
STEP: delete the pod
Feb 20 21:54:10.407: INFO: Waiting for pod downward-api-0c72caf8-355a-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:54:10.410: INFO: Pod downward-api-0c72caf8-355a-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:54:10.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-r8q7q" for this suite.
Feb 20 21:54:16.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:54:16.544: INFO: namespace: e2e-tests-downward-api-r8q7q, resource: bindings, ignored listing per whitelist
Feb 20 21:54:16.585: INFO: namespace e2e-tests-downward-api-r8q7q deletion completed in 6.170978611s

â€¢ [SLOW TEST:8.453 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:54:16.585: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5ppnx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 20 21:54:16.881: INFO: Waiting up to 5m0s for pod "pod-11845928-355a-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-5ppnx" to be "success or failure"
Feb 20 21:54:16.885: INFO: Pod "pod-11845928-355a-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259493ms
Feb 20 21:54:18.890: INFO: Pod "pod-11845928-355a-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009134408s
STEP: Saw pod success
Feb 20 21:54:18.890: INFO: Pod "pod-11845928-355a-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:54:18.893: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-11845928-355a-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:54:18.914: INFO: Waiting for pod pod-11845928-355a-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:54:18.917: INFO: Pod pod-11845928-355a-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:54:18.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5ppnx" for this suite.
Feb 20 21:54:24.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:54:25.034: INFO: namespace: e2e-tests-emptydir-5ppnx, resource: bindings, ignored listing per whitelist
Feb 20 21:54:25.144: INFO: namespace e2e-tests-emptydir-5ppnx deletion completed in 6.223022794s

â€¢ [SLOW TEST:8.559 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:54:25.144: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-h6ltc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-2pflc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb 20 21:54:34.679: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-twdrb
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:54:51.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-h6ltc" for this suite.
Feb 20 21:54:57.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:54:57.789: INFO: namespace: e2e-tests-namespaces-h6ltc, resource: bindings, ignored listing per whitelist
Feb 20 21:54:57.945: INFO: namespace e2e-tests-namespaces-h6ltc deletion completed in 6.203600199s
STEP: Destroying namespace "e2e-tests-nsdeletetest-2pflc" for this suite.
Feb 20 21:54:57.948: INFO: Namespace e2e-tests-nsdeletetest-2pflc was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-twdrb" for this suite.
Feb 20 21:55:03.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:55:04.053: INFO: namespace: e2e-tests-nsdeletetest-twdrb, resource: bindings, ignored listing per whitelist
Feb 20 21:55:04.169: INFO: namespace e2e-tests-nsdeletetest-twdrb deletion completed in 6.221515827s

â€¢ [SLOW TEST:39.025 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:55:04.171: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rrth4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 21:55:04.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2dd987b2-355a-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-rrth4" to be "success or failure"
Feb 20 21:55:04.418: INFO: Pod "downwardapi-volume-2dd987b2-355a-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.585414ms
Feb 20 21:55:06.427: INFO: Pod "downwardapi-volume-2dd987b2-355a-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012298804s
STEP: Saw pod success
Feb 20 21:55:06.427: INFO: Pod "downwardapi-volume-2dd987b2-355a-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:55:06.431: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-2dd987b2-355a-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 21:55:06.469: INFO: Waiting for pod downwardapi-volume-2dd987b2-355a-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:55:06.475: INFO: Pod downwardapi-volume-2dd987b2-355a-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:55:06.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rrth4" for this suite.
Feb 20 21:55:12.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:55:12.690: INFO: namespace: e2e-tests-projected-rrth4, resource: bindings, ignored listing per whitelist
Feb 20 21:55:12.719: INFO: namespace e2e-tests-projected-rrth4 deletion completed in 6.237275947s

â€¢ [SLOW TEST:8.548 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:55:12.719: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-92djl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 21:55:13.001: INFO: Waiting up to 5m0s for pod "downward-api-32f70e71-355a-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-92djl" to be "success or failure"
Feb 20 21:55:13.005: INFO: Pod "downward-api-32f70e71-355a-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.192109ms
Feb 20 21:55:15.013: INFO: Pod "downward-api-32f70e71-355a-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01194683s
STEP: Saw pod success
Feb 20 21:55:15.013: INFO: Pod "downward-api-32f70e71-355a-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:55:15.019: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downward-api-32f70e71-355a-11e9-8ceb-aaeae242ec87 container dapi-container: <nil>
STEP: delete the pod
Feb 20 21:55:15.040: INFO: Waiting for pod downward-api-32f70e71-355a-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:55:15.044: INFO: Pod downward-api-32f70e71-355a-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:55:15.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-92djl" for this suite.
Feb 20 21:55:21.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:55:21.111: INFO: namespace: e2e-tests-downward-api-92djl, resource: bindings, ignored listing per whitelist
Feb 20 21:55:21.290: INFO: namespace e2e-tests-downward-api-92djl deletion completed in 6.241993712s

â€¢ [SLOW TEST:8.571 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:55:21.290: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-r8nct
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:55:23.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-r8nct" for this suite.
Feb 20 21:56:01.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:56:01.784: INFO: namespace: e2e-tests-kubelet-test-r8nct, resource: bindings, ignored listing per whitelist
Feb 20 21:56:01.790: INFO: namespace e2e-tests-kubelet-test-r8nct deletion completed in 38.225187992s

â€¢ [SLOW TEST:40.500 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:56:01.791: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-v675j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 21:56:02.066: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-v675j'
Feb 20 21:56:03.104: INFO: stderr: ""
Feb 20 21:56:03.104: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Feb 20 21:56:03.108: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-v675j'
Feb 20 21:56:10.206: INFO: stderr: ""
Feb 20 21:56:10.206: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:56:10.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v675j" for this suite.
Feb 20 21:56:16.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:56:16.347: INFO: namespace: e2e-tests-kubectl-v675j, resource: bindings, ignored listing per whitelist
Feb 20 21:56:16.456: INFO: namespace e2e-tests-kubectl-v675j deletion completed in 6.244099519s

â€¢ [SLOW TEST:14.665 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:56:16.456: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-22nls
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-58f817dd-355a-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 21:56:16.762: INFO: Waiting up to 5m0s for pod "pod-secrets-58f8a7e4-355a-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-secrets-22nls" to be "success or failure"
Feb 20 21:56:16.766: INFO: Pod "pod-secrets-58f8a7e4-355a-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.414274ms
Feb 20 21:56:18.771: INFO: Pod "pod-secrets-58f8a7e4-355a-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009392304s
STEP: Saw pod success
Feb 20 21:56:18.771: INFO: Pod "pod-secrets-58f8a7e4-355a-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:56:18.774: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-secrets-58f8a7e4-355a-11e9-8ceb-aaeae242ec87 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 21:56:18.796: INFO: Waiting for pod pod-secrets-58f8a7e4-355a-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:56:18.799: INFO: Pod pod-secrets-58f8a7e4-355a-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:56:18.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-22nls" for this suite.
Feb 20 21:56:24.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:56:24.867: INFO: namespace: e2e-tests-secrets-22nls, resource: bindings, ignored listing per whitelist
Feb 20 21:56:25.012: INFO: namespace e2e-tests-secrets-22nls deletion completed in 6.208254343s

â€¢ [SLOW TEST:8.555 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:56:25.012: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-v9kgt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-v9kgt
I0220 21:56:25.256859   32391 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-v9kgt, replica count: 1
I0220 21:56:26.307426   32391 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 21:56:27.307786   32391 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 21:56:27.427: INFO: Created: latency-svc-z7zm5
Feb 20 21:56:27.435: INFO: Got endpoints: latency-svc-z7zm5 [27.356905ms]
Feb 20 21:56:27.448: INFO: Created: latency-svc-tf4zd
Feb 20 21:56:27.450: INFO: Got endpoints: latency-svc-tf4zd [15.282663ms]
Feb 20 21:56:27.455: INFO: Created: latency-svc-pnk86
Feb 20 21:56:27.458: INFO: Got endpoints: latency-svc-pnk86 [23.215787ms]
Feb 20 21:56:27.463: INFO: Created: latency-svc-8v5hs
Feb 20 21:56:27.466: INFO: Got endpoints: latency-svc-8v5hs [30.623471ms]
Feb 20 21:56:27.470: INFO: Created: latency-svc-5jr2p
Feb 20 21:56:27.473: INFO: Got endpoints: latency-svc-5jr2p [37.798033ms]
Feb 20 21:56:27.478: INFO: Created: latency-svc-tds9x
Feb 20 21:56:27.480: INFO: Got endpoints: latency-svc-tds9x [44.925865ms]
Feb 20 21:56:27.487: INFO: Created: latency-svc-2l7b5
Feb 20 21:56:27.493: INFO: Created: latency-svc-w6szs
Feb 20 21:56:27.513: INFO: Got endpoints: latency-svc-2l7b5 [77.12889ms]
Feb 20 21:56:27.513: INFO: Got endpoints: latency-svc-w6szs [77.309272ms]
Feb 20 21:56:27.522: INFO: Created: latency-svc-j6wpf
Feb 20 21:56:27.534: INFO: Created: latency-svc-h8lzg
Feb 20 21:56:27.545: INFO: Got endpoints: latency-svc-h8lzg [109.683746ms]
Feb 20 21:56:27.545: INFO: Got endpoints: latency-svc-j6wpf [110.08357ms]
Feb 20 21:56:27.547: INFO: Created: latency-svc-bjmb7
Feb 20 21:56:27.553: INFO: Got endpoints: latency-svc-bjmb7 [117.78952ms]
Feb 20 21:56:27.557: INFO: Created: latency-svc-4rcd8
Feb 20 21:56:27.560: INFO: Got endpoints: latency-svc-4rcd8 [124.257017ms]
Feb 20 21:56:27.565: INFO: Created: latency-svc-qz5d9
Feb 20 21:56:27.570: INFO: Got endpoints: latency-svc-qz5d9 [134.685298ms]
Feb 20 21:56:27.575: INFO: Created: latency-svc-gltcc
Feb 20 21:56:27.576: INFO: Got endpoints: latency-svc-gltcc [140.746331ms]
Feb 20 21:56:27.584: INFO: Created: latency-svc-vqws9
Feb 20 21:56:27.586: INFO: Got endpoints: latency-svc-vqws9 [150.440913ms]
Feb 20 21:56:27.595: INFO: Created: latency-svc-nqrbg
Feb 20 21:56:27.607: INFO: Got endpoints: latency-svc-nqrbg [171.963959ms]
Feb 20 21:56:27.613: INFO: Created: latency-svc-x7rdj
Feb 20 21:56:27.616: INFO: Got endpoints: latency-svc-x7rdj [165.572958ms]
Feb 20 21:56:27.630: INFO: Created: latency-svc-mnj4f
Feb 20 21:56:27.633: INFO: Got endpoints: latency-svc-mnj4f [174.751174ms]
Feb 20 21:56:27.642: INFO: Created: latency-svc-wdkcj
Feb 20 21:56:27.654: INFO: Got endpoints: latency-svc-wdkcj [188.440598ms]
Feb 20 21:56:27.658: INFO: Created: latency-svc-pcbfx
Feb 20 21:56:27.663: INFO: Got endpoints: latency-svc-pcbfx [189.433256ms]
Feb 20 21:56:27.663: INFO: Created: latency-svc-c75tt
Feb 20 21:56:27.669: INFO: Got endpoints: latency-svc-c75tt [188.665993ms]
Feb 20 21:56:27.670: INFO: Created: latency-svc-k5jsh
Feb 20 21:56:27.672: INFO: Got endpoints: latency-svc-k5jsh [159.181718ms]
Feb 20 21:56:27.677: INFO: Created: latency-svc-qj2wl
Feb 20 21:56:27.680: INFO: Got endpoints: latency-svc-qj2wl [167.377179ms]
Feb 20 21:56:27.685: INFO: Created: latency-svc-p5vwq
Feb 20 21:56:27.687: INFO: Got endpoints: latency-svc-p5vwq [142.337285ms]
Feb 20 21:56:27.694: INFO: Created: latency-svc-8fpkj
Feb 20 21:56:27.704: INFO: Got endpoints: latency-svc-8fpkj [158.847874ms]
Feb 20 21:56:27.713: INFO: Created: latency-svc-hdz9b
Feb 20 21:56:27.722: INFO: Got endpoints: latency-svc-hdz9b [168.834093ms]
Feb 20 21:56:27.728: INFO: Created: latency-svc-s6x44
Feb 20 21:56:27.734: INFO: Got endpoints: latency-svc-s6x44 [173.863236ms]
Feb 20 21:56:27.743: INFO: Created: latency-svc-vkr46
Feb 20 21:56:27.745: INFO: Got endpoints: latency-svc-vkr46 [174.931298ms]
Feb 20 21:56:27.751: INFO: Created: latency-svc-rd7c5
Feb 20 21:56:27.752: INFO: Got endpoints: latency-svc-rd7c5 [176.046629ms]
Feb 20 21:56:27.763: INFO: Created: latency-svc-smtkt
Feb 20 21:56:27.768: INFO: Got endpoints: latency-svc-smtkt [182.299882ms]
Feb 20 21:56:27.769: INFO: Created: latency-svc-vqrjq
Feb 20 21:56:27.772: INFO: Got endpoints: latency-svc-vqrjq [164.257304ms]
Feb 20 21:56:27.776: INFO: Created: latency-svc-qbbz5
Feb 20 21:56:27.779: INFO: Got endpoints: latency-svc-qbbz5 [162.70055ms]
Feb 20 21:56:27.782: INFO: Created: latency-svc-htqwj
Feb 20 21:56:27.785: INFO: Got endpoints: latency-svc-htqwj [151.691615ms]
Feb 20 21:56:27.789: INFO: Created: latency-svc-jkwwr
Feb 20 21:56:27.793: INFO: Got endpoints: latency-svc-jkwwr [138.501833ms]
Feb 20 21:56:27.828: INFO: Created: latency-svc-p8njk
Feb 20 21:56:27.831: INFO: Got endpoints: latency-svc-p8njk [167.64699ms]
Feb 20 21:56:27.835: INFO: Created: latency-svc-wkb6q
Feb 20 21:56:27.842: INFO: Created: latency-svc-vh8ld
Feb 20 21:56:27.842: INFO: Got endpoints: latency-svc-wkb6q [172.875364ms]
Feb 20 21:56:27.845: INFO: Got endpoints: latency-svc-vh8ld [172.987031ms]
Feb 20 21:56:27.850: INFO: Created: latency-svc-6nqd9
Feb 20 21:56:27.851: INFO: Got endpoints: latency-svc-6nqd9 [170.58816ms]
Feb 20 21:56:27.857: INFO: Created: latency-svc-w5rhl
Feb 20 21:56:27.870: INFO: Created: latency-svc-hmxlg
Feb 20 21:56:27.878: INFO: Created: latency-svc-5jv9x
Feb 20 21:56:27.884: INFO: Got endpoints: latency-svc-w5rhl [196.714686ms]
Feb 20 21:56:27.885: INFO: Created: latency-svc-94xfj
Feb 20 21:56:27.889: INFO: Created: latency-svc-rmrr7
Feb 20 21:56:27.912: INFO: Created: latency-svc-rhd9j
Feb 20 21:56:27.934: INFO: Got endpoints: latency-svc-hmxlg [230.013859ms]
Feb 20 21:56:27.936: INFO: Created: latency-svc-t8kpg
Feb 20 21:56:27.943: INFO: Created: latency-svc-ps6pn
Feb 20 21:56:27.949: INFO: Created: latency-svc-jgmxx
Feb 20 21:56:27.956: INFO: Created: latency-svc-wsndd
Feb 20 21:56:27.964: INFO: Created: latency-svc-ngktx
Feb 20 21:56:27.976: INFO: Created: latency-svc-47dcz
Feb 20 21:56:27.982: INFO: Got endpoints: latency-svc-5jv9x [259.809754ms]
Feb 20 21:56:27.983: INFO: Created: latency-svc-lq7v6
Feb 20 21:56:27.990: INFO: Created: latency-svc-x8l57
Feb 20 21:56:28.001: INFO: Created: latency-svc-qgx2d
Feb 20 21:56:28.012: INFO: Created: latency-svc-hsw42
Feb 20 21:56:28.020: INFO: Created: latency-svc-frmnb
Feb 20 21:56:28.034: INFO: Got endpoints: latency-svc-94xfj [300.475701ms]
Feb 20 21:56:28.034: INFO: Created: latency-svc-gqf9s
Feb 20 21:56:28.048: INFO: Created: latency-svc-hrqfn
Feb 20 21:56:28.081: INFO: Got endpoints: latency-svc-rmrr7 [336.271079ms]
Feb 20 21:56:28.093: INFO: Created: latency-svc-lkxtg
Feb 20 21:56:28.131: INFO: Got endpoints: latency-svc-rhd9j [378.689947ms]
Feb 20 21:56:28.146: INFO: Created: latency-svc-n8mdn
Feb 20 21:56:28.187: INFO: Got endpoints: latency-svc-t8kpg [418.593555ms]
Feb 20 21:56:28.202: INFO: Created: latency-svc-prjjb
Feb 20 21:56:28.231: INFO: Got endpoints: latency-svc-ps6pn [459.507065ms]
Feb 20 21:56:28.248: INFO: Created: latency-svc-5bm94
Feb 20 21:56:28.281: INFO: Got endpoints: latency-svc-jgmxx [501.988398ms]
Feb 20 21:56:28.296: INFO: Created: latency-svc-5mxpx
Feb 20 21:56:28.331: INFO: Got endpoints: latency-svc-wsndd [546.391746ms]
Feb 20 21:56:28.343: INFO: Created: latency-svc-w8swl
Feb 20 21:56:28.429: INFO: Got endpoints: latency-svc-ngktx [635.696328ms]
Feb 20 21:56:28.431: INFO: Got endpoints: latency-svc-47dcz [600.585747ms]
Feb 20 21:56:28.441: INFO: Created: latency-svc-ghbgs
Feb 20 21:56:28.447: INFO: Created: latency-svc-xc2bf
Feb 20 21:56:28.481: INFO: Got endpoints: latency-svc-lq7v6 [639.555562ms]
Feb 20 21:56:28.492: INFO: Created: latency-svc-nsn2j
Feb 20 21:56:28.531: INFO: Got endpoints: latency-svc-x8l57 [686.54152ms]
Feb 20 21:56:28.543: INFO: Created: latency-svc-t4lhn
Feb 20 21:56:28.582: INFO: Got endpoints: latency-svc-qgx2d [730.543644ms]
Feb 20 21:56:28.592: INFO: Created: latency-svc-xlv4l
Feb 20 21:56:28.632: INFO: Got endpoints: latency-svc-hsw42 [747.810728ms]
Feb 20 21:56:28.648: INFO: Created: latency-svc-ccl2g
Feb 20 21:56:28.681: INFO: Got endpoints: latency-svc-frmnb [746.98249ms]
Feb 20 21:56:28.692: INFO: Created: latency-svc-g2d9d
Feb 20 21:56:28.731: INFO: Got endpoints: latency-svc-gqf9s [749.502362ms]
Feb 20 21:56:28.742: INFO: Created: latency-svc-7qq89
Feb 20 21:56:28.782: INFO: Got endpoints: latency-svc-hrqfn [747.697275ms]
Feb 20 21:56:28.829: INFO: Created: latency-svc-rrlzj
Feb 20 21:56:28.831: INFO: Got endpoints: latency-svc-lkxtg [749.652784ms]
Feb 20 21:56:28.843: INFO: Created: latency-svc-jbmq7
Feb 20 21:56:28.881: INFO: Got endpoints: latency-svc-n8mdn [749.782895ms]
Feb 20 21:56:28.892: INFO: Created: latency-svc-t9tpx
Feb 20 21:56:28.931: INFO: Got endpoints: latency-svc-prjjb [744.197796ms]
Feb 20 21:56:28.944: INFO: Created: latency-svc-6x7k8
Feb 20 21:56:28.981: INFO: Got endpoints: latency-svc-5bm94 [749.754972ms]
Feb 20 21:56:28.991: INFO: Created: latency-svc-s66zp
Feb 20 21:56:29.032: INFO: Got endpoints: latency-svc-5mxpx [751.03197ms]
Feb 20 21:56:29.042: INFO: Created: latency-svc-9pbj9
Feb 20 21:56:29.081: INFO: Got endpoints: latency-svc-w8swl [749.987968ms]
Feb 20 21:56:29.094: INFO: Created: latency-svc-rfbxs
Feb 20 21:56:29.132: INFO: Got endpoints: latency-svc-ghbgs [702.99484ms]
Feb 20 21:56:29.142: INFO: Created: latency-svc-hjkdm
Feb 20 21:56:29.190: INFO: Got endpoints: latency-svc-xc2bf [758.899916ms]
Feb 20 21:56:29.200: INFO: Created: latency-svc-79kd9
Feb 20 21:56:29.231: INFO: Got endpoints: latency-svc-nsn2j [749.750862ms]
Feb 20 21:56:29.245: INFO: Created: latency-svc-rntfc
Feb 20 21:56:29.282: INFO: Got endpoints: latency-svc-t4lhn [750.321164ms]
Feb 20 21:56:29.293: INFO: Created: latency-svc-lxkzx
Feb 20 21:56:29.332: INFO: Got endpoints: latency-svc-xlv4l [750.202748ms]
Feb 20 21:56:29.345: INFO: Created: latency-svc-w8gdr
Feb 20 21:56:29.382: INFO: Got endpoints: latency-svc-ccl2g [749.50909ms]
Feb 20 21:56:29.393: INFO: Created: latency-svc-922dk
Feb 20 21:56:29.432: INFO: Got endpoints: latency-svc-g2d9d [750.566352ms]
Feb 20 21:56:29.444: INFO: Created: latency-svc-xf4m7
Feb 20 21:56:29.481: INFO: Got endpoints: latency-svc-7qq89 [749.61874ms]
Feb 20 21:56:29.492: INFO: Created: latency-svc-47ncj
Feb 20 21:56:29.532: INFO: Got endpoints: latency-svc-rrlzj [749.701283ms]
Feb 20 21:56:29.542: INFO: Created: latency-svc-wzl88
Feb 20 21:56:29.582: INFO: Got endpoints: latency-svc-jbmq7 [750.641229ms]
Feb 20 21:56:29.592: INFO: Created: latency-svc-m68qx
Feb 20 21:56:29.632: INFO: Got endpoints: latency-svc-t9tpx [751.114709ms]
Feb 20 21:56:29.646: INFO: Created: latency-svc-8nxwk
Feb 20 21:56:29.681: INFO: Got endpoints: latency-svc-6x7k8 [749.672414ms]
Feb 20 21:56:29.693: INFO: Created: latency-svc-pzjbc
Feb 20 21:56:29.731: INFO: Got endpoints: latency-svc-s66zp [749.915701ms]
Feb 20 21:56:29.743: INFO: Created: latency-svc-jrgn2
Feb 20 21:56:29.781: INFO: Got endpoints: latency-svc-9pbj9 [749.501594ms]
Feb 20 21:56:29.793: INFO: Created: latency-svc-d9rrl
Feb 20 21:56:29.832: INFO: Got endpoints: latency-svc-rfbxs [750.814743ms]
Feb 20 21:56:29.848: INFO: Created: latency-svc-6x79r
Feb 20 21:56:29.882: INFO: Got endpoints: latency-svc-hjkdm [749.724082ms]
Feb 20 21:56:29.892: INFO: Created: latency-svc-x4jrb
Feb 20 21:56:29.931: INFO: Got endpoints: latency-svc-79kd9 [741.354385ms]
Feb 20 21:56:29.948: INFO: Created: latency-svc-nt527
Feb 20 21:56:29.982: INFO: Got endpoints: latency-svc-rntfc [750.658586ms]
Feb 20 21:56:29.993: INFO: Created: latency-svc-n4j7d
Feb 20 21:56:30.032: INFO: Got endpoints: latency-svc-lxkzx [750.331021ms]
Feb 20 21:56:30.046: INFO: Created: latency-svc-bg4h2
Feb 20 21:56:30.082: INFO: Got endpoints: latency-svc-w8gdr [750.270804ms]
Feb 20 21:56:30.094: INFO: Created: latency-svc-2gvlf
Feb 20 21:56:30.132: INFO: Got endpoints: latency-svc-922dk [750.576154ms]
Feb 20 21:56:30.145: INFO: Created: latency-svc-kc7rj
Feb 20 21:56:30.181: INFO: Got endpoints: latency-svc-xf4m7 [749.121094ms]
Feb 20 21:56:30.192: INFO: Created: latency-svc-s5ccb
Feb 20 21:56:30.232: INFO: Got endpoints: latency-svc-47ncj [750.558044ms]
Feb 20 21:56:30.248: INFO: Created: latency-svc-gwtmr
Feb 20 21:56:30.283: INFO: Got endpoints: latency-svc-wzl88 [751.090739ms]
Feb 20 21:56:30.296: INFO: Created: latency-svc-76h2g
Feb 20 21:56:30.331: INFO: Got endpoints: latency-svc-m68qx [749.228142ms]
Feb 20 21:56:30.342: INFO: Created: latency-svc-sxxww
Feb 20 21:56:30.381: INFO: Got endpoints: latency-svc-8nxwk [749.011101ms]
Feb 20 21:56:30.393: INFO: Created: latency-svc-qnt6n
Feb 20 21:56:30.431: INFO: Got endpoints: latency-svc-pzjbc [750.167868ms]
Feb 20 21:56:30.443: INFO: Created: latency-svc-d8rr7
Feb 20 21:56:30.484: INFO: Got endpoints: latency-svc-jrgn2 [752.889362ms]
Feb 20 21:56:30.496: INFO: Created: latency-svc-hdx2n
Feb 20 21:56:30.532: INFO: Got endpoints: latency-svc-d9rrl [750.516192ms]
Feb 20 21:56:30.547: INFO: Created: latency-svc-vnlw6
Feb 20 21:56:30.581: INFO: Got endpoints: latency-svc-6x79r [748.914811ms]
Feb 20 21:56:30.595: INFO: Created: latency-svc-znk26
Feb 20 21:56:30.633: INFO: Got endpoints: latency-svc-x4jrb [751.018452ms]
Feb 20 21:56:30.645: INFO: Created: latency-svc-sr5fz
Feb 20 21:56:30.684: INFO: Got endpoints: latency-svc-nt527 [752.719853ms]
Feb 20 21:56:30.698: INFO: Created: latency-svc-7vsrl
Feb 20 21:56:30.732: INFO: Got endpoints: latency-svc-n4j7d [750.378263ms]
Feb 20 21:56:30.745: INFO: Created: latency-svc-nksv8
Feb 20 21:56:30.784: INFO: Got endpoints: latency-svc-bg4h2 [752.247505ms]
Feb 20 21:56:30.802: INFO: Created: latency-svc-c848f
Feb 20 21:56:30.840: INFO: Got endpoints: latency-svc-2gvlf [757.74372ms]
Feb 20 21:56:30.908: INFO: Got endpoints: latency-svc-kc7rj [775.638013ms]
Feb 20 21:56:30.908: INFO: Created: latency-svc-wrpnm
Feb 20 21:56:30.921: INFO: Created: latency-svc-8m42w
Feb 20 21:56:30.935: INFO: Got endpoints: latency-svc-s5ccb [753.406102ms]
Feb 20 21:56:30.954: INFO: Created: latency-svc-mh4f2
Feb 20 21:56:30.982: INFO: Got endpoints: latency-svc-gwtmr [750.045366ms]
Feb 20 21:56:31.062: INFO: Got endpoints: latency-svc-76h2g [778.609094ms]
Feb 20 21:56:31.076: INFO: Created: latency-svc-5hjsg
Feb 20 21:56:31.082: INFO: Got endpoints: latency-svc-sxxww [750.861811ms]
Feb 20 21:56:31.086: INFO: Created: latency-svc-l4tn9
Feb 20 21:56:31.095: INFO: Created: latency-svc-44lx2
Feb 20 21:56:31.131: INFO: Got endpoints: latency-svc-qnt6n [750.162903ms]
Feb 20 21:56:31.143: INFO: Created: latency-svc-hfwb5
Feb 20 21:56:31.181: INFO: Got endpoints: latency-svc-d8rr7 [749.432458ms]
Feb 20 21:56:31.196: INFO: Created: latency-svc-z4wkn
Feb 20 21:56:31.232: INFO: Got endpoints: latency-svc-hdx2n [747.352555ms]
Feb 20 21:56:31.250: INFO: Created: latency-svc-bjdsd
Feb 20 21:56:31.283: INFO: Got endpoints: latency-svc-vnlw6 [750.572701ms]
Feb 20 21:56:31.298: INFO: Created: latency-svc-9pbf2
Feb 20 21:56:31.331: INFO: Got endpoints: latency-svc-znk26 [749.879428ms]
Feb 20 21:56:31.345: INFO: Created: latency-svc-5mxml
Feb 20 21:56:31.389: INFO: Got endpoints: latency-svc-sr5fz [756.23959ms]
Feb 20 21:56:31.399: INFO: Created: latency-svc-2q65n
Feb 20 21:56:31.446: INFO: Got endpoints: latency-svc-7vsrl [761.92605ms]
Feb 20 21:56:31.457: INFO: Created: latency-svc-jmb4v
Feb 20 21:56:31.481: INFO: Got endpoints: latency-svc-nksv8 [748.343909ms]
Feb 20 21:56:31.492: INFO: Created: latency-svc-cnwxt
Feb 20 21:56:31.532: INFO: Got endpoints: latency-svc-c848f [747.277049ms]
Feb 20 21:56:31.544: INFO: Created: latency-svc-pwj9n
Feb 20 21:56:31.582: INFO: Got endpoints: latency-svc-wrpnm [741.444331ms]
Feb 20 21:56:31.593: INFO: Created: latency-svc-p8wcq
Feb 20 21:56:31.631: INFO: Got endpoints: latency-svc-8m42w [723.340793ms]
Feb 20 21:56:31.704: INFO: Got endpoints: latency-svc-mh4f2 [769.31393ms]
Feb 20 21:56:31.704: INFO: Created: latency-svc-6kgq2
Feb 20 21:56:31.715: INFO: Created: latency-svc-4rk72
Feb 20 21:56:31.732: INFO: Got endpoints: latency-svc-5hjsg [750.476001ms]
Feb 20 21:56:31.743: INFO: Created: latency-svc-2n4c7
Feb 20 21:56:31.782: INFO: Got endpoints: latency-svc-l4tn9 [719.894172ms]
Feb 20 21:56:31.794: INFO: Created: latency-svc-gqjnh
Feb 20 21:56:31.831: INFO: Got endpoints: latency-svc-44lx2 [749.378324ms]
Feb 20 21:56:31.844: INFO: Created: latency-svc-7j925
Feb 20 21:56:31.882: INFO: Got endpoints: latency-svc-hfwb5 [750.169249ms]
Feb 20 21:56:31.895: INFO: Created: latency-svc-qhbd2
Feb 20 21:56:31.932: INFO: Got endpoints: latency-svc-z4wkn [751.126093ms]
Feb 20 21:56:31.943: INFO: Created: latency-svc-qwn58
Feb 20 21:56:31.981: INFO: Got endpoints: latency-svc-bjdsd [749.56437ms]
Feb 20 21:56:31.991: INFO: Created: latency-svc-bx9xr
Feb 20 21:56:32.032: INFO: Got endpoints: latency-svc-9pbf2 [748.89698ms]
Feb 20 21:56:32.064: INFO: Created: latency-svc-rzrwh
Feb 20 21:56:32.087: INFO: Got endpoints: latency-svc-5mxml [755.556972ms]
Feb 20 21:56:32.098: INFO: Created: latency-svc-72qmk
Feb 20 21:56:32.132: INFO: Got endpoints: latency-svc-2q65n [742.91566ms]
Feb 20 21:56:32.151: INFO: Created: latency-svc-hw2x6
Feb 20 21:56:32.181: INFO: Got endpoints: latency-svc-jmb4v [734.734814ms]
Feb 20 21:56:32.193: INFO: Created: latency-svc-6t746
Feb 20 21:56:32.231: INFO: Got endpoints: latency-svc-cnwxt [750.401907ms]
Feb 20 21:56:32.245: INFO: Created: latency-svc-52w97
Feb 20 21:56:32.281: INFO: Got endpoints: latency-svc-pwj9n [749.157584ms]
Feb 20 21:56:32.295: INFO: Created: latency-svc-m5qxm
Feb 20 21:56:32.331: INFO: Got endpoints: latency-svc-p8wcq [749.827352ms]
Feb 20 21:56:32.343: INFO: Created: latency-svc-hjkxb
Feb 20 21:56:32.385: INFO: Got endpoints: latency-svc-6kgq2 [753.226927ms]
Feb 20 21:56:32.396: INFO: Created: latency-svc-c8x4q
Feb 20 21:56:32.432: INFO: Got endpoints: latency-svc-4rk72 [727.50285ms]
Feb 20 21:56:32.445: INFO: Created: latency-svc-c2wf7
Feb 20 21:56:32.482: INFO: Got endpoints: latency-svc-2n4c7 [749.730379ms]
Feb 20 21:56:32.559: INFO: Created: latency-svc-gbn5t
Feb 20 21:56:32.559: INFO: Got endpoints: latency-svc-gqjnh [777.329592ms]
Feb 20 21:56:32.581: INFO: Created: latency-svc-pchnm
Feb 20 21:56:32.584: INFO: Got endpoints: latency-svc-7j925 [752.323075ms]
Feb 20 21:56:32.595: INFO: Created: latency-svc-5gdbm
Feb 20 21:56:32.632: INFO: Got endpoints: latency-svc-qhbd2 [750.730061ms]
Feb 20 21:56:32.646: INFO: Created: latency-svc-7md7t
Feb 20 21:56:32.682: INFO: Got endpoints: latency-svc-qwn58 [750.112687ms]
Feb 20 21:56:32.696: INFO: Created: latency-svc-qr99p
Feb 20 21:56:32.731: INFO: Got endpoints: latency-svc-bx9xr [749.480822ms]
Feb 20 21:56:32.743: INFO: Created: latency-svc-94w5n
Feb 20 21:56:32.784: INFO: Got endpoints: latency-svc-rzrwh [752.412593ms]
Feb 20 21:56:32.798: INFO: Created: latency-svc-m5ffn
Feb 20 21:56:32.834: INFO: Got endpoints: latency-svc-72qmk [746.78232ms]
Feb 20 21:56:32.846: INFO: Created: latency-svc-nx2fj
Feb 20 21:56:32.883: INFO: Got endpoints: latency-svc-hw2x6 [750.854914ms]
Feb 20 21:56:32.896: INFO: Created: latency-svc-mmhqc
Feb 20 21:56:32.934: INFO: Got endpoints: latency-svc-6t746 [752.408819ms]
Feb 20 21:56:32.947: INFO: Created: latency-svc-xjzfg
Feb 20 21:56:32.981: INFO: Got endpoints: latency-svc-52w97 [750.03249ms]
Feb 20 21:56:32.994: INFO: Created: latency-svc-7pftl
Feb 20 21:56:33.033: INFO: Got endpoints: latency-svc-m5qxm [751.62835ms]
Feb 20 21:56:33.044: INFO: Created: latency-svc-tk28b
Feb 20 21:56:33.083: INFO: Got endpoints: latency-svc-hjkxb [751.463353ms]
Feb 20 21:56:33.099: INFO: Created: latency-svc-pfrrb
Feb 20 21:56:33.132: INFO: Got endpoints: latency-svc-c8x4q [746.787433ms]
Feb 20 21:56:33.143: INFO: Created: latency-svc-5bd5q
Feb 20 21:56:33.184: INFO: Got endpoints: latency-svc-c2wf7 [752.318047ms]
Feb 20 21:56:33.198: INFO: Created: latency-svc-l9tr5
Feb 20 21:56:33.232: INFO: Got endpoints: latency-svc-gbn5t [749.552537ms]
Feb 20 21:56:33.243: INFO: Created: latency-svc-l4qvj
Feb 20 21:56:33.282: INFO: Got endpoints: latency-svc-pchnm [722.772041ms]
Feb 20 21:56:33.302: INFO: Created: latency-svc-kq2r7
Feb 20 21:56:33.332: INFO: Got endpoints: latency-svc-5gdbm [748.475676ms]
Feb 20 21:56:33.347: INFO: Created: latency-svc-4w2b7
Feb 20 21:56:33.381: INFO: Got endpoints: latency-svc-7md7t [748.956053ms]
Feb 20 21:56:33.394: INFO: Created: latency-svc-wnm97
Feb 20 21:56:33.433: INFO: Got endpoints: latency-svc-qr99p [750.803485ms]
Feb 20 21:56:33.458: INFO: Created: latency-svc-9sbcw
Feb 20 21:56:33.482: INFO: Got endpoints: latency-svc-94w5n [750.760413ms]
Feb 20 21:56:33.494: INFO: Created: latency-svc-rz2k6
Feb 20 21:56:33.532: INFO: Got endpoints: latency-svc-m5ffn [747.629805ms]
Feb 20 21:56:33.545: INFO: Created: latency-svc-282ns
Feb 20 21:56:33.582: INFO: Got endpoints: latency-svc-nx2fj [748.181093ms]
Feb 20 21:56:33.596: INFO: Created: latency-svc-c2xkj
Feb 20 21:56:33.633: INFO: Got endpoints: latency-svc-mmhqc [750.216358ms]
Feb 20 21:56:33.645: INFO: Created: latency-svc-dr5nd
Feb 20 21:56:33.687: INFO: Got endpoints: latency-svc-xjzfg [753.85091ms]
Feb 20 21:56:33.699: INFO: Created: latency-svc-vsbvr
Feb 20 21:56:33.731: INFO: Got endpoints: latency-svc-7pftl [749.762107ms]
Feb 20 21:56:33.744: INFO: Created: latency-svc-jspk8
Feb 20 21:56:33.781: INFO: Got endpoints: latency-svc-tk28b [748.276071ms]
Feb 20 21:56:33.795: INFO: Created: latency-svc-hsvt5
Feb 20 21:56:33.834: INFO: Got endpoints: latency-svc-pfrrb [750.621106ms]
Feb 20 21:56:33.845: INFO: Created: latency-svc-prhxk
Feb 20 21:56:33.882: INFO: Got endpoints: latency-svc-5bd5q [749.955054ms]
Feb 20 21:56:33.892: INFO: Created: latency-svc-pbdmq
Feb 20 21:56:33.931: INFO: Got endpoints: latency-svc-l9tr5 [747.091357ms]
Feb 20 21:56:33.944: INFO: Created: latency-svc-tp87j
Feb 20 21:56:33.982: INFO: Got endpoints: latency-svc-l4qvj [749.731597ms]
Feb 20 21:56:33.994: INFO: Created: latency-svc-6xtct
Feb 20 21:56:34.035: INFO: Got endpoints: latency-svc-kq2r7 [753.083974ms]
Feb 20 21:56:34.049: INFO: Created: latency-svc-9cwpm
Feb 20 21:56:34.086: INFO: Got endpoints: latency-svc-4w2b7 [753.242912ms]
Feb 20 21:56:34.101: INFO: Created: latency-svc-mq6pz
Feb 20 21:56:34.133: INFO: Got endpoints: latency-svc-wnm97 [751.315699ms]
Feb 20 21:56:34.149: INFO: Created: latency-svc-ht4vj
Feb 20 21:56:34.181: INFO: Got endpoints: latency-svc-9sbcw [748.269956ms]
Feb 20 21:56:34.199: INFO: Created: latency-svc-nqkqb
Feb 20 21:56:34.235: INFO: Got endpoints: latency-svc-rz2k6 [753.461659ms]
Feb 20 21:56:34.253: INFO: Created: latency-svc-zb5cw
Feb 20 21:56:34.283: INFO: Got endpoints: latency-svc-282ns [751.015231ms]
Feb 20 21:56:34.332: INFO: Got endpoints: latency-svc-c2xkj [750.343727ms]
Feb 20 21:56:34.342: INFO: Created: latency-svc-zsjq2
Feb 20 21:56:34.350: INFO: Created: latency-svc-7rq8z
Feb 20 21:56:34.382: INFO: Got endpoints: latency-svc-dr5nd [748.957075ms]
Feb 20 21:56:34.400: INFO: Created: latency-svc-qhwt4
Feb 20 21:56:34.433: INFO: Got endpoints: latency-svc-vsbvr [745.909649ms]
Feb 20 21:56:34.482: INFO: Got endpoints: latency-svc-jspk8 [750.737487ms]
Feb 20 21:56:34.514: INFO: Created: latency-svc-nzn9w
Feb 20 21:56:34.521: INFO: Created: latency-svc-lbxx2
Feb 20 21:56:34.532: INFO: Got endpoints: latency-svc-hsvt5 [750.944329ms]
Feb 20 21:56:34.544: INFO: Created: latency-svc-s5g6v
Feb 20 21:56:34.581: INFO: Got endpoints: latency-svc-prhxk [747.635299ms]
Feb 20 21:56:34.594: INFO: Created: latency-svc-x6czn
Feb 20 21:56:34.635: INFO: Got endpoints: latency-svc-pbdmq [753.21581ms]
Feb 20 21:56:34.646: INFO: Created: latency-svc-hsrc5
Feb 20 21:56:34.682: INFO: Got endpoints: latency-svc-tp87j [750.64733ms]
Feb 20 21:56:34.701: INFO: Created: latency-svc-84nwz
Feb 20 21:56:34.733: INFO: Got endpoints: latency-svc-6xtct [750.817553ms]
Feb 20 21:56:34.748: INFO: Created: latency-svc-wpmvt
Feb 20 21:56:34.782: INFO: Got endpoints: latency-svc-9cwpm [746.561067ms]
Feb 20 21:56:34.793: INFO: Created: latency-svc-cmfw2
Feb 20 21:56:34.833: INFO: Got endpoints: latency-svc-mq6pz [747.654373ms]
Feb 20 21:56:34.846: INFO: Created: latency-svc-f6w7x
Feb 20 21:56:34.881: INFO: Got endpoints: latency-svc-ht4vj [748.297278ms]
Feb 20 21:56:34.897: INFO: Created: latency-svc-787vm
Feb 20 21:56:34.931: INFO: Got endpoints: latency-svc-nqkqb [750.073302ms]
Feb 20 21:56:34.944: INFO: Created: latency-svc-74cpl
Feb 20 21:56:34.982: INFO: Got endpoints: latency-svc-zb5cw [746.333386ms]
Feb 20 21:56:34.995: INFO: Created: latency-svc-dnn2q
Feb 20 21:56:35.033: INFO: Got endpoints: latency-svc-zsjq2 [749.64235ms]
Feb 20 21:56:35.045: INFO: Created: latency-svc-kz9gq
Feb 20 21:56:35.084: INFO: Got endpoints: latency-svc-7rq8z [751.483316ms]
Feb 20 21:56:35.102: INFO: Created: latency-svc-m4fc8
Feb 20 21:56:35.133: INFO: Got endpoints: latency-svc-qhwt4 [750.670279ms]
Feb 20 21:56:35.148: INFO: Created: latency-svc-c4grm
Feb 20 21:56:35.182: INFO: Got endpoints: latency-svc-nzn9w [748.848015ms]
Feb 20 21:56:35.196: INFO: Created: latency-svc-q6b52
Feb 20 21:56:35.232: INFO: Got endpoints: latency-svc-lbxx2 [749.326637ms]
Feb 20 21:56:35.244: INFO: Created: latency-svc-78c4n
Feb 20 21:56:35.282: INFO: Got endpoints: latency-svc-s5g6v [749.405977ms]
Feb 20 21:56:35.332: INFO: Got endpoints: latency-svc-x6czn [750.853201ms]
Feb 20 21:56:35.382: INFO: Got endpoints: latency-svc-hsrc5 [747.56898ms]
Feb 20 21:56:35.432: INFO: Got endpoints: latency-svc-84nwz [750.299281ms]
Feb 20 21:56:35.491: INFO: Got endpoints: latency-svc-wpmvt [758.020609ms]
Feb 20 21:56:35.532: INFO: Got endpoints: latency-svc-cmfw2 [750.35704ms]
Feb 20 21:56:35.581: INFO: Got endpoints: latency-svc-f6w7x [747.90141ms]
Feb 20 21:56:35.634: INFO: Got endpoints: latency-svc-787vm [752.885088ms]
Feb 20 21:56:35.681: INFO: Got endpoints: latency-svc-74cpl [750.021455ms]
Feb 20 21:56:35.736: INFO: Got endpoints: latency-svc-dnn2q [753.988196ms]
Feb 20 21:56:35.782: INFO: Got endpoints: latency-svc-kz9gq [749.32177ms]
Feb 20 21:56:35.832: INFO: Got endpoints: latency-svc-m4fc8 [747.978782ms]
Feb 20 21:56:35.884: INFO: Got endpoints: latency-svc-c4grm [751.52444ms]
Feb 20 21:56:35.932: INFO: Got endpoints: latency-svc-q6b52 [749.958224ms]
Feb 20 21:56:35.984: INFO: Got endpoints: latency-svc-78c4n [752.192178ms]
Feb 20 21:56:35.984: INFO: Latencies: [15.282663ms 23.215787ms 30.623471ms 37.798033ms 44.925865ms 77.12889ms 77.309272ms 109.683746ms 110.08357ms 117.78952ms 124.257017ms 134.685298ms 138.501833ms 140.746331ms 142.337285ms 150.440913ms 151.691615ms 158.847874ms 159.181718ms 162.70055ms 164.257304ms 165.572958ms 167.377179ms 167.64699ms 168.834093ms 170.58816ms 171.963959ms 172.875364ms 172.987031ms 173.863236ms 174.751174ms 174.931298ms 176.046629ms 182.299882ms 188.440598ms 188.665993ms 189.433256ms 196.714686ms 230.013859ms 259.809754ms 300.475701ms 336.271079ms 378.689947ms 418.593555ms 459.507065ms 501.988398ms 546.391746ms 600.585747ms 635.696328ms 639.555562ms 686.54152ms 702.99484ms 719.894172ms 722.772041ms 723.340793ms 727.50285ms 730.543644ms 734.734814ms 741.354385ms 741.444331ms 742.91566ms 744.197796ms 745.909649ms 746.333386ms 746.561067ms 746.78232ms 746.787433ms 746.98249ms 747.091357ms 747.277049ms 747.352555ms 747.56898ms 747.629805ms 747.635299ms 747.654373ms 747.697275ms 747.810728ms 747.90141ms 747.978782ms 748.181093ms 748.269956ms 748.276071ms 748.297278ms 748.343909ms 748.475676ms 748.848015ms 748.89698ms 748.914811ms 748.956053ms 748.957075ms 749.011101ms 749.121094ms 749.157584ms 749.228142ms 749.32177ms 749.326637ms 749.378324ms 749.405977ms 749.432458ms 749.480822ms 749.501594ms 749.502362ms 749.50909ms 749.552537ms 749.56437ms 749.61874ms 749.64235ms 749.652784ms 749.672414ms 749.701283ms 749.724082ms 749.730379ms 749.731597ms 749.750862ms 749.754972ms 749.762107ms 749.782895ms 749.827352ms 749.879428ms 749.915701ms 749.955054ms 749.958224ms 749.987968ms 750.021455ms 750.03249ms 750.045366ms 750.073302ms 750.112687ms 750.162903ms 750.167868ms 750.169249ms 750.202748ms 750.216358ms 750.270804ms 750.299281ms 750.321164ms 750.331021ms 750.343727ms 750.35704ms 750.378263ms 750.401907ms 750.476001ms 750.516192ms 750.558044ms 750.566352ms 750.572701ms 750.576154ms 750.621106ms 750.641229ms 750.64733ms 750.658586ms 750.670279ms 750.730061ms 750.737487ms 750.760413ms 750.803485ms 750.814743ms 750.817553ms 750.853201ms 750.854914ms 750.861811ms 750.944329ms 751.015231ms 751.018452ms 751.03197ms 751.090739ms 751.114709ms 751.126093ms 751.315699ms 751.463353ms 751.483316ms 751.52444ms 751.62835ms 752.192178ms 752.247505ms 752.318047ms 752.323075ms 752.408819ms 752.412593ms 752.719853ms 752.885088ms 752.889362ms 753.083974ms 753.21581ms 753.226927ms 753.242912ms 753.406102ms 753.461659ms 753.85091ms 753.988196ms 755.556972ms 756.23959ms 757.74372ms 758.020609ms 758.899916ms 761.92605ms 769.31393ms 775.638013ms 777.329592ms 778.609094ms]
Feb 20 21:56:35.984: INFO: 50 %ile: 749.501594ms
Feb 20 21:56:35.984: INFO: 90 %ile: 752.885088ms
Feb 20 21:56:35.984: INFO: 99 %ile: 777.329592ms
Feb 20 21:56:35.984: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:56:35.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-v9kgt" for this suite.
Feb 20 21:56:54.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:56:54.084: INFO: namespace: e2e-tests-svc-latency-v9kgt, resource: bindings, ignored listing per whitelist
Feb 20 21:56:54.187: INFO: namespace e2e-tests-svc-latency-v9kgt deletion completed in 18.198080923s

â€¢ [SLOW TEST:29.175 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:56:54.187: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-qzvlg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 20 21:56:54.418: INFO: Waiting up to 5m0s for pod "pod-6f6ad782-355a-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-qzvlg" to be "success or failure"
Feb 20 21:56:54.422: INFO: Pod "pod-6f6ad782-355a-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.849977ms
Feb 20 21:56:56.426: INFO: Pod "pod-6f6ad782-355a-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008232169s
STEP: Saw pod success
Feb 20 21:56:56.426: INFO: Pod "pod-6f6ad782-355a-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:56:56.430: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-6f6ad782-355a-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 21:56:56.448: INFO: Waiting for pod pod-6f6ad782-355a-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:56:56.451: INFO: Pod pod-6f6ad782-355a-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:56:56.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qzvlg" for this suite.
Feb 20 21:57:02.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:57:02.545: INFO: namespace: e2e-tests-emptydir-qzvlg, resource: bindings, ignored listing per whitelist
Feb 20 21:57:02.612: INFO: namespace e2e-tests-emptydir-qzvlg deletion completed in 6.156710578s

â€¢ [SLOW TEST:8.425 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:57:02.612: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-kjgkm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 21:57:03.101: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74979ffb-355a-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-kjgkm" to be "success or failure"
Feb 20 21:57:03.106: INFO: Pod "downwardapi-volume-74979ffb-355a-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.579898ms
Feb 20 21:57:05.111: INFO: Pod "downwardapi-volume-74979ffb-355a-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010509284s
STEP: Saw pod success
Feb 20 21:57:05.111: INFO: Pod "downwardapi-volume-74979ffb-355a-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:57:05.115: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-74979ffb-355a-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 21:57:05.132: INFO: Waiting for pod downwardapi-volume-74979ffb-355a-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:57:05.136: INFO: Pod downwardapi-volume-74979ffb-355a-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:57:05.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kjgkm" for this suite.
Feb 20 21:57:11.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:57:11.344: INFO: namespace: e2e-tests-downward-api-kjgkm, resource: bindings, ignored listing per whitelist
Feb 20 21:57:11.362: INFO: namespace e2e-tests-downward-api-kjgkm deletion completed in 6.220268514s

â€¢ [SLOW TEST:8.750 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:57:11.362: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-f8b9n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 21:57:11.615: INFO: Waiting up to 5m0s for pod "downward-api-79aae5ee-355a-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-f8b9n" to be "success or failure"
Feb 20 21:57:11.621: INFO: Pod "downward-api-79aae5ee-355a-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.271138ms
Feb 20 21:57:13.626: INFO: Pod "downward-api-79aae5ee-355a-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011363553s
STEP: Saw pod success
Feb 20 21:57:13.626: INFO: Pod "downward-api-79aae5ee-355a-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 21:57:13.631: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downward-api-79aae5ee-355a-11e9-8ceb-aaeae242ec87 container dapi-container: <nil>
STEP: delete the pod
Feb 20 21:57:13.649: INFO: Waiting for pod downward-api-79aae5ee-355a-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 21:57:13.654: INFO: Pod downward-api-79aae5ee-355a-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 21:57:13.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-f8b9n" for this suite.
Feb 20 21:57:19.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 21:57:19.792: INFO: namespace: e2e-tests-downward-api-f8b9n, resource: bindings, ignored listing per whitelist
Feb 20 21:57:19.878: INFO: namespace e2e-tests-downward-api-f8b9n deletion completed in 6.218504457s

â€¢ [SLOW TEST:8.516 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 21:57:19.879: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zdpds
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 20 21:57:20.100: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 20 21:57:20.100: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 21:57:20.504: INFO: stderr: ""
Feb 20 21:57:20.504: INFO: stdout: "service/redis-slave created\n"
Feb 20 21:57:20.504: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 20 21:57:20.504: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 21:57:20.745: INFO: stderr: ""
Feb 20 21:57:20.745: INFO: stdout: "service/redis-master created\n"
Feb 20 21:57:20.745: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 20 21:57:20.745: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 21:57:20.992: INFO: stderr: ""
Feb 20 21:57:20.992: INFO: stdout: "service/frontend created\n"
Feb 20 21:57:20.993: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 20 21:57:20.993: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 21:57:21.214: INFO: stderr: ""
Feb 20 21:57:21.214: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 20 21:57:21.214: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 20 21:57:21.214: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 21:57:21.439: INFO: stderr: ""
Feb 20 21:57:21.439: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 20 21:57:21.439: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 20 21:57:21.439: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 21:57:21.651: INFO: stderr: ""
Feb 20 21:57:21.651: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 20 21:57:21.651: INFO: Waiting for all frontend pods to be Running.
Feb 20 22:00:01.711: INFO: Waiting for frontend to serve content.
Feb 20 22:00:06.773: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 20 22:00:16.835: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 20 22:00:26.955: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 20 22:00:32.045: INFO: Trying to add a new entry to the guestbook.
Feb 20 22:00:32.169: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 20 22:00:32.293: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 22:00:32.473: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 22:00:32.473: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 22:00:32.473: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 22:00:32.595: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 22:00:32.595: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 22:00:32.595: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 22:00:32.702: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 22:00:32.702: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 22:00:32.702: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 22:00:32.807: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 22:00:32.807: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 22:00:32.807: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 22:00:32.932: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 22:00:32.932: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 22:00:32.932: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zdpds'
Feb 20 22:00:33.040: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 22:00:33.040: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:00:33.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zdpds" for this suite.
Feb 20 22:01:11.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:01:11.217: INFO: namespace: e2e-tests-kubectl-zdpds, resource: bindings, ignored listing per whitelist
Feb 20 22:01:11.285: INFO: namespace e2e-tests-kubectl-zdpds deletion completed in 38.23927479s

â€¢ [SLOW TEST:231.406 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:01:11.285: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-snhlf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-t4rg
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 22:01:11.583: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-t4rg" in namespace "e2e-tests-subpath-snhlf" to be "success or failure"
Feb 20 22:01:11.586: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.331184ms
Feb 20 22:01:13.592: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009374592s
Feb 20 22:01:15.599: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Running", Reason="", readiness=false. Elapsed: 4.016045876s
Feb 20 22:01:17.604: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Running", Reason="", readiness=false. Elapsed: 6.021424685s
Feb 20 22:01:19.611: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Running", Reason="", readiness=false. Elapsed: 8.028257041s
Feb 20 22:01:21.617: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Running", Reason="", readiness=false. Elapsed: 10.033714756s
Feb 20 22:01:23.622: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Running", Reason="", readiness=false. Elapsed: 12.03871315s
Feb 20 22:01:25.627: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Running", Reason="", readiness=false. Elapsed: 14.043988058s
Feb 20 22:01:27.632: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Running", Reason="", readiness=false. Elapsed: 16.048704848s
Feb 20 22:01:29.637: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Running", Reason="", readiness=false. Elapsed: 18.054006293s
Feb 20 22:01:31.643: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Running", Reason="", readiness=false. Elapsed: 20.059520133s
Feb 20 22:01:33.648: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Running", Reason="", readiness=false. Elapsed: 22.064526659s
Feb 20 22:01:35.653: INFO: Pod "pod-subpath-test-projected-t4rg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.070020833s
STEP: Saw pod success
Feb 20 22:01:35.653: INFO: Pod "pod-subpath-test-projected-t4rg" satisfied condition "success or failure"
Feb 20 22:01:35.656: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-subpath-test-projected-t4rg container test-container-subpath-projected-t4rg: <nil>
STEP: delete the pod
Feb 20 22:01:35.680: INFO: Waiting for pod pod-subpath-test-projected-t4rg to disappear
Feb 20 22:01:35.686: INFO: Pod pod-subpath-test-projected-t4rg no longer exists
STEP: Deleting pod pod-subpath-test-projected-t4rg
Feb 20 22:01:35.686: INFO: Deleting pod "pod-subpath-test-projected-t4rg" in namespace "e2e-tests-subpath-snhlf"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:01:35.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-snhlf" for this suite.
Feb 20 22:01:41.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:01:41.799: INFO: namespace: e2e-tests-subpath-snhlf, resource: bindings, ignored listing per whitelist
Feb 20 22:01:41.938: INFO: namespace e2e-tests-subpath-snhlf deletion completed in 6.227033574s

â€¢ [SLOW TEST:30.653 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:01:41.941: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-vdgbc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 20 22:01:42.231: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-vdgbc,SelfLink:/api/v1/namespaces/e2e-tests-watch-vdgbc/configmaps/e2e-watch-test-label-changed,UID:1af4fcbf-355b-11e9-91b0-3e11857925bf,ResourceVersion:16128,Generation:0,CreationTimestamp:2019-02-20 22:01:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 22:01:42.231: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-vdgbc,SelfLink:/api/v1/namespaces/e2e-tests-watch-vdgbc/configmaps/e2e-watch-test-label-changed,UID:1af4fcbf-355b-11e9-91b0-3e11857925bf,ResourceVersion:16129,Generation:0,CreationTimestamp:2019-02-20 22:01:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 20 22:01:42.231: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-vdgbc,SelfLink:/api/v1/namespaces/e2e-tests-watch-vdgbc/configmaps/e2e-watch-test-label-changed,UID:1af4fcbf-355b-11e9-91b0-3e11857925bf,ResourceVersion:16130,Generation:0,CreationTimestamp:2019-02-20 22:01:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 20 22:01:52.264: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-vdgbc,SelfLink:/api/v1/namespaces/e2e-tests-watch-vdgbc/configmaps/e2e-watch-test-label-changed,UID:1af4fcbf-355b-11e9-91b0-3e11857925bf,ResourceVersion:16151,Generation:0,CreationTimestamp:2019-02-20 22:01:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 22:01:52.265: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-vdgbc,SelfLink:/api/v1/namespaces/e2e-tests-watch-vdgbc/configmaps/e2e-watch-test-label-changed,UID:1af4fcbf-355b-11e9-91b0-3e11857925bf,ResourceVersion:16152,Generation:0,CreationTimestamp:2019-02-20 22:01:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 20 22:01:52.265: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-vdgbc,SelfLink:/api/v1/namespaces/e2e-tests-watch-vdgbc/configmaps/e2e-watch-test-label-changed,UID:1af4fcbf-355b-11e9-91b0-3e11857925bf,ResourceVersion:16153,Generation:0,CreationTimestamp:2019-02-20 22:01:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:01:52.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-vdgbc" for this suite.
Feb 20 22:01:58.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:01:58.313: INFO: namespace: e2e-tests-watch-vdgbc, resource: bindings, ignored listing per whitelist
Feb 20 22:01:58.459: INFO: namespace e2e-tests-watch-vdgbc deletion completed in 6.188650257s

â€¢ [SLOW TEST:16.519 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:01:58.460: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-5pppt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-24c75b8c-355b-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 22:01:58.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-24c8057e-355b-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-configmap-5pppt" to be "success or failure"
Feb 20 22:01:58.700: INFO: Pod "pod-configmaps-24c8057e-355b-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.239644ms
Feb 20 22:02:00.735: INFO: Pod "pod-configmaps-24c8057e-355b-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.038304135s
STEP: Saw pod success
Feb 20 22:02:00.735: INFO: Pod "pod-configmaps-24c8057e-355b-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:02:00.739: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-configmaps-24c8057e-355b-11e9-8ceb-aaeae242ec87 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 22:02:00.764: INFO: Waiting for pod pod-configmaps-24c8057e-355b-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:02:00.768: INFO: Pod pod-configmaps-24c8057e-355b-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:02:00.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5pppt" for this suite.
Feb 20 22:02:06.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:02:06.855: INFO: namespace: e2e-tests-configmap-5pppt, resource: bindings, ignored listing per whitelist
Feb 20 22:02:06.984: INFO: namespace e2e-tests-configmap-5pppt deletion completed in 6.211771624s

â€¢ [SLOW TEST:8.525 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:02:06.987: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-dbt6h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 22:02:07.251: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 20 22:02:07.259: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 20 22:02:12.266: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 20 22:02:12.266: INFO: Creating deployment "test-rolling-update-deployment"
Feb 20 22:02:12.271: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 20 22:02:12.279: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 20 22:02:14.289: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 20 22:02:14.292: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 22:02:14.304: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-dbt6h,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dbt6h/deployments/test-rolling-update-deployment,UID:2ce00a85-355b-11e9-91b0-3e11857925bf,ResourceVersion:16255,Generation:1,CreationTimestamp:2019-02-20 22:02:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-20 22:02:12 +0000 UTC 2019-02-20 22:02:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-20 22:02:13 +0000 UTC 2019-02-20 22:02:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 22:02:14.315: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-dbt6h,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dbt6h/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:2ce29552-355b-11e9-91b0-3e11857925bf,ResourceVersion:16248,Generation:1,CreationTimestamp:2019-02-20 22:02:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2ce00a85-355b-11e9-91b0-3e11857925bf 0xc002f1cbe7 0xc002f1cbe8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 20 22:02:14.315: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 20 22:02:14.315: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-dbt6h,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-dbt6h/replicasets/test-rolling-update-controller,UID:29e2cc02-355b-11e9-91b0-3e11857925bf,ResourceVersion:16254,Generation:2,CreationTimestamp:2019-02-20 22:02:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2ce00a85-355b-11e9-91b0-3e11857925bf 0xc002f1cb27 0xc002f1cb28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 22:02:14.322: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-7gdfw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-7gdfw,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-dbt6h,SelfLink:/api/v1/namespaces/e2e-tests-deployment-dbt6h/pods/test-rolling-update-deployment-68b55d7bc6-7gdfw,UID:2ce2fa62-355b-11e9-91b0-3e11857925bf,ResourceVersion:16247,Generation:0,CreationTimestamp:2019-02-20 22:02:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.171/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 2ce29552-355b-11e9-91b0-3e11857925bf 0xc0010e9ed7 0xc0010e9ed8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fm45b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fm45b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-fm45b true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e9f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e9f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 22:02:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 22:02:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 22:02:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 22:02:12 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:100.96.1.171,StartTime:2019-02-20 22:02:12 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-20 22:02:13 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://0d284264805b4d2e641d4ded6358d70bd9054751543194a90627e1be7c17fb00}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:02:14.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-dbt6h" for this suite.
Feb 20 22:02:20.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:02:20.506: INFO: namespace: e2e-tests-deployment-dbt6h, resource: bindings, ignored listing per whitelist
Feb 20 22:02:20.518: INFO: namespace e2e-tests-deployment-dbt6h deletion completed in 6.189226632s

â€¢ [SLOW TEST:13.531 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:02:20.518: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-4nx94
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 20 22:02:20.771: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 22:02:20.779: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 22:02:20.782: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs before test
Feb 20 22:02:20.798: INFO: addons-nginx-ingress-controller-d74bfff57-wsvnp from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.798: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 20 22:02:20.798: INFO: addons-kubernetes-dashboard-6579b646c5-ttbst from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.798: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 20 22:02:20.798: INFO: calico-node-9f88m from kube-system started at 2019-02-20 20:36:04 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.798: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:02:20.798: INFO: kube-proxy-slfj9 from kube-system started at 2019-02-20 20:36:04 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.798: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:02:20.798: INFO: node-exporter-zfb8s from kube-system started at 2019-02-20 20:36:04 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.798: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:02:20.798: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-h9ds6 from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.798: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 20 22:02:20.798: INFO: vpn-shoot-67f9c4c5ff-z2l8j from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.798: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 20 22:02:20.798: INFO: addons-kube-lego-69bbdc96b6-7c626 from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.798: INFO: 	Container kube-lego ready: true, restart count 0
Feb 20 22:02:20.798: INFO: metrics-server-7594c945c8-pndpd from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.799: INFO: 	Container metrics-server ready: true, restart count 0
Feb 20 22:02:20.799: INFO: coredns-67df79bbdd-6dv5v from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.799: INFO: 	Container coredns ready: true, restart count 0
Feb 20 22:02:20.799: INFO: blackbox-exporter-d6c46f9fc-jt4nc from kube-system started at 2019-02-20 20:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.799: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 20 22:02:20.799: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq before test
Feb 20 22:02:20.851: INFO: node-exporter-kqq7b from kube-system started at 2019-02-20 20:36:26 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.851: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:02:20.851: INFO: calico-node-wdqxm from kube-system started at 2019-02-20 20:36:26 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.851: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:02:20.851: INFO: kube-proxy-pp9s2 from kube-system started at 2019-02-20 20:36:26 +0000 UTC (1 container statuses recorded)
Feb 20 22:02:20.851: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1585318fe9f267bf], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:02:21.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-4nx94" for this suite.
Feb 20 22:02:27.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:02:28.022: INFO: namespace: e2e-tests-sched-pred-4nx94, resource: bindings, ignored listing per whitelist
Feb 20 22:02:28.077: INFO: namespace e2e-tests-sched-pred-4nx94 deletion completed in 6.193242735s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.559 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:02:28.078: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-qzcs6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-qzcs6
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qzcs6 to expose endpoints map[]
Feb 20 22:02:28.354: INFO: Get endpoints failed (3.315215ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 20 22:02:29.359: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qzcs6 exposes endpoints map[] (1.008059972s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-qzcs6
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qzcs6 to expose endpoints map[pod1:[80]]
Feb 20 22:02:33.414: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.048198018s elapsed, will retry)
Feb 20 22:02:38.463: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (9.09721385s elapsed, will retry)
Feb 20 22:02:43.507: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (14.142145686s elapsed, will retry)
Feb 20 22:02:45.530: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qzcs6 exposes endpoints map[pod1:[80]] (16.164410217s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-qzcs6
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qzcs6 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 20 22:02:47.573: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qzcs6 exposes endpoints map[pod1:[80] pod2:[80]] (2.038131467s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-qzcs6
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qzcs6 to expose endpoints map[pod2:[80]]
Feb 20 22:02:47.591: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qzcs6 exposes endpoints map[pod2:[80]] (7.508363ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-qzcs6
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-qzcs6 to expose endpoints map[]
Feb 20 22:02:48.605: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-qzcs6 exposes endpoints map[] (1.008626179s elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:02:48.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-qzcs6" for this suite.
Feb 20 22:03:10.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:03:10.728: INFO: namespace: e2e-tests-services-qzcs6, resource: bindings, ignored listing per whitelist
Feb 20 22:03:10.825: INFO: namespace e2e-tests-services-qzcs6 deletion completed in 22.19564362s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:42.747 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:03:10.825: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-tml6n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 22:03:11.062: INFO: Creating ReplicaSet my-hostname-basic-4feb3693-355b-11e9-8ceb-aaeae242ec87
Feb 20 22:03:11.072: INFO: Pod name my-hostname-basic-4feb3693-355b-11e9-8ceb-aaeae242ec87: Found 0 pods out of 1
Feb 20 22:03:16.077: INFO: Pod name my-hostname-basic-4feb3693-355b-11e9-8ceb-aaeae242ec87: Found 1 pods out of 1
Feb 20 22:03:16.077: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4feb3693-355b-11e9-8ceb-aaeae242ec87" is running
Feb 20 22:03:16.080: INFO: Pod "my-hostname-basic-4feb3693-355b-11e9-8ceb-aaeae242ec87-v6hm4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 22:03:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 22:03:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 22:03:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 22:03:11 +0000 UTC Reason: Message:}])
Feb 20 22:03:16.080: INFO: Trying to dial the pod
Feb 20 22:03:21.173: INFO: Controller my-hostname-basic-4feb3693-355b-11e9-8ceb-aaeae242ec87: Got expected result from replica 1 [my-hostname-basic-4feb3693-355b-11e9-8ceb-aaeae242ec87-v6hm4]: "my-hostname-basic-4feb3693-355b-11e9-8ceb-aaeae242ec87-v6hm4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:03:21.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-tml6n" for this suite.
Feb 20 22:03:27.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:03:27.400: INFO: namespace: e2e-tests-replicaset-tml6n, resource: bindings, ignored listing per whitelist
Feb 20 22:03:27.400: INFO: namespace e2e-tests-replicaset-tml6n deletion completed in 6.220488155s

â€¢ [SLOW TEST:16.575 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:03:27.401: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-t8h9q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 22:03:27.670: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 20 22:03:27.677: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-t8h9q/daemonsets","resourceVersion":"16496"},"items":null}

Feb 20 22:03:27.680: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-t8h9q/pods","resourceVersion":"16496"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:03:27.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-t8h9q" for this suite.
Feb 20 22:03:33.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:03:33.792: INFO: namespace: e2e-tests-daemonsets-t8h9q, resource: bindings, ignored listing per whitelist
Feb 20 22:03:33.928: INFO: namespace e2e-tests-daemonsets-t8h9q deletion completed in 6.231334201s

S [SKIPPING] [6.527 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 20 22:03:27.670: Requires at least 2 nodes (not -1)

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:03:33.928: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-slpdd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 20 22:03:34.237: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 20 22:03:39.244: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:03:40.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-slpdd" for this suite.
Feb 20 22:03:46.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:03:46.424: INFO: namespace: e2e-tests-replication-controller-slpdd, resource: bindings, ignored listing per whitelist
Feb 20 22:03:46.474: INFO: namespace e2e-tests-replication-controller-slpdd deletion completed in 6.206885244s

â€¢ [SLOW TEST:12.546 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:03:46.474: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-lklg4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 22:03:46.767: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65326838-355b-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-lklg4" to be "success or failure"
Feb 20 22:03:46.773: INFO: Pod "downwardapi-volume-65326838-355b-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.320297ms
Feb 20 22:03:48.779: INFO: Pod "downwardapi-volume-65326838-355b-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011282366s
STEP: Saw pod success
Feb 20 22:03:48.779: INFO: Pod "downwardapi-volume-65326838-355b-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:03:48.785: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-65326838-355b-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 22:03:48.804: INFO: Waiting for pod downwardapi-volume-65326838-355b-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:03:48.807: INFO: Pod downwardapi-volume-65326838-355b-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:03:48.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lklg4" for this suite.
Feb 20 22:03:54.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:03:55.030: INFO: namespace: e2e-tests-downward-api-lklg4, resource: bindings, ignored listing per whitelist
Feb 20 22:03:55.045: INFO: namespace e2e-tests-downward-api-lklg4 deletion completed in 6.233599352s

â€¢ [SLOW TEST:8.571 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:03:55.045: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tj4d5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 22:03:55.354: INFO: Waiting up to 5m0s for pod "downward-api-6a50a2bf-355b-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-tj4d5" to be "success or failure"
Feb 20 22:03:55.358: INFO: Pod "downward-api-6a50a2bf-355b-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.413704ms
Feb 20 22:03:57.363: INFO: Pod "downward-api-6a50a2bf-355b-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008380082s
STEP: Saw pod success
Feb 20 22:03:57.363: INFO: Pod "downward-api-6a50a2bf-355b-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:03:57.366: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downward-api-6a50a2bf-355b-11e9-8ceb-aaeae242ec87 container dapi-container: <nil>
STEP: delete the pod
Feb 20 22:03:57.389: INFO: Waiting for pod downward-api-6a50a2bf-355b-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:03:57.393: INFO: Pod downward-api-6a50a2bf-355b-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:03:57.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tj4d5" for this suite.
Feb 20 22:04:03.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:04:03.574: INFO: namespace: e2e-tests-downward-api-tj4d5, resource: bindings, ignored listing per whitelist
Feb 20 22:04:03.645: INFO: namespace e2e-tests-downward-api-tj4d5 deletion completed in 6.247566575s

â€¢ [SLOW TEST:8.600 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:04:03.645: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qnqw8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 22:04:03.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f6da170-355b-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-qnqw8" to be "success or failure"
Feb 20 22:04:03.939: INFO: Pod "downwardapi-volume-6f6da170-355b-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.606055ms
Feb 20 22:04:05.945: INFO: Pod "downwardapi-volume-6f6da170-355b-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011409229s
STEP: Saw pod success
Feb 20 22:04:05.945: INFO: Pod "downwardapi-volume-6f6da170-355b-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:04:05.950: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-6f6da170-355b-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 22:04:05.974: INFO: Waiting for pod downwardapi-volume-6f6da170-355b-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:04:05.977: INFO: Pod downwardapi-volume-6f6da170-355b-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:04:05.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qnqw8" for this suite.
Feb 20 22:04:11.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:04:12.185: INFO: namespace: e2e-tests-downward-api-qnqw8, resource: bindings, ignored listing per whitelist
Feb 20 22:04:12.268: INFO: namespace e2e-tests-downward-api-qnqw8 deletion completed in 6.285901643s

â€¢ [SLOW TEST:8.623 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:04:12.268: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-89pps
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 20 22:04:12.540: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml cluster-info'
Feb 20 22:04:12.747: INFO: stderr: ""
Feb 20 22:04:12.747: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:04:12.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-89pps" for this suite.
Feb 20 22:04:18.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:04:18.919: INFO: namespace: e2e-tests-kubectl-89pps, resource: bindings, ignored listing per whitelist
Feb 20 22:04:19.020: INFO: namespace e2e-tests-kubectl-89pps deletion completed in 6.268892917s

â€¢ [SLOW TEST:6.752 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:04:19.020: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8htvp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 22:04:19.290: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7894abbf-355b-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-8htvp" to be "success or failure"
Feb 20 22:04:19.295: INFO: Pod "downwardapi-volume-7894abbf-355b-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.260784ms
Feb 20 22:04:21.301: INFO: Pod "downwardapi-volume-7894abbf-355b-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010343545s
STEP: Saw pod success
Feb 20 22:04:21.301: INFO: Pod "downwardapi-volume-7894abbf-355b-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:04:21.304: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-7894abbf-355b-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 22:04:21.326: INFO: Waiting for pod downwardapi-volume-7894abbf-355b-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:04:21.330: INFO: Pod downwardapi-volume-7894abbf-355b-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:04:21.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8htvp" for this suite.
Feb 20 22:04:27.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:04:27.573: INFO: namespace: e2e-tests-projected-8htvp, resource: bindings, ignored listing per whitelist
Feb 20 22:04:27.588: INFO: namespace e2e-tests-projected-8htvp deletion completed in 6.252543585s

â€¢ [SLOW TEST:8.568 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:04:27.588: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qxrq7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 20 22:04:27.880: INFO: namespace e2e-tests-kubectl-qxrq7
Feb 20 22:04:27.880: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-qxrq7'
Feb 20 22:04:28.248: INFO: stderr: ""
Feb 20 22:04:28.248: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 20 22:04:29.253: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 22:04:29.254: INFO: Found 0 / 1
Feb 20 22:04:30.253: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 22:04:30.253: INFO: Found 1 / 1
Feb 20 22:04:30.253: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 22:04:30.257: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 22:04:30.257: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 22:04:30.257: INFO: wait on redis-master startup in e2e-tests-kubectl-qxrq7 
Feb 20 22:04:30.257: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs redis-master-vv98p redis-master --namespace=e2e-tests-kubectl-qxrq7'
Feb 20 22:04:30.374: INFO: stderr: ""
Feb 20 22:04:30.374: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 20 Feb 22:04:29.095 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Feb 22:04:29.095 # Server started, Redis version 3.2.12\n1:M 20 Feb 22:04:29.095 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Feb 22:04:29.095 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 20 22:04:30.374: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-qxrq7'
Feb 20 22:04:30.537: INFO: stderr: ""
Feb 20 22:04:30.538: INFO: stdout: "service/rm2 exposed\n"
Feb 20 22:04:30.541: INFO: Service rm2 in namespace e2e-tests-kubectl-qxrq7 found.
STEP: exposing service
Feb 20 22:04:32.550: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-qxrq7'
Feb 20 22:04:32.689: INFO: stderr: ""
Feb 20 22:04:32.689: INFO: stdout: "service/rm3 exposed\n"
Feb 20 22:04:32.693: INFO: Service rm3 in namespace e2e-tests-kubectl-qxrq7 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:04:34.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qxrq7" for this suite.
Feb 20 22:04:56.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:04:56.950: INFO: namespace: e2e-tests-kubectl-qxrq7, resource: bindings, ignored listing per whitelist
Feb 20 22:04:56.955: INFO: namespace e2e-tests-kubectl-qxrq7 deletion completed in 22.248932633s

â€¢ [SLOW TEST:29.367 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:04:56.956: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-8pfzc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 20 22:04:57.277: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-8pfzc,SelfLink:/api/v1/namespaces/e2e-tests-watch-8pfzc/configmaps/e2e-watch-test-resource-version,UID:8f360899-355b-11e9-91b0-3e11857925bf,ResourceVersion:16840,Generation:0,CreationTimestamp:2019-02-20 22:04:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 22:04:57.278: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-8pfzc,SelfLink:/api/v1/namespaces/e2e-tests-watch-8pfzc/configmaps/e2e-watch-test-resource-version,UID:8f360899-355b-11e9-91b0-3e11857925bf,ResourceVersion:16841,Generation:0,CreationTimestamp:2019-02-20 22:04:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:04:57.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-8pfzc" for this suite.
Feb 20 22:05:03.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:05:03.479: INFO: namespace: e2e-tests-watch-8pfzc, resource: bindings, ignored listing per whitelist
Feb 20 22:05:03.528: INFO: namespace e2e-tests-watch-8pfzc deletion completed in 6.245393399s

â€¢ [SLOW TEST:6.572 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:05:03.528: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2pn7c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 22:05:03.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93188d94-355b-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-2pn7c" to be "success or failure"
Feb 20 22:05:03.778: INFO: Pod "downwardapi-volume-93188d94-355b-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.450861ms
Feb 20 22:05:05.783: INFO: Pod "downwardapi-volume-93188d94-355b-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008497979s
STEP: Saw pod success
Feb 20 22:05:05.783: INFO: Pod "downwardapi-volume-93188d94-355b-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:05:05.786: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-93188d94-355b-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 22:05:05.816: INFO: Waiting for pod downwardapi-volume-93188d94-355b-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:05:05.821: INFO: Pod downwardapi-volume-93188d94-355b-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:05:05.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2pn7c" for this suite.
Feb 20 22:05:11.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:05:11.930: INFO: namespace: e2e-tests-projected-2pn7c, resource: bindings, ignored listing per whitelist
Feb 20 22:05:12.073: INFO: namespace e2e-tests-projected-2pn7c deletion completed in 6.246960649s

â€¢ [SLOW TEST:8.545 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:05:12.073: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-jghzp
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-9836303c-355b-11e9-8ceb-aaeae242ec87
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:05:14.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jghzp" for this suite.
Feb 20 22:05:36.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:05:36.588: INFO: namespace: e2e-tests-configmap-jghzp, resource: bindings, ignored listing per whitelist
Feb 20 22:05:36.683: INFO: namespace e2e-tests-configmap-jghzp deletion completed in 22.202053864s

â€¢ [SLOW TEST:24.610 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:05:36.683: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jm4fv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 22:05:36.956: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml version --client'
Feb 20 22:05:37.050: INFO: stderr: ""
Feb 20 22:05:37.050: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"archive\", BuildDate:\"2019-02-20T20:51:55Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 20 22:05:37.053: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-jm4fv'
Feb 20 22:05:37.430: INFO: stderr: ""
Feb 20 22:05:37.430: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 20 22:05:37.430: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-jm4fv'
Feb 20 22:05:37.708: INFO: stderr: ""
Feb 20 22:05:37.708: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 20 22:05:38.719: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 22:05:38.719: INFO: Found 1 / 1
Feb 20 22:05:38.719: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 22:05:38.723: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 22:05:38.723: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 22:05:38.723: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe pod redis-master-qzd8k --namespace=e2e-tests-kubectl-jm4fv'
Feb 20 22:05:38.934: INFO: stderr: ""
Feb 20 22:05:38.934: INFO: stdout: "Name:               redis-master-qzd8k\nNamespace:          e2e-tests-kubectl-jm4fv\nPriority:           0\nPriorityClassName:  <none>\nNode:               shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq/10.250.0.17\nStart Time:         Wed, 20 Feb 2019 22:05:37 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.183/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.183\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://e4b14d34e707318072d55ff8f8a7403fb35290004250c77e01889490e138bbde\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 20 Feb 2019 22:05:38 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fbkz5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-fbkz5:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-fbkz5\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                             Message\n  ----    ------     ----  ----                                                             -------\n  Normal  Scheduled  1s    default-scheduler                                                Successfully assigned e2e-tests-kubectl-jm4fv/redis-master-qzd8k to shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq\n  Normal  Pulled     0s    kubelet, shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    0s    kubelet, shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Created container\n  Normal  Started    0s    kubelet, shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq  Started container\n"
Feb 20 22:05:38.935: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe rc redis-master --namespace=e2e-tests-kubectl-jm4fv'
Feb 20 22:05:39.166: INFO: stderr: ""
Feb 20 22:05:39.166: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-jm4fv\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-qzd8k\n"
Feb 20 22:05:39.166: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe service redis-master --namespace=e2e-tests-kubectl-jm4fv'
Feb 20 22:05:39.345: INFO: stderr: ""
Feb 20 22:05:39.345: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-jm4fv\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.67.42.104\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.183:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 20 22:05:39.353: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs'
Feb 20 22:05:39.558: INFO: stderr: ""
Feb 20 22:05:39.558: INFO: stdout: "Name:               shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecf69421-6238-4433-a0a0-70bb24198e09\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/zone=rot_1_1\n                    kubernetes.io/hostname=shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.10/19\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 20 Feb 2019 20:35:58 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 20 Feb 2019 22:05:36 +0000   Wed, 20 Feb 2019 20:35:58 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 20 Feb 2019 22:05:36 +0000   Wed, 20 Feb 2019 20:35:58 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 20 Feb 2019 22:05:36 +0000   Wed, 20 Feb 2019 20:35:58 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 20 Feb 2019 22:05:36 +0000   Wed, 20 Feb 2019 20:37:49 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.0.10\nCapacity:\n cpu:                2\n ephemeral-storage:  38216108Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             4042360Ki\n pods:               110\nAllocatable:\n cpu:                1920m\n ephemeral-storage:  37176629834\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             2858665981\n pods:               110\nSystem Info:\n Machine ID:                 f32cbfab31204cefa940ea8299d3d65d\n System UUID:                F32CBFAB-3120-4CEF-A940-EA8299D3D65D\n Boot ID:                    3021d1e1-7be8-4af8-b9eb-84de8f89f3b9\n Kernel Version:             4.14.96-coreos\n OS Image:                   Container Linux by CoreOS 1967.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.13.3\n Kube-Proxy Version:         v1.13.3\nPodCIDR:                     100.96.0.0/24\nProviderID:                  openstack:///f32cbfab-3120-4cef-a940-ea8299d3d65d\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                              ------------  ----------  ---------------  -------------  ---\n  kube-system                addons-kube-lego-69bbdc96b6-7c626                                 20m (1%)      50m (2%)    8Mi (0%)         32Mi (1%)      93m\n  kube-system                addons-kubernetes-dashboard-6579b646c5-ttbst                      50m (2%)      100m (5%)   50Mi (1%)        256Mi (9%)     93m\n  kube-system                addons-nginx-ingress-controller-d74bfff57-wsvnp                   100m (5%)     2 (104%)    100Mi (3%)       800Mi (29%)    93m\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-h9ds6    0 (0%)        0 (0%)      0 (0%)           0 (0%)         93m\n  kube-system                blackbox-exporter-d6c46f9fc-jt4nc                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (1%)      93m\n  kube-system                calico-node-9f88m                                                 100m (5%)     500m (26%)  100Mi (3%)       700Mi (25%)    89m\n  kube-system                coredns-67df79bbdd-6dv5v                                          50m (2%)      100m (5%)   15Mi (0%)        100Mi (3%)     93m\n  kube-system                kube-proxy-slfj9                                                  20m (1%)      900m (46%)  64Mi (2%)        200Mi (7%)     89m\n  kube-system                metrics-server-7594c945c8-pndpd                                   20m (1%)      80m (4%)    100Mi (3%)       400Mi (14%)    93m\n  kube-system                node-exporter-zfb8s                                               5m (0%)       15m (0%)    10Mi (0%)        50Mi (1%)      89m\n  kube-system                vpn-shoot-67f9c4c5ff-z2l8j                                        50m (2%)      100m (5%)   50Mi (1%)        100Mi (3%)     93m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                420m (21%)   3855m (200%)\n  memory             502Mi (18%)  2673Mi (98%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:              <none>\n"
Feb 20 22:05:39.559: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe namespace e2e-tests-kubectl-jm4fv'
Feb 20 22:05:39.740: INFO: stderr: ""
Feb 20 22:05:39.740: INFO: stdout: "Name:         e2e-tests-kubectl-jm4fv\nLabels:       e2e-framework=kubectl\n              e2e-run=a0182cec-3551-11e9-8ceb-aaeae242ec87\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:05:39.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jm4fv" for this suite.
Feb 20 22:06:01.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:06:01.814: INFO: namespace: e2e-tests-kubectl-jm4fv, resource: bindings, ignored listing per whitelist
Feb 20 22:06:02.000: INFO: namespace e2e-tests-kubectl-jm4fv deletion completed in 22.253513416s

â€¢ [SLOW TEST:25.317 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:06:02.000: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-7q77b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-7q77b/configmap-test-b60518fd-355b-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 22:06:02.371: INFO: Waiting up to 5m0s for pod "pod-configmaps-b605dc36-355b-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-configmap-7q77b" to be "success or failure"
Feb 20 22:06:02.385: INFO: Pod "pod-configmaps-b605dc36-355b-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 13.870366ms
Feb 20 22:06:04.391: INFO: Pod "pod-configmaps-b605dc36-355b-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019625873s
STEP: Saw pod success
Feb 20 22:06:04.391: INFO: Pod "pod-configmaps-b605dc36-355b-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:06:04.399: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-configmaps-b605dc36-355b-11e9-8ceb-aaeae242ec87 container env-test: <nil>
STEP: delete the pod
Feb 20 22:06:04.421: INFO: Waiting for pod pod-configmaps-b605dc36-355b-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:06:04.424: INFO: Pod pod-configmaps-b605dc36-355b-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:06:04.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7q77b" for this suite.
Feb 20 22:06:10.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:06:10.525: INFO: namespace: e2e-tests-configmap-7q77b, resource: bindings, ignored listing per whitelist
Feb 20 22:06:10.677: INFO: namespace e2e-tests-configmap-7q77b deletion completed in 6.248278123s

â€¢ [SLOW TEST:8.676 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:06:10.677: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-79f9t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:06:33.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-79f9t" for this suite.
Feb 20 22:06:39.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:06:39.606: INFO: namespace: e2e-tests-container-runtime-79f9t, resource: bindings, ignored listing per whitelist
Feb 20 22:06:39.676: INFO: namespace e2e-tests-container-runtime-79f9t deletion completed in 6.178182397s

â€¢ [SLOW TEST:28.999 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:06:39.676: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qmd7r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 22:06:42.465: INFO: Successfully updated pod "annotationupdatecc66a1b7-355b-11e9-8ceb-aaeae242ec87"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:06:44.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qmd7r" for this suite.
Feb 20 22:07:06.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:07:06.618: INFO: namespace: e2e-tests-projected-qmd7r, resource: bindings, ignored listing per whitelist
Feb 20 22:07:06.729: INFO: namespace e2e-tests-projected-qmd7r deletion completed in 22.230538003s

â€¢ [SLOW TEST:27.053 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:07:06.730: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-qmjbw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 22:07:07.001: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:07:09.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-qmjbw" for this suite.
Feb 20 22:07:55.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:07:55.419: INFO: namespace: e2e-tests-pods-qmjbw, resource: bindings, ignored listing per whitelist
Feb 20 22:07:55.481: INFO: namespace e2e-tests-pods-qmjbw deletion completed in 46.212208428s

â€¢ [SLOW TEST:48.751 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:07:55.481: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-kflq4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 22:07:55.713: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-kflq4'
Feb 20 22:07:56.774: INFO: stderr: ""
Feb 20 22:07:56.774: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 20 22:08:01.825: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-kflq4 -o json'
Feb 20 22:08:02.009: INFO: stderr: ""
Feb 20 22:08:02.009: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.190/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-02-20T22:07:56Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-kflq4\",\n        \"resourceVersion\": \"17379\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-kflq4/pods/e2e-test-nginx-pod\",\n        \"uid\": \"fa344f63-355b-11e9-91b0-3e11857925bf\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-clpqq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-clpqq\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-clpqq\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T22:07:56Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T22:07:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T22:07:58Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T22:07:56Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a2a1c42f820546b4baf0af233f62894794b4ba2c79ca8754d2b2aec058121912\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-20T22:07:57Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.17\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.190\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-20T22:07:56Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 20 22:08:02.009: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml replace -f - --namespace=e2e-tests-kubectl-kflq4'
Feb 20 22:08:02.334: INFO: stderr: ""
Feb 20 22:08:02.334: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Feb 20 22:08:02.338: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-kflq4'
Feb 20 22:08:10.208: INFO: stderr: ""
Feb 20 22:08:10.208: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:08:10.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kflq4" for this suite.
Feb 20 22:08:16.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:08:16.405: INFO: namespace: e2e-tests-kubectl-kflq4, resource: bindings, ignored listing per whitelist
Feb 20 22:08:16.424: INFO: namespace e2e-tests-kubectl-kflq4 deletion completed in 6.211596777s

â€¢ [SLOW TEST:20.943 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:08:16.424: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-k59xp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 20 22:08:20.761: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 22:08:20.766: INFO: Pod pod-with-poststart-http-hook still exists
Feb 20 22:08:22.767: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 22:08:22.771: INFO: Pod pod-with-poststart-http-hook still exists
Feb 20 22:08:24.766: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 22:08:24.770: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:08:24.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-k59xp" for this suite.
Feb 20 22:08:46.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:08:46.820: INFO: namespace: e2e-tests-container-lifecycle-hook-k59xp, resource: bindings, ignored listing per whitelist
Feb 20 22:08:46.959: INFO: namespace e2e-tests-container-lifecycle-hook-k59xp deletion completed in 22.185159957s

â€¢ [SLOW TEST:30.535 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:08:46.959: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-hpdkf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-hpdkf A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-hpdkf A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-hpdkf.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-hpdkf.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-hpdkf.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 48.207.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.207.48_udp@PTR;check="$$(dig +tcp +noall +answer +search 48.207.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.207.48_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-hpdkf A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-hpdkf;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-hpdkf A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-hpdkf.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-hpdkf.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-hpdkf.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-hpdkf.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 48.207.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.207.48_udp@PTR;check="$$(dig +tcp +noall +answer +search 48.207.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.207.48_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 20 22:11:17.322: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.362: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.368: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.373: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.380: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.385: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.391: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.396: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.883: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.892: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.899: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.905: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.911: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.917: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.923: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.929: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.934: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.939: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.944: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:17.949: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:18.093: INFO: Lookups using e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf.svc wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-hpdkf jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf jessie_udp@dns-test-service.e2e-tests-dns-hpdkf.svc jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 20 22:11:23.100: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.143: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.149: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.155: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.161: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.166: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.172: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.179: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.678: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.684: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.690: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.695: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.703: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.708: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.714: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.721: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.729: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.734: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.740: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.745: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:23.872: INFO: Lookups using e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf.svc wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-hpdkf jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf jessie_udp@dns-test-service.e2e-tests-dns-hpdkf.svc jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 20 22:11:28.101: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.143: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.149: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.156: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.203: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.211: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.217: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.226: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.722: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.730: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.736: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.742: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.748: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.754: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.762: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.769: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.775: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.782: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.788: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.794: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87)
Feb 20 22:11:28.925: INFO: Lookups using e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf wheezy_udp@dns-test-service.e2e-tests-dns-hpdkf.svc wheezy_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-hpdkf jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf jessie_udp@dns-test-service.e2e-tests-dns-hpdkf.svc jessie_tcp@dns-test-service.e2e-tests-dns-hpdkf.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-hpdkf.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-hpdkf.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 20 22:11:35.237: INFO: DNS probes using e2e-tests-dns-hpdkf/dns-test-184846cc-355c-11e9-8ceb-aaeae242ec87 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:11:35.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-hpdkf" for this suite.
Feb 20 22:11:41.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:11:41.468: INFO: namespace: e2e-tests-dns-hpdkf, resource: bindings, ignored listing per whitelist
Feb 20 22:11:41.501: INFO: namespace e2e-tests-dns-hpdkf deletion completed in 6.203376578s

â€¢ [SLOW TEST:174.542 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:11:41.503: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-wn72z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 20 22:11:45.920: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:11:45.924: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 22:11:47.924: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:11:47.929: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 22:11:49.924: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:11:49.930: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 22:11:51.924: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:11:51.929: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 22:11:53.924: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:11:53.929: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 22:11:55.924: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:11:55.930: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 22:11:57.924: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:11:57.928: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 22:11:59.924: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:11:59.929: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 22:12:01.924: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:12:01.931: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 22:12:03.925: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:12:03.930: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 22:12:05.925: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 22:12:05.929: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:12:05.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-wn72z" for this suite.
Feb 20 22:12:25.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:12:25.985: INFO: namespace: e2e-tests-container-lifecycle-hook-wn72z, resource: bindings, ignored listing per whitelist
Feb 20 22:12:26.117: INFO: namespace e2e-tests-container-lifecycle-hook-wn72z deletion completed in 20.169931275s

â€¢ [SLOW TEST:44.614 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:12:26.117: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-nnw2n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-nnw2n
Feb 20 22:12:28.355: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-nnw2n
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 22:12:28.363: INFO: Initial restart count of pod liveness-http is 0
Feb 20 22:12:50.435: INFO: Restart count of pod e2e-tests-container-probe-nnw2n/liveness-http is now 1 (22.072241163s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:12:50.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-nnw2n" for this suite.
Feb 20 22:12:56.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:12:56.592: INFO: namespace: e2e-tests-container-probe-nnw2n, resource: bindings, ignored listing per whitelist
Feb 20 22:12:56.613: INFO: namespace e2e-tests-container-probe-nnw2n deletion completed in 6.163421336s

â€¢ [SLOW TEST:30.496 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:12:56.614: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-mhs6x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 22:12:56.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad152952-355c-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-mhs6x" to be "success or failure"
Feb 20 22:12:56.873: INFO: Pod "downwardapi-volume-ad152952-355c-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.508603ms
Feb 20 22:12:58.878: INFO: Pod "downwardapi-volume-ad152952-355c-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008711419s
STEP: Saw pod success
Feb 20 22:12:58.878: INFO: Pod "downwardapi-volume-ad152952-355c-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:12:58.882: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-ad152952-355c-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 22:12:58.902: INFO: Waiting for pod downwardapi-volume-ad152952-355c-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:12:58.906: INFO: Pod downwardapi-volume-ad152952-355c-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:12:58.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mhs6x" for this suite.
Feb 20 22:13:04.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:13:05.101: INFO: namespace: e2e-tests-downward-api-mhs6x, resource: bindings, ignored listing per whitelist
Feb 20 22:13:05.176: INFO: namespace e2e-tests-downward-api-mhs6x deletion completed in 6.262633224s

â€¢ [SLOW TEST:8.563 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:13:05.176: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-sz2xl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-b22ffe3d-355c-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 22:13:05.438: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b230d084-355c-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-sz2xl" to be "success or failure"
Feb 20 22:13:05.442: INFO: Pod "pod-projected-configmaps-b230d084-355c-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.927161ms
Feb 20 22:13:07.447: INFO: Pod "pod-projected-configmaps-b230d084-355c-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008930063s
STEP: Saw pod success
Feb 20 22:13:07.447: INFO: Pod "pod-projected-configmaps-b230d084-355c-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:13:07.451: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-configmaps-b230d084-355c-11e9-8ceb-aaeae242ec87 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 22:13:07.472: INFO: Waiting for pod pod-projected-configmaps-b230d084-355c-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:13:07.474: INFO: Pod pod-projected-configmaps-b230d084-355c-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:13:07.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sz2xl" for this suite.
Feb 20 22:13:13.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:13:13.623: INFO: namespace: e2e-tests-projected-sz2xl, resource: bindings, ignored listing per whitelist
Feb 20 22:13:13.647: INFO: namespace e2e-tests-projected-sz2xl deletion completed in 6.166045783s

â€¢ [SLOW TEST:8.471 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:13:13.647: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-l5rck
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4kmrt in namespace e2e-tests-proxy-l5rck
I0220 22:13:14.068687   32391 runners.go:184] Created replication controller with name: proxy-service-4kmrt, namespace: e2e-tests-proxy-l5rck, replica count: 1
I0220 22:13:15.119207   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:16.120196   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:17.120392   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:18.120611   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:19.120778   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:20.121136   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:21.121533   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:22.122261   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:23.122590   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:24.125872   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:25.126178   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:26.126438   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:27.126647   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:28.126860   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:29.127090   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:30.127399   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:31.127632   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:32.127823   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:33.128095   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:34.128359   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:35.128549   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:36.128770   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:37.129014   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:38.129265   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:39.129518   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:40.131190   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:41.131459   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:42.131736   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:43.131944   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:44.132217   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:45.132479   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:46.132788   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:47.133065   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:48.133381   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:49.133685   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:50.135186   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:51.135443   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:52.135757   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:53.136010   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:54.136258   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:55.136714   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:56.136951   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:57.137285   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:58.137564   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:13:59.137827   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:14:00.138010   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:14:01.138291   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:14:02.138627   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:14:03.138927   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:14:04.139221   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:14:05.139482   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0220 22:14:06.139790   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0220 22:14:07.140044   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0220 22:14:08.140357   32391 runners.go:184] proxy-service-4kmrt Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 22:14:08.144: INFO: setup took 54.0958962s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 20 22:14:08.155: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 10.972751ms)
Feb 20 22:14:08.160: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 14.639002ms)
Feb 20 22:14:08.160: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 15.08024ms)
Feb 20 22:14:08.160: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 15.038606ms)
Feb 20 22:14:08.160: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 15.10322ms)
Feb 20 22:14:08.160: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 15.183552ms)
Feb 20 22:14:08.160: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 15.057744ms)
Feb 20 22:14:08.163: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 18.25575ms)
Feb 20 22:14:08.163: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 18.096583ms)
Feb 20 22:14:08.164: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 19.524441ms)
Feb 20 22:14:08.164: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 19.467068ms)
Feb 20 22:14:08.166: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 21.221499ms)
Feb 20 22:14:08.166: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 21.259883ms)
Feb 20 22:14:08.166: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 21.586736ms)
Feb 20 22:14:08.167: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 22.155614ms)
Feb 20 22:14:08.209: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 64.527307ms)
Feb 20 22:14:08.219: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 9.268931ms)
Feb 20 22:14:08.219: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 9.244466ms)
Feb 20 22:14:08.219: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 9.413144ms)
Feb 20 22:14:08.219: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 9.305025ms)
Feb 20 22:14:08.219: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 9.815404ms)
Feb 20 22:14:08.219: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 10.085366ms)
Feb 20 22:14:08.219: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 9.416933ms)
Feb 20 22:14:08.219: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 9.629561ms)
Feb 20 22:14:08.219: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 9.390341ms)
Feb 20 22:14:08.219: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 9.560983ms)
Feb 20 22:14:08.220: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 9.65052ms)
Feb 20 22:14:08.220: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 10.505442ms)
Feb 20 22:14:08.220: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 10.208334ms)
Feb 20 22:14:08.220: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 9.995834ms)
Feb 20 22:14:08.221: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 11.075897ms)
Feb 20 22:14:08.221: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 10.977342ms)
Feb 20 22:14:08.229: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 8.057758ms)
Feb 20 22:14:08.229: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 8.220719ms)
Feb 20 22:14:08.229: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 8.383446ms)
Feb 20 22:14:08.229: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 8.361605ms)
Feb 20 22:14:08.230: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 8.692133ms)
Feb 20 22:14:08.230: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 8.566966ms)
Feb 20 22:14:08.231: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 9.639809ms)
Feb 20 22:14:08.231: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 9.693586ms)
Feb 20 22:14:08.231: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 9.743474ms)
Feb 20 22:14:08.231: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 9.866629ms)
Feb 20 22:14:08.231: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 9.976654ms)
Feb 20 22:14:08.233: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 11.953275ms)
Feb 20 22:14:08.233: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 11.984443ms)
Feb 20 22:14:08.233: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 12.20273ms)
Feb 20 22:14:08.233: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 12.126887ms)
Feb 20 22:14:08.234: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 12.435464ms)
Feb 20 22:14:08.240: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 6.782337ms)
Feb 20 22:14:08.243: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 8.848509ms)
Feb 20 22:14:08.243: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 8.538824ms)
Feb 20 22:14:08.243: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 9.376302ms)
Feb 20 22:14:08.243: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 8.654846ms)
Feb 20 22:14:08.244: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 9.354482ms)
Feb 20 22:14:08.244: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 9.609357ms)
Feb 20 22:14:08.244: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 9.527069ms)
Feb 20 22:14:08.244: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 9.279627ms)
Feb 20 22:14:08.244: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 9.299671ms)
Feb 20 22:14:08.244: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 10.124958ms)
Feb 20 22:14:08.244: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 9.601476ms)
Feb 20 22:14:08.245: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 10.535015ms)
Feb 20 22:14:08.245: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 10.376362ms)
Feb 20 22:14:08.245: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 11.657809ms)
Feb 20 22:14:08.246: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 11.162789ms)
Feb 20 22:14:08.255: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 8.768592ms)
Feb 20 22:14:08.255: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 8.494065ms)
Feb 20 22:14:08.255: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 8.646377ms)
Feb 20 22:14:08.255: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 8.89928ms)
Feb 20 22:14:08.256: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 8.867213ms)
Feb 20 22:14:08.256: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 9.481145ms)
Feb 20 22:14:08.256: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 9.501576ms)
Feb 20 22:14:08.256: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 9.560946ms)
Feb 20 22:14:08.256: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 9.396898ms)
Feb 20 22:14:08.256: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 10.254596ms)
Feb 20 22:14:08.258: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 11.635966ms)
Feb 20 22:14:08.258: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 11.079669ms)
Feb 20 22:14:08.258: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 11.993273ms)
Feb 20 22:14:08.258: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 11.640674ms)
Feb 20 22:14:08.258: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 11.280924ms)
Feb 20 22:14:08.258: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 11.856058ms)
Feb 20 22:14:08.273: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 13.916231ms)
Feb 20 22:14:08.274: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 14.758716ms)
Feb 20 22:14:08.274: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 15.999879ms)
Feb 20 22:14:08.274: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 15.848648ms)
Feb 20 22:14:08.274: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 16.201806ms)
Feb 20 22:14:08.275: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 15.716298ms)
Feb 20 22:14:08.275: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 16.851556ms)
Feb 20 22:14:08.275: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 16.220795ms)
Feb 20 22:14:08.276: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 17.080497ms)
Feb 20 22:14:08.276: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 17.180828ms)
Feb 20 22:14:08.276: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 17.080259ms)
Feb 20 22:14:08.276: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 17.322954ms)
Feb 20 22:14:08.276: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 16.922347ms)
Feb 20 22:14:08.276: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 17.143921ms)
Feb 20 22:14:08.276: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 17.740586ms)
Feb 20 22:14:08.278: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 20.042526ms)
Feb 20 22:14:08.287: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 8.805381ms)
Feb 20 22:14:08.287: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 8.92889ms)
Feb 20 22:14:08.287: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 8.740854ms)
Feb 20 22:14:08.287: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 8.825895ms)
Feb 20 22:14:08.288: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 9.130881ms)
Feb 20 22:14:08.288: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 9.824201ms)
Feb 20 22:14:08.288: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 9.676926ms)
Feb 20 22:14:08.288: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 9.819002ms)
Feb 20 22:14:08.288: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 9.657871ms)
Feb 20 22:14:08.289: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 9.881021ms)
Feb 20 22:14:08.289: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 10.009526ms)
Feb 20 22:14:08.289: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 10.280019ms)
Feb 20 22:14:08.289: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 10.55273ms)
Feb 20 22:14:08.289: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 10.432282ms)
Feb 20 22:14:08.289: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 10.489551ms)
Feb 20 22:14:08.290: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 11.754939ms)
Feb 20 22:14:08.300: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 8.797969ms)
Feb 20 22:14:08.300: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 9.73924ms)
Feb 20 22:14:08.300: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 9.856926ms)
Feb 20 22:14:08.300: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 9.703368ms)
Feb 20 22:14:08.300: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 9.567258ms)
Feb 20 22:14:08.300: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 9.292243ms)
Feb 20 22:14:08.300: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 9.909118ms)
Feb 20 22:14:08.301: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 10.045424ms)
Feb 20 22:14:08.301: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 9.941175ms)
Feb 20 22:14:08.301: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 10.250732ms)
Feb 20 22:14:08.301: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 10.606981ms)
Feb 20 22:14:08.301: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 9.927805ms)
Feb 20 22:14:08.301: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 10.508261ms)
Feb 20 22:14:08.301: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 10.663035ms)
Feb 20 22:14:08.301: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 10.460165ms)
Feb 20 22:14:08.303: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 12.04841ms)
Feb 20 22:14:08.315: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 11.020739ms)
Feb 20 22:14:08.315: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 11.016106ms)
Feb 20 22:14:08.315: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 11.679477ms)
Feb 20 22:14:08.315: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 11.766702ms)
Feb 20 22:14:08.315: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 11.550465ms)
Feb 20 22:14:08.315: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 11.44413ms)
Feb 20 22:14:08.315: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 11.811375ms)
Feb 20 22:14:08.315: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 11.531961ms)
Feb 20 22:14:08.315: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 11.921163ms)
Feb 20 22:14:08.316: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 11.973041ms)
Feb 20 22:14:08.316: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 12.003927ms)
Feb 20 22:14:08.315: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 11.403298ms)
Feb 20 22:14:08.316: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 12.253831ms)
Feb 20 22:14:08.317: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 13.387183ms)
Feb 20 22:14:08.317: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 13.021447ms)
Feb 20 22:14:08.317: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 13.320474ms)
Feb 20 22:14:08.325: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 7.554421ms)
Feb 20 22:14:08.325: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 7.635193ms)
Feb 20 22:14:08.325: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 7.463344ms)
Feb 20 22:14:08.325: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 8.065343ms)
Feb 20 22:14:08.327: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 8.669002ms)
Feb 20 22:14:08.327: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 9.110394ms)
Feb 20 22:14:08.327: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 10.034924ms)
Feb 20 22:14:08.327: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 9.051607ms)
Feb 20 22:14:08.327: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 9.229948ms)
Feb 20 22:14:08.327: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 9.50338ms)
Feb 20 22:14:08.327: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 10.148251ms)
Feb 20 22:14:08.327: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 9.375334ms)
Feb 20 22:14:08.327: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 10.083351ms)
Feb 20 22:14:08.328: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 10.799263ms)
Feb 20 22:14:08.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 10.786233ms)
Feb 20 22:14:08.329: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 10.702517ms)
Feb 20 22:14:08.337: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 7.141467ms)
Feb 20 22:14:08.337: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 8.009233ms)
Feb 20 22:14:08.337: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 7.699214ms)
Feb 20 22:14:08.337: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 7.839098ms)
Feb 20 22:14:08.337: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 8.384357ms)
Feb 20 22:14:08.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 8.348708ms)
Feb 20 22:14:08.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 8.282754ms)
Feb 20 22:14:08.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 8.622336ms)
Feb 20 22:14:08.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 9.065992ms)
Feb 20 22:14:08.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 9.382731ms)
Feb 20 22:14:08.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 8.86808ms)
Feb 20 22:14:08.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 8.8319ms)
Feb 20 22:14:08.338: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 9.054043ms)
Feb 20 22:14:08.339: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 9.738469ms)
Feb 20 22:14:08.340: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 10.188728ms)
Feb 20 22:14:08.340: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 9.931862ms)
Feb 20 22:14:08.348: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 8.014832ms)
Feb 20 22:14:08.348: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 7.592005ms)
Feb 20 22:14:08.348: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 7.738755ms)
Feb 20 22:14:08.349: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 8.718173ms)
Feb 20 22:14:08.349: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 8.771313ms)
Feb 20 22:14:08.349: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 8.935165ms)
Feb 20 22:14:08.349: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 9.2559ms)
Feb 20 22:14:08.349: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 9.230759ms)
Feb 20 22:14:08.350: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 9.273348ms)
Feb 20 22:14:08.350: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 9.438109ms)
Feb 20 22:14:08.350: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 9.476249ms)
Feb 20 22:14:08.352: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 11.655919ms)
Feb 20 22:14:08.352: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 11.795869ms)
Feb 20 22:14:08.352: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 11.773811ms)
Feb 20 22:14:08.352: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 11.749139ms)
Feb 20 22:14:08.357: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 16.15127ms)
Feb 20 22:14:08.365: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 7.479969ms)
Feb 20 22:14:08.365: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 8.401688ms)
Feb 20 22:14:08.365: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 8.675951ms)
Feb 20 22:14:08.366: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 8.799857ms)
Feb 20 22:14:08.366: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 8.871407ms)
Feb 20 22:14:08.366: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 8.893116ms)
Feb 20 22:14:08.366: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 8.867364ms)
Feb 20 22:14:08.366: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 9.143026ms)
Feb 20 22:14:08.366: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 9.388408ms)
Feb 20 22:14:08.366: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 9.292042ms)
Feb 20 22:14:08.367: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 9.923202ms)
Feb 20 22:14:08.368: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 10.649623ms)
Feb 20 22:14:08.368: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 10.819492ms)
Feb 20 22:14:08.368: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 10.91841ms)
Feb 20 22:14:08.368: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 10.99284ms)
Feb 20 22:14:08.368: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 11.598054ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 11.379484ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 11.416155ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 11.638187ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 11.546173ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 11.741252ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 11.66074ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 11.818967ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 11.617225ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 11.679127ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 11.92117ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 11.698227ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 11.764675ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 11.727798ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 11.675808ms)
Feb 20 22:14:08.380: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 11.664716ms)
Feb 20 22:14:08.381: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 12.386606ms)
Feb 20 22:14:08.392: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 10.363834ms)
Feb 20 22:14:08.392: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 10.513773ms)
Feb 20 22:14:08.392: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 10.365464ms)
Feb 20 22:14:08.392: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 10.514629ms)
Feb 20 22:14:08.392: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 10.400786ms)
Feb 20 22:14:08.392: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 10.400481ms)
Feb 20 22:14:08.392: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 10.701652ms)
Feb 20 22:14:08.392: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 10.739481ms)
Feb 20 22:14:08.392: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 10.969725ms)
Feb 20 22:14:08.393: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 11.689111ms)
Feb 20 22:14:08.393: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 11.602815ms)
Feb 20 22:14:08.393: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 11.588053ms)
Feb 20 22:14:08.393: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 11.762945ms)
Feb 20 22:14:08.393: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 11.810811ms)
Feb 20 22:14:08.393: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 11.775041ms)
Feb 20 22:14:08.393: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 11.832664ms)
Feb 20 22:14:08.402: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 8.617374ms)
Feb 20 22:14:08.402: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 8.989022ms)
Feb 20 22:14:08.402: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 9.140945ms)
Feb 20 22:14:08.402: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 9.022927ms)
Feb 20 22:14:08.402: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 9.078665ms)
Feb 20 22:14:08.402: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 9.020075ms)
Feb 20 22:14:08.402: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 9.174879ms)
Feb 20 22:14:08.402: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 9.097454ms)
Feb 20 22:14:08.402: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 9.158638ms)
Feb 20 22:14:08.403: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 9.274584ms)
Feb 20 22:14:08.403: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 9.251025ms)
Feb 20 22:14:08.403: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 9.211884ms)
Feb 20 22:14:08.403: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 9.402776ms)
Feb 20 22:14:08.403: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 9.849743ms)
Feb 20 22:14:08.403: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 10.22454ms)
Feb 20 22:14:08.403: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 10.136279ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 13.083036ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 13.009212ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 13.088511ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 13.007313ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 13.009455ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 13.007937ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 13.176265ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 13.618897ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 13.544619ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 13.3248ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 13.373828ms)
Feb 20 22:14:08.417: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 13.461608ms)
Feb 20 22:14:08.418: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 14.375846ms)
Feb 20 22:14:08.418: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 14.475218ms)
Feb 20 22:14:08.419: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 15.082276ms)
Feb 20 22:14:08.419: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 15.338112ms)
Feb 20 22:14:08.429: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 9.529986ms)
Feb 20 22:14:08.429: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 10.280444ms)
Feb 20 22:14:08.429: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 10.286226ms)
Feb 20 22:14:08.429: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 10.173024ms)
Feb 20 22:14:08.429: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 10.282175ms)
Feb 20 22:14:08.430: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 10.834789ms)
Feb 20 22:14:08.430: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 10.672482ms)
Feb 20 22:14:08.430: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 10.730126ms)
Feb 20 22:14:08.430: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 10.77633ms)
Feb 20 22:14:08.430: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 10.889615ms)
Feb 20 22:14:08.430: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 10.968046ms)
Feb 20 22:14:08.430: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 11.061909ms)
Feb 20 22:14:08.472: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 52.521399ms)
Feb 20 22:14:08.472: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 52.600204ms)
Feb 20 22:14:08.472: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 52.796711ms)
Feb 20 22:14:08.472: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 52.694189ms)
Feb 20 22:14:08.478: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 5.829833ms)
Feb 20 22:14:08.482: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 9.957736ms)
Feb 20 22:14:08.482: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 9.922792ms)
Feb 20 22:14:08.482: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 10.121949ms)
Feb 20 22:14:08.482: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 10.043692ms)
Feb 20 22:14:08.482: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 10.101408ms)
Feb 20 22:14:08.482: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 10.055209ms)
Feb 20 22:14:08.482: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 10.214122ms)
Feb 20 22:14:08.482: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 10.071865ms)
Feb 20 22:14:08.482: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 10.299586ms)
Feb 20 22:14:08.483: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 10.618717ms)
Feb 20 22:14:08.483: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 10.686668ms)
Feb 20 22:14:08.483: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 10.74096ms)
Feb 20 22:14:08.483: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 10.745205ms)
Feb 20 22:14:08.483: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 10.987329ms)
Feb 20 22:14:08.483: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 11.019682ms)
Feb 20 22:14:08.490: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2/proxy/rewriteme"... (200; 6.435555ms)
Feb 20 22:14:08.490: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 6.267885ms)
Feb 20 22:14:08.490: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:443/proxy/... (200; 6.36642ms)
Feb 20 22:14:08.490: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 6.436909ms)
Feb 20 22:14:08.490: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:162/proxy/: bar (200; 6.728774ms)
Feb 20 22:14:08.490: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:462/proxy/: tls qux (200; 6.687336ms)
Feb 20 22:14:08.490: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/http:proxy-service-4kmrt-klgl2:1080/proxy/... (200; 6.863724ms)
Feb 20 22:14:08.491: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/https:proxy-service-4kmrt-klgl2:460/proxy/: tls baz (200; 7.482561ms)
Feb 20 22:14:08.491: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:1080/proxy/rewri... (200; 7.569588ms)
Feb 20 22:14:08.491: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/pods/proxy-service-4kmrt-klgl2:160/proxy/: foo (200; 7.602542ms)
Feb 20 22:14:08.492: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname1/proxy/: foo (200; 8.633285ms)
Feb 20 22:14:08.492: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname2/proxy/: bar (200; 8.902761ms)
Feb 20 22:14:08.492: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname2/proxy/: tls qux (200; 9.076066ms)
Feb 20 22:14:08.493: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/https:proxy-service-4kmrt:tlsportname1/proxy/: tls baz (200; 9.048997ms)
Feb 20 22:14:08.532: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/proxy-service-4kmrt:portname1/proxy/: foo (200; 48.233851ms)
Feb 20 22:14:08.532: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-l5rck/services/http:proxy-service-4kmrt:portname2/proxy/: bar (200; 48.270406ms)
STEP: deleting ReplicationController proxy-service-4kmrt in namespace e2e-tests-proxy-l5rck, will wait for the garbage collector to delete the pods
Feb 20 22:14:08.594: INFO: Deleting ReplicationController proxy-service-4kmrt took: 7.367322ms
Feb 20 22:14:08.794: INFO: Terminating ReplicationController proxy-service-4kmrt pods took: 200.233341ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:14:11.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-l5rck" for this suite.
Feb 20 22:14:17.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:14:17.224: INFO: namespace: e2e-tests-proxy-l5rck, resource: bindings, ignored listing per whitelist
Feb 20 22:14:17.358: INFO: namespace e2e-tests-proxy-l5rck deletion completed in 6.254006308s

â€¢ [SLOW TEST:63.711 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:14:17.359: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-f4lnl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-n8rd
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 22:14:17.711: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-n8rd" in namespace "e2e-tests-subpath-f4lnl" to be "success or failure"
Feb 20 22:14:17.715: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624785ms
Feb 20 22:14:19.720: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008944628s
Feb 20 22:14:21.725: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Running", Reason="", readiness=false. Elapsed: 4.014203809s
Feb 20 22:14:23.731: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Running", Reason="", readiness=false. Elapsed: 6.019861776s
Feb 20 22:14:25.736: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Running", Reason="", readiness=false. Elapsed: 8.025004244s
Feb 20 22:14:27.741: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Running", Reason="", readiness=false. Elapsed: 10.030276358s
Feb 20 22:14:29.748: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Running", Reason="", readiness=false. Elapsed: 12.03739401s
Feb 20 22:14:31.753: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Running", Reason="", readiness=false. Elapsed: 14.042423007s
Feb 20 22:14:33.759: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Running", Reason="", readiness=false. Elapsed: 16.047868718s
Feb 20 22:14:35.764: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Running", Reason="", readiness=false. Elapsed: 18.052978729s
Feb 20 22:14:37.769: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Running", Reason="", readiness=false. Elapsed: 20.05806279s
Feb 20 22:14:39.774: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Running", Reason="", readiness=false. Elapsed: 22.063018005s
Feb 20 22:14:41.780: INFO: Pod "pod-subpath-test-configmap-n8rd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.068823946s
STEP: Saw pod success
Feb 20 22:14:41.780: INFO: Pod "pod-subpath-test-configmap-n8rd" satisfied condition "success or failure"
Feb 20 22:14:41.788: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-subpath-test-configmap-n8rd container test-container-subpath-configmap-n8rd: <nil>
STEP: delete the pod
Feb 20 22:14:41.812: INFO: Waiting for pod pod-subpath-test-configmap-n8rd to disappear
Feb 20 22:14:41.816: INFO: Pod pod-subpath-test-configmap-n8rd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-n8rd
Feb 20 22:14:41.816: INFO: Deleting pod "pod-subpath-test-configmap-n8rd" in namespace "e2e-tests-subpath-f4lnl"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:14:41.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-f4lnl" for this suite.
Feb 20 22:14:47.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:14:48.070: INFO: namespace: e2e-tests-subpath-f4lnl, resource: bindings, ignored listing per whitelist
Feb 20 22:14:48.078: INFO: namespace e2e-tests-subpath-f4lnl deletion completed in 6.248785135s

â€¢ [SLOW TEST:30.719 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:14:48.078: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-t7vvj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-ef894d92-355c-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 22:14:48.364: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ef89eb69-355c-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-t7vvj" to be "success or failure"
Feb 20 22:14:48.367: INFO: Pod "pod-projected-secrets-ef89eb69-355c-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867306ms
Feb 20 22:14:50.373: INFO: Pod "pod-projected-secrets-ef89eb69-355c-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00984517s
STEP: Saw pod success
Feb 20 22:14:50.374: INFO: Pod "pod-projected-secrets-ef89eb69-355c-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:14:50.379: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-secrets-ef89eb69-355c-11e9-8ceb-aaeae242ec87 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 22:14:50.400: INFO: Waiting for pod pod-projected-secrets-ef89eb69-355c-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:14:50.403: INFO: Pod pod-projected-secrets-ef89eb69-355c-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:14:50.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t7vvj" for this suite.
Feb 20 22:14:56.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:14:56.577: INFO: namespace: e2e-tests-projected-t7vvj, resource: bindings, ignored listing per whitelist
Feb 20 22:14:56.590: INFO: namespace e2e-tests-projected-t7vvj deletion completed in 6.183077875s

â€¢ [SLOW TEST:8.512 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:14:56.590: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-hpvxz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 20 22:14:56.850: INFO: Waiting up to 5m0s for pod "client-containers-f499094d-355c-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-containers-hpvxz" to be "success or failure"
Feb 20 22:14:56.854: INFO: Pod "client-containers-f499094d-355c-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.330669ms
Feb 20 22:14:58.859: INFO: Pod "client-containers-f499094d-355c-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008944238s
STEP: Saw pod success
Feb 20 22:14:58.860: INFO: Pod "client-containers-f499094d-355c-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:14:58.863: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod client-containers-f499094d-355c-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 22:14:58.882: INFO: Waiting for pod client-containers-f499094d-355c-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:14:58.885: INFO: Pod client-containers-f499094d-355c-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:14:58.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-hpvxz" for this suite.
Feb 20 22:15:04.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:15:04.979: INFO: namespace: e2e-tests-containers-hpvxz, resource: bindings, ignored listing per whitelist
Feb 20 22:15:05.130: INFO: namespace e2e-tests-containers-hpvxz deletion completed in 6.240415069s

â€¢ [SLOW TEST:8.540 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:15:05.130: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-x4bh2
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-f9b44ba4-355c-11e9-8ceb-aaeae242ec87
STEP: Creating configMap with name cm-test-opt-upd-f9b44bea-355c-11e9-8ceb-aaeae242ec87
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f9b44ba4-355c-11e9-8ceb-aaeae242ec87
STEP: Updating configmap cm-test-opt-upd-f9b44bea-355c-11e9-8ceb-aaeae242ec87
STEP: Creating configMap with name cm-test-opt-create-f9b44ccf-355c-11e9-8ceb-aaeae242ec87
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:16:22.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-x4bh2" for this suite.
Feb 20 22:16:44.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:16:44.568: INFO: namespace: e2e-tests-configmap-x4bh2, resource: bindings, ignored listing per whitelist
Feb 20 22:16:44.654: INFO: namespace e2e-tests-configmap-x4bh2 deletion completed in 22.152525171s

â€¢ [SLOW TEST:99.524 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:16:44.654: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-9wfrx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 20 22:16:44.904: INFO: Waiting up to 5m0s for pod "pod-35006dc0-355d-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-9wfrx" to be "success or failure"
Feb 20 22:16:44.908: INFO: Pod "pod-35006dc0-355d-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.367006ms
Feb 20 22:16:46.913: INFO: Pod "pod-35006dc0-355d-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009380887s
STEP: Saw pod success
Feb 20 22:16:46.914: INFO: Pod "pod-35006dc0-355d-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:16:46.917: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-35006dc0-355d-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 22:16:46.938: INFO: Waiting for pod pod-35006dc0-355d-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:16:46.941: INFO: Pod pod-35006dc0-355d-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:16:46.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9wfrx" for this suite.
Feb 20 22:16:52.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:16:53.127: INFO: namespace: e2e-tests-emptydir-9wfrx, resource: bindings, ignored listing per whitelist
Feb 20 22:16:53.143: INFO: namespace e2e-tests-emptydir-9wfrx deletion completed in 6.197016482s

â€¢ [SLOW TEST:8.489 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:16:53.143: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-lvgh2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-3a0f4492-355d-11e9-8ceb-aaeae242ec87
Feb 20 22:16:53.389: INFO: Pod name my-hostname-basic-3a0f4492-355d-11e9-8ceb-aaeae242ec87: Found 0 pods out of 1
Feb 20 22:16:58.394: INFO: Pod name my-hostname-basic-3a0f4492-355d-11e9-8ceb-aaeae242ec87: Found 1 pods out of 1
Feb 20 22:16:58.394: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3a0f4492-355d-11e9-8ceb-aaeae242ec87" are running
Feb 20 22:16:58.398: INFO: Pod "my-hostname-basic-3a0f4492-355d-11e9-8ceb-aaeae242ec87-8jctv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 22:16:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 22:16:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 22:16:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 22:16:53 +0000 UTC Reason: Message:}])
Feb 20 22:16:58.398: INFO: Trying to dial the pod
Feb 20 22:17:03.493: INFO: Controller my-hostname-basic-3a0f4492-355d-11e9-8ceb-aaeae242ec87: Got expected result from replica 1 [my-hostname-basic-3a0f4492-355d-11e9-8ceb-aaeae242ec87-8jctv]: "my-hostname-basic-3a0f4492-355d-11e9-8ceb-aaeae242ec87-8jctv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:17:03.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-lvgh2" for this suite.
Feb 20 22:17:09.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:17:09.683: INFO: namespace: e2e-tests-replication-controller-lvgh2, resource: bindings, ignored listing per whitelist
Feb 20 22:17:09.694: INFO: namespace e2e-tests-replication-controller-lvgh2 deletion completed in 6.19540181s

â€¢ [SLOW TEST:16.550 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:17:09.694: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-7jgdt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-7jgdt.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-7jgdt.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 20 22:17:12.387: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.395: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.402: INFO: Unable to read wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.408: INFO: Unable to read wheezy_hosts@dns-querier-1 from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.413: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.420: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.750: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.757: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.766: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.773: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.781: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.790: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:12.790: INFO: Lookups using e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local wheezy_hosts@dns-querier-1 wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 20 22:17:18.084: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.092: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.098: INFO: Unable to read wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.104: INFO: Unable to read wheezy_hosts@dns-querier-1 from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.111: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.117: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.530: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.536: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.542: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.547: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.553: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:18.553: INFO: Lookups using e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local wheezy_hosts@dns-querier-1 wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_tcp@kubernetes.default.svc.cluster.local jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 20 22:17:23.463: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:23.468: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:23.880: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:23.888: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:23.893: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:23.899: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:23.905: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:23.905: INFO: Lookups using e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_tcp@kubernetes.default.svc.cluster.local jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7jgdt.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 20 22:17:28.368: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:28.374: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:28.939: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:28.945: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:28.950: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:28.950: INFO: Lookups using e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 20 22:17:33.410: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:33.416: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:33.990: INFO: Unable to read jessie_hosts@dns-querier-1 from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:33.996: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:34.002: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:34.002: INFO: Lookups using e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 20 22:17:38.430: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:38.436: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:39.090: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:39.098: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:39.098: INFO: Lookups using e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 20 22:17:43.391: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:44.293: INFO: Lookups using e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@PodARecord]

Feb 20 22:17:48.406: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87: the server could not find the requested resource (get pods dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87)
Feb 20 22:17:49.345: INFO: Lookups using e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87 failed for: [wheezy_udp@PodARecord]

Feb 20 22:17:54.421: INFO: DNS probes using e2e-tests-dns-7jgdt/dns-test-43f6582d-355d-11e9-8ceb-aaeae242ec87 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:17:54.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-7jgdt" for this suite.
Feb 20 22:18:00.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:18:00.528: INFO: namespace: e2e-tests-dns-7jgdt, resource: bindings, ignored listing per whitelist
Feb 20 22:18:00.638: INFO: namespace e2e-tests-dns-7jgdt deletion completed in 6.197288666s

â€¢ [SLOW TEST:50.944 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:18:00.638: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-l74rt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:18:04.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-l74rt" for this suite.
Feb 20 22:18:10.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:18:11.018: INFO: namespace: e2e-tests-kubelet-test-l74rt, resource: bindings, ignored listing per whitelist
Feb 20 22:18:11.137: INFO: namespace e2e-tests-kubelet-test-l74rt deletion completed in 6.16660596s

â€¢ [SLOW TEST:10.498 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:18:11.137: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8m74n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 22:18:13.911: INFO: Successfully updated pod "labelsupdate688ae8de-355d-11e9-8ceb-aaeae242ec87"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:18:17.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8m74n" for this suite.
Feb 20 22:18:39.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:18:39.992: INFO: namespace: e2e-tests-projected-8m74n, resource: bindings, ignored listing per whitelist
Feb 20 22:18:40.171: INFO: namespace e2e-tests-projected-8m74n deletion completed in 22.214134232s

â€¢ [SLOW TEST:29.034 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:18:40.171: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-97b6z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 22:18:40.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79e056c0-355d-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-97b6z" to be "success or failure"
Feb 20 22:18:40.458: INFO: Pod "downwardapi-volume-79e056c0-355d-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.407806ms
Feb 20 22:18:42.465: INFO: Pod "downwardapi-volume-79e056c0-355d-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009633019s
STEP: Saw pod success
Feb 20 22:18:42.465: INFO: Pod "downwardapi-volume-79e056c0-355d-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:18:42.468: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-79e056c0-355d-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 22:18:42.487: INFO: Waiting for pod downwardapi-volume-79e056c0-355d-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:18:42.491: INFO: Pod downwardapi-volume-79e056c0-355d-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:18:42.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-97b6z" for this suite.
Feb 20 22:18:48.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:18:48.605: INFO: namespace: e2e-tests-projected-97b6z, resource: bindings, ignored listing per whitelist
Feb 20 22:18:48.692: INFO: namespace e2e-tests-projected-97b6z deletion completed in 6.19641339s

â€¢ [SLOW TEST:8.521 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:18:48.692: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-ccrpj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 20 22:18:48.961: INFO: Number of nodes with available pods: 0
Feb 20 22:18:48.961: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 22:18:49.971: INFO: Number of nodes with available pods: 0
Feb 20 22:18:49.971: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-9rlgs is running more than one daemon pod
Feb 20 22:18:50.970: INFO: Number of nodes with available pods: 2
Feb 20 22:18:50.971: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 20 22:18:50.992: INFO: Number of nodes with available pods: 1
Feb 20 22:18:50.992: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:18:52.008: INFO: Number of nodes with available pods: 1
Feb 20 22:18:52.009: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:18:53.009: INFO: Number of nodes with available pods: 1
Feb 20 22:18:53.009: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:18:54.009: INFO: Number of nodes with available pods: 1
Feb 20 22:18:54.009: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:18:55.007: INFO: Number of nodes with available pods: 1
Feb 20 22:18:55.007: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:18:56.004: INFO: Number of nodes with available pods: 1
Feb 20 22:18:56.004: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:18:57.006: INFO: Number of nodes with available pods: 1
Feb 20 22:18:57.006: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:18:58.009: INFO: Number of nodes with available pods: 1
Feb 20 22:18:58.009: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:18:59.003: INFO: Number of nodes with available pods: 1
Feb 20 22:18:59.003: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:00.001: INFO: Number of nodes with available pods: 1
Feb 20 22:19:00.001: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:01.003: INFO: Number of nodes with available pods: 1
Feb 20 22:19:01.003: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:02.003: INFO: Number of nodes with available pods: 1
Feb 20 22:19:02.003: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:03.003: INFO: Number of nodes with available pods: 1
Feb 20 22:19:03.003: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:04.010: INFO: Number of nodes with available pods: 1
Feb 20 22:19:04.010: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:05.009: INFO: Number of nodes with available pods: 1
Feb 20 22:19:05.009: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:06.019: INFO: Number of nodes with available pods: 1
Feb 20 22:19:06.019: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:07.019: INFO: Number of nodes with available pods: 1
Feb 20 22:19:07.019: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:08.011: INFO: Number of nodes with available pods: 1
Feb 20 22:19:08.011: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:09.009: INFO: Number of nodes with available pods: 1
Feb 20 22:19:09.009: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:10.003: INFO: Number of nodes with available pods: 1
Feb 20 22:19:10.003: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:11.004: INFO: Number of nodes with available pods: 1
Feb 20 22:19:11.004: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:12.005: INFO: Number of nodes with available pods: 1
Feb 20 22:19:12.005: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:13.002: INFO: Number of nodes with available pods: 1
Feb 20 22:19:13.002: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:14.003: INFO: Number of nodes with available pods: 1
Feb 20 22:19:14.003: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:15.004: INFO: Number of nodes with available pods: 1
Feb 20 22:19:15.005: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:16.009: INFO: Number of nodes with available pods: 1
Feb 20 22:19:16.009: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:17.003: INFO: Number of nodes with available pods: 1
Feb 20 22:19:17.003: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:18.000: INFO: Number of nodes with available pods: 1
Feb 20 22:19:18.000: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:19.002: INFO: Number of nodes with available pods: 1
Feb 20 22:19:19.002: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:20.003: INFO: Number of nodes with available pods: 1
Feb 20 22:19:20.003: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:21.004: INFO: Number of nodes with available pods: 1
Feb 20 22:19:21.004: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:22.000: INFO: Number of nodes with available pods: 1
Feb 20 22:19:22.000: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:23.002: INFO: Number of nodes with available pods: 1
Feb 20 22:19:23.002: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:24.004: INFO: Number of nodes with available pods: 1
Feb 20 22:19:24.004: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:25.001: INFO: Number of nodes with available pods: 1
Feb 20 22:19:25.001: INFO: Node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq is running more than one daemon pod
Feb 20 22:19:26.003: INFO: Number of nodes with available pods: 2
Feb 20 22:19:26.004: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-ccrpj, will wait for the garbage collector to delete the pods
Feb 20 22:19:26.071: INFO: Deleting DaemonSet.extensions daemon-set took: 6.588582ms
Feb 20 22:19:26.171: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.26861ms
Feb 20 22:20:10.276: INFO: Number of nodes with available pods: 0
Feb 20 22:20:10.276: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 22:20:10.279: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-ccrpj/daemonsets","resourceVersion":"19248"},"items":null}

Feb 20 22:20:10.282: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-ccrpj/pods","resourceVersion":"19248"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:20:10.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-ccrpj" for this suite.
Feb 20 22:20:16.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:20:16.492: INFO: namespace: e2e-tests-daemonsets-ccrpj, resource: bindings, ignored listing per whitelist
Feb 20 22:20:16.532: INFO: namespace e2e-tests-daemonsets-ccrpj deletion completed in 6.235049866s

â€¢ [SLOW TEST:87.841 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:20:16.533: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5qr2w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 20 22:20:16.763: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:20:16.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5qr2w" for this suite.
Feb 20 22:20:22.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:20:23.180: INFO: namespace: e2e-tests-kubectl-5qr2w, resource: bindings, ignored listing per whitelist
Feb 20 22:20:23.204: INFO: namespace e2e-tests-kubectl-5qr2w deletion completed in 6.234694544s

â€¢ [SLOW TEST:6.671 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:20:23.204: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2q2vn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-b75a3039-355d-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 22:20:23.609: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b75aecf9-355d-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-2q2vn" to be "success or failure"
Feb 20 22:20:23.616: INFO: Pod "pod-projected-secrets-b75aecf9-355d-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.898554ms
Feb 20 22:20:25.621: INFO: Pod "pod-projected-secrets-b75aecf9-355d-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01192764s
STEP: Saw pod success
Feb 20 22:20:25.621: INFO: Pod "pod-projected-secrets-b75aecf9-355d-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:20:25.625: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-secrets-b75aecf9-355d-11e9-8ceb-aaeae242ec87 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 22:20:25.651: INFO: Waiting for pod pod-projected-secrets-b75aecf9-355d-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:20:25.656: INFO: Pod pod-projected-secrets-b75aecf9-355d-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:20:25.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2q2vn" for this suite.
Feb 20 22:20:31.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:20:31.772: INFO: namespace: e2e-tests-projected-2q2vn, resource: bindings, ignored listing per whitelist
Feb 20 22:20:31.850: INFO: namespace e2e-tests-projected-2q2vn deletion completed in 6.189329431s

â€¢ [SLOW TEST:8.646 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:20:31.850: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bfbtw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-bc6f6cc2-355d-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 22:20:32.127: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bc702021-355d-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-bfbtw" to be "success or failure"
Feb 20 22:20:32.132: INFO: Pod "pod-projected-configmaps-bc702021-355d-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.766112ms
Feb 20 22:20:34.137: INFO: Pod "pod-projected-configmaps-bc702021-355d-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009659322s
STEP: Saw pod success
Feb 20 22:20:34.137: INFO: Pod "pod-projected-configmaps-bc702021-355d-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:20:34.140: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-configmaps-bc702021-355d-11e9-8ceb-aaeae242ec87 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 22:20:34.160: INFO: Waiting for pod pod-projected-configmaps-bc702021-355d-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:20:34.164: INFO: Pod pod-projected-configmaps-bc702021-355d-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:20:34.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bfbtw" for this suite.
Feb 20 22:20:40.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:20:40.359: INFO: namespace: e2e-tests-projected-bfbtw, resource: bindings, ignored listing per whitelist
Feb 20 22:20:40.365: INFO: namespace e2e-tests-projected-bfbtw deletion completed in 6.194568168s

â€¢ [SLOW TEST:8.514 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:20:40.365: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ccmxg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 20 22:20:40.660: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:41.355: INFO: stderr: ""
Feb 20 22:20:41.355: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 22:20:41.355: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:41.470: INFO: stderr: ""
Feb 20 22:20:41.470: INFO: stdout: "update-demo-nautilus-kpn64 update-demo-nautilus-wh5mk "
Feb 20 22:20:41.471: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-kpn64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:41.670: INFO: stderr: ""
Feb 20 22:20:41.670: INFO: stdout: ""
Feb 20 22:20:41.670: INFO: update-demo-nautilus-kpn64 is created but not running
Feb 20 22:20:46.670: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:46.767: INFO: stderr: ""
Feb 20 22:20:46.767: INFO: stdout: "update-demo-nautilus-kpn64 update-demo-nautilus-wh5mk "
Feb 20 22:20:46.767: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-kpn64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:46.869: INFO: stderr: ""
Feb 20 22:20:46.869: INFO: stdout: "true"
Feb 20 22:20:46.869: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-kpn64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:46.960: INFO: stderr: ""
Feb 20 22:20:46.960: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 22:20:46.960: INFO: validating pod update-demo-nautilus-kpn64
Feb 20 22:20:47.053: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:20:47.053: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:20:47.053: INFO: update-demo-nautilus-kpn64 is verified up and running
Feb 20 22:20:47.053: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-wh5mk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:47.176: INFO: stderr: ""
Feb 20 22:20:47.176: INFO: stdout: "true"
Feb 20 22:20:47.176: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-wh5mk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:47.277: INFO: stderr: ""
Feb 20 22:20:47.277: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 22:20:47.277: INFO: validating pod update-demo-nautilus-wh5mk
Feb 20 22:20:47.366: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:20:47.366: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:20:47.366: INFO: update-demo-nautilus-wh5mk is verified up and running
STEP: scaling down the replication controller
Feb 20 22:20:47.379: INFO: scanned /root for discovery docs: <nil>
Feb 20 22:20:47.379: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:48.522: INFO: stderr: ""
Feb 20 22:20:48.522: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 22:20:48.522: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:48.613: INFO: stderr: ""
Feb 20 22:20:48.613: INFO: stdout: "update-demo-nautilus-kpn64 update-demo-nautilus-wh5mk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 20 22:20:53.614: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:53.708: INFO: stderr: ""
Feb 20 22:20:53.708: INFO: stdout: "update-demo-nautilus-kpn64 "
Feb 20 22:20:53.708: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-kpn64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:53.807: INFO: stderr: ""
Feb 20 22:20:53.807: INFO: stdout: "true"
Feb 20 22:20:53.807: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-kpn64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:53.902: INFO: stderr: ""
Feb 20 22:20:53.902: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 22:20:53.902: INFO: validating pod update-demo-nautilus-kpn64
Feb 20 22:20:53.911: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:20:53.911: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:20:53.911: INFO: update-demo-nautilus-kpn64 is verified up and running
STEP: scaling up the replication controller
Feb 20 22:20:53.915: INFO: scanned /root for discovery docs: <nil>
Feb 20 22:20:53.915: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:55.056: INFO: stderr: ""
Feb 20 22:20:55.056: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 22:20:55.056: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:55.282: INFO: stderr: ""
Feb 20 22:20:55.282: INFO: stdout: "update-demo-nautilus-kpn64 update-demo-nautilus-zndnr "
Feb 20 22:20:55.282: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-kpn64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:55.487: INFO: stderr: ""
Feb 20 22:20:55.487: INFO: stdout: "true"
Feb 20 22:20:55.487: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-kpn64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:55.657: INFO: stderr: ""
Feb 20 22:20:55.657: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 22:20:55.657: INFO: validating pod update-demo-nautilus-kpn64
Feb 20 22:20:55.663: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:20:55.663: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:20:55.663: INFO: update-demo-nautilus-kpn64 is verified up and running
Feb 20 22:20:55.663: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-zndnr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:55.798: INFO: stderr: ""
Feb 20 22:20:55.798: INFO: stdout: "true"
Feb 20 22:20:55.799: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-zndnr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:55.950: INFO: stderr: ""
Feb 20 22:20:55.950: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 22:20:55.950: INFO: validating pod update-demo-nautilus-zndnr
Feb 20 22:20:56.037: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:20:56.038: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:20:56.038: INFO: update-demo-nautilus-zndnr is verified up and running
STEP: using delete to clean up resources
Feb 20 22:20:56.038: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:56.183: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 22:20:56.183: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 20 22:20:56.183: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-ccmxg'
Feb 20 22:20:56.350: INFO: stderr: "No resources found.\n"
Feb 20 22:20:56.350: INFO: stdout: ""
Feb 20 22:20:56.350: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-ccmxg -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 22:20:56.489: INFO: stderr: ""
Feb 20 22:20:56.489: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:20:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ccmxg" for this suite.
Feb 20 22:21:18.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:21:18.684: INFO: namespace: e2e-tests-kubectl-ccmxg, resource: bindings, ignored listing per whitelist
Feb 20 22:21:18.728: INFO: namespace e2e-tests-kubectl-ccmxg deletion completed in 22.233038064s

â€¢ [SLOW TEST:38.364 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:21:18.729: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-8ct8x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 22:21:19.001: INFO: PodSpec: initContainers in spec.initContainers
Feb 20 22:22:04.624: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d8619bd0-355d-11e9-8ceb-aaeae242ec87", GenerateName:"", Namespace:"e2e-tests-init-container-8ct8x", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-8ct8x/pods/pod-init-d8619bd0-355d-11e9-8ceb-aaeae242ec87", UID:"d862a32c-355d-11e9-91b0-3e11857925bf", ResourceVersion:"19594", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686298079, loc:(*time.Location)(0x791ea40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"1702206"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.216/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-z72cr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00136ac00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z72cr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z72cr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z72cr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001cd6af8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00144e9c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001cd72c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001cd72e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001cd72e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001cd72ec)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686298079, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686298079, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686298079, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686298079, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.17", PodIP:"100.96.1.216", StartTime:(*v1.Time)(0xc0016c8c60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000413b20)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000413b90)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://e50a13ca7e984b02303a92a9b527c80bbe3f2c450889b7b9fc368324d1e2f74c"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0016c8ca0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0016c8c80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:22:04.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-8ct8x" for this suite.
Feb 20 22:22:26.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:22:26.859: INFO: namespace: e2e-tests-init-container-8ct8x, resource: bindings, ignored listing per whitelist
Feb 20 22:22:26.871: INFO: namespace e2e-tests-init-container-8ct8x deletion completed in 22.242275689s

â€¢ [SLOW TEST:68.142 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:22:26.871: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-p4ksd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 20 22:22:27.144: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:22:27.446: INFO: stderr: ""
Feb 20 22:22:27.446: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 22:22:27.446: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:22:27.551: INFO: stderr: ""
Feb 20 22:22:27.552: INFO: stdout: "update-demo-nautilus-lg6x7 update-demo-nautilus-pnktk "
Feb 20 22:22:27.552: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-lg6x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:22:27.652: INFO: stderr: ""
Feb 20 22:22:27.652: INFO: stdout: ""
Feb 20 22:22:27.652: INFO: update-demo-nautilus-lg6x7 is created but not running
Feb 20 22:22:32.652: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:22:37.751: INFO: stderr: ""
Feb 20 22:22:37.751: INFO: stdout: "update-demo-nautilus-lg6x7 update-demo-nautilus-pnktk "
Feb 20 22:22:37.751: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-lg6x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:22:37.851: INFO: stderr: ""
Feb 20 22:22:37.851: INFO: stdout: "true"
Feb 20 22:22:37.851: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-lg6x7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:22:37.946: INFO: stderr: ""
Feb 20 22:22:37.946: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 22:22:37.946: INFO: validating pod update-demo-nautilus-lg6x7
Feb 20 22:22:38.030: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:22:38.030: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:22:38.030: INFO: update-demo-nautilus-lg6x7 is verified up and running
Feb 20 22:22:38.030: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-pnktk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:22:38.113: INFO: stderr: ""
Feb 20 22:22:38.113: INFO: stdout: "true"
Feb 20 22:22:38.113: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-pnktk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:22:38.204: INFO: stderr: ""
Feb 20 22:22:38.204: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 22:22:38.204: INFO: validating pod update-demo-nautilus-pnktk
Feb 20 22:22:38.289: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:22:38.289: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:22:38.289: INFO: update-demo-nautilus-pnktk is verified up and running
STEP: rolling-update to new replication controller
Feb 20 22:22:38.294: INFO: scanned /root for discovery docs: <nil>
Feb 20 22:22:38.294: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:24:32.724: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 20 22:24:32.724: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 22:24:32.724: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:24:32.884: INFO: stderr: ""
Feb 20 22:24:32.884: INFO: stdout: "update-demo-kitten-mpl25 update-demo-kitten-qnbhl "
Feb 20 22:24:32.884: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-mpl25 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:24:32.978: INFO: stderr: ""
Feb 20 22:24:32.978: INFO: stdout: "true"
Feb 20 22:24:32.978: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-mpl25 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:24:33.079: INFO: stderr: ""
Feb 20 22:24:33.079: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 20 22:24:33.079: INFO: validating pod update-demo-kitten-mpl25
Feb 20 22:24:33.181: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 20 22:24:33.181: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 20 22:24:33.182: INFO: update-demo-kitten-mpl25 is verified up and running
Feb 20 22:24:33.182: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-qnbhl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:24:33.287: INFO: stderr: ""
Feb 20 22:24:33.287: INFO: stdout: "true"
Feb 20 22:24:33.287: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-qnbhl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-p4ksd'
Feb 20 22:24:33.390: INFO: stderr: ""
Feb 20 22:24:33.390: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 20 22:24:33.390: INFO: validating pod update-demo-kitten-qnbhl
Feb 20 22:24:33.477: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 20 22:24:33.477: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 20 22:24:33.477: INFO: update-demo-kitten-qnbhl is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:24:33.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-p4ksd" for this suite.
Feb 20 22:24:55.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:24:55.654: INFO: namespace: e2e-tests-kubectl-p4ksd, resource: bindings, ignored listing per whitelist
Feb 20 22:24:55.670: INFO: namespace e2e-tests-kubectl-p4ksd deletion completed in 22.188018349s

â€¢ [SLOW TEST:148.799 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:24:55.670: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-7t6rb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 20 22:24:59.954: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-7t6rb PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:24:59.954: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:25:00.349: INFO: Exec stderr: ""
Feb 20 22:25:00.349: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-7t6rb PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:25:00.349: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:25:00.731: INFO: Exec stderr: ""
Feb 20 22:25:00.731: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-7t6rb PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:25:00.731: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:25:01.129: INFO: Exec stderr: ""
Feb 20 22:25:01.129: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-7t6rb PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:25:01.129: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:25:01.475: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 20 22:25:01.475: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-7t6rb PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:25:01.475: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:25:01.866: INFO: Exec stderr: ""
Feb 20 22:25:01.866: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-7t6rb PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:25:01.866: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:25:02.258: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 20 22:25:02.258: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-7t6rb PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:25:02.258: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:25:02.663: INFO: Exec stderr: ""
Feb 20 22:25:02.663: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-7t6rb PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:25:02.664: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:25:03.069: INFO: Exec stderr: ""
Feb 20 22:25:03.069: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-7t6rb PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:25:03.069: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:25:03.589: INFO: Exec stderr: ""
Feb 20 22:25:03.590: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-7t6rb PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:25:03.590: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:25:04.077: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:25:04.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-7t6rb" for this suite.
Feb 20 22:25:50.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:25:50.214: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-7t6rb, resource: bindings, ignored listing per whitelist
Feb 20 22:25:50.237: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-7t6rb deletion completed in 46.152793461s

â€¢ [SLOW TEST:54.566 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:25:50.237: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-cjs7j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 22:26:12.532: INFO: Container started at 2019-02-20 22:25:51 +0000 UTC, pod became ready at 2019-02-20 22:26:11 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:26:12.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cjs7j" for this suite.
Feb 20 22:26:34.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:26:34.787: INFO: namespace: e2e-tests-container-probe-cjs7j, resource: bindings, ignored listing per whitelist
Feb 20 22:26:34.816: INFO: namespace e2e-tests-container-probe-cjs7j deletion completed in 22.279543658s

â€¢ [SLOW TEST:44.580 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:26:34.817: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-4zbzw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-4zbzw
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-4zbzw
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-4zbzw
Feb 20 22:26:35.174: INFO: Found 0 stateful pods, waiting for 1
Feb 20 22:26:45.180: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 20 22:26:45.185: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zbzw ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 22:26:45.828: INFO: stderr: ""
Feb 20 22:26:45.828: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 22:26:45.828: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 22:26:45.835: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 20 22:26:55.841: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 22:26:55.841: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 22:26:55.858: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999773s
Feb 20 22:26:56.863: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994878784s
Feb 20 22:26:57.868: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99030396s
Feb 20 22:26:58.872: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985326918s
Feb 20 22:26:59.878: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980720692s
Feb 20 22:27:00.884: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97517464s
Feb 20 22:27:01.890: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968963739s
Feb 20 22:27:02.896: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.963028523s
Feb 20 22:27:03.902: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.957076725s
Feb 20 22:27:04.907: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.09915ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-4zbzw
Feb 20 22:27:05.913: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zbzw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 22:27:06.413: INFO: stderr: ""
Feb 20 22:27:06.413: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 22:27:06.413: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 22:27:06.418: INFO: Found 1 stateful pods, waiting for 3
Feb 20 22:27:16.424: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 22:27:16.424: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 22:27:16.424: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 20 22:27:16.432: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zbzw ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 22:27:16.925: INFO: stderr: ""
Feb 20 22:27:16.925: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 22:27:16.925: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 22:27:16.925: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zbzw ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 22:27:17.434: INFO: stderr: ""
Feb 20 22:27:17.434: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 22:27:17.434: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 22:27:17.434: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zbzw ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 22:27:17.948: INFO: stderr: ""
Feb 20 22:27:17.948: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 22:27:17.948: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 22:27:17.948: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 22:27:17.952: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 20 22:27:27.962: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 22:27:27.962: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 22:27:27.962: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 22:27:27.977: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999784s
Feb 20 22:27:28.982: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994813909s
Feb 20 22:27:29.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990048434s
Feb 20 22:27:30.993: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984219684s
Feb 20 22:27:31.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978405763s
Feb 20 22:27:33.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972684147s
Feb 20 22:27:34.009: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967421955s
Feb 20 22:27:35.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962690865s
Feb 20 22:27:36.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95567928s
Feb 20 22:27:37.029: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.824668ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-4zbzw
Feb 20 22:27:38.037: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zbzw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 22:27:38.577: INFO: stderr: ""
Feb 20 22:27:38.577: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 22:27:38.577: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 22:27:38.577: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zbzw ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 22:27:39.052: INFO: stderr: ""
Feb 20 22:27:39.052: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 22:27:39.052: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 22:27:39.052: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-4zbzw ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 22:27:39.521: INFO: stderr: ""
Feb 20 22:27:39.522: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 22:27:39.522: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 22:27:39.522: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 22:28:19.580: INFO: Deleting all statefulset in ns e2e-tests-statefulset-4zbzw
Feb 20 22:28:19.583: INFO: Scaling statefulset ss to 0
Feb 20 22:28:19.593: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 22:28:19.597: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:28:19.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-4zbzw" for this suite.
Feb 20 22:28:25.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:28:25.716: INFO: namespace: e2e-tests-statefulset-4zbzw, resource: bindings, ignored listing per whitelist
Feb 20 22:28:25.841: INFO: namespace e2e-tests-statefulset-4zbzw deletion completed in 6.227705011s

â€¢ [SLOW TEST:111.024 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:28:25.841: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-b744h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Feb 20 22:28:26.064: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b744h'
Feb 20 22:28:26.325: INFO: stderr: ""
Feb 20 22:28:26.325: INFO: stdout: "pod/pause created\n"
Feb 20 22:28:26.325: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 20 22:28:26.325: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-b744h" to be "running and ready"
Feb 20 22:28:26.336: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.273494ms
Feb 20 22:28:28.343: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.017967568s
Feb 20 22:28:28.343: INFO: Pod "pause" satisfied condition "running and ready"
Feb 20 22:28:28.343: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 20 22:28:28.343: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-b744h'
Feb 20 22:28:28.532: INFO: stderr: ""
Feb 20 22:28:28.532: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 20 22:28:28.532: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-b744h'
Feb 20 22:28:28.711: INFO: stderr: ""
Feb 20 22:28:28.711: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 20 22:28:28.711: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml label pods pause testing-label- --namespace=e2e-tests-kubectl-b744h'
Feb 20 22:28:28.910: INFO: stderr: ""
Feb 20 22:28:28.910: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 20 22:28:28.910: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-b744h'
Feb 20 22:28:29.083: INFO: stderr: ""
Feb 20 22:28:29.083: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Feb 20 22:28:29.083: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b744h'
Feb 20 22:28:29.387: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 22:28:29.387: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 20 22:28:29.387: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-b744h'
Feb 20 22:28:29.663: INFO: stderr: "No resources found.\n"
Feb 20 22:28:29.663: INFO: stdout: ""
Feb 20 22:28:29.663: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=pause --namespace=e2e-tests-kubectl-b744h -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 22:28:29.942: INFO: stderr: ""
Feb 20 22:28:29.942: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:28:29.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b744h" for this suite.
Feb 20 22:28:35.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:28:36.041: INFO: namespace: e2e-tests-kubectl-b744h, resource: bindings, ignored listing per whitelist
Feb 20 22:28:36.153: INFO: namespace e2e-tests-kubectl-b744h deletion completed in 6.206166134s

â€¢ [SLOW TEST:10.312 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:28:36.153: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-stqqj
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-dd1c92f7-355e-11e9-8ceb-aaeae242ec87
STEP: Creating configMap with name cm-test-opt-upd-dd1c9332-355e-11e9-8ceb-aaeae242ec87
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-dd1c92f7-355e-11e9-8ceb-aaeae242ec87
STEP: Updating configmap cm-test-opt-upd-dd1c9332-355e-11e9-8ceb-aaeae242ec87
STEP: Creating configMap with name cm-test-opt-create-dd1c934c-355e-11e9-8ceb-aaeae242ec87
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:28:42.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-stqqj" for this suite.
Feb 20 22:29:04.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:29:05.183: INFO: namespace: e2e-tests-projected-stqqj, resource: bindings, ignored listing per whitelist
Feb 20 22:29:05.183: INFO: namespace e2e-tests-projected-stqqj deletion completed in 22.215109959s

â€¢ [SLOW TEST:29.030 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:29:05.183: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-nxw6w
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 20 22:29:05.454: INFO: Waiting up to 5m0s for pod "pod-ee67a604-355e-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-nxw6w" to be "success or failure"
Feb 20 22:29:05.458: INFO: Pod "pod-ee67a604-355e-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16455ms
Feb 20 22:29:07.467: INFO: Pod "pod-ee67a604-355e-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012649099s
STEP: Saw pod success
Feb 20 22:29:07.470: INFO: Pod "pod-ee67a604-355e-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:29:07.473: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-ee67a604-355e-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 22:29:07.526: INFO: Waiting for pod pod-ee67a604-355e-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:29:07.530: INFO: Pod pod-ee67a604-355e-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:29:07.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nxw6w" for this suite.
Feb 20 22:29:13.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:29:13.707: INFO: namespace: e2e-tests-emptydir-nxw6w, resource: bindings, ignored listing per whitelist
Feb 20 22:29:13.753: INFO: namespace e2e-tests-emptydir-nxw6w deletion completed in 6.217341662s

â€¢ [SLOW TEST:8.570 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:29:13.753: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-nmdpt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-r7ndd
STEP: Creating secret with name secret-test-f380bca7-355e-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 22:29:14.159: INFO: Waiting up to 5m0s for pod "pod-secrets-f397f223-355e-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-secrets-nmdpt" to be "success or failure"
Feb 20 22:29:14.163: INFO: Pod "pod-secrets-f397f223-355e-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369355ms
Feb 20 22:29:16.168: INFO: Pod "pod-secrets-f397f223-355e-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009060961s
STEP: Saw pod success
Feb 20 22:29:16.168: INFO: Pod "pod-secrets-f397f223-355e-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:29:16.172: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-secrets-f397f223-355e-11e9-8ceb-aaeae242ec87 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 22:29:16.226: INFO: Waiting for pod pod-secrets-f397f223-355e-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:29:16.229: INFO: Pod pod-secrets-f397f223-355e-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:29:16.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-nmdpt" for this suite.
Feb 20 22:29:22.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:29:22.325: INFO: namespace: e2e-tests-secrets-nmdpt, resource: bindings, ignored listing per whitelist
Feb 20 22:29:22.419: INFO: namespace e2e-tests-secrets-nmdpt deletion completed in 6.18635073s
STEP: Destroying namespace "e2e-tests-secret-namespace-r7ndd" for this suite.
Feb 20 22:29:28.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:29:28.661: INFO: namespace: e2e-tests-secret-namespace-r7ndd, resource: bindings, ignored listing per whitelist
Feb 20 22:29:28.675: INFO: namespace e2e-tests-secret-namespace-r7ndd deletion completed in 6.255543824s

â€¢ [SLOW TEST:14.922 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:29:28.675: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-mz49n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 20 22:29:31.476: INFO: Successfully updated pod "pod-update-activedeadlineseconds-fc6765b2-355e-11e9-8ceb-aaeae242ec87"
Feb 20 22:29:31.476: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-fc6765b2-355e-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-pods-mz49n" to be "terminated due to deadline exceeded"
Feb 20 22:29:31.480: INFO: Pod "pod-update-activedeadlineseconds-fc6765b2-355e-11e9-8ceb-aaeae242ec87": Phase="Running", Reason="", readiness=true. Elapsed: 3.942213ms
Feb 20 22:29:33.485: INFO: Pod "pod-update-activedeadlineseconds-fc6765b2-355e-11e9-8ceb-aaeae242ec87": Phase="Running", Reason="", readiness=true. Elapsed: 2.009126378s
Feb 20 22:29:35.490: INFO: Pod "pod-update-activedeadlineseconds-fc6765b2-355e-11e9-8ceb-aaeae242ec87": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.014329392s
Feb 20 22:29:35.490: INFO: Pod "pod-update-activedeadlineseconds-fc6765b2-355e-11e9-8ceb-aaeae242ec87" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:29:35.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-mz49n" for this suite.
Feb 20 22:29:41.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:29:41.538: INFO: namespace: e2e-tests-pods-mz49n, resource: bindings, ignored listing per whitelist
Feb 20 22:29:41.658: INFO: namespace e2e-tests-pods-mz49n deletion completed in 6.162741861s

â€¢ [SLOW TEST:12.983 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:29:41.658: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-5shqq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-5shqq
Feb 20 22:29:43.967: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-5shqq
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 22:29:43.973: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:33:44.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-5shqq" for this suite.
Feb 20 22:33:50.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:33:50.870: INFO: namespace: e2e-tests-container-probe-5shqq, resource: bindings, ignored listing per whitelist
Feb 20 22:33:50.997: INFO: namespace e2e-tests-container-probe-5shqq deletion completed in 6.234384559s

â€¢ [SLOW TEST:249.339 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:33:50.997: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jpq6d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 22:33:51.269: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-jpq6d'
Feb 20 22:33:51.946: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 20 22:33:51.946: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Feb 20 22:33:51.949: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-jpq6d'
Feb 20 22:33:52.061: INFO: stderr: ""
Feb 20 22:33:52.061: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:33:52.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jpq6d" for this suite.
Feb 20 22:33:58.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:33:58.165: INFO: namespace: e2e-tests-kubectl-jpq6d, resource: bindings, ignored listing per whitelist
Feb 20 22:33:58.309: INFO: namespace e2e-tests-kubectl-jpq6d deletion completed in 6.243211046s

â€¢ [SLOW TEST:7.312 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:33:58.309: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-2bfdp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 20 22:33:58.588: INFO: Waiting up to 5m0s for pod "var-expansion-9d205f36-355f-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-var-expansion-2bfdp" to be "success or failure"
Feb 20 22:33:58.594: INFO: Pod "var-expansion-9d205f36-355f-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.772966ms
Feb 20 22:34:00.600: INFO: Pod "var-expansion-9d205f36-355f-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011622545s
STEP: Saw pod success
Feb 20 22:34:00.600: INFO: Pod "var-expansion-9d205f36-355f-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:34:00.604: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod var-expansion-9d205f36-355f-11e9-8ceb-aaeae242ec87 container dapi-container: <nil>
STEP: delete the pod
Feb 20 22:34:00.624: INFO: Waiting for pod var-expansion-9d205f36-355f-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:34:00.627: INFO: Pod var-expansion-9d205f36-355f-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:34:00.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-2bfdp" for this suite.
Feb 20 22:34:06.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:34:06.671: INFO: namespace: e2e-tests-var-expansion-2bfdp, resource: bindings, ignored listing per whitelist
Feb 20 22:34:06.860: INFO: namespace e2e-tests-var-expansion-2bfdp deletion completed in 6.229276975s

â€¢ [SLOW TEST:8.551 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:34:06.861: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-whgb4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-a23b9713-355f-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 22:34:07.160: INFO: Waiting up to 5m0s for pod "pod-configmaps-a23c480a-355f-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-configmap-whgb4" to be "success or failure"
Feb 20 22:34:07.163: INFO: Pod "pod-configmaps-a23c480a-355f-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.219231ms
Feb 20 22:34:09.168: INFO: Pod "pod-configmaps-a23c480a-355f-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007761422s
STEP: Saw pod success
Feb 20 22:34:09.168: INFO: Pod "pod-configmaps-a23c480a-355f-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:34:09.171: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-configmaps-a23c480a-355f-11e9-8ceb-aaeae242ec87 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 22:34:09.228: INFO: Waiting for pod pod-configmaps-a23c480a-355f-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:34:09.231: INFO: Pod pod-configmaps-a23c480a-355f-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:34:09.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-whgb4" for this suite.
Feb 20 22:34:15.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:34:15.263: INFO: namespace: e2e-tests-configmap-whgb4, resource: bindings, ignored listing per whitelist
Feb 20 22:34:15.432: INFO: namespace e2e-tests-configmap-whgb4 deletion completed in 6.196393007s

â€¢ [SLOW TEST:8.572 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:34:15.432: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-78qc7
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-a754763b-355f-11e9-8ceb-aaeae242ec87
STEP: Creating secret with name s-test-opt-upd-a75476a2-355f-11e9-8ceb-aaeae242ec87
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a754763b-355f-11e9-8ceb-aaeae242ec87
STEP: Updating secret s-test-opt-upd-a75476a2-355f-11e9-8ceb-aaeae242ec87
STEP: Creating secret with name s-test-opt-create-a75476c5-355f-11e9-8ceb-aaeae242ec87
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:34:20.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-78qc7" for this suite.
Feb 20 22:34:42.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:34:42.277: INFO: namespace: e2e-tests-projected-78qc7, resource: bindings, ignored listing per whitelist
Feb 20 22:34:42.413: INFO: namespace e2e-tests-projected-78qc7 deletion completed in 22.201643857s

â€¢ [SLOW TEST:26.981 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:34:42.413: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jfbmw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 22:34:42.676: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-jfbmw'
Feb 20 22:34:42.798: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 20 22:34:42.798: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Feb 20 22:34:44.816: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-jfbmw'
Feb 20 22:34:44.939: INFO: stderr: ""
Feb 20 22:34:44.939: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:34:44.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jfbmw" for this suite.
Feb 20 22:35:06.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:35:07.054: INFO: namespace: e2e-tests-kubectl-jfbmw, resource: bindings, ignored listing per whitelist
Feb 20 22:35:07.181: INFO: namespace e2e-tests-kubectl-jfbmw deletion completed in 22.237526191s

â€¢ [SLOW TEST:24.768 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:35:07.181: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-vxt7g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 20 22:35:07.462: INFO: Waiting up to 5m0s for pod "pod-c62d8e73-355f-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-emptydir-vxt7g" to be "success or failure"
Feb 20 22:35:07.482: INFO: Pod "pod-c62d8e73-355f-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 19.875227ms
Feb 20 22:35:09.488: INFO: Pod "pod-c62d8e73-355f-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025752296s
STEP: Saw pod success
Feb 20 22:35:09.488: INFO: Pod "pod-c62d8e73-355f-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:35:09.494: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-c62d8e73-355f-11e9-8ceb-aaeae242ec87 container test-container: <nil>
STEP: delete the pod
Feb 20 22:35:09.518: INFO: Waiting for pod pod-c62d8e73-355f-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:35:09.522: INFO: Pod pod-c62d8e73-355f-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:35:09.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vxt7g" for this suite.
Feb 20 22:35:15.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:35:15.614: INFO: namespace: e2e-tests-emptydir-vxt7g, resource: bindings, ignored listing per whitelist
Feb 20 22:35:15.784: INFO: namespace e2e-tests-emptydir-vxt7g deletion completed in 6.255228477s

â€¢ [SLOW TEST:8.602 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:35:15.784: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-h7lmp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 22:35:16.071: INFO: Creating deployment "test-recreate-deployment"
Feb 20 22:35:16.075: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 20 22:35:16.083: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Feb 20 22:35:18.092: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 20 22:35:18.096: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 20 22:35:18.105: INFO: Updating deployment test-recreate-deployment
Feb 20 22:35:18.105: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 22:35:18.157: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-h7lmp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h7lmp/deployments/test-recreate-deployment,UID:cb50b5c2-355f-11e9-91b0-3e11857925bf,ResourceVersion:21694,Generation:2,CreationTimestamp:2019-02-20 22:35:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-20 22:35:18 +0000 UTC 2019-02-20 22:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-20 22:35:18 +0000 UTC 2019-02-20 22:35:16 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 20 22:35:18.161: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-h7lmp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h7lmp/replicasets/test-recreate-deployment-697fbf54bf,UID:cc8b114b-355f-11e9-91b0-3e11857925bf,ResourceVersion:21693,Generation:1,CreationTimestamp:2019-02-20 22:35:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment cb50b5c2-355f-11e9-91b0-3e11857925bf 0xc001db3917 0xc001db3918}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 22:35:18.161: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 20 22:35:18.161: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-h7lmp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h7lmp/replicasets/test-recreate-deployment-5dfdcc846d,UID:cb521202-355f-11e9-91b0-3e11857925bf,ResourceVersion:21686,Generation:2,CreationTimestamp:2019-02-20 22:35:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment cb50b5c2-355f-11e9-91b0-3e11857925bf 0xc001db3647 0xc001db3648}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 22:35:18.165: INFO: Pod "test-recreate-deployment-697fbf54bf-vqscx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-vqscx,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-h7lmp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h7lmp/pods/test-recreate-deployment-697fbf54bf-vqscx,UID:cc8b9b6d-355f-11e9-91b0-3e11857925bf,ResourceVersion:21695,Generation:0,CreationTimestamp:2019-02-20 22:35:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf cc8b114b-355f-11e9-91b0-3e11857925bf 0xc001da71d7 0xc001da71d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-9ndjw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ndjw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ndjw true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001da74d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001da74f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 22:35:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 22:35:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 22:35:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 22:35:18 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.17,PodIP:,StartTime:2019-02-20 22:35:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:35:18.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-h7lmp" for this suite.
Feb 20 22:35:24.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:35:24.297: INFO: namespace: e2e-tests-deployment-h7lmp, resource: bindings, ignored listing per whitelist
Feb 20 22:35:24.391: INFO: namespace e2e-tests-deployment-h7lmp deletion completed in 6.221554161s

â€¢ [SLOW TEST:8.607 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:35:24.391: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-x8t4k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 20 22:35:25.292: INFO: created pod pod-service-account-defaultsa
Feb 20 22:35:25.292: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 20 22:35:25.297: INFO: created pod pod-service-account-mountsa
Feb 20 22:35:25.297: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 20 22:35:25.302: INFO: created pod pod-service-account-nomountsa
Feb 20 22:35:25.303: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 20 22:35:25.308: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 20 22:35:25.308: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 20 22:35:25.314: INFO: created pod pod-service-account-mountsa-mountspec
Feb 20 22:35:25.314: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 20 22:35:25.321: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 20 22:35:25.321: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 20 22:35:25.359: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 20 22:35:25.359: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 20 22:35:25.364: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 20 22:35:25.364: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 20 22:35:25.369: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 20 22:35:25.369: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:35:25.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-x8t4k" for this suite.
Feb 20 22:35:47.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:35:47.441: INFO: namespace: e2e-tests-svcaccounts-x8t4k, resource: bindings, ignored listing per whitelist
Feb 20 22:35:47.632: INFO: namespace e2e-tests-svcaccounts-x8t4k deletion completed in 22.257698377s

â€¢ [SLOW TEST:23.241 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:35:47.632: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2m9jt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 22:35:47.903: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de485dbb-355f-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-2m9jt" to be "success or failure"
Feb 20 22:35:47.906: INFO: Pod "downwardapi-volume-de485dbb-355f-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.367633ms
Feb 20 22:35:49.912: INFO: Pod "downwardapi-volume-de485dbb-355f-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008760466s
STEP: Saw pod success
Feb 20 22:35:49.912: INFO: Pod "downwardapi-volume-de485dbb-355f-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:35:49.916: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-de485dbb-355f-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 22:35:49.946: INFO: Waiting for pod downwardapi-volume-de485dbb-355f-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:35:49.949: INFO: Pod downwardapi-volume-de485dbb-355f-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:35:49.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2m9jt" for this suite.
Feb 20 22:35:55.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:35:56.134: INFO: namespace: e2e-tests-projected-2m9jt, resource: bindings, ignored listing per whitelist
Feb 20 22:35:56.184: INFO: namespace e2e-tests-projected-2m9jt deletion completed in 6.229913044s

â€¢ [SLOW TEST:8.552 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:35:56.184: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-92dwn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 22:35:56.454: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3613c9c-355f-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-92dwn" to be "success or failure"
Feb 20 22:35:56.457: INFO: Pod "downwardapi-volume-e3613c9c-355f-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.483613ms
Feb 20 22:35:58.462: INFO: Pod "downwardapi-volume-e3613c9c-355f-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008710252s
STEP: Saw pod success
Feb 20 22:35:58.462: INFO: Pod "downwardapi-volume-e3613c9c-355f-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:35:58.467: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-e3613c9c-355f-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 22:35:58.489: INFO: Waiting for pod downwardapi-volume-e3613c9c-355f-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:35:58.493: INFO: Pod downwardapi-volume-e3613c9c-355f-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:35:58.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-92dwn" for this suite.
Feb 20 22:36:04.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:36:04.655: INFO: namespace: e2e-tests-downward-api-92dwn, resource: bindings, ignored listing per whitelist
Feb 20 22:36:04.743: INFO: namespace e2e-tests-downward-api-92dwn deletion completed in 6.243752561s

â€¢ [SLOW TEST:8.559 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:36:04.743: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-59xrw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 22:36:04.965: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml version'
Feb 20 22:36:05.089: INFO: stderr: ""
Feb 20 22:36:05.089: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"archive\", BuildDate:\"2019-02-20T20:51:55Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:36:05.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-59xrw" for this suite.
Feb 20 22:36:11.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:36:11.499: INFO: namespace: e2e-tests-kubectl-59xrw, resource: bindings, ignored listing per whitelist
Feb 20 22:36:11.536: INFO: namespace e2e-tests-kubectl-59xrw deletion completed in 6.442161786s

â€¢ [SLOW TEST:6.793 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:36:11.536: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-lvp4g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-lvp4g
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 22:36:11.851: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 22:36:33.940: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.245:8080/dial?request=hostName&protocol=udp&host=100.96.0.68&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-lvp4g PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:36:33.940: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:36:34.341: INFO: Waiting for endpoints: map[]
Feb 20 22:36:34.350: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.245:8080/dial?request=hostName&protocol=udp&host=100.96.1.244&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-lvp4g PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 22:36:34.350: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 20 22:36:34.776: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:36:34.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-lvp4g" for this suite.
Feb 20 22:36:56.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:36:56.925: INFO: namespace: e2e-tests-pod-network-test-lvp4g, resource: bindings, ignored listing per whitelist
Feb 20 22:36:57.139: INFO: namespace e2e-tests-pod-network-test-lvp4g deletion completed in 22.357461448s

â€¢ [SLOW TEST:45.603 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:36:57.140: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4lqk5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-07c1d639-3560-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 22:36:57.492: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07c2b942-3560-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-4lqk5" to be "success or failure"
Feb 20 22:36:57.506: INFO: Pod "pod-projected-configmaps-07c2b942-3560-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 13.149293ms
Feb 20 22:36:59.511: INFO: Pod "pod-projected-configmaps-07c2b942-3560-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018005292s
STEP: Saw pod success
Feb 20 22:36:59.511: INFO: Pod "pod-projected-configmaps-07c2b942-3560-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:36:59.514: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-configmaps-07c2b942-3560-11e9-8ceb-aaeae242ec87 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 22:36:59.538: INFO: Waiting for pod pod-projected-configmaps-07c2b942-3560-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:36:59.542: INFO: Pod pod-projected-configmaps-07c2b942-3560-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:36:59.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4lqk5" for this suite.
Feb 20 22:37:05.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:37:05.730: INFO: namespace: e2e-tests-projected-4lqk5, resource: bindings, ignored listing per whitelist
Feb 20 22:37:05.943: INFO: namespace e2e-tests-projected-4lqk5 deletion completed in 6.396114257s

â€¢ [SLOW TEST:8.804 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:37:05.943: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-7l2s4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-0d0303ca-3560-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 22:37:06.314: INFO: Waiting up to 5m0s for pod "pod-secrets-0d044064-3560-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-secrets-7l2s4" to be "success or failure"
Feb 20 22:37:06.323: INFO: Pod "pod-secrets-0d044064-3560-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 8.452992ms
Feb 20 22:37:08.332: INFO: Pod "pod-secrets-0d044064-3560-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017654297s
STEP: Saw pod success
Feb 20 22:37:08.332: INFO: Pod "pod-secrets-0d044064-3560-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:37:08.342: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-secrets-0d044064-3560-11e9-8ceb-aaeae242ec87 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 22:37:08.393: INFO: Waiting for pod pod-secrets-0d044064-3560-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:37:08.401: INFO: Pod pod-secrets-0d044064-3560-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:37:08.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7l2s4" for this suite.
Feb 20 22:37:14.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:37:14.758: INFO: namespace: e2e-tests-secrets-7l2s4, resource: bindings, ignored listing per whitelist
Feb 20 22:37:14.759: INFO: namespace e2e-tests-secrets-7l2s4 deletion completed in 6.344018102s

â€¢ [SLOW TEST:8.815 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:37:14.759: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-h4bwx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 22:37:15.154: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1249681b-3560-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-h4bwx" to be "success or failure"
Feb 20 22:37:15.162: INFO: Pod "downwardapi-volume-1249681b-3560-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 7.486538ms
Feb 20 22:37:17.168: INFO: Pod "downwardapi-volume-1249681b-3560-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013486721s
STEP: Saw pod success
Feb 20 22:37:17.168: INFO: Pod "downwardapi-volume-1249681b-3560-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:37:17.173: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-1249681b-3560-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 22:37:17.216: INFO: Waiting for pod downwardapi-volume-1249681b-3560-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:37:17.220: INFO: Pod downwardapi-volume-1249681b-3560-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:37:17.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h4bwx" for this suite.
Feb 20 22:37:23.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:37:23.427: INFO: namespace: e2e-tests-projected-h4bwx, resource: bindings, ignored listing per whitelist
Feb 20 22:37:23.493: INFO: namespace e2e-tests-projected-h4bwx deletion completed in 6.263321923s

â€¢ [SLOW TEST:8.734 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:37:23.493: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-6x7pc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-6x7pc
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-6x7pc
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-6x7pc
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-6x7pc
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-6x7pc
Feb 20 22:37:25.747: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-6x7pc, name: ss-0, uid: 1899f0a1-3560-11e9-91b0-3e11857925bf, status phase: Pending. Waiting for statefulset controller to delete.
Feb 20 22:37:25.753: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-6x7pc, name: ss-0, uid: 1899f0a1-3560-11e9-91b0-3e11857925bf, status phase: Failed. Waiting for statefulset controller to delete.
Feb 20 22:37:25.843: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-6x7pc, name: ss-0, uid: 1899f0a1-3560-11e9-91b0-3e11857925bf, status phase: Failed. Waiting for statefulset controller to delete.
Feb 20 22:37:25.845: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-6x7pc
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-6x7pc
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-6x7pc and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 22:37:27.876: INFO: Deleting all statefulset in ns e2e-tests-statefulset-6x7pc
Feb 20 22:37:27.882: INFO: Scaling statefulset ss to 0
Feb 20 22:37:47.916: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 22:37:47.920: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:37:47.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-6x7pc" for this suite.
Feb 20 22:37:53.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:37:54.065: INFO: namespace: e2e-tests-statefulset-6x7pc, resource: bindings, ignored listing per whitelist
Feb 20 22:37:54.139: INFO: namespace e2e-tests-statefulset-6x7pc deletion completed in 6.201037092s

â€¢ [SLOW TEST:30.647 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:37:54.141: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6kbf9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-29acabc8-3560-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 22:37:54.394: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-29ad6389-3560-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-projected-6kbf9" to be "success or failure"
Feb 20 22:37:54.403: INFO: Pod "pod-projected-configmaps-29ad6389-3560-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 8.476751ms
Feb 20 22:37:56.409: INFO: Pod "pod-projected-configmaps-29ad6389-3560-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014252212s
STEP: Saw pod success
Feb 20 22:37:56.409: INFO: Pod "pod-projected-configmaps-29ad6389-3560-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:37:56.412: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-projected-configmaps-29ad6389-3560-11e9-8ceb-aaeae242ec87 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 22:37:56.434: INFO: Waiting for pod pod-projected-configmaps-29ad6389-3560-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:37:56.437: INFO: Pod pod-projected-configmaps-29ad6389-3560-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:37:56.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6kbf9" for this suite.
Feb 20 22:38:02.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:38:02.625: INFO: namespace: e2e-tests-projected-6kbf9, resource: bindings, ignored listing per whitelist
Feb 20 22:38:02.664: INFO: namespace e2e-tests-projected-6kbf9 deletion completed in 6.21757362s

â€¢ [SLOW TEST:8.523 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:38:02.664: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-cdm4k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-cdm4k/configmap-test-2ec38aa1-3560-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume configMaps
Feb 20 22:38:02.932: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ec43e55-3560-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-configmap-cdm4k" to be "success or failure"
Feb 20 22:38:02.937: INFO: Pod "pod-configmaps-2ec43e55-3560-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088664ms
Feb 20 22:38:04.941: INFO: Pod "pod-configmaps-2ec43e55-3560-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008230118s
STEP: Saw pod success
Feb 20 22:38:04.941: INFO: Pod "pod-configmaps-2ec43e55-3560-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:38:04.945: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-configmaps-2ec43e55-3560-11e9-8ceb-aaeae242ec87 container env-test: <nil>
STEP: delete the pod
Feb 20 22:38:04.964: INFO: Waiting for pod pod-configmaps-2ec43e55-3560-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:38:04.967: INFO: Pod pod-configmaps-2ec43e55-3560-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:38:04.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cdm4k" for this suite.
Feb 20 22:38:10.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:38:11.139: INFO: namespace: e2e-tests-configmap-cdm4k, resource: bindings, ignored listing per whitelist
Feb 20 22:38:11.197: INFO: namespace e2e-tests-configmap-cdm4k deletion completed in 6.226811466s

â€¢ [SLOW TEST:8.533 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:38:11.199: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-cjhs7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:38:13.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-cjhs7" for this suite.
Feb 20 22:38:19.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:38:19.599: INFO: namespace: e2e-tests-emptydir-wrapper-cjhs7, resource: bindings, ignored listing per whitelist
Feb 20 22:38:19.693: INFO: namespace e2e-tests-emptydir-wrapper-cjhs7 deletion completed in 6.194121332s

â€¢ [SLOW TEST:8.494 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:38:19.693: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-tl6t5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 20 22:38:20.176: INFO: Pod name wrapped-volume-race-390a39d8-3560-11e9-8ceb-aaeae242ec87: Found 0 pods out of 5
Feb 20 22:38:25.190: INFO: Pod name wrapped-volume-race-390a39d8-3560-11e9-8ceb-aaeae242ec87: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-390a39d8-3560-11e9-8ceb-aaeae242ec87 in namespace e2e-tests-emptydir-wrapper-tl6t5, will wait for the garbage collector to delete the pods
Feb 20 22:40:29.275: INFO: Deleting ReplicationController wrapped-volume-race-390a39d8-3560-11e9-8ceb-aaeae242ec87 took: 7.609613ms
Feb 20 22:40:29.376: INFO: Terminating ReplicationController wrapped-volume-race-390a39d8-3560-11e9-8ceb-aaeae242ec87 pods took: 100.287293ms
STEP: Creating RC which spawns configmap-volume pods
Feb 20 22:41:08.595: INFO: Pod name wrapped-volume-race-9d6c4ee0-3560-11e9-8ceb-aaeae242ec87: Found 0 pods out of 5
Feb 20 22:41:13.605: INFO: Pod name wrapped-volume-race-9d6c4ee0-3560-11e9-8ceb-aaeae242ec87: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9d6c4ee0-3560-11e9-8ceb-aaeae242ec87 in namespace e2e-tests-emptydir-wrapper-tl6t5, will wait for the garbage collector to delete the pods
Feb 20 22:41:13.694: INFO: Deleting ReplicationController wrapped-volume-race-9d6c4ee0-3560-11e9-8ceb-aaeae242ec87 took: 8.469158ms
Feb 20 22:41:13.794: INFO: Terminating ReplicationController wrapped-volume-race-9d6c4ee0-3560-11e9-8ceb-aaeae242ec87 pods took: 100.238215ms
STEP: Creating RC which spawns configmap-volume pods
Feb 20 22:41:58.516: INFO: Pod name wrapped-volume-race-bb2d46d6-3560-11e9-8ceb-aaeae242ec87: Found 0 pods out of 5
Feb 20 22:42:03.532: INFO: Pod name wrapped-volume-race-bb2d46d6-3560-11e9-8ceb-aaeae242ec87: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bb2d46d6-3560-11e9-8ceb-aaeae242ec87 in namespace e2e-tests-emptydir-wrapper-tl6t5, will wait for the garbage collector to delete the pods
Feb 20 22:42:03.620: INFO: Deleting ReplicationController wrapped-volume-race-bb2d46d6-3560-11e9-8ceb-aaeae242ec87 took: 7.479414ms
Feb 20 22:42:03.720: INFO: Terminating ReplicationController wrapped-volume-race-bb2d46d6-3560-11e9-8ceb-aaeae242ec87 pods took: 100.238586ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:42:48.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-tl6t5" for this suite.
Feb 20 22:42:54.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:42:55.014: INFO: namespace: e2e-tests-emptydir-wrapper-tl6t5, resource: bindings, ignored listing per whitelist
Feb 20 22:42:55.014: INFO: namespace e2e-tests-emptydir-wrapper-tl6t5 deletion completed in 6.211559549s

â€¢ [SLOW TEST:275.321 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:42:55.015: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-v2bv7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 20 22:42:55.297: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml api-versions'
Feb 20 22:42:55.546: INFO: stderr: ""
Feb 20 22:42:55.546: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:42:55.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v2bv7" for this suite.
Feb 20 22:43:01.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:43:01.638: INFO: namespace: e2e-tests-kubectl-v2bv7, resource: bindings, ignored listing per whitelist
Feb 20 22:43:01.718: INFO: namespace e2e-tests-kubectl-v2bv7 deletion completed in 6.167181947s

â€¢ [SLOW TEST:6.704 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:43:01.719: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-w68z9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-w68z9
Feb 20 22:43:03.957: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-w68z9
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 22:43:03.961: INFO: Initial restart count of pod liveness-exec is 0
Feb 20 22:44:00.121: INFO: Restart count of pod e2e-tests-container-probe-w68z9/liveness-exec is now 1 (56.160295828s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:44:00.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-w68z9" for this suite.
Feb 20 22:44:06.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:44:06.366: INFO: namespace: e2e-tests-container-probe-w68z9, resource: bindings, ignored listing per whitelist
Feb 20 22:44:06.421: INFO: namespace e2e-tests-container-probe-w68z9 deletion completed in 6.275080006s

â€¢ [SLOW TEST:64.702 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:44:06.421: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-wtcsv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 20 22:44:08.674: INFO: Pod pod-hostip-078ffbe2-3561-11e9-8ceb-aaeae242ec87 has hostIP: 10.250.0.17
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:44:08.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-wtcsv" for this suite.
Feb 20 22:44:30.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:44:30.872: INFO: namespace: e2e-tests-pods-wtcsv, resource: bindings, ignored listing per whitelist
Feb 20 22:44:30.933: INFO: namespace e2e-tests-pods-wtcsv deletion completed in 22.253219869s

â€¢ [SLOW TEST:24.512 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:44:30.934: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qxsxq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 22:44:31.210: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-qxsxq'
Feb 20 22:44:32.083: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 20 22:44:32.083: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 20 22:44:32.090: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 20 22:44:32.094: INFO: scanned /root for discovery docs: <nil>
Feb 20 22:44:32.094: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-qxsxq'
Feb 20 22:44:47.906: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 20 22:44:47.906: INFO: stdout: "Created e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a\nScaling up e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 20 22:44:47.906: INFO: stdout: "Created e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a\nScaling up e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 20 22:44:47.906: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qxsxq'
Feb 20 22:44:48.057: INFO: stderr: ""
Feb 20 22:44:48.057: INFO: stdout: "e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a-rhqzb "
Feb 20 22:44:48.057: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a-rhqzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qxsxq'
Feb 20 22:44:48.151: INFO: stderr: ""
Feb 20 22:44:48.151: INFO: stdout: "true"
Feb 20 22:44:48.151: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a-rhqzb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qxsxq'
Feb 20 22:44:48.253: INFO: stderr: ""
Feb 20 22:44:48.253: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 20 22:44:48.253: INFO: e2e-test-nginx-rc-19c5924f8e0d21975a3adf5f22f0053a-rhqzb is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Feb 20 22:44:48.254: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-nyl25.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qxsxq'
Feb 20 22:44:48.370: INFO: stderr: ""
Feb 20 22:44:48.370: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:44:48.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qxsxq" for this suite.
Feb 20 22:44:54.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:44:54.593: INFO: namespace: e2e-tests-kubectl-qxsxq, resource: bindings, ignored listing per whitelist
Feb 20 22:44:54.634: INFO: namespace e2e-tests-kubectl-qxsxq deletion completed in 6.251282749s

â€¢ [SLOW TEST:23.701 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:44:54.635: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2jbbm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 22:44:54.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-244c4cc3-3561-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-downward-api-2jbbm" to be "success or failure"
Feb 20 22:44:54.870: INFO: Pod "downwardapi-volume-244c4cc3-3561-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.321906ms
Feb 20 22:44:56.874: INFO: Pod "downwardapi-volume-244c4cc3-3561-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007802022s
STEP: Saw pod success
Feb 20 22:44:56.874: INFO: Pod "downwardapi-volume-244c4cc3-3561-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:44:56.878: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod downwardapi-volume-244c4cc3-3561-11e9-8ceb-aaeae242ec87 container client-container: <nil>
STEP: delete the pod
Feb 20 22:44:56.897: INFO: Waiting for pod downwardapi-volume-244c4cc3-3561-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:44:56.900: INFO: Pod downwardapi-volume-244c4cc3-3561-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:44:56.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2jbbm" for this suite.
Feb 20 22:45:02.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:45:03.140: INFO: namespace: e2e-tests-downward-api-2jbbm, resource: bindings, ignored listing per whitelist
Feb 20 22:45:03.148: INFO: namespace e2e-tests-downward-api-2jbbm deletion completed in 6.242880402s

â€¢ [SLOW TEST:8.513 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:45:03.148: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-mkzb2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 22:45:05.959: INFO: Successfully updated pod "annotationupdate2961eeec-3561-11e9-8ceb-aaeae242ec87"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:45:09.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mkzb2" for this suite.
Feb 20 22:45:32.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:45:32.087: INFO: namespace: e2e-tests-downward-api-mkzb2, resource: bindings, ignored listing per whitelist
Feb 20 22:45:32.151: INFO: namespace e2e-tests-downward-api-mkzb2 deletion completed in 22.151976819s

â€¢ [SLOW TEST:29.003 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:45:32.152: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-hr6bd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 20 22:45:32.394: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-hr6bd,SelfLink:/api/v1/namespaces/e2e-tests-watch-hr6bd/configmaps/e2e-watch-test-watch-closed,UID:3aaa4e8d-3561-11e9-91b0-3e11857925bf,ResourceVersion:23761,Generation:0,CreationTimestamp:2019-02-20 22:45:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 22:45:32.394: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-hr6bd,SelfLink:/api/v1/namespaces/e2e-tests-watch-hr6bd/configmaps/e2e-watch-test-watch-closed,UID:3aaa4e8d-3561-11e9-91b0-3e11857925bf,ResourceVersion:23762,Generation:0,CreationTimestamp:2019-02-20 22:45:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 20 22:45:32.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-hr6bd,SelfLink:/api/v1/namespaces/e2e-tests-watch-hr6bd/configmaps/e2e-watch-test-watch-closed,UID:3aaa4e8d-3561-11e9-91b0-3e11857925bf,ResourceVersion:23763,Generation:0,CreationTimestamp:2019-02-20 22:45:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 22:45:32.409: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-hr6bd,SelfLink:/api/v1/namespaces/e2e-tests-watch-hr6bd/configmaps/e2e-watch-test-watch-closed,UID:3aaa4e8d-3561-11e9-91b0-3e11857925bf,ResourceVersion:23764,Generation:0,CreationTimestamp:2019-02-20 22:45:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:45:32.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-hr6bd" for this suite.
Feb 20 22:45:38.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:45:38.536: INFO: namespace: e2e-tests-watch-hr6bd, resource: bindings, ignored listing per whitelist
Feb 20 22:45:38.611: INFO: namespace e2e-tests-watch-hr6bd deletion completed in 6.19742503s

â€¢ [SLOW TEST:6.459 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:45:38.611: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-8g2tk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:46:38.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-8g2tk" for this suite.
Feb 20 22:47:02.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:47:02.954: INFO: namespace: e2e-tests-container-probe-8g2tk, resource: bindings, ignored listing per whitelist
Feb 20 22:47:03.098: INFO: namespace e2e-tests-container-probe-8g2tk deletion completed in 24.253145551s

â€¢ [SLOW TEST:84.487 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 20 22:47:03.098: INFO: >>> kubeConfig: /tmp/build/a8535ce5/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-zk9qp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-70f28d37-3561-11e9-8ceb-aaeae242ec87
STEP: Creating a pod to test consume secrets
Feb 20 22:47:03.472: INFO: Waiting up to 5m0s for pod "pod-secrets-70f37e71-3561-11e9-8ceb-aaeae242ec87" in namespace "e2e-tests-secrets-zk9qp" to be "success or failure"
Feb 20 22:47:03.478: INFO: Pod "pod-secrets-70f37e71-3561-11e9-8ceb-aaeae242ec87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.284893ms
Feb 20 22:47:05.489: INFO: Pod "pod-secrets-70f37e71-3561-11e9-8ceb-aaeae242ec87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017203408s
STEP: Saw pod success
Feb 20 22:47:05.489: INFO: Pod "pod-secrets-70f37e71-3561-11e9-8ceb-aaeae242ec87" satisfied condition "success or failure"
Feb 20 22:47:05.498: INFO: Trying to get logs from node shoot--it--pub-os-nyl25-cpu-worker-z1-666b4c6cf5-mxgbq pod pod-secrets-70f37e71-3561-11e9-8ceb-aaeae242ec87 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 22:47:05.529: INFO: Waiting for pod pod-secrets-70f37e71-3561-11e9-8ceb-aaeae242ec87 to disappear
Feb 20 22:47:05.540: INFO: Pod pod-secrets-70f37e71-3561-11e9-8ceb-aaeae242ec87 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 20 22:47:05.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-zk9qp" for this suite.
Feb 20 22:47:11.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 22:47:11.621: INFO: namespace: e2e-tests-secrets-zk9qp, resource: bindings, ignored listing per whitelist
Feb 20 22:47:11.710: INFO: namespace e2e-tests-secrets-zk9qp deletion completed in 6.158060068s

â€¢ [SLOW TEST:8.611 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSFeb 20 22:47:11.710: INFO: Running AfterSuite actions on all nodes
Feb 20 22:47:11.710: INFO: Running AfterSuite actions on node 1
Feb 20 22:47:11.710: INFO: Skipping dumping logs from cluster

Ran 200 of 2161 Specs in 6800.349 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Flaked | 0 Pending | 1961 Skipped PASS

Ginkgo ran 1 suite in 1h53m21.198960441s
Test Suite Passed
