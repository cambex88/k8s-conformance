Conformance test: not doing test setup.
I0213 22:31:12.965783   31687 e2e.go:224] Starting e2e run "111bad8d-2fdf-11e9-9de3-466a6591423c" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1550097072 - Will randomize all specs
Will run 201 of 2161 specs

Feb 13 22:31:13.185: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 13 22:31:13.191: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 13 22:31:13.302: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 13 22:31:13.424: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 13 22:31:13.424: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 13 22:31:13.424: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 13 22:31:13.452: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 13 22:31:13.452: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 13 22:31:13.452: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 13 22:31:13.452: INFO: e2e test version: v1.13.3
Feb 13 22:31:13.473: INFO: kube-apiserver version: v1.13.3
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:31:13.473: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
Feb 13 22:31:14.479: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 13 22:31:14.547: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-4gzcn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 22:31:14.738: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:31:21.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4gzcn" for this suite.
Feb 13 22:32:01.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:32:01.362: INFO: namespace: e2e-tests-pods-4gzcn, resource: bindings, ignored listing per whitelist
Feb 13 22:32:02.048: INFO: namespace e2e-tests-pods-4gzcn deletion completed in 40.941978286s

• [SLOW TEST:48.575 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:32:02.048: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-vfgdq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 13 22:32:13.317: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 13 22:32:13.339: INFO: Pod pod-with-prestop-http-hook still exists
Feb 13 22:32:15.339: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 13 22:32:15.362: INFO: Pod pod-with-prestop-http-hook still exists
Feb 13 22:32:17.339: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 13 22:32:17.361: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:32:17.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-vfgdq" for this suite.
Feb 13 22:32:41.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:32:42.067: INFO: namespace: e2e-tests-container-lifecycle-hook-vfgdq, resource: bindings, ignored listing per whitelist
Feb 13 22:32:42.386: INFO: namespace e2e-tests-container-lifecycle-hook-vfgdq deletion completed in 24.969687999s

• [SLOW TEST:40.338 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:32:42.386: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-k26z5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-476bb316-2fdf-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 13 22:32:43.442: INFO: Waiting up to 5m0s for pod "pod-secrets-476efaca-2fdf-11e9-9de3-466a6591423c" in namespace "e2e-tests-secrets-k26z5" to be "success or failure"
Feb 13 22:32:43.463: INFO: Pod "pod-secrets-476efaca-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.797405ms
Feb 13 22:32:45.486: INFO: Pod "pod-secrets-476efaca-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042999355s
Feb 13 22:32:47.508: INFO: Pod "pod-secrets-476efaca-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06596472s
Feb 13 22:32:49.531: INFO: Pod "pod-secrets-476efaca-2fdf-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08802677s
STEP: Saw pod success
Feb 13 22:32:49.531: INFO: Pod "pod-secrets-476efaca-2fdf-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:32:49.552: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-secrets-476efaca-2fdf-11e9-9de3-466a6591423c container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 22:32:49.611: INFO: Waiting for pod pod-secrets-476efaca-2fdf-11e9-9de3-466a6591423c to disappear
Feb 13 22:32:49.634: INFO: Pod pod-secrets-476efaca-2fdf-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:32:49.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-k26z5" for this suite.
Feb 13 22:32:55.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:32:56.097: INFO: namespace: e2e-tests-secrets-k26z5, resource: bindings, ignored listing per whitelist
Feb 13 22:32:56.548: INFO: namespace e2e-tests-secrets-k26z5 deletion completed in 6.892796598s

• [SLOW TEST:14.162 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:32:56.548: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-2vhqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-4fe77937-2fdf-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 13 22:32:57.675: INFO: Waiting up to 5m0s for pod "pod-secrets-4feadf58-2fdf-11e9-9de3-466a6591423c" in namespace "e2e-tests-secrets-2vhqg" to be "success or failure"
Feb 13 22:32:57.697: INFO: Pod "pod-secrets-4feadf58-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.085934ms
Feb 13 22:32:59.720: INFO: Pod "pod-secrets-4feadf58-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044765018s
Feb 13 22:33:01.743: INFO: Pod "pod-secrets-4feadf58-2fdf-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067419953s
STEP: Saw pod success
Feb 13 22:33:01.743: INFO: Pod "pod-secrets-4feadf58-2fdf-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:33:01.764: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-secrets-4feadf58-2fdf-11e9-9de3-466a6591423c container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 22:33:01.831: INFO: Waiting for pod pod-secrets-4feadf58-2fdf-11e9-9de3-466a6591423c to disappear
Feb 13 22:33:01.853: INFO: Pod pod-secrets-4feadf58-2fdf-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:33:01.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2vhqg" for this suite.
Feb 13 22:33:07.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:33:08.318: INFO: namespace: e2e-tests-secrets-2vhqg, resource: bindings, ignored listing per whitelist
Feb 13 22:33:08.820: INFO: namespace e2e-tests-secrets-2vhqg deletion completed in 6.94517817s

• [SLOW TEST:12.272 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:33:08.820: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-56n8r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-57284d1c-2fdf-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 13 22:33:09.844: INFO: Waiting up to 5m0s for pod "pod-secrets-572b9d8c-2fdf-11e9-9de3-466a6591423c" in namespace "e2e-tests-secrets-56n8r" to be "success or failure"
Feb 13 22:33:09.865: INFO: Pod "pod-secrets-572b9d8c-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.344574ms
Feb 13 22:33:11.888: INFO: Pod "pod-secrets-572b9d8c-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043723891s
Feb 13 22:33:13.910: INFO: Pod "pod-secrets-572b9d8c-2fdf-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065790441s
STEP: Saw pod success
Feb 13 22:33:13.910: INFO: Pod "pod-secrets-572b9d8c-2fdf-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:33:13.931: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-secrets-572b9d8c-2fdf-11e9-9de3-466a6591423c container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 22:33:13.996: INFO: Waiting for pod pod-secrets-572b9d8c-2fdf-11e9-9de3-466a6591423c to disappear
Feb 13 22:33:14.017: INFO: Pod pod-secrets-572b9d8c-2fdf-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:33:14.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-56n8r" for this suite.
Feb 13 22:33:20.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:33:20.557: INFO: namespace: e2e-tests-secrets-56n8r, resource: bindings, ignored listing per whitelist
Feb 13 22:33:20.918: INFO: namespace e2e-tests-secrets-56n8r deletion completed in 6.878802808s

• [SLOW TEST:12.098 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:33:20.918: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-d6jdw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 22:33:21.905: INFO: Creating ReplicaSet my-hostname-basic-5e5fa90d-2fdf-11e9-9de3-466a6591423c
Feb 13 22:33:21.948: INFO: Pod name my-hostname-basic-5e5fa90d-2fdf-11e9-9de3-466a6591423c: Found 0 pods out of 1
Feb 13 22:33:26.971: INFO: Pod name my-hostname-basic-5e5fa90d-2fdf-11e9-9de3-466a6591423c: Found 1 pods out of 1
Feb 13 22:33:26.971: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5e5fa90d-2fdf-11e9-9de3-466a6591423c" is running
Feb 13 22:33:26.992: INFO: Pod "my-hostname-basic-5e5fa90d-2fdf-11e9-9de3-466a6591423c-l2jnf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-13 22:33:21 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-13 22:33:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-13 22:33:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-13 22:33:21 +0000 UTC Reason: Message:}])
Feb 13 22:33:26.992: INFO: Trying to dial the pod
Feb 13 22:33:32.150: INFO: Controller my-hostname-basic-5e5fa90d-2fdf-11e9-9de3-466a6591423c: Got expected result from replica 1 [my-hostname-basic-5e5fa90d-2fdf-11e9-9de3-466a6591423c-l2jnf]: "my-hostname-basic-5e5fa90d-2fdf-11e9-9de3-466a6591423c-l2jnf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:33:32.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-d6jdw" for this suite.
Feb 13 22:33:38.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:33:38.780: INFO: namespace: e2e-tests-replicaset-d6jdw, resource: bindings, ignored listing per whitelist
Feb 13 22:33:39.058: INFO: namespace e2e-tests-replicaset-d6jdw deletion completed in 6.88608847s

• [SLOW TEST:18.140 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:33:39.059: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-97scm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-97scm/configmap-test-6938d258-2fdf-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 13 22:33:40.159: INFO: Waiting up to 5m0s for pod "pod-configmaps-693c51cb-2fdf-11e9-9de3-466a6591423c" in namespace "e2e-tests-configmap-97scm" to be "success or failure"
Feb 13 22:33:40.180: INFO: Pod "pod-configmaps-693c51cb-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.297265ms
Feb 13 22:33:42.202: INFO: Pod "pod-configmaps-693c51cb-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043497907s
Feb 13 22:33:44.225: INFO: Pod "pod-configmaps-693c51cb-2fdf-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065844494s
STEP: Saw pod success
Feb 13 22:33:44.225: INFO: Pod "pod-configmaps-693c51cb-2fdf-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:33:44.246: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-configmaps-693c51cb-2fdf-11e9-9de3-466a6591423c container env-test: <nil>
STEP: delete the pod
Feb 13 22:33:44.303: INFO: Waiting for pod pod-configmaps-693c51cb-2fdf-11e9-9de3-466a6591423c to disappear
Feb 13 22:33:44.325: INFO: Pod pod-configmaps-693c51cb-2fdf-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:33:44.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-97scm" for this suite.
Feb 13 22:33:50.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:33:50.787: INFO: namespace: e2e-tests-configmap-97scm, resource: bindings, ignored listing per whitelist
Feb 13 22:33:51.238: INFO: namespace e2e-tests-configmap-97scm deletion completed in 6.889657128s

• [SLOW TEST:12.179 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:33:51.238: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-lmkqf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 13 22:33:53.426: INFO: Pod name wrapped-volume-race-711e4fde-2fdf-11e9-9de3-466a6591423c: Found 3 pods out of 5
Feb 13 22:33:58.451: INFO: Pod name wrapped-volume-race-711e4fde-2fdf-11e9-9de3-466a6591423c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-711e4fde-2fdf-11e9-9de3-466a6591423c in namespace e2e-tests-emptydir-wrapper-lmkqf, will wait for the garbage collector to delete the pods
Feb 13 22:34:04.737: INFO: Deleting ReplicationController wrapped-volume-race-711e4fde-2fdf-11e9-9de3-466a6591423c took: 24.433286ms
Feb 13 22:34:04.937: INFO: Terminating ReplicationController wrapped-volume-race-711e4fde-2fdf-11e9-9de3-466a6591423c pods took: 200.280137ms
STEP: Creating RC which spawns configmap-volume pods
Feb 13 22:35:15.421: INFO: Pod name wrapped-volume-race-a1fc1975-2fdf-11e9-9de3-466a6591423c: Found 3 pods out of 5
Feb 13 22:35:20.446: INFO: Pod name wrapped-volume-race-a1fc1975-2fdf-11e9-9de3-466a6591423c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a1fc1975-2fdf-11e9-9de3-466a6591423c in namespace e2e-tests-emptydir-wrapper-lmkqf, will wait for the garbage collector to delete the pods
Feb 13 22:35:28.674: INFO: Deleting ReplicationController wrapped-volume-race-a1fc1975-2fdf-11e9-9de3-466a6591423c took: 24.40904ms
Feb 13 22:35:28.775: INFO: Terminating ReplicationController wrapped-volume-race-a1fc1975-2fdf-11e9-9de3-466a6591423c pods took: 100.250042ms
STEP: Creating RC which spawns configmap-volume pods
Feb 13 22:36:15.343: INFO: Pod name wrapped-volume-race-c5b5e2d2-2fdf-11e9-9de3-466a6591423c: Found 0 pods out of 5
Feb 13 22:36:49.903: INFO: Pod name wrapped-volume-race-c5b5e2d2-2fdf-11e9-9de3-466a6591423c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c5b5e2d2-2fdf-11e9-9de3-466a6591423c in namespace e2e-tests-emptydir-wrapper-lmkqf, will wait for the garbage collector to delete the pods
Feb 13 22:36:56.149: INFO: Deleting ReplicationController wrapped-volume-race-c5b5e2d2-2fdf-11e9-9de3-466a6591423c took: 24.985015ms
Feb 13 22:36:56.250: INFO: Terminating ReplicationController wrapped-volume-race-c5b5e2d2-2fdf-11e9-9de3-466a6591423c pods took: 100.236406ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:37:36.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-lmkqf" for this suite.
Feb 13 22:37:44.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:37:45.060: INFO: namespace: e2e-tests-emptydir-wrapper-lmkqf, resource: bindings, ignored listing per whitelist
Feb 13 22:37:45.629: INFO: namespace e2e-tests-emptydir-wrapper-lmkqf deletion completed in 8.968510241s

• [SLOW TEST:234.390 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:37:45.629: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-kzd6x
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 13 22:37:46.727: INFO: Waiting up to 5m0s for pod "pod-fc3485f0-2fdf-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-kzd6x" to be "success or failure"
Feb 13 22:37:46.748: INFO: Pod "pod-fc3485f0-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.013874ms
Feb 13 22:37:48.771: INFO: Pod "pod-fc3485f0-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044009993s
Feb 13 22:37:50.793: INFO: Pod "pod-fc3485f0-2fdf-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066271736s
Feb 13 22:37:52.815: INFO: Pod "pod-fc3485f0-2fdf-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08838589s
STEP: Saw pod success
Feb 13 22:37:52.815: INFO: Pod "pod-fc3485f0-2fdf-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:37:52.837: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-fc3485f0-2fdf-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 22:37:52.895: INFO: Waiting for pod pod-fc3485f0-2fdf-11e9-9de3-466a6591423c to disappear
Feb 13 22:37:52.916: INFO: Pod pod-fc3485f0-2fdf-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:37:52.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kzd6x" for this suite.
Feb 13 22:37:59.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:37:59.674: INFO: namespace: e2e-tests-emptydir-kzd6x, resource: bindings, ignored listing per whitelist
Feb 13 22:37:59.848: INFO: namespace e2e-tests-emptydir-kzd6x deletion completed in 6.910474813s

• [SLOW TEST:14.219 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:37:59.849: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-t9q2q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 13 22:38:11.058: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:11.085: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 22:38:13.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:13.107: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 22:38:15.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:15.107: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 22:38:17.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:17.107: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 22:38:19.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:19.107: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 22:38:21.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:21.115: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 22:38:23.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:23.107: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 22:38:25.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:25.107: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 22:38:27.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:27.108: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 22:38:29.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:29.109: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 22:38:31.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 22:38:31.107: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:38:31.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-t9q2q" for this suite.
Feb 13 22:38:55.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:38:55.720: INFO: namespace: e2e-tests-container-lifecycle-hook-t9q2q, resource: bindings, ignored listing per whitelist
Feb 13 22:38:56.044: INFO: namespace e2e-tests-container-lifecycle-hook-t9q2q deletion completed in 24.901818349s

• [SLOW TEST:56.195 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:38:56.044: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-nnlbf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 22:38:57.033: INFO: Waiting up to 5m0s for pod "downwardapi-volume-261c5b90-2fe0-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-nnlbf" to be "success or failure"
Feb 13 22:38:57.107: INFO: Pod "downwardapi-volume-261c5b90-2fe0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 73.936617ms
Feb 13 22:38:59.129: INFO: Pod "downwardapi-volume-261c5b90-2fe0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09652089s
Feb 13 22:39:01.152: INFO: Pod "downwardapi-volume-261c5b90-2fe0-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.119048018s
STEP: Saw pod success
Feb 13 22:39:01.152: INFO: Pod "downwardapi-volume-261c5b90-2fe0-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:39:01.173: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-261c5b90-2fe0-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 22:39:01.232: INFO: Waiting for pod downwardapi-volume-261c5b90-2fe0-11e9-9de3-466a6591423c to disappear
Feb 13 22:39:01.253: INFO: Pod downwardapi-volume-261c5b90-2fe0-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:39:01.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nnlbf" for this suite.
Feb 13 22:39:07.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:39:07.847: INFO: namespace: e2e-tests-projected-nnlbf, resource: bindings, ignored listing per whitelist
Feb 13 22:39:08.191: INFO: namespace e2e-tests-projected-nnlbf deletion completed in 6.915107218s

• [SLOW TEST:12.146 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:39:08.191: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vrfcj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 13 22:39:09.111: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 13 22:39:09.111: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:39:09.511: INFO: stderr: ""
Feb 13 22:39:09.511: INFO: stdout: "service/redis-slave created\n"
Feb 13 22:39:09.511: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 13 22:39:09.511: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:39:09.825: INFO: stderr: ""
Feb 13 22:39:09.825: INFO: stdout: "service/redis-master created\n"
Feb 13 22:39:09.825: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 13 22:39:09.825: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:39:10.144: INFO: stderr: ""
Feb 13 22:39:10.144: INFO: stdout: "service/frontend created\n"
Feb 13 22:39:10.144: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 13 22:39:10.144: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:39:10.426: INFO: stderr: ""
Feb 13 22:39:10.426: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 13 22:39:10.426: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 13 22:39:10.426: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:39:10.717: INFO: stderr: ""
Feb 13 22:39:10.717: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 13 22:39:10.717: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 13 22:39:10.718: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:39:10.998: INFO: stderr: ""
Feb 13 22:39:10.998: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 13 22:39:10.998: INFO: Waiting for all frontend pods to be Running.
Feb 13 22:39:46.050: INFO: Waiting for frontend to serve content.
Feb 13 22:39:46.164: INFO: Trying to add a new entry to the guestbook.
Feb 13 22:39:46.298: INFO: Verifying that added entry can be retrieved.
Feb 13 22:39:46.432: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 13 22:39:51.548: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 13 22:39:56.662: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 13 22:40:01.777: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 13 22:40:06.893: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 13 22:40:12.010: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 13 22:40:31.689: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 13 22:40:37.122: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Feb 13 22:40:42.212: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Feb 13 22:40:47.326: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:40:47.540: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 13 22:40:47.540: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 13 22:40:47.540: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:40:47.737: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 13 22:40:47.737: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 13 22:40:47.737: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:40:47.941: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 13 22:40:47.941: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 13 22:40:47.941: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:40:48.135: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 13 22:40:48.135: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 13 22:40:48.136: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:40:48.328: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 13 22:40:48.328: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 13 22:40:48.329: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vrfcj'
Feb 13 22:40:48.514: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 13 22:40:48.514: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:40:48.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vrfcj" for this suite.
Feb 13 22:41:53.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:41:53.907: INFO: namespace: e2e-tests-kubectl-vrfcj, resource: bindings, ignored listing per whitelist
Feb 13 22:41:54.357: INFO: namespace e2e-tests-kubectl-vrfcj deletion completed in 1m5.813212164s

• [SLOW TEST:166.166 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:41:54.357: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-wq6q2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 13 22:41:55.489: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-wq6q2,SelfLink:/api/v1/namespaces/e2e-tests-watch-wq6q2/configmaps/e2e-watch-test-watch-closed,UID:9075149f-2fe0-11e9-9f3f-da917295d151,ResourceVersion:6141,Generation:0,CreationTimestamp:2019-02-13 22:41:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 13 22:41:55.489: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-wq6q2,SelfLink:/api/v1/namespaces/e2e-tests-watch-wq6q2/configmaps/e2e-watch-test-watch-closed,UID:9075149f-2fe0-11e9-9f3f-da917295d151,ResourceVersion:6142,Generation:0,CreationTimestamp:2019-02-13 22:41:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 13 22:41:55.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-wq6q2,SelfLink:/api/v1/namespaces/e2e-tests-watch-wq6q2/configmaps/e2e-watch-test-watch-closed,UID:9075149f-2fe0-11e9-9f3f-da917295d151,ResourceVersion:6143,Generation:0,CreationTimestamp:2019-02-13 22:41:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 13 22:41:55.581: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-wq6q2,SelfLink:/api/v1/namespaces/e2e-tests-watch-wq6q2/configmaps/e2e-watch-test-watch-closed,UID:9075149f-2fe0-11e9-9f3f-da917295d151,ResourceVersion:6144,Generation:0,CreationTimestamp:2019-02-13 22:41:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:41:55.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-wq6q2" for this suite.
Feb 13 22:42:01.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:42:02.294: INFO: namespace: e2e-tests-watch-wq6q2, resource: bindings, ignored listing per whitelist
Feb 13 22:42:02.548: INFO: namespace e2e-tests-watch-wq6q2 deletion completed in 6.94471004s

• [SLOW TEST:8.191 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:42:02.548: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-78k28
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-955482dd-2fe0-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 13 22:42:03.649: INFO: Waiting up to 5m0s for pod "pod-secrets-9557cb55-2fe0-11e9-9de3-466a6591423c" in namespace "e2e-tests-secrets-78k28" to be "success or failure"
Feb 13 22:42:03.670: INFO: Pod "pod-secrets-9557cb55-2fe0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.024548ms
Feb 13 22:42:05.693: INFO: Pod "pod-secrets-9557cb55-2fe0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044183116s
Feb 13 22:42:07.715: INFO: Pod "pod-secrets-9557cb55-2fe0-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066627624s
STEP: Saw pod success
Feb 13 22:42:07.715: INFO: Pod "pod-secrets-9557cb55-2fe0-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:42:07.736: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-secrets-9557cb55-2fe0-11e9-9de3-466a6591423c container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 22:42:07.798: INFO: Waiting for pod pod-secrets-9557cb55-2fe0-11e9-9de3-466a6591423c to disappear
Feb 13 22:42:07.819: INFO: Pod pod-secrets-9557cb55-2fe0-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:42:07.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-78k28" for this suite.
Feb 13 22:42:13.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:42:13.998: INFO: namespace: e2e-tests-secrets-78k28, resource: bindings, ignored listing per whitelist
Feb 13 22:42:14.747: INFO: namespace e2e-tests-secrets-78k28 deletion completed in 6.906417491s

• [SLOW TEST:12.200 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:42:14.748: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-vxcld
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 13 22:42:15.801: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:42:19.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-vxcld" for this suite.
Feb 13 22:42:26.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:42:26.634: INFO: namespace: e2e-tests-init-container-vxcld, resource: bindings, ignored listing per whitelist
Feb 13 22:42:26.917: INFO: namespace e2e-tests-init-container-vxcld deletion completed in 6.919268579s

• [SLOW TEST:12.169 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:42:26.917: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-m68bc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 22:42:28.015: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 13 22:42:28.061: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-m68bc/daemonsets","resourceVersion":"6263"},"items":null}

Feb 13 22:42:28.083: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-m68bc/pods","resourceVersion":"6263"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:42:28.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-m68bc" for this suite.
Feb 13 22:42:34.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:42:34.946: INFO: namespace: e2e-tests-daemonsets-m68bc, resource: bindings, ignored listing per whitelist
Feb 13 22:42:35.098: INFO: namespace e2e-tests-daemonsets-m68bc deletion completed in 6.928345047s

S [SKIPPING] [8.181 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 13 22:42:28.015: Requires at least 2 nodes (not -1)

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:42:35.098: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kwqj6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-a8b2f56f-2fe0-11e9-9de3-466a6591423c
STEP: Creating secret with name secret-projected-all-test-volume-a8b2f55b-2fe0-11e9-9de3-466a6591423c
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 13 22:42:36.174: INFO: Waiting up to 5m0s for pod "projected-volume-a8b2f518-2fe0-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-kwqj6" to be "success or failure"
Feb 13 22:42:36.196: INFO: Pod "projected-volume-a8b2f518-2fe0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.245431ms
Feb 13 22:42:38.219: INFO: Pod "projected-volume-a8b2f518-2fe0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044921451s
Feb 13 22:42:40.258: INFO: Pod "projected-volume-a8b2f518-2fe0-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08422076s
STEP: Saw pod success
Feb 13 22:42:40.258: INFO: Pod "projected-volume-a8b2f518-2fe0-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:42:40.282: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod projected-volume-a8b2f518-2fe0-11e9-9de3-466a6591423c container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 13 22:42:40.345: INFO: Waiting for pod projected-volume-a8b2f518-2fe0-11e9-9de3-466a6591423c to disappear
Feb 13 22:42:40.366: INFO: Pod projected-volume-a8b2f518-2fe0-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:42:40.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kwqj6" for this suite.
Feb 13 22:42:46.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:42:46.619: INFO: namespace: e2e-tests-projected-kwqj6, resource: bindings, ignored listing per whitelist
Feb 13 22:42:47.317: INFO: namespace e2e-tests-projected-kwqj6 deletion completed in 6.925874972s

• [SLOW TEST:12.219 seconds]
[sig-storage] Projected combined
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:42:47.317: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-8cmdz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-4ct94
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-7424b
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:42:55.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-8cmdz" for this suite.
Feb 13 22:43:01.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:43:01.597: INFO: namespace: e2e-tests-namespaces-8cmdz, resource: bindings, ignored listing per whitelist
Feb 13 22:43:02.006: INFO: namespace e2e-tests-namespaces-8cmdz deletion completed in 6.94902882s
STEP: Destroying namespace "e2e-tests-nsdeletetest-4ct94" for this suite.
Feb 13 22:43:02.027: INFO: Namespace e2e-tests-nsdeletetest-4ct94 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-7424b" for this suite.
Feb 13 22:43:08.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:43:08.552: INFO: namespace: e2e-tests-nsdeletetest-7424b, resource: bindings, ignored listing per whitelist
Feb 13 22:43:08.978: INFO: namespace e2e-tests-nsdeletetest-7424b deletion completed in 6.950262083s

• [SLOW TEST:21.660 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:43:08.978: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-fvh8n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 13 22:43:09.994: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:43:13.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-fvh8n" for this suite.
Feb 13 22:43:19.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:43:20.460: INFO: namespace: e2e-tests-init-container-fvh8n, resource: bindings, ignored listing per whitelist
Feb 13 22:43:20.503: INFO: namespace e2e-tests-init-container-fvh8n deletion completed in 6.966207075s

• [SLOW TEST:11.525 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:43:20.503: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-2rxzh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 13 22:43:22.171: INFO: Waiting up to 5m0s for pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-szzxd" in namespace "e2e-tests-svcaccounts-2rxzh" to be "success or failure"
Feb 13 22:43:22.192: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-szzxd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.998663ms
Feb 13 22:43:24.214: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-szzxd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043187733s
Feb 13 22:43:26.236: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-szzxd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06522009s
Feb 13 22:43:28.259: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-szzxd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087692479s
STEP: Saw pod success
Feb 13 22:43:28.259: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-szzxd" satisfied condition "success or failure"
Feb 13 22:43:28.280: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-szzxd container token-test: <nil>
STEP: delete the pod
Feb 13 22:43:28.378: INFO: Waiting for pod pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-szzxd to disappear
Feb 13 22:43:28.400: INFO: Pod pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-szzxd no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 13 22:43:28.422: INFO: Waiting up to 5m0s for pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-vxxtp" in namespace "e2e-tests-svcaccounts-2rxzh" to be "success or failure"
Feb 13 22:43:28.447: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-vxxtp": Phase="Pending", Reason="", readiness=false. Elapsed: 25.049974ms
Feb 13 22:43:30.471: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-vxxtp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048251854s
Feb 13 22:43:32.493: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-vxxtp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070390346s
Feb 13 22:43:34.515: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-vxxtp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092898616s
STEP: Saw pod success
Feb 13 22:43:34.515: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-vxxtp" satisfied condition "success or failure"
Feb 13 22:43:34.537: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-vxxtp container root-ca-test: <nil>
STEP: delete the pod
Feb 13 22:43:34.601: INFO: Waiting for pod pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-vxxtp to disappear
Feb 13 22:43:34.622: INFO: Pod pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-vxxtp no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 13 22:43:34.645: INFO: Waiting up to 5m0s for pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-9l7gp" in namespace "e2e-tests-svcaccounts-2rxzh" to be "success or failure"
Feb 13 22:43:34.676: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-9l7gp": Phase="Pending", Reason="", readiness=false. Elapsed: 30.971858ms
Feb 13 22:43:36.700: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-9l7gp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054580102s
Feb 13 22:43:38.723: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-9l7gp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077858753s
Feb 13 22:43:40.746: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-9l7gp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.100639564s
STEP: Saw pod success
Feb 13 22:43:40.746: INFO: Pod "pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-9l7gp" satisfied condition "success or failure"
Feb 13 22:43:40.767: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-9l7gp container namespace-test: <nil>
STEP: delete the pod
Feb 13 22:43:40.831: INFO: Waiting for pod pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-9l7gp to disappear
Feb 13 22:43:40.852: INFO: Pod pod-service-account-c4251914-2fe0-11e9-9de3-466a6591423c-9l7gp no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:43:40.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-2rxzh" for this suite.
Feb 13 22:43:46.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:43:47.740: INFO: namespace: e2e-tests-svcaccounts-2rxzh, resource: bindings, ignored listing per whitelist
Feb 13 22:43:47.804: INFO: namespace e2e-tests-svcaccounts-2rxzh deletion completed in 6.929358006s

• [SLOW TEST:27.301 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:43:47.804: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-crds5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-m2gts
STEP: Creating secret with name secret-test-d3f9d396-2fe0-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 13 22:43:49.080: INFO: Waiting up to 5m0s for pod "pod-secrets-d42f4c05-2fe0-11e9-9de3-466a6591423c" in namespace "e2e-tests-secrets-crds5" to be "success or failure"
Feb 13 22:43:49.128: INFO: Pod "pod-secrets-d42f4c05-2fe0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 48.580726ms
Feb 13 22:43:51.156: INFO: Pod "pod-secrets-d42f4c05-2fe0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075801066s
Feb 13 22:43:53.178: INFO: Pod "pod-secrets-d42f4c05-2fe0-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098211662s
STEP: Saw pod success
Feb 13 22:43:53.178: INFO: Pod "pod-secrets-d42f4c05-2fe0-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:43:53.200: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-secrets-d42f4c05-2fe0-11e9-9de3-466a6591423c container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 22:43:53.262: INFO: Waiting for pod pod-secrets-d42f4c05-2fe0-11e9-9de3-466a6591423c to disappear
Feb 13 22:43:53.282: INFO: Pod pod-secrets-d42f4c05-2fe0-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:43:53.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-crds5" for this suite.
Feb 13 22:43:59.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:44:00.264: INFO: namespace: e2e-tests-secrets-crds5, resource: bindings, ignored listing per whitelist
Feb 13 22:44:00.264: INFO: namespace e2e-tests-secrets-crds5 deletion completed in 6.9596964s
STEP: Destroying namespace "e2e-tests-secret-namespace-m2gts" for this suite.
Feb 13 22:44:08.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:44:08.950: INFO: namespace: e2e-tests-secret-namespace-m2gts, resource: bindings, ignored listing per whitelist
Feb 13 22:44:09.256: INFO: namespace e2e-tests-secret-namespace-m2gts deletion completed in 8.991238691s

• [SLOW TEST:21.451 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:44:09.256: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-mqvsr
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 22:44:10.344: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:44:11.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-mqvsr" for this suite.
Feb 13 22:44:17.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:44:18.042: INFO: namespace: e2e-tests-custom-resource-definition-mqvsr, resource: bindings, ignored listing per whitelist
Feb 13 22:44:18.042: INFO: namespace e2e-tests-custom-resource-definition-mqvsr deletion completed in 6.941370343s

• [SLOW TEST:8.786 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:44:18.042: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-bz9zm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 22:44:19.127: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6180925-2fe0-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-bz9zm" to be "success or failure"
Feb 13 22:44:19.148: INFO: Pod "downwardapi-volume-e6180925-2fe0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.975607ms
Feb 13 22:44:21.171: INFO: Pod "downwardapi-volume-e6180925-2fe0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043958381s
Feb 13 22:44:23.193: INFO: Pod "downwardapi-volume-e6180925-2fe0-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066319611s
STEP: Saw pod success
Feb 13 22:44:23.193: INFO: Pod "downwardapi-volume-e6180925-2fe0-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:44:23.215: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-e6180925-2fe0-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 22:44:23.314: INFO: Waiting for pod downwardapi-volume-e6180925-2fe0-11e9-9de3-466a6591423c to disappear
Feb 13 22:44:23.335: INFO: Pod downwardapi-volume-e6180925-2fe0-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:44:23.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bz9zm" for this suite.
Feb 13 22:44:29.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:44:29.899: INFO: namespace: e2e-tests-downward-api-bz9zm, resource: bindings, ignored listing per whitelist
Feb 13 22:44:30.286: INFO: namespace e2e-tests-downward-api-bz9zm deletion completed in 6.929358761s

• [SLOW TEST:12.244 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:44:30.286: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8psbx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 13 22:44:31.397: INFO: namespace e2e-tests-kubectl-8psbx
Feb 13 22:44:31.397: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-8psbx'
Feb 13 22:44:33.829: INFO: stderr: ""
Feb 13 22:44:33.829: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 13 22:44:34.851: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 22:44:34.851: INFO: Found 0 / 1
Feb 13 22:44:35.851: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 22:44:35.851: INFO: Found 0 / 1
Feb 13 22:44:36.852: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 22:44:36.852: INFO: Found 1 / 1
Feb 13 22:44:36.852: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 13 22:44:36.874: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 22:44:36.875: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 13 22:44:36.875: INFO: wait on redis-master startup in e2e-tests-kubectl-8psbx 
Feb 13 22:44:36.875: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml logs redis-master-zgkvk redis-master --namespace=e2e-tests-kubectl-8psbx'
Feb 13 22:44:37.230: INFO: stderr: ""
Feb 13 22:44:37.230: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Feb 22:44:36.301 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Feb 22:44:36.302 # Server started, Redis version 3.2.12\n1:M 13 Feb 22:44:36.302 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Feb 22:44:36.302 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 13 22:44:37.230: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-8psbx'
Feb 13 22:44:37.517: INFO: stderr: ""
Feb 13 22:44:37.517: INFO: stdout: "service/rm2 exposed\n"
Feb 13 22:44:37.541: INFO: Service rm2 in namespace e2e-tests-kubectl-8psbx found.
STEP: exposing service
Feb 13 22:44:39.586: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-8psbx'
Feb 13 22:44:39.850: INFO: stderr: ""
Feb 13 22:44:39.850: INFO: stdout: "service/rm3 exposed\n"
Feb 13 22:44:39.872: INFO: Service rm3 in namespace e2e-tests-kubectl-8psbx found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:44:41.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8psbx" for this suite.
Feb 13 22:45:06.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:45:06.271: INFO: namespace: e2e-tests-kubectl-8psbx, resource: bindings, ignored listing per whitelist
Feb 13 22:45:06.890: INFO: namespace e2e-tests-kubectl-8psbx deletion completed in 24.951988414s

• [SLOW TEST:36.604 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:45:06.890: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-zt2p4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-0331733b-2fe1-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 13 22:45:07.968: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0334cb54-2fe1-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-zt2p4" to be "success or failure"
Feb 13 22:45:08.049: INFO: Pod "pod-projected-secrets-0334cb54-2fe1-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 80.342013ms
Feb 13 22:45:10.072: INFO: Pod "pod-projected-secrets-0334cb54-2fe1-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103000693s
Feb 13 22:45:12.095: INFO: Pod "pod-projected-secrets-0334cb54-2fe1-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.126313784s
Feb 13 22:45:14.117: INFO: Pod "pod-projected-secrets-0334cb54-2fe1-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.148416982s
STEP: Saw pod success
Feb 13 22:45:14.117: INFO: Pod "pod-projected-secrets-0334cb54-2fe1-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:45:14.138: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-secrets-0334cb54-2fe1-11e9-9de3-466a6591423c container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 13 22:45:14.211: INFO: Waiting for pod pod-projected-secrets-0334cb54-2fe1-11e9-9de3-466a6591423c to disappear
Feb 13 22:45:14.233: INFO: Pod pod-projected-secrets-0334cb54-2fe1-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:45:14.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zt2p4" for this suite.
Feb 13 22:45:20.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:45:20.981: INFO: namespace: e2e-tests-projected-zt2p4, resource: bindings, ignored listing per whitelist
Feb 13 22:45:21.233: INFO: namespace e2e-tests-projected-zt2p4 deletion completed in 6.978226987s

• [SLOW TEST:14.343 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:45:21.233: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-892xz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-0bc57fa5-2fe1-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 13 22:45:22.362: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bc8e040-2fe1-11e9-9de3-466a6591423c" in namespace "e2e-tests-configmap-892xz" to be "success or failure"
Feb 13 22:45:22.383: INFO: Pod "pod-configmaps-0bc8e040-2fe1-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.015167ms
Feb 13 22:45:24.405: INFO: Pod "pod-configmaps-0bc8e040-2fe1-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043254973s
Feb 13 22:45:26.429: INFO: Pod "pod-configmaps-0bc8e040-2fe1-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066833963s
STEP: Saw pod success
Feb 13 22:45:26.429: INFO: Pod "pod-configmaps-0bc8e040-2fe1-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:45:26.450: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-configmaps-0bc8e040-2fe1-11e9-9de3-466a6591423c container configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 22:45:26.528: INFO: Waiting for pod pod-configmaps-0bc8e040-2fe1-11e9-9de3-466a6591423c to disappear
Feb 13 22:45:26.549: INFO: Pod pod-configmaps-0bc8e040-2fe1-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:45:26.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-892xz" for this suite.
Feb 13 22:45:32.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:45:33.452: INFO: namespace: e2e-tests-configmap-892xz, resource: bindings, ignored listing per whitelist
Feb 13 22:45:33.498: INFO: namespace e2e-tests-configmap-892xz deletion completed in 6.92711713s

• [SLOW TEST:12.265 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:45:33.499: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-s8j7d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-s8j7d
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 13 22:45:34.608: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 13 22:46:00.981: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.46:8080/dial?request=hostName&protocol=udp&host=100.96.0.12&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-s8j7d PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 13 22:46:00.982: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 13 22:46:01.522: INFO: Waiting for endpoints: map[]
Feb 13 22:46:01.544: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.46:8080/dial?request=hostName&protocol=udp&host=100.96.1.45&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-s8j7d PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 13 22:46:01.545: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 13 22:46:02.045: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:46:02.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-s8j7d" for this suite.
Feb 13 22:46:26.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:46:26.257: INFO: namespace: e2e-tests-pod-network-test-s8j7d, resource: bindings, ignored listing per whitelist
Feb 13 22:46:27.032: INFO: namespace e2e-tests-pod-network-test-s8j7d deletion completed in 24.964236148s

• [SLOW TEST:53.533 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:46:27.032: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-2z7jq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 13 22:46:28.008: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 13 22:46:28.053: INFO: Waiting for terminating namespaces to be deleted...
Feb 13 22:46:28.074: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx before test
Feb 13 22:46:28.106: INFO: calico-node-skgqz from kube-system started at 2019-02-13 22:03:50 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.106: INFO: 	Container calico-node ready: true, restart count 0
Feb 13 22:46:28.106: INFO: node-exporter-clvlv from kube-system started at 2019-02-13 22:03:50 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.106: INFO: 	Container node-exporter ready: true, restart count 0
Feb 13 22:46:28.106: INFO: kube-proxy-n2tgw from kube-system started at 2019-02-13 22:03:50 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.106: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 13 22:46:28.106: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp before test
Feb 13 22:46:28.267: INFO: addons-nginx-ingress-controller-d74bfff57-87jx9 from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 13 22:46:28.267: INFO: blackbox-exporter-d6c46f9fc-5xt9g from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 13 22:46:28.267: INFO: kube-proxy-6688x from kube-system started at 2019-02-13 22:03:43 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 13 22:46:28.267: INFO: calico-node-psbns from kube-system started at 2019-02-13 22:03:43 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container calico-node ready: true, restart count 0
Feb 13 22:46:28.267: INFO: node-exporter-tmhn5 from kube-system started at 2019-02-13 22:03:43 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container node-exporter ready: true, restart count 0
Feb 13 22:46:28.267: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-kj5ch from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 13 22:46:28.267: INFO: addons-kubernetes-dashboard-6579b646c5-g48vq from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 13 22:46:28.267: INFO: addons-kube-lego-69bbdc96b6-6m8wg from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container kube-lego ready: true, restart count 0
Feb 13 22:46:28.267: INFO: vpn-shoot-c76596df-6nsrb from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 13 22:46:28.267: INFO: metrics-server-65cc55bd79-8kjg8 from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container metrics-server ready: true, restart count 0
Feb 13 22:46:28.267: INFO: coredns-67df79bbdd-96tpz from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 13 22:46:28.267: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15830de85a358909], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:46:29.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-2z7jq" for this suite.
Feb 13 22:46:35.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:46:36.046: INFO: namespace: e2e-tests-sched-pred-2z7jq, resource: bindings, ignored listing per whitelist
Feb 13 22:46:36.299: INFO: namespace e2e-tests-sched-pred-2z7jq deletion completed in 6.894957389s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.268 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:46:36.300: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-q9dql
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 22:46:37.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3879c980-2fe1-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-q9dql" to be "success or failure"
Feb 13 22:46:37.373: INFO: Pod "downwardapi-volume-3879c980-2fe1-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 32.06729ms
Feb 13 22:46:39.396: INFO: Pod "downwardapi-volume-3879c980-2fe1-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055152461s
Feb 13 22:46:41.419: INFO: Pod "downwardapi-volume-3879c980-2fe1-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077930524s
STEP: Saw pod success
Feb 13 22:46:41.419: INFO: Pod "downwardapi-volume-3879c980-2fe1-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:46:41.440: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-3879c980-2fe1-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 22:46:41.501: INFO: Waiting for pod downwardapi-volume-3879c980-2fe1-11e9-9de3-466a6591423c to disappear
Feb 13 22:46:41.522: INFO: Pod downwardapi-volume-3879c980-2fe1-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:46:41.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-q9dql" for this suite.
Feb 13 22:46:47.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:46:48.457: INFO: namespace: e2e-tests-projected-q9dql, resource: bindings, ignored listing per whitelist
Feb 13 22:46:48.501: INFO: namespace e2e-tests-projected-q9dql deletion completed in 6.955968573s

• [SLOW TEST:12.201 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:46:48.503: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-dpwx2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 13 22:46:49.510: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:46:58.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-dpwx2" for this suite.
Feb 13 22:47:22.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:47:22.794: INFO: namespace: e2e-tests-init-container-dpwx2, resource: bindings, ignored listing per whitelist
Feb 13 22:47:23.296: INFO: namespace e2e-tests-init-container-dpwx2 deletion completed in 24.929837761s

• [SLOW TEST:34.794 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:47:23.297: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-87hg4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 22:47:24.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54b8b9ff-2fe1-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-87hg4" to be "success or failure"
Feb 13 22:47:24.751: INFO: Pod "downwardapi-volume-54b8b9ff-2fe1-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.095535ms
Feb 13 22:47:26.773: INFO: Pod "downwardapi-volume-54b8b9ff-2fe1-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043254008s
Feb 13 22:47:28.796: INFO: Pod "downwardapi-volume-54b8b9ff-2fe1-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066314063s
STEP: Saw pod success
Feb 13 22:47:28.796: INFO: Pod "downwardapi-volume-54b8b9ff-2fe1-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:47:28.818: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-54b8b9ff-2fe1-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 22:47:28.904: INFO: Waiting for pod downwardapi-volume-54b8b9ff-2fe1-11e9-9de3-466a6591423c to disappear
Feb 13 22:47:28.925: INFO: Pod downwardapi-volume-54b8b9ff-2fe1-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:47:28.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-87hg4" for this suite.
Feb 13 22:47:35.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:47:35.221: INFO: namespace: e2e-tests-downward-api-87hg4, resource: bindings, ignored listing per whitelist
Feb 13 22:47:35.843: INFO: namespace e2e-tests-downward-api-87hg4 deletion completed in 6.896771916s

• [SLOW TEST:12.547 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:47:35.844: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-runtime-mvl6h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:48:09.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-mvl6h" for this suite.
Feb 13 22:48:15.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:48:15.778: INFO: namespace: e2e-tests-container-runtime-mvl6h, resource: bindings, ignored listing per whitelist
Feb 13 22:48:16.014: INFO: namespace e2e-tests-container-runtime-mvl6h deletion completed in 6.933126952s

• [SLOW TEST:40.170 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:48:16.014: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-zg562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 22:53:17.099: INFO: Unexpected error occurred: timed out waiting for the condition
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-kubelet-test-zg562".
STEP: Found 2 events.
Feb 13 22:53:17.122: INFO: At 2019-02-13 22:48:17 +0000 UTC - event for busybox-readonly-fs111e4dc4-2fdf-11e9-9de3-466a6591423c: {default-scheduler } Scheduled: Successfully assigned e2e-tests-kubelet-test-zg562/busybox-readonly-fs111e4dc4-2fdf-11e9-9de3-466a6591423c to shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 13 22:53:17.122: INFO: At 2019-02-13 22:48:48 +0000 UTC - event for busybox-readonly-fs111e4dc4-2fdf-11e9-9de3-466a6591423c: {kubelet shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx} FailedCreatePodSandBox: Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "21832db3106e8af072cf8b786d0f7af5a66d6b04f47b3af1b5e5e9c8ec623a94" network for pod "busybox-readonly-fs111e4dc4-2fdf-11e9-9de3-466a6591423c": NetworkPlugin cni failed to set up pod "busybox-readonly-fs111e4dc4-2fdf-11e9-9de3-466a6591423c_e2e-tests-kubelet-test-zg562" network: error getting ClusterInformation: Get https://[100.64.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default: dial tcp 100.64.0.1:443: i/o timeout
Feb 13 22:53:17.168: INFO: POD                                                             NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:53:17.168: INFO: busybox-readonly-fs111e4dc4-2fdf-11e9-9de3-466a6591423c         shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:48:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:48:17 +0000 UTC ContainersNotReady containers with unready status: [busybox-readonly-fs111e4dc4-2fdf-11e9-9de3-466a6591423c]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:48:17 +0000 UTC ContainersNotReady containers with unready status: [busybox-readonly-fs111e4dc4-2fdf-11e9-9de3-466a6591423c]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:48:17 +0000 UTC  }]
Feb 13 22:53:17.168: INFO: addons-kube-lego-69bbdc96b6-6m8wg                               shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 22:53:17.168: INFO: addons-kubernetes-dashboard-6579b646c5-g48vq                    shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 22:53:17.168: INFO: addons-nginx-ingress-controller-d74bfff57-87jx9                 shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 22:53:17.168: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-kj5ch  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 22:53:17.168: INFO: blackbox-exporter-d6c46f9fc-5xt9g                               shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 22:53:17.168: INFO: calico-node-psbns                                               shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:42 +0000 UTC  }]
Feb 13 22:53:17.168: INFO: calico-node-skgqz                                               shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:50 +0000 UTC  }]
Feb 13 22:53:17.168: INFO: coredns-67df79bbdd-96tpz                                        shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 22:53:17.169: INFO: kube-proxy-6688x                                                shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:42 +0000 UTC  }]
Feb 13 22:53:17.169: INFO: kube-proxy-n2tgw                                                shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:50 +0000 UTC  }]
Feb 13 22:53:17.169: INFO: metrics-server-65cc55bd79-8kjg8                                 shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 22:53:17.169: INFO: node-exporter-clvlv                                             shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:50 +0000 UTC  }]
Feb 13 22:53:17.169: INFO: node-exporter-tmhn5                                             shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:42 +0000 UTC  }]
Feb 13 22:53:17.169: INFO: vpn-shoot-c76596df-6nsrb                                        shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 22:53:17.169: INFO: 
Feb 13 22:53:17.190: INFO: 
Logging node info for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 13 22:53:17.212: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,UID:3e65adb9-2fdb-11e9-9f3f-da917295d151,ResourceVersion:7942,Generation:0,CreationTimestamp:2019-02-13 22:03:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: Standard_DS2_v2,beta.kubernetes.io/os: linux,failure-domain.beta.kubernetes.io/region: westeurope,failure-domain.beta.kubernetes.io/zone: 1,kubernetes.io/hostname: shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,kubernetes.io/role: node,node-role.kubernetes.io/node: ,worker.garden.sapcloud.io/group: cpu-worker,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 10.250.0.5/19,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.96.1.0/24,DoNotUse_ExternalID:,ProviderID:azure:///subscriptions/00d2caa5-cd29-46f7-845a-2f8ee0360ef5/resourceGroups/shoot--it--pub-az-7eoqa/providers/Microsoft.Compute/virtualMachines/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-azure-disk: {{8 0} {<nil>} 8 DecimalSI},cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33931702272 0} {<nil>} 33136428Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{7286583296 0} {<nil>} 7115804Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-azure-disk: {{8 0} {<nil>} 8 DecimalSI},cpu: {{1920 -3} {<nil>} 1920m DecimalSI},ephemeral-storage: {{32235117134 0} {<nil>} 32235117134 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{5848512302 0} {<nil>} 5848512302 DecimalSI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-02-13 22:04:14 +0000 UTC 2019-02-13 22:04:14 +0000 UTC RouteCreated RouteController created a route} {MemoryPressure False 2019-02-13 22:53:09 +0000 UTC 2019-02-13 22:03:50 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-02-13 22:53:09 +0000 UTC 2019-02-13 22:03:50 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-02-13 22:53:09 +0000 UTC 2019-02-13 22:03:50 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-02-13 22:53:09 +0000 UTC 2019-02-13 22:04:10 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.250.0.5} {Hostname shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:c6a974bd9e9a471f9985af77c270be63,SystemUUID:485BC509-BC48-AC4D-A484-A78B36A6CFF6,BootID:e730366f-e2e0-4aff-bae0-eb62accbb532,KernelVersion:4.14.96-coreos,OSImage:Container Linux by CoreOS 1967.5.0 (Rhyolite),ContainerRuntimeVersion:docker://18.6.1,KubeletVersion:v1.13.3,KubeProxyVersion:v1.13.3,OperatingSystem:linux,Architecture:amd64,},Images:[{[k8s.gcr.io/hyperkube@sha256:a36f9497dacfac277ca706ef245a04682ec9f5a7afa0ad27036d08e7c108d57b k8s.gcr.io/hyperkube:v1.12.3] 635332234} {[k8s.gcr.io/hyperkube@sha256:388312116707c0420c492accc15ad3ba400911c2c337c8b33049f8d92f9c7bb9 k8s.gcr.io/hyperkube:v1.13.3] 564951434} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[quay.io/calico/node@sha256:264d7de573a956f8914de93fbade66bdc7fd86f22a8f4e9a99ebdbb9e45b66f7 quay.io/calico/node:v3.4.0] 75919845} {[quay.io/calico/cni@sha256:526f0c585f66fc70d79062463d78a324f053c406b69a600fc282c2aaa74c4428 quay.io/calico/cni:v3.4.0] 75434816} {[ruby@sha256:83eebc6b30281ff43dc919cdd361803fb296f74f3d4f1cc42036860a49de9caf ruby:2.5.3-alpine] 50827145} {[quay.io/prometheus/node-exporter@sha256:1b129a3801a0440f9c5b2afb20082dfdb31bf6092b561f5f249531130000cb83 quay.io/prometheus/node-exporter:v0.17.0] 20982005} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 busybox:1.29] 1154361} {[gcr.io/google_containers/pause-amd64@sha256:59eec8837a4d942cc19a52b8c09ea75121acc38114a2c68b98983ce9356b8610 k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea gcr.io/google_containers/pause-amd64:3.1 k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Feb 13 22:53:17.213: INFO: 
Logging kubelet events for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 13 22:53:17.238: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 13 22:53:17.272: INFO: calico-node-skgqz started at 2019-02-13 22:03:50 +0000 UTC (1+1 container statuses recorded)
Feb 13 22:53:17.272: INFO: 	Init container install-cni ready: true, restart count 0
Feb 13 22:53:17.272: INFO: 	Container calico-node ready: true, restart count 0
Feb 13 22:53:17.272: INFO: node-exporter-clvlv started at 2019-02-13 22:03:50 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.272: INFO: 	Container node-exporter ready: true, restart count 0
Feb 13 22:53:17.272: INFO: kube-proxy-n2tgw started at 2019-02-13 22:03:50 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.272: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 13 22:53:17.272: INFO: busybox-readonly-fs111e4dc4-2fdf-11e9-9de3-466a6591423c started at 2019-02-13 22:48:17 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.272: INFO: 	Container busybox-readonly-fs111e4dc4-2fdf-11e9-9de3-466a6591423c ready: false, restart count 0
W0213 22:53:17.295350   31687 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 22:53:17.424: INFO: 
Latency metrics for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 13 22:53:17.424: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.5 Latency:31.526587s}
Feb 13 22:53:17.424: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.9 Latency:31.526587s}
Feb 13 22:53:17.424: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.99 Latency:31.526587s}
Feb 13 22:53:17.424: INFO: 
Logging node info for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 13 22:53:17.446: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,UID:396631bc-2fdb-11e9-9f3f-da917295d151,ResourceVersion:7950,Generation:0,CreationTimestamp:2019-02-13 22:03:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: Standard_DS2_v2,beta.kubernetes.io/os: linux,failure-domain.beta.kubernetes.io/region: westeurope,failure-domain.beta.kubernetes.io/zone: 0,kubernetes.io/hostname: shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,kubernetes.io/role: node,node-role.kubernetes.io/node: ,worker.garden.sapcloud.io/group: cpu-worker,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 10.250.0.4/19,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.96.0.0/24,DoNotUse_ExternalID:,ProviderID:azure:///subscriptions/00d2caa5-cd29-46f7-845a-2f8ee0360ef5/resourceGroups/shoot--it--pub-az-7eoqa/providers/Microsoft.Compute/virtualMachines/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-azure-disk: {{8 0} {<nil>} 8 DecimalSI},cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33931702272 0} {<nil>} 33136428Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{7286583296 0} {<nil>} 7115804Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-azure-disk: {{8 0} {<nil>} 8 DecimalSI},cpu: {{1920 -3} {<nil>} 1920m DecimalSI},ephemeral-storage: {{32235117134 0} {<nil>} 32235117134 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{5848512302 0} {<nil>} 5848512302 DecimalSI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-02-13 22:04:31 +0000 UTC 2019-02-13 22:04:31 +0000 UTC RouteCreated RouteController created a route} {MemoryPressure False 2019-02-13 22:53:13 +0000 UTC 2019-02-13 22:03:41 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-02-13 22:53:13 +0000 UTC 2019-02-13 22:03:41 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-02-13 22:53:13 +0000 UTC 2019-02-13 22:03:41 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-02-13 22:53:13 +0000 UTC 2019-02-13 22:04:02 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.250.0.4} {Hostname shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:6c8c7e4d4c77480683bae4659ecfe2c8,SystemUUID:ACBA3163-164E-654B-A3CA-359ACDA743E9,BootID:6655f8f9-c370-4c96-b50d-24dd05e8cb6f,KernelVersion:4.14.96-coreos,OSImage:Container Linux by CoreOS 1967.5.0 (Rhyolite),ContainerRuntimeVersion:docker://18.6.1,KubeletVersion:v1.13.3,KubeProxyVersion:v1.13.3,OperatingSystem:linux,Architecture:amd64,},Images:[{[k8s.gcr.io/hyperkube@sha256:a36f9497dacfac277ca706ef245a04682ec9f5a7afa0ad27036d08e7c108d57b k8s.gcr.io/hyperkube:v1.12.3] 635332234} {[quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:617076c3e3d4d0638a4927702530d8456bda64c67194f6daed272a59e93b992f quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0] 568075227} {[k8s.gcr.io/hyperkube@sha256:388312116707c0420c492accc15ad3ba400911c2c337c8b33049f8d92f9c7bb9 k8s.gcr.io/hyperkube:v1.13.3] 564951434} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[quay.io/calico/node@sha256:264d7de573a956f8914de93fbade66bdc7fd86f22a8f4e9a99ebdbb9e45b66f7 quay.io/calico/node:v3.4.0] 75919845} {[quay.io/calico/cni@sha256:526f0c585f66fc70d79062463d78a324f053c406b69a600fc282c2aaa74c4428 quay.io/calico/cni:v3.4.0] 75434816} {[eu.gcr.io/gardener-project/gardener/ingress-default-backend@sha256:17b68928ead12cc9df88ee60d9c638d3fd642a7e122c2bb7586da1a21eb2de45 eu.gcr.io/gardener-project/gardener/ingress-default-backend:0.7.0] 69546830} {[ruby@sha256:83eebc6b30281ff43dc919cdd361803fb296f74f3d4f1cc42036860a49de9caf ruby:2.5.3-alpine] 50827145} {[jetstack/kube-lego@sha256:7824e0c58ee4403364227453dda6c2b0bc88ea6159e521973e5d8b2b7a64851b jetstack/kube-lego:0.1.7] 48639481} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[coredns/coredns@sha256:e030773c7fee285435ed7fc7623532ee54c4c1c4911fb24d95cd0170a8a768bc coredns/coredns:1.3.0] 40415528} {[quay.io/prometheus/node-exporter@sha256:1b129a3801a0440f9c5b2afb20082dfdb31bf6092b561f5f249531130000cb83 quay.io/prometheus/node-exporter:v0.17.0] 20982005} {[quay.io/prometheus/blackbox-exporter@sha256:0958c966dca13db9880a55e03221f507a2cf72ccf401b34642bdb0ba739d056f quay.io/prometheus/blackbox-exporter:v0.13.0] 17490231} {[eu.gcr.io/gardener-project/gardener/vpn-shoot@sha256:411918b22b7e3dbfee1838bed73b621777cb28896410e06b53ed4cdaa4aa1c1d eu.gcr.io/gardener-project/gardener/vpn-shoot:0.13.0] 13638102} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/google_containers/pause-amd64@sha256:59eec8837a4d942cc19a52b8c09ea75121acc38114a2c68b98983ce9356b8610 gcr.io/google_containers/pause-amd64:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Feb 13 22:53:17.446: INFO: 
Logging kubelet events for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 13 22:53:17.472: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 13 22:53:17.514: INFO: metrics-server-65cc55bd79-8kjg8 started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Container metrics-server ready: true, restart count 0
Feb 13 22:53:17.514: INFO: coredns-67df79bbdd-96tpz started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Container coredns ready: true, restart count 0
Feb 13 22:53:17.514: INFO: addons-kubernetes-dashboard-6579b646c5-g48vq started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 13 22:53:17.514: INFO: addons-kube-lego-69bbdc96b6-6m8wg started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Container kube-lego ready: true, restart count 0
Feb 13 22:53:17.514: INFO: vpn-shoot-c76596df-6nsrb started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 13 22:53:17.514: INFO: node-exporter-tmhn5 started at 2019-02-13 22:03:43 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Container node-exporter ready: true, restart count 0
Feb 13 22:53:17.514: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-kj5ch started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 13 22:53:17.514: INFO: addons-nginx-ingress-controller-d74bfff57-87jx9 started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 13 22:53:17.514: INFO: blackbox-exporter-d6c46f9fc-5xt9g started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 13 22:53:17.514: INFO: kube-proxy-6688x started at 2019-02-13 22:03:43 +0000 UTC (0+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 13 22:53:17.514: INFO: calico-node-psbns started at 2019-02-13 22:03:43 +0000 UTC (1+1 container statuses recorded)
Feb 13 22:53:17.514: INFO: 	Init container install-cni ready: true, restart count 0
Feb 13 22:53:17.514: INFO: 	Container calico-node ready: true, restart count 0
W0213 22:53:17.536089   31687 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 22:53:17.623: INFO: 
Latency metrics for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 13 22:53:17.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-zg562" for this suite.
Feb 13 22:53:39.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:53:40.207: INFO: namespace: e2e-tests-kubelet-test-zg562, resource: bindings, ignored listing per whitelist
Feb 13 22:53:40.547: INFO: namespace e2e-tests-kubelet-test-zg562 deletion completed in 22.901230874s

• Failure [324.533 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance] [It]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

    Expected error:
        <*errors.errorString | 0xc0000c5850>: {
            s: "timed out waiting for the condition",
        }
        timed out waiting for the condition
    not to have occurred

    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/pods.go:110
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:53:40.547: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-qw5ps
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:53:45.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-qw5ps" for this suite.
Feb 13 22:54:39.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:54:39.878: INFO: namespace: e2e-tests-kubelet-test-qw5ps, resource: bindings, ignored listing per whitelist
Feb 13 22:54:40.656: INFO: namespace e2e-tests-kubelet-test-qw5ps deletion completed in 54.924063775s

• [SLOW TEST:60.109 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:54:40.656: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-zwb86
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-592394aa-2fe2-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 13 22:54:41.679: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5926e1d6-2fe2-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-zwb86" to be "success or failure"
Feb 13 22:54:41.700: INFO: Pod "pod-projected-secrets-5926e1d6-2fe2-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.510523ms
Feb 13 22:54:43.723: INFO: Pod "pod-projected-secrets-5926e1d6-2fe2-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044215364s
Feb 13 22:54:45.746: INFO: Pod "pod-projected-secrets-5926e1d6-2fe2-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067082001s
STEP: Saw pod success
Feb 13 22:54:45.746: INFO: Pod "pod-projected-secrets-5926e1d6-2fe2-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:54:45.767: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-secrets-5926e1d6-2fe2-11e9-9de3-466a6591423c container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 22:54:45.861: INFO: Waiting for pod pod-projected-secrets-5926e1d6-2fe2-11e9-9de3-466a6591423c to disappear
Feb 13 22:54:45.882: INFO: Pod pod-projected-secrets-5926e1d6-2fe2-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:54:45.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zwb86" for this suite.
Feb 13 22:54:51.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:54:52.308: INFO: namespace: e2e-tests-projected-zwb86, resource: bindings, ignored listing per whitelist
Feb 13 22:54:52.868: INFO: namespace e2e-tests-projected-zwb86 deletion completed in 6.964125785s

• [SLOW TEST:12.212 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:54:52.868: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-2dnts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 13 22:54:58.138: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-6089d5cd-2fe2-11e9-9de3-466a6591423c,GenerateName:,Namespace:e2e-tests-events-2dnts,SelfLink:/api/v1/namespaces/e2e-tests-events-2dnts/pods/send-events-6089d5cd-2fe2-11e9-9de3-466a6591423c,UID:608ac834-2fe2-11e9-9f3f-da917295d151,ResourceVersion:8211,Generation:0,CreationTimestamp:2019-02-13 22:54:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 27110401,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.55/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-5d5dt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5d5dt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-5d5dt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001557280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0015572a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:54:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:54:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:54:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:54:54 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.55,StartTime:2019-02-13 22:54:54 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-13 22:54:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://01d16e92fc8d7a9610b159290d861704b20930a3ed746c51dad41aede96c0943}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 13 22:55:00.161: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 13 22:55:02.184: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:55:02.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-2dnts" for this suite.
Feb 13 22:55:46.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:55:46.664: INFO: namespace: e2e-tests-events-2dnts, resource: bindings, ignored listing per whitelist
Feb 13 22:55:47.176: INFO: namespace e2e-tests-events-2dnts deletion completed in 44.946133666s

• [SLOW TEST:54.308 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:55:47.177: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-r6jr5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 22:55:48.228: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80d46c26-2fe2-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-r6jr5" to be "success or failure"
Feb 13 22:55:48.256: INFO: Pod "downwardapi-volume-80d46c26-2fe2-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 27.955824ms
Feb 13 22:55:50.278: INFO: Pod "downwardapi-volume-80d46c26-2fe2-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049955262s
Feb 13 22:55:52.300: INFO: Pod "downwardapi-volume-80d46c26-2fe2-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072528941s
STEP: Saw pod success
Feb 13 22:55:52.300: INFO: Pod "downwardapi-volume-80d46c26-2fe2-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:55:52.323: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-80d46c26-2fe2-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 22:55:52.380: INFO: Waiting for pod downwardapi-volume-80d46c26-2fe2-11e9-9de3-466a6591423c to disappear
Feb 13 22:55:52.402: INFO: Pod downwardapi-volume-80d46c26-2fe2-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:55:52.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-r6jr5" for this suite.
Feb 13 22:56:23.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:56:23.857: INFO: namespace: e2e-tests-downward-api-r6jr5, resource: bindings, ignored listing per whitelist
Feb 13 22:56:24.260: INFO: namespace e2e-tests-downward-api-r6jr5 deletion completed in 31.836351364s

• [SLOW TEST:37.083 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:56:24.260: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-z6xsq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 13 22:56:25.226: INFO: Waiting up to 5m0s for pod "pod-96e1ef59-2fe2-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-z6xsq" to be "success or failure"
Feb 13 22:56:25.269: INFO: Pod "pod-96e1ef59-2fe2-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 43.484769ms
Feb 13 22:56:27.292: INFO: Pod "pod-96e1ef59-2fe2-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066233407s
Feb 13 22:56:29.315: INFO: Pod "pod-96e1ef59-2fe2-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088755994s
STEP: Saw pod success
Feb 13 22:56:29.315: INFO: Pod "pod-96e1ef59-2fe2-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 22:56:29.336: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-96e1ef59-2fe2-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 22:56:29.399: INFO: Waiting for pod pod-96e1ef59-2fe2-11e9-9de3-466a6591423c to disappear
Feb 13 22:56:29.420: INFO: Pod pod-96e1ef59-2fe2-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:56:29.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-z6xsq" for this suite.
Feb 13 22:56:35.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:56:35.611: INFO: namespace: e2e-tests-emptydir-z6xsq, resource: bindings, ignored listing per whitelist
Feb 13 22:56:36.418: INFO: namespace e2e-tests-emptydir-z6xsq deletion completed in 6.97616276s

• [SLOW TEST:12.158 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:56:36.418: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-q6wxs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:56:37.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-q6wxs" for this suite.
Feb 13 22:56:43.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:56:43.958: INFO: namespace: e2e-tests-services-q6wxs, resource: bindings, ignored listing per whitelist
Feb 13 22:56:44.427: INFO: namespace e2e-tests-services-q6wxs deletion completed in 6.944094806s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:8.009 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:56:44.427: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-7mw5x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 13 22:56:45.641: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 22:56:46.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-7mw5x" for this suite.
Feb 13 22:56:52.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:56:53.053: INFO: namespace: e2e-tests-replication-controller-7mw5x, resource: bindings, ignored listing per whitelist
Feb 13 22:56:53.781: INFO: namespace e2e-tests-replication-controller-7mw5x deletion completed in 6.989308076s

• [SLOW TEST:9.354 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 22:56:53.781: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-hd8kf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-hd8kf
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-hd8kf
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-hd8kf
Feb 13 22:56:54.784: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 13 22:57:04.813: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 13 22:57:04.835: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 13 22:57:05.711: INFO: stderr: ""
Feb 13 22:57:05.711: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 13 22:57:05.711: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 13 22:57:05.733: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 13 22:57:15.756: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 13 22:57:15.756: INFO: Waiting for statefulset status.replicas updated to 0
Feb 13 22:57:15.849: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999962s
Feb 13 22:57:16.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.970654145s
Feb 13 22:57:17.895: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.948001163s
Feb 13 22:57:18.919: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.925393s
Feb 13 22:57:19.942: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.901329612s
Feb 13 22:57:20.964: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.878261297s
Feb 13 22:57:21.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.855755022s
Feb 13 22:57:23.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.832833948s
Feb 13 22:57:24.034: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.809552968s
Feb 13 22:57:25.057: INFO: Verifying statefulset ss doesn't scale past 3 for another 786.12639ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-hd8kf
Feb 13 22:57:26.081: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:57:26.759: INFO: stderr: ""
Feb 13 22:57:26.759: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 13 22:57:26.759: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 13 22:57:26.759: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:57:27.491: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 13 22:57:27.491: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 13 22:57:27.491: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 13 22:57:27.491: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:57:28.228: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 13 22:57:28.228: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 13 22:57:28.228: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 13 22:57:28.251: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 22:57:28.251: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 22:57:28.251: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 13 22:57:28.273: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 13 22:57:28.978: INFO: stderr: ""
Feb 13 22:57:28.978: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 13 22:57:28.978: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 13 22:57:28.978: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 13 22:57:29.597: INFO: stderr: ""
Feb 13 22:57:29.597: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 13 22:57:29.597: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 13 22:57:29.597: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 13 22:57:30.349: INFO: stderr: ""
Feb 13 22:57:30.349: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 13 22:57:30.349: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 13 22:57:30.349: INFO: Waiting for statefulset status.replicas updated to 0
Feb 13 22:57:30.371: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 13 22:57:40.419: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 13 22:57:40.419: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 13 22:57:40.419: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 13 22:57:40.493: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:57:40.493: INFO: ss-0  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  }]
Feb 13 22:57:40.493: INFO: ss-1  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:40.493: INFO: ss-2  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:40.493: INFO: 
Feb 13 22:57:40.493: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 13 22:57:41.516: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:57:41.516: INFO: ss-0  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  }]
Feb 13 22:57:41.516: INFO: ss-1  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:41.516: INFO: ss-2  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:41.516: INFO: 
Feb 13 22:57:41.516: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 13 22:57:42.539: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:57:42.539: INFO: ss-0  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  }]
Feb 13 22:57:42.539: INFO: ss-1  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:42.539: INFO: ss-2  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:42.539: INFO: 
Feb 13 22:57:42.539: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 13 22:57:43.562: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:57:43.562: INFO: ss-0  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  }]
Feb 13 22:57:43.562: INFO: ss-2  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:43.562: INFO: 
Feb 13 22:57:43.562: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 13 22:57:44.585: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:57:44.586: INFO: ss-0  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  }]
Feb 13 22:57:44.586: INFO: ss-2  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:44.586: INFO: 
Feb 13 22:57:44.586: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 13 22:57:45.609: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:57:45.609: INFO: ss-0  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  }]
Feb 13 22:57:45.609: INFO: ss-2  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:45.609: INFO: 
Feb 13 22:57:45.609: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 13 22:57:46.632: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:57:46.632: INFO: ss-0  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  }]
Feb 13 22:57:46.632: INFO: ss-2  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:46.632: INFO: 
Feb 13 22:57:46.632: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 13 22:57:47.655: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:57:47.655: INFO: ss-0  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  }]
Feb 13 22:57:47.655: INFO: ss-2  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:47.655: INFO: 
Feb 13 22:57:47.655: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 13 22:57:48.678: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:57:48.678: INFO: ss-0  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  }]
Feb 13 22:57:48.678: INFO: ss-2  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:48.678: INFO: 
Feb 13 22:57:48.678: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 13 22:57:49.701: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 22:57:49.701: INFO: ss-0  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:56:54 +0000 UTC  }]
Feb 13 22:57:49.701: INFO: ss-2  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:57:15 +0000 UTC  }]
Feb 13 22:57:49.701: INFO: 
Feb 13 22:57:49.701: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-hd8kf
Feb 13 22:57:50.724: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:57:51.189: INFO: rc: 1
Feb 13 22:57:51.189: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc000cb77a0 exit status 1 <nil> <nil> true [0xc00000ea78 0xc00000eac0 0xc00000eae0] [0xc00000ea78 0xc00000eac0 0xc00000eae0] [0xc00000eab8 0xc00000ead0] [0x933040 0x933040] 0xc0018381e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 13 22:58:01.189: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:58:01.341: INFO: rc: 1
Feb 13 22:58:01.341: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000cb7b60 exit status 1 <nil> <nil> true [0xc00000eaf0 0xc00000ee68 0xc00000ee90] [0xc00000eaf0 0xc00000ee68 0xc00000ee90] [0xc00000ee50 0xc00000ee80] [0x933040 0x933040] 0xc001838540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:58:11.341: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:58:11.564: INFO: rc: 1
Feb 13 22:58:11.564: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0017e5d10 exit status 1 <nil> <nil> true [0xc0018bc370 0xc0018bc3d0 0xc0018bc3f8] [0xc0018bc370 0xc0018bc3d0 0xc0018bc3f8] [0xc0018bc3b0 0xc0018bc3f0] [0x933040 0x933040] 0xc0016b5140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:58:21.565: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:58:21.738: INFO: rc: 1
Feb 13 22:58:21.738: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0017e5fb0 exit status 1 <nil> <nil> true [0xc0018bc400 0xc0018bc438 0xc0018bc468] [0xc0018bc400 0xc0018bc438 0xc0018bc468] [0xc0018bc430 0xc0018bc448] [0x933040 0x933040] 0xc0016b5440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:58:31.739: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:58:31.889: INFO: rc: 1
Feb 13 22:58:31.889: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000cb7e60 exit status 1 <nil> <nil> true [0xc00000eea8 0xc00000eee8 0xc00000ef20] [0xc00000eea8 0xc00000eee8 0xc00000ef20] [0xc00000eec8 0xc00000ef10] [0x933040 0x933040] 0xc0018388a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:58:41.889: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:58:42.076: INFO: rc: 1
Feb 13 22:58:42.076: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008203f0 exit status 1 <nil> <nil> true [0xc0000d5898 0xc0000d5908 0xc0000d5990] [0xc0000d5898 0xc0000d5908 0xc0000d5990] [0xc0000d58f0 0xc0000d5948] [0x933040 0x933040] 0xc0018d5680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:58:52.076: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:58:52.250: INFO: rc: 1
Feb 13 22:58:52.250: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008a8180 exit status 1 <nil> <nil> true [0xc00000ef28 0xc00000ef50 0xc00000ef80] [0xc00000ef28 0xc00000ef50 0xc00000ef80] [0xc00000ef40 0xc00000ef70] [0x933040 0x933040] 0xc001838c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:59:02.250: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:59:02.404: INFO: rc: 1
Feb 13 22:59:02.404: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008a8420 exit status 1 <nil> <nil> true [0xc00000ef88 0xc00000efa8 0xc00000efd8] [0xc00000ef88 0xc00000efa8 0xc00000efd8] [0xc00000efa0 0xc00000efc0] [0x933040 0x933040] 0xc001838f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:59:12.404: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:59:12.611: INFO: rc: 1
Feb 13 22:59:12.612: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008a86f0 exit status 1 <nil> <nil> true [0xc00000efe0 0xc00000f018 0xc00000f038] [0xc00000efe0 0xc00000f018 0xc00000f038] [0xc00000f000 0xc00000f030] [0x933040 0x933040] 0xc001839260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:59:22.613: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:59:22.778: INFO: rc: 1
Feb 13 22:59:22.778: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008a8c30 exit status 1 <nil> <nil> true [0xc00000f060 0xc00000f080 0xc00000f0a0] [0xc00000f060 0xc00000f080 0xc00000f0a0] [0xc00000f078 0xc00000f090] [0x933040 0x933040] 0xc000fea4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:59:32.778: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:59:32.929: INFO: rc: 1
Feb 13 22:59:32.929: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000cb6300 exit status 1 <nil> <nil> true [0xc0018bc000 0xc0018bc028 0xc0018bc068] [0xc0018bc000 0xc0018bc028 0xc0018bc068] [0xc0018bc010 0xc0018bc060] [0x933040 0x933040] 0xc001838360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:59:42.930: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:59:43.087: INFO: rc: 1
Feb 13 22:59:43.087: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ba5c80 exit status 1 <nil> <nil> true [0xc000178000 0xc000178040 0xc000178070] [0xc000178000 0xc000178040 0xc000178070] [0xc000178028 0xc000178068] [0x933040 0x933040] 0xc001ee6240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 22:59:53.087: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 22:59:53.274: INFO: rc: 1
Feb 13 22:59:53.274: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ba5fb0 exit status 1 <nil> <nil> true [0xc000178100 0xc000178130 0xc0001781a0] [0xc000178100 0xc000178130 0xc0001781a0] [0xc000178128 0xc000178148] [0x933040 0x933040] 0xc001ee7680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:00:03.274: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:00:03.423: INFO: rc: 1
Feb 13 23:00:03.423: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000cb6780 exit status 1 <nil> <nil> true [0xc0018bc070 0xc0018bc0b8 0xc0018bc108] [0xc0018bc070 0xc0018bc0b8 0xc0018bc108] [0xc0018bc098 0xc0018bc0f8] [0x933040 0x933040] 0xc001838660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:00:13.424: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:00:13.572: INFO: rc: 1
Feb 13 23:00:13.572: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0017e4150 exit status 1 <nil> <nil> true [0xc0001781a8 0xc0001781e0 0xc000178208] [0xc0001781a8 0xc0001781e0 0xc000178208] [0xc0001781d0 0xc000178200] [0x933040 0x933040] 0xc001c88540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:00:23.572: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:00:23.729: INFO: rc: 1
Feb 13 23:00:23.729: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0017e43c0 exit status 1 <nil> <nil> true [0xc000178240 0xc000178260 0xc000178280] [0xc000178240 0xc000178260 0xc000178280] [0xc000178258 0xc000178278] [0x933040 0x933040] 0xc001c888a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:00:33.730: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:00:33.897: INFO: rc: 1
Feb 13 23:00:33.897: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0017e4690 exit status 1 <nil> <nil> true [0xc0001782a8 0xc000178300 0xc000178330] [0xc0001782a8 0xc000178300 0xc000178330] [0xc0001782d8 0xc000178328] [0x933040 0x933040] 0xc001c88ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:00:43.897: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:00:44.066: INFO: rc: 1
Feb 13 23:00:44.066: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000cb6ae0 exit status 1 <nil> <nil> true [0xc0018bc110 0xc0018bc148 0xc0018bc180] [0xc0018bc110 0xc0018bc148 0xc0018bc180] [0xc0018bc130 0xc0018bc160] [0x933040 0x933040] 0xc0018389c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:00:54.067: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:00:54.228: INFO: rc: 1
Feb 13 23:00:54.228: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0017e4900 exit status 1 <nil> <nil> true [0xc000178340 0xc0001783d8 0xc000178450] [0xc000178340 0xc0001783d8 0xc000178450] [0xc0001783b8 0xc000178430] [0x933040 0x933040] 0xc0019fc120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:01:04.228: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:01:09.391: INFO: rc: 1
Feb 13 23:01:09.391: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b662d0 exit status 1 <nil> <nil> true [0xc0000d40a0 0xc0000d4978 0xc0000d4b40] [0xc0000d40a0 0xc0000d4978 0xc0000d4b40] [0xc0000d4788 0xc0000d49e8] [0x933040 0x933040] 0xc0016b4960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:01:19.391: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:01:19.573: INFO: rc: 1
Feb 13 23:01:19.573: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000cb6e40 exit status 1 <nil> <nil> true [0xc0018bc1a0 0xc0018bc1e0 0xc0018bc1f8] [0xc0018bc1a0 0xc0018bc1e0 0xc0018bc1f8] [0xc0018bc1d8 0xc0018bc1f0] [0x933040 0x933040] 0xc001838d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:01:29.573: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:01:29.749: INFO: rc: 1
Feb 13 23:01:29.749: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ba5b90 exit status 1 <nil> <nil> true [0xc000178018 0xc000178050 0xc000178100] [0xc000178018 0xc000178050 0xc000178100] [0xc000178040 0xc000178070] [0x933040 0x933040] 0xc001c886c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:01:39.749: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:01:39.920: INFO: rc: 1
Feb 13 23:01:39.920: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ba5f20 exit status 1 <nil> <nil> true [0xc000178118 0xc000178140 0xc0001781a8] [0xc000178118 0xc000178140 0xc0001781a8] [0xc000178130 0xc0001781a0] [0x933040 0x933040] 0xc001c88a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:01:49.920: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:01:50.082: INFO: rc: 1
Feb 13 23:01:50.082: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0017e4240 exit status 1 <nil> <nil> true [0xc0001781c0 0xc0001781f0 0xc000178240] [0xc0001781c0 0xc0001781f0 0xc000178240] [0xc0001781e0 0xc000178208] [0x933040 0x933040] 0xc001c88d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:02:00.082: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:02:00.264: INFO: rc: 1
Feb 13 23:02:00.264: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b66270 exit status 1 <nil> <nil> true [0xc0000d40a0 0xc0000d4978 0xc0000d4b40] [0xc0000d40a0 0xc0000d4978 0xc0000d4b40] [0xc0000d4788 0xc0000d49e8] [0x933040 0x933040] 0xc0019fc240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:02:10.264: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:02:10.450: INFO: rc: 1
Feb 13 23:02:10.450: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0017e44e0 exit status 1 <nil> <nil> true [0xc000178248 0xc000178268 0xc0001782a8] [0xc000178248 0xc000178268 0xc0001782a8] [0xc000178260 0xc000178280] [0x933040 0x933040] 0xc001ee62a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:02:20.450: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:02:20.604: INFO: rc: 1
Feb 13 23:02:20.604: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0017e4780 exit status 1 <nil> <nil> true [0xc0001782c8 0xc000178310 0xc000178340] [0xc0001782c8 0xc000178310 0xc000178340] [0xc000178300 0xc000178330] [0x933040 0x933040] 0xc001ee76e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:02:30.604: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:02:30.760: INFO: rc: 1
Feb 13 23:02:30.760: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000cb63f0 exit status 1 <nil> <nil> true [0xc0018bc000 0xc0018bc028 0xc0018bc068] [0xc0018bc000 0xc0018bc028 0xc0018bc068] [0xc0018bc010 0xc0018bc060] [0x933040 0x933040] 0xc0016b4960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:02:40.761: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:02:40.986: INFO: rc: 1
Feb 13 23:02:40.986: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000cb6750 exit status 1 <nil> <nil> true [0xc0018bc070 0xc0018bc0b8 0xc0018bc108] [0xc0018bc070 0xc0018bc0b8 0xc0018bc108] [0xc0018bc098 0xc0018bc0f8] [0x933040 0x933040] 0xc0016b4c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 13 23:02:50.986: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hd8kf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:03:06.437: INFO: rc: 1
Feb 13 23:03:06.437: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Feb 13 23:03:06.437: INFO: Scaling statefulset ss to 0
Feb 13 23:03:06.510: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 13 23:03:06.532: INFO: Deleting all statefulset in ns e2e-tests-statefulset-hd8kf
Feb 13 23:03:06.553: INFO: Scaling statefulset ss to 0
Feb 13 23:03:06.618: INFO: Waiting for statefulset status.replicas updated to 0
Feb 13 23:03:06.639: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:03:06.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-hd8kf" for this suite.
Feb 13 23:03:12.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:03:13.601: INFO: namespace: e2e-tests-statefulset-hd8kf, resource: bindings, ignored listing per whitelist
Feb 13 23:03:13.666: INFO: namespace e2e-tests-statefulset-hd8kf deletion completed in 6.937164587s

• [SLOW TEST:379.885 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:03:13.666: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-ftlpz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-ftlpz
Feb 13 23:03:18.772: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-ftlpz
STEP: checking the pod's current state and verifying that restartCount is present
Feb 13 23:03:18.793: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:07:19.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-ftlpz" for this suite.
Feb 13 23:07:25.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:07:25.797: INFO: namespace: e2e-tests-container-probe-ftlpz, resource: bindings, ignored listing per whitelist
Feb 13 23:07:26.530: INFO: namespace e2e-tests-container-probe-ftlpz deletion completed in 6.925844213s

• [SLOW TEST:252.864 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:07:26.530: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wpwll
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-21b5fb52-2fe4-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 13 23:07:27.661: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-21b97f5c-2fe4-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-wpwll" to be "success or failure"
Feb 13 23:07:27.682: INFO: Pod "pod-projected-configmaps-21b97f5c-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.104334ms
Feb 13 23:07:29.705: INFO: Pod "pod-projected-configmaps-21b97f5c-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044276498s
Feb 13 23:07:31.735: INFO: Pod "pod-projected-configmaps-21b97f5c-2fe4-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073965846s
STEP: Saw pod success
Feb 13 23:07:31.735: INFO: Pod "pod-projected-configmaps-21b97f5c-2fe4-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:07:31.756: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-configmaps-21b97f5c-2fe4-11e9-9de3-466a6591423c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 23:07:31.845: INFO: Waiting for pod pod-projected-configmaps-21b97f5c-2fe4-11e9-9de3-466a6591423c to disappear
Feb 13 23:07:31.866: INFO: Pod pod-projected-configmaps-21b97f5c-2fe4-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:07:31.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wpwll" for this suite.
Feb 13 23:07:37.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:07:38.296: INFO: namespace: e2e-tests-projected-wpwll, resource: bindings, ignored listing per whitelist
Feb 13 23:07:38.879: INFO: namespace e2e-tests-projected-wpwll deletion completed in 6.991010885s

• [SLOW TEST:12.349 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:07:38.879: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4zk8n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-2918f4fd-2fe4-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 13 23:07:40.052: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-291c4c8b-2fe4-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-4zk8n" to be "success or failure"
Feb 13 23:07:40.092: INFO: Pod "pod-projected-configmaps-291c4c8b-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 40.21079ms
Feb 13 23:07:42.115: INFO: Pod "pod-projected-configmaps-291c4c8b-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06269168s
Feb 13 23:07:44.136: INFO: Pod "pod-projected-configmaps-291c4c8b-2fe4-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084562343s
STEP: Saw pod success
Feb 13 23:07:44.136: INFO: Pod "pod-projected-configmaps-291c4c8b-2fe4-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:07:44.158: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-configmaps-291c4c8b-2fe4-11e9-9de3-466a6591423c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 23:07:44.217: INFO: Waiting for pod pod-projected-configmaps-291c4c8b-2fe4-11e9-9de3-466a6591423c to disappear
Feb 13 23:07:44.241: INFO: Pod pod-projected-configmaps-291c4c8b-2fe4-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:07:44.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4zk8n" for this suite.
Feb 13 23:07:50.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:07:51.190: INFO: namespace: e2e-tests-projected-4zk8n, resource: bindings, ignored listing per whitelist
Feb 13 23:07:51.190: INFO: namespace e2e-tests-projected-4zk8n deletion completed in 6.926856938s

• [SLOW TEST:12.311 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:07:51.190: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-jbpk7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:07:52.150: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 13 23:07:56.194: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 13 23:07:58.216: INFO: Creating deployment "test-rollover-deployment"
Feb 13 23:07:58.262: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 13 23:08:00.306: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 13 23:08:00.349: INFO: Ensure that both replica sets have 1 created replica
Feb 13 23:08:00.393: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 13 23:08:00.441: INFO: Updating deployment test-rollover-deployment
Feb 13 23:08:00.441: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 13 23:08:02.487: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 13 23:08:02.530: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 13 23:08:02.574: INFO: all replica sets need to contain the pod-template-hash label
Feb 13 23:08:02.574: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696080, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:08:04.619: INFO: all replica sets need to contain the pod-template-hash label
Feb 13 23:08:04.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696080, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:08:06.619: INFO: all replica sets need to contain the pod-template-hash label
Feb 13 23:08:06.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696086, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:08:08.619: INFO: all replica sets need to contain the pod-template-hash label
Feb 13 23:08:08.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696086, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:08:10.619: INFO: all replica sets need to contain the pod-template-hash label
Feb 13 23:08:10.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696086, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:08:12.619: INFO: all replica sets need to contain the pod-template-hash label
Feb 13 23:08:12.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696086, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:08:14.618: INFO: all replica sets need to contain the pod-template-hash label
Feb 13 23:08:14.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696086, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685696078, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:08:16.618: INFO: 
Feb 13 23:08:16.618: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 13 23:08:16.683: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-jbpk7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jbpk7/deployments/test-rollover-deployment,UID:33f46312-2fe4-11e9-9f3f-da917295d151,ResourceVersion:10054,Generation:2,CreationTimestamp:2019-02-13 23:07:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-13 23:07:58 +0000 UTC 2019-02-13 23:07:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-13 23:08:16 +0000 UTC 2019-02-13 23:07:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 13 23:08:16.705: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-jbpk7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jbpk7/replicasets/test-rollover-deployment-6b7f9d6597,UID:3544f58f-2fe4-11e9-9f3f-da917295d151,ResourceVersion:10047,Generation:2,CreationTimestamp:2019-02-13 23:08:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 33f46312-2fe4-11e9-9f3f-da917295d151 0xc001b15b37 0xc001b15b38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 13 23:08:16.705: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 13 23:08:16.705: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-jbpk7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jbpk7/replicasets/test-rollover-controller,UID:30501d29-2fe4-11e9-9f3f-da917295d151,ResourceVersion:10053,Generation:2,CreationTimestamp:2019-02-13 23:07:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 33f46312-2fe4-11e9-9f3f-da917295d151 0xc001b158f7 0xc001b158f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 13 23:08:16.705: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-jbpk7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jbpk7/replicasets/test-rollover-deployment-6586df867b,UID:33f9bd8b-2fe4-11e9-9f3f-da917295d151,ResourceVersion:10005,Generation:2,CreationTimestamp:2019-02-13 23:07:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 33f46312-2fe4-11e9-9f3f-da917295d151 0xc001b15a67 0xc001b15a68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 13 23:08:16.727: INFO: Pod "test-rollover-deployment-6b7f9d6597-tjdz2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-tjdz2,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-jbpk7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jbpk7/pods/test-rollover-deployment-6b7f9d6597-tjdz2,UID:3551b569-2fe4-11e9-9f3f-da917295d151,ResourceVersion:10024,Generation:0,CreationTimestamp:2019-02-13 23:08:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.67/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 3544f58f-2fe4-11e9-9f3f-da917295d151 0xc001e3d1d7 0xc001e3d1d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-lzfwv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lzfwv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lzfwv true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3d240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3d260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:08:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:08:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:08:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:08:00 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.67,StartTime:2019-02-13 23:08:00 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-13 23:08:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7247720050e33f22a4dcb763dc61796713e32c4b376aa3623f383dccc32cc29b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:08:16.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-jbpk7" for this suite.
Feb 13 23:08:22.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:08:23.208: INFO: namespace: e2e-tests-deployment-jbpk7, resource: bindings, ignored listing per whitelist
Feb 13 23:08:23.720: INFO: namespace e2e-tests-deployment-jbpk7 deletion completed in 6.970276303s

• [SLOW TEST:32.529 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:08:23.720: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-nlkpz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:08:24.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43bd97a5-2fe4-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-nlkpz" to be "success or failure"
Feb 13 23:08:24.752: INFO: Pod "downwardapi-volume-43bd97a5-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.295151ms
Feb 13 23:08:26.774: INFO: Pod "downwardapi-volume-43bd97a5-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043558156s
Feb 13 23:08:28.797: INFO: Pod "downwardapi-volume-43bd97a5-2fe4-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066080954s
STEP: Saw pod success
Feb 13 23:08:28.797: INFO: Pod "downwardapi-volume-43bd97a5-2fe4-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:08:28.818: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-43bd97a5-2fe4-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 23:08:28.883: INFO: Waiting for pod downwardapi-volume-43bd97a5-2fe4-11e9-9de3-466a6591423c to disappear
Feb 13 23:08:28.906: INFO: Pod downwardapi-volume-43bd97a5-2fe4-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:08:28.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nlkpz" for this suite.
Feb 13 23:08:34.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:08:35.296: INFO: namespace: e2e-tests-downward-api-nlkpz, resource: bindings, ignored listing per whitelist
Feb 13 23:08:35.882: INFO: namespace e2e-tests-downward-api-nlkpz deletion completed in 6.953315084s

• [SLOW TEST:12.162 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:08:35.882: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-pnv9r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0213 23:09:17.067238   31687 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 23:09:17.067: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:09:17.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-pnv9r" for this suite.
Feb 13 23:09:23.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:09:23.788: INFO: namespace: e2e-tests-gc-pnv9r, resource: bindings, ignored listing per whitelist
Feb 13 23:09:24.025: INFO: namespace e2e-tests-gc-pnv9r deletion completed in 6.936239721s

• [SLOW TEST:48.143 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:09:24.025: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-sv7d9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:09:25.163: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 13 23:09:29.209: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 13 23:09:33.437: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-sv7d9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sv7d9/deployments/test-cleanup-deployment,UID:6a37205f-2fe4-11e9-9f3f-da917295d151,ResourceVersion:10370,Generation:1,CreationTimestamp:2019-02-13 23:09:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-13 23:09:29 +0000 UTC 2019-02-13 23:09:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-13 23:09:32 +0000 UTC 2019-02-13 23:09:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-7dbbfcf846" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 13 23:09:33.459: INFO: New ReplicaSet "test-cleanup-deployment-7dbbfcf846" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846,GenerateName:,Namespace:e2e-tests-deployment-sv7d9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sv7d9/replicasets/test-cleanup-deployment-7dbbfcf846,UID:6a393a48-2fe4-11e9-9f3f-da917295d151,ResourceVersion:10362,Generation:1,CreationTimestamp:2019-02-13 23:09:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 6a37205f-2fe4-11e9-9f3f-da917295d151 0xc0004bb207 0xc0004bb208}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 13 23:09:33.481: INFO: Pod "test-cleanup-deployment-7dbbfcf846-rk2kf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-7dbbfcf846-rk2kf,GenerateName:test-cleanup-deployment-7dbbfcf846-,Namespace:e2e-tests-deployment-sv7d9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sv7d9/pods/test-cleanup-deployment-7dbbfcf846-rk2kf,UID:6a39bbe6-2fe4-11e9-9f3f-da917295d151,ResourceVersion:10361,Generation:0,CreationTimestamp:2019-02-13 23:09:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 7dbbfcf846,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.75/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-7dbbfcf846 6a393a48-2fe4-11e9-9f3f-da917295d151 0xc00047c347 0xc00047c348}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-pb78x {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pb78x,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pb78x true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00047c780} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00047c7d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:09:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:09:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:09:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:09:29 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.75,StartTime:2019-02-13 23:09:29 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-13 23:09:31 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7d3cc4a7cdba703f77395dc8c9e44de9a2572521c12a2c4d9abc56feea349bd5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:09:33.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-sv7d9" for this suite.
Feb 13 23:09:39.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:09:39.754: INFO: namespace: e2e-tests-deployment-sv7d9, resource: bindings, ignored listing per whitelist
Feb 13 23:09:40.455: INFO: namespace e2e-tests-deployment-sv7d9 deletion completed in 6.950964725s

• [SLOW TEST:16.429 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:09:40.455: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-m59b9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:09:41.526: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71839e86-2fe4-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-m59b9" to be "success or failure"
Feb 13 23:09:41.547: INFO: Pod "downwardapi-volume-71839e86-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.827333ms
Feb 13 23:09:43.589: INFO: Pod "downwardapi-volume-71839e86-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06364923s
Feb 13 23:09:45.612: INFO: Pod "downwardapi-volume-71839e86-2fe4-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086665131s
STEP: Saw pod success
Feb 13 23:09:45.612: INFO: Pod "downwardapi-volume-71839e86-2fe4-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:09:45.633: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-71839e86-2fe4-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 23:09:45.692: INFO: Waiting for pod downwardapi-volume-71839e86-2fe4-11e9-9de3-466a6591423c to disappear
Feb 13 23:09:45.712: INFO: Pod downwardapi-volume-71839e86-2fe4-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:09:45.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m59b9" for this suite.
Feb 13 23:09:51.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:09:52.619: INFO: namespace: e2e-tests-projected-m59b9, resource: bindings, ignored listing per whitelist
Feb 13 23:09:52.706: INFO: namespace e2e-tests-projected-m59b9 deletion completed in 6.971550842s

• [SLOW TEST:12.251 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:09:52.706: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-9k6zj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 13 23:09:53.726: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-9k6zj'
Feb 13 23:09:55.154: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 13 23:09:55.154: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Feb 13 23:09:55.230: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-9k6zj'
Feb 13 23:09:55.429: INFO: stderr: ""
Feb 13 23:09:55.429: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:09:55.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9k6zj" for this suite.
Feb 13 23:10:01.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:10:02.027: INFO: namespace: e2e-tests-kubectl-9k6zj, resource: bindings, ignored listing per whitelist
Feb 13 23:10:02.394: INFO: namespace e2e-tests-kubectl-9k6zj deletion completed in 6.943071341s

• [SLOW TEST:9.688 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:10:02.394: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-snzsf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:10:03.432: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e92314c-2fe4-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-snzsf" to be "success or failure"
Feb 13 23:10:03.453: INFO: Pod "downwardapi-volume-7e92314c-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.420815ms
Feb 13 23:10:05.476: INFO: Pod "downwardapi-volume-7e92314c-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044113416s
Feb 13 23:10:07.499: INFO: Pod "downwardapi-volume-7e92314c-2fe4-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066760465s
STEP: Saw pod success
Feb 13 23:10:07.499: INFO: Pod "downwardapi-volume-7e92314c-2fe4-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:10:07.520: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-7e92314c-2fe4-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 23:10:07.585: INFO: Waiting for pod downwardapi-volume-7e92314c-2fe4-11e9-9de3-466a6591423c to disappear
Feb 13 23:10:07.607: INFO: Pod downwardapi-volume-7e92314c-2fe4-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:10:07.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-snzsf" for this suite.
Feb 13 23:10:13.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:10:13.952: INFO: namespace: e2e-tests-downward-api-snzsf, resource: bindings, ignored listing per whitelist
Feb 13 23:10:14.609: INFO: namespace e2e-tests-downward-api-snzsf deletion completed in 6.980072945s

• [SLOW TEST:12.215 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:10:14.609: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-wlvt6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 13 23:10:20.362: INFO: Successfully updated pod "pod-update-85e6b25c-2fe4-11e9-9de3-466a6591423c"
STEP: verifying the updated pod is in kubernetes
Feb 13 23:10:20.410: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:10:20.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-wlvt6" for this suite.
Feb 13 23:10:42.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:10:43.039: INFO: namespace: e2e-tests-pods-wlvt6, resource: bindings, ignored listing per whitelist
Feb 13 23:10:43.361: INFO: namespace e2e-tests-pods-wlvt6 deletion completed in 22.928099722s

• [SLOW TEST:28.752 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:10:43.361: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-6zb6j
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-9703f183-2fe4-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 13 23:10:44.463: INFO: Waiting up to 5m0s for pod "pod-configmaps-97073608-2fe4-11e9-9de3-466a6591423c" in namespace "e2e-tests-configmap-6zb6j" to be "success or failure"
Feb 13 23:10:44.484: INFO: Pod "pod-configmaps-97073608-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.270767ms
Feb 13 23:10:46.506: INFO: Pod "pod-configmaps-97073608-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043262914s
Feb 13 23:10:48.529: INFO: Pod "pod-configmaps-97073608-2fe4-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065610377s
STEP: Saw pod success
Feb 13 23:10:48.529: INFO: Pod "pod-configmaps-97073608-2fe4-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:10:48.550: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-configmaps-97073608-2fe4-11e9-9de3-466a6591423c container configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 23:10:48.617: INFO: Waiting for pod pod-configmaps-97073608-2fe4-11e9-9de3-466a6591423c to disappear
Feb 13 23:10:48.639: INFO: Pod pod-configmaps-97073608-2fe4-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:10:48.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-6zb6j" for this suite.
Feb 13 23:10:54.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:10:55.563: INFO: namespace: e2e-tests-configmap-6zb6j, resource: bindings, ignored listing per whitelist
Feb 13 23:10:55.606: INFO: namespace e2e-tests-configmap-6zb6j deletion completed in 6.945528605s

• [SLOW TEST:12.245 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:10:55.606: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-l99nd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 13 23:10:56.725: INFO: Waiting up to 5m0s for pod "pod-9e49882b-2fe4-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-l99nd" to be "success or failure"
Feb 13 23:10:56.747: INFO: Pod "pod-9e49882b-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.698032ms
Feb 13 23:10:58.769: INFO: Pod "pod-9e49882b-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043860989s
Feb 13 23:11:00.792: INFO: Pod "pod-9e49882b-2fe4-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066641517s
Feb 13 23:11:02.814: INFO: Pod "pod-9e49882b-2fe4-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089107466s
STEP: Saw pod success
Feb 13 23:11:02.814: INFO: Pod "pod-9e49882b-2fe4-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:11:02.836: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-9e49882b-2fe4-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 23:11:02.907: INFO: Waiting for pod pod-9e49882b-2fe4-11e9-9de3-466a6591423c to disappear
Feb 13 23:11:02.928: INFO: Pod pod-9e49882b-2fe4-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:11:02.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-l99nd" for this suite.
Feb 13 23:11:09.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:11:09.761: INFO: namespace: e2e-tests-emptydir-l99nd, resource: bindings, ignored listing per whitelist
Feb 13 23:11:09.887: INFO: namespace e2e-tests-emptydir-l99nd deletion completed in 6.936707634s

• [SLOW TEST:14.280 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:11:09.887: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-ftdjt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-ftdjt
Feb 13 23:11:14.981: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-ftdjt
STEP: checking the pod's current state and verifying that restartCount is present
Feb 13 23:11:15.002: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:15:17.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-ftdjt" for this suite.
Feb 13 23:15:23.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:15:23.381: INFO: namespace: e2e-tests-container-probe-ftdjt, resource: bindings, ignored listing per whitelist
Feb 13 23:15:23.940: INFO: namespace e2e-tests-container-probe-ftdjt deletion completed in 6.904631971s

• [SLOW TEST:254.053 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:15:23.940: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wrapper-jnv7t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:15:31.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-jnv7t" for this suite.
Feb 13 23:15:37.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:15:37.711: INFO: namespace: e2e-tests-emptydir-wrapper-jnv7t, resource: bindings, ignored listing per whitelist
Feb 13 23:15:38.034: INFO: namespace e2e-tests-emptydir-wrapper-jnv7t deletion completed in 6.895506551s

• [SLOW TEST:14.094 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:15:38.034: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-9c96n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 13 23:15:39.107: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-9c96n'
Feb 13 23:15:39.301: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 13 23:15:39.301: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 13 23:15:39.355: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-r5m6s]
Feb 13 23:15:39.355: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-r5m6s" in namespace "e2e-tests-kubectl-9c96n" to be "running and ready"
Feb 13 23:15:39.377: INFO: Pod "e2e-test-nginx-rc-r5m6s": Phase="Pending", Reason="", readiness=false. Elapsed: 21.486948ms
Feb 13 23:15:41.399: INFO: Pod "e2e-test-nginx-rc-r5m6s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043900832s
Feb 13 23:15:43.423: INFO: Pod "e2e-test-nginx-rc-r5m6s": Phase="Running", Reason="", readiness=true. Elapsed: 4.067406681s
Feb 13 23:15:43.423: INFO: Pod "e2e-test-nginx-rc-r5m6s" satisfied condition "running and ready"
Feb 13 23:15:43.423: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-r5m6s]
Feb 13 23:15:43.423: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-9c96n'
Feb 13 23:15:43.719: INFO: stderr: ""
Feb 13 23:15:43.719: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Feb 13 23:15:43.719: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-9c96n'
Feb 13 23:15:43.986: INFO: stderr: ""
Feb 13 23:15:43.986: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:15:43.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9c96n" for this suite.
Feb 13 23:16:06.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:16:06.803: INFO: namespace: e2e-tests-kubectl-9c96n, resource: bindings, ignored listing per whitelist
Feb 13 23:16:06.931: INFO: namespace e2e-tests-kubectl-9c96n deletion completed in 22.922938726s

• [SLOW TEST:28.897 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:16:06.931: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-fx7dx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-kcbt
STEP: Creating a pod to test atomic-volume-subpath
Feb 13 23:16:08.059: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kcbt" in namespace "e2e-tests-subpath-fx7dx" to be "success or failure"
Feb 13 23:16:08.087: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Pending", Reason="", readiness=false. Elapsed: 27.513541ms
Feb 13 23:16:10.109: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050009798s
Feb 13 23:16:12.132: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072541623s
Feb 13 23:16:14.154: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Running", Reason="", readiness=false. Elapsed: 6.095050224s
Feb 13 23:16:16.177: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Running", Reason="", readiness=false. Elapsed: 8.118094038s
Feb 13 23:16:18.200: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Running", Reason="", readiness=false. Elapsed: 10.14088093s
Feb 13 23:16:20.226: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Running", Reason="", readiness=false. Elapsed: 12.167362203s
Feb 13 23:16:22.249: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Running", Reason="", readiness=false. Elapsed: 14.189666388s
Feb 13 23:16:24.271: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Running", Reason="", readiness=false. Elapsed: 16.212216346s
Feb 13 23:16:26.295: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Running", Reason="", readiness=false. Elapsed: 18.235617561s
Feb 13 23:16:28.317: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Running", Reason="", readiness=false. Elapsed: 20.258096511s
Feb 13 23:16:30.340: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Running", Reason="", readiness=false. Elapsed: 22.280949638s
Feb 13 23:16:32.363: INFO: Pod "pod-subpath-test-downwardapi-kcbt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.304151584s
STEP: Saw pod success
Feb 13 23:16:32.363: INFO: Pod "pod-subpath-test-downwardapi-kcbt" satisfied condition "success or failure"
Feb 13 23:16:32.385: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-subpath-test-downwardapi-kcbt container test-container-subpath-downwardapi-kcbt: <nil>
STEP: delete the pod
Feb 13 23:16:32.534: INFO: Waiting for pod pod-subpath-test-downwardapi-kcbt to disappear
Feb 13 23:16:32.572: INFO: Pod pod-subpath-test-downwardapi-kcbt no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-kcbt
Feb 13 23:16:32.573: INFO: Deleting pod "pod-subpath-test-downwardapi-kcbt" in namespace "e2e-tests-subpath-fx7dx"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:16:32.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-fx7dx" for this suite.
Feb 13 23:16:38.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:16:39.457: INFO: namespace: e2e-tests-subpath-fx7dx, resource: bindings, ignored listing per whitelist
Feb 13 23:16:39.525: INFO: namespace e2e-tests-subpath-fx7dx deletion completed in 6.909837538s

• [SLOW TEST:32.594 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:16:39.526: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-spzd2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 13 23:16:48.715: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:16:48.736: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:16:50.736: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:16:50.759: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:16:52.736: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:16:52.758: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:16:54.736: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:16:54.759: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:16:56.736: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:16:56.759: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:16:58.736: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:16:58.759: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:17:00.736: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:17:00.759: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:17:02.736: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:17:02.759: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:17:04.736: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:17:04.758: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:17:06.736: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:17:06.758: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:17:08.736: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:17:08.758: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:17:08.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-spzd2" for this suite.
Feb 13 23:17:32.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:17:33.713: INFO: namespace: e2e-tests-container-lifecycle-hook-spzd2, resource: bindings, ignored listing per whitelist
Feb 13 23:17:33.734: INFO: namespace e2e-tests-container-lifecycle-hook-spzd2 deletion completed in 24.923248173s

• [SLOW TEST:54.209 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:17:33.735: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-gk78q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:17:34.812: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 13 23:17:34.857: INFO: Number of nodes with available pods: 0
Feb 13 23:17:34.857: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 13 23:17:34.951: INFO: Number of nodes with available pods: 0
Feb 13 23:17:34.951: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:35.974: INFO: Number of nodes with available pods: 0
Feb 13 23:17:35.974: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:36.973: INFO: Number of nodes with available pods: 0
Feb 13 23:17:36.973: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:37.975: INFO: Number of nodes with available pods: 0
Feb 13 23:17:37.975: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:38.974: INFO: Number of nodes with available pods: 1
Feb 13 23:17:38.974: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 13 23:17:39.070: INFO: Number of nodes with available pods: 0
Feb 13 23:17:39.070: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 13 23:17:39.115: INFO: Number of nodes with available pods: 0
Feb 13 23:17:39.115: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:40.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:40.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:41.138: INFO: Number of nodes with available pods: 0
Feb 13 23:17:41.138: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:42.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:42.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:43.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:43.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:44.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:44.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:45.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:45.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:46.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:46.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:47.136: INFO: Number of nodes with available pods: 0
Feb 13 23:17:47.136: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:48.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:48.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:49.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:49.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:50.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:50.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:51.138: INFO: Number of nodes with available pods: 0
Feb 13 23:17:51.138: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:52.138: INFO: Number of nodes with available pods: 0
Feb 13 23:17:52.138: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:53.139: INFO: Number of nodes with available pods: 0
Feb 13 23:17:53.139: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:54.138: INFO: Number of nodes with available pods: 0
Feb 13 23:17:54.138: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:55.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:55.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:56.138: INFO: Number of nodes with available pods: 0
Feb 13 23:17:56.139: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:57.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:57.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:58.137: INFO: Number of nodes with available pods: 0
Feb 13 23:17:58.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:17:59.139: INFO: Number of nodes with available pods: 0
Feb 13 23:17:59.139: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:00.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:00.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:01.138: INFO: Number of nodes with available pods: 0
Feb 13 23:18:01.138: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:02.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:02.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:03.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:03.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:04.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:04.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:05.138: INFO: Number of nodes with available pods: 0
Feb 13 23:18:05.138: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:06.138: INFO: Number of nodes with available pods: 0
Feb 13 23:18:06.138: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:07.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:07.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:08.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:08.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:09.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:09.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:10.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:10.138: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:11.138: INFO: Number of nodes with available pods: 0
Feb 13 23:18:11.138: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:12.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:12.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:13.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:13.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:14.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:14.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:15.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:15.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:16.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:16.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:17.137: INFO: Number of nodes with available pods: 0
Feb 13 23:18:17.137: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:18:18.138: INFO: Number of nodes with available pods: 1
Feb 13 23:18:18.138: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-gk78q, will wait for the garbage collector to delete the pods
Feb 13 23:18:18.277: INFO: Deleting DaemonSet.extensions daemon-set took: 24.392867ms
Feb 13 23:18:18.377: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.319289ms
Feb 13 23:18:51.998: INFO: Number of nodes with available pods: 0
Feb 13 23:18:51.998: INFO: Number of running nodes: 0, number of available pods: 0
Feb 13 23:18:52.020: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-gk78q/daemonsets","resourceVersion":"11751"},"items":null}

Feb 13 23:18:52.041: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-gk78q/pods","resourceVersion":"11751"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:18:52.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-gk78q" for this suite.
Feb 13 23:18:58.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:18:58.503: INFO: namespace: e2e-tests-daemonsets-gk78q, resource: bindings, ignored listing per whitelist
Feb 13 23:18:59.110: INFO: namespace e2e-tests-daemonsets-gk78q deletion completed in 6.949493413s

• [SLOW TEST:85.376 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:18:59.111: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-hsxwj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:19:00.110: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml version --client'
Feb 13 23:19:00.178: INFO: stderr: ""
Feb 13 23:19:00.178: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"archive\", BuildDate:\"2019-02-13T22:27:45Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 13 23:19:00.199: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-hsxwj'
Feb 13 23:19:00.596: INFO: stderr: ""
Feb 13 23:19:00.596: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 13 23:19:00.596: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-hsxwj'
Feb 13 23:19:00.914: INFO: stderr: ""
Feb 13 23:19:00.914: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 13 23:19:01.937: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 23:19:01.937: INFO: Found 0 / 1
Feb 13 23:19:32.460: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 23:19:32.461: INFO: Found 1 / 1
Feb 13 23:19:32.461: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 13 23:19:32.483: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 23:19:32.483: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 13 23:19:32.483: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml describe pod redis-master-hg7zh --namespace=e2e-tests-kubectl-hsxwj'
Feb 13 23:19:32.714: INFO: stderr: ""
Feb 13 23:19:32.714: INFO: stdout: "Name:               redis-master-hg7zh\nNamespace:          e2e-tests-kubectl-hsxwj\nPriority:           0\nPriorityClassName:  <none>\nNode:               shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/10.250.0.5\nStart Time:         Wed, 13 Feb 2019 23:19:00 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.90/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.90\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://d59b2e846b2f5ebcf8ffa2d9fef4c84cee63f9a6198ffb11cbbbbe140d49fa66\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 13 Feb 2019 23:19:02 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rbb89 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rbb89:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rbb89\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                          Message\n  ----    ------     ----  ----                                                          -------\n  Normal  Scheduled  32s   default-scheduler                                             Successfully assigned e2e-tests-kubectl-hsxwj/redis-master-hg7zh to shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx\n  Normal  Pulled     31s   kubelet, shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    30s   kubelet, shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Created container\n  Normal  Started    30s   kubelet, shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Started container\n"
Feb 13 23:19:32.714: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml describe rc redis-master --namespace=e2e-tests-kubectl-hsxwj'
Feb 13 23:19:32.954: INFO: stderr: ""
Feb 13 23:19:32.954: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-hsxwj\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  32s   replication-controller  Created pod: redis-master-hg7zh\n"
Feb 13 23:19:32.954: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml describe service redis-master --namespace=e2e-tests-kubectl-hsxwj'
Feb 13 23:19:33.188: INFO: stderr: ""
Feb 13 23:19:33.188: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-hsxwj\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.68.205.70\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.90:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 13 23:19:33.210: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml describe node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx'
Feb 13 23:19:33.475: INFO: stderr: ""
Feb 13 23:19:33.475: INFO: stdout: "Name:               shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_DS2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=westeurope\n                    failure-domain.beta.kubernetes.io/zone=1\n                    kubernetes.io/hostname=shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.5/19\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 13 Feb 2019 22:03:50 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 13 Feb 2019 22:04:14 +0000   Wed, 13 Feb 2019 22:04:14 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Wed, 13 Feb 2019 23:19:23 +0000   Wed, 13 Feb 2019 22:03:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 13 Feb 2019 23:19:23 +0000   Wed, 13 Feb 2019 22:03:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 13 Feb 2019 23:19:23 +0000   Wed, 13 Feb 2019 22:03:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 13 Feb 2019 23:19:23 +0000   Wed, 13 Feb 2019 22:04:10 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.0.5\n  Hostname:    shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx\nCapacity:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              33136428Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         7115804Ki\n pods:                           110\nAllocatable:\n attachable-volumes-azure-disk:  8\n cpu:                            1920m\n ephemeral-storage:              32235117134\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         5848512302\n pods:                           110\nSystem Info:\n Machine ID:                 c6a974bd9e9a471f9985af77c270be63\n System UUID:                485BC509-BC48-AC4D-A484-A78B36A6CFF6\n Boot ID:                    e730366f-e2e0-4aff-bae0-eb62accbb532\n Kernel Version:             4.14.96-coreos\n OS Image:                   Container Linux by CoreOS 1967.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.13.3\n Kube-Proxy Version:         v1.13.3\nPodCIDR:                     100.96.1.0/24\nProviderID:                  azure:///subscriptions/00d2caa5-cd29-46f7-845a-2f8ee0360ef5/resourceGroups/shoot--it--pub-az-7eoqa/providers/Microsoft.Compute/virtualMachines/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx\nNon-terminated Pods:         (4 in total)\n  Namespace                  Name                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                   ------------  ----------  ---------------  -------------  ---\n  e2e-tests-kubectl-hsxwj    redis-master-hg7zh     0 (0%)        0 (0%)      0 (0%)           0 (0%)         33s\n  kube-system                calico-node-skgqz      100m (5%)     500m (26%)  100Mi (1%)       700Mi (12%)    75m\n  kube-system                kube-proxy-n2tgw       20m (1%)      900m (46%)  64Mi (1%)        200Mi (3%)     75m\n  kube-system                node-exporter-clvlv    5m (0%)       15m (0%)    10Mi (0%)        50Mi (0%)      75m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            125m (6%)   1415m (73%)\n  memory                         174Mi (3%)  950Mi (17%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\nEvents:                          <none>\n"
Feb 13 23:19:33.476: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml describe namespace e2e-tests-kubectl-hsxwj'
Feb 13 23:19:33.721: INFO: stderr: ""
Feb 13 23:19:33.721: INFO: stdout: "Name:         e2e-tests-kubectl-hsxwj\nLabels:       e2e-framework=kubectl\n              e2e-run=111bad8d-2fdf-11e9-9de3-466a6591423c\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:19:33.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hsxwj" for this suite.
Feb 13 23:19:55.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:19:56.458: INFO: namespace: e2e-tests-kubectl-hsxwj, resource: bindings, ignored listing per whitelist
Feb 13 23:19:56.655: INFO: namespace e2e-tests-kubectl-hsxwj deletion completed in 22.910900911s

• [SLOW TEST:57.544 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:19:56.655: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-w6k2d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 13 23:19:57.639: INFO: Waiting up to 5m0s for pod "client-containers-e0bf1b55-2fe5-11e9-9de3-466a6591423c" in namespace "e2e-tests-containers-w6k2d" to be "success or failure"
Feb 13 23:19:57.661: INFO: Pod "client-containers-e0bf1b55-2fe5-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.310874ms
Feb 13 23:19:59.683: INFO: Pod "client-containers-e0bf1b55-2fe5-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043176568s
Feb 13 23:20:01.706: INFO: Pod "client-containers-e0bf1b55-2fe5-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06636942s
Feb 13 23:20:03.728: INFO: Pod "client-containers-e0bf1b55-2fe5-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089027782s
STEP: Saw pod success
Feb 13 23:20:03.728: INFO: Pod "client-containers-e0bf1b55-2fe5-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:20:03.751: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod client-containers-e0bf1b55-2fe5-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 23:20:03.837: INFO: Waiting for pod client-containers-e0bf1b55-2fe5-11e9-9de3-466a6591423c to disappear
Feb 13 23:20:03.859: INFO: Pod client-containers-e0bf1b55-2fe5-11e9-9de3-466a6591423c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:20:03.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-w6k2d" for this suite.
Feb 13 23:20:09.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:20:10.890: INFO: namespace: e2e-tests-containers-w6k2d, resource: bindings, ignored listing per whitelist
Feb 13 23:20:10.976: INFO: namespace e2e-tests-containers-w6k2d deletion completed in 7.094012034s

• [SLOW TEST:14.321 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:20:10.976: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-dt6zw
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-e984c0eb-2fe5-11e9-9de3-466a6591423c
STEP: Creating configMap with name cm-test-opt-upd-e984c12c-2fe5-11e9-9de3-466a6591423c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e984c0eb-2fe5-11e9-9de3-466a6591423c
STEP: Updating configmap cm-test-opt-upd-e984c12c-2fe5-11e9-9de3-466a6591423c
STEP: Creating configMap with name cm-test-opt-create-e984c141-2fe5-11e9-9de3-466a6591423c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:21:42.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-dt6zw" for this suite.
Feb 13 23:22:04.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:22:05.464: INFO: namespace: e2e-tests-configmap-dt6zw, resource: bindings, ignored listing per whitelist
Feb 13 23:22:05.697: INFO: namespace e2e-tests-configmap-dt6zw deletion completed in 22.983736515s

• [SLOW TEST:114.721 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:22:05.697: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-5whr7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-5whr7
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-5whr7
STEP: Deleting pre-stop pod
Feb 13 23:22:23.933: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:22:23.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-5whr7" for this suite.
Feb 13 23:23:04.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:23:04.891: INFO: namespace: e2e-tests-prestop-5whr7, resource: bindings, ignored listing per whitelist
Feb 13 23:23:04.979: INFO: namespace e2e-tests-prestop-5whr7 deletion completed in 40.960950613s

• [SLOW TEST:59.282 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:23:04.979: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-9wjv6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:23:06.077: INFO: (0) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 48.767219ms)
Feb 13 23:23:06.123: INFO: (1) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 45.627655ms)
Feb 13 23:23:06.147: INFO: (2) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.341084ms)
Feb 13 23:23:06.172: INFO: (3) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.131973ms)
Feb 13 23:23:06.198: INFO: (4) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.992764ms)
Feb 13 23:23:06.223: INFO: (5) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.026152ms)
Feb 13 23:23:06.248: INFO: (6) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.257823ms)
Feb 13 23:23:06.273: INFO: (7) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.711452ms)
Feb 13 23:23:06.297: INFO: (8) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.073706ms)
Feb 13 23:23:06.322: INFO: (9) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.818362ms)
Feb 13 23:23:06.346: INFO: (10) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.398654ms)
Feb 13 23:23:06.371: INFO: (11) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.629875ms)
Feb 13 23:23:06.397: INFO: (12) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.529477ms)
Feb 13 23:23:06.422: INFO: (13) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.502152ms)
Feb 13 23:23:06.447: INFO: (14) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.63714ms)
Feb 13 23:23:06.471: INFO: (15) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.605572ms)
Feb 13 23:23:06.496: INFO: (16) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.412497ms)
Feb 13 23:23:06.522: INFO: (17) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.374285ms)
Feb 13 23:23:06.547: INFO: (18) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.805636ms)
Feb 13 23:23:06.633: INFO: (19) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 85.505271ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:23:06.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-9wjv6" for this suite.
Feb 13 23:23:12.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:23:13.191: INFO: namespace: e2e-tests-proxy-9wjv6, resource: bindings, ignored listing per whitelist
Feb 13 23:23:13.640: INFO: namespace e2e-tests-proxy-9wjv6 deletion completed in 6.985143684s

• [SLOW TEST:8.660 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:23:13.640: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-hqhsr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 13 23:23:14.714: INFO: Waiting up to 5m0s for pod "pod-56366208-2fe6-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-hqhsr" to be "success or failure"
Feb 13 23:23:14.735: INFO: Pod "pod-56366208-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.113145ms
Feb 13 23:23:16.757: INFO: Pod "pod-56366208-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043565581s
Feb 13 23:23:18.787: INFO: Pod "pod-56366208-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072906423s
Feb 13 23:23:20.809: INFO: Pod "pod-56366208-2fe6-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.095041563s
STEP: Saw pod success
Feb 13 23:23:20.809: INFO: Pod "pod-56366208-2fe6-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:23:20.836: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-56366208-2fe6-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 23:23:20.894: INFO: Waiting for pod pod-56366208-2fe6-11e9-9de3-466a6591423c to disappear
Feb 13 23:23:20.922: INFO: Pod pod-56366208-2fe6-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:23:20.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hqhsr" for this suite.
Feb 13 23:23:27.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:23:27.160: INFO: namespace: e2e-tests-emptydir-hqhsr, resource: bindings, ignored listing per whitelist
Feb 13 23:23:27.904: INFO: namespace e2e-tests-emptydir-hqhsr deletion completed in 6.960057253s

• [SLOW TEST:14.264 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:23:27.904: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-tjpvs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-tjpvs
I0213 23:23:28.923701   31687 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-tjpvs, replica count: 1
I0213 23:23:29.974199   31687 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:23:30.974470   31687 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:23:31.974790   31687 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 13 23:23:32.109: INFO: Created: latency-svc-kvvwf
Feb 13 23:23:32.124: INFO: Got endpoints: latency-svc-kvvwf [49.77241ms]
Feb 13 23:23:32.185: INFO: Created: latency-svc-pvfp6
Feb 13 23:23:32.197: INFO: Got endpoints: latency-svc-pvfp6 [71.727024ms]
Feb 13 23:23:32.207: INFO: Created: latency-svc-h2rf6
Feb 13 23:23:32.217: INFO: Got endpoints: latency-svc-h2rf6 [92.722588ms]
Feb 13 23:23:32.227: INFO: Created: latency-svc-g4zz4
Feb 13 23:23:32.239: INFO: Got endpoints: latency-svc-g4zz4 [114.467736ms]
Feb 13 23:23:32.249: INFO: Created: latency-svc-2rdfw
Feb 13 23:23:32.266: INFO: Got endpoints: latency-svc-2rdfw [141.875494ms]
Feb 13 23:23:32.295: INFO: Created: latency-svc-89rcx
Feb 13 23:23:32.298: INFO: Got endpoints: latency-svc-89rcx [172.530438ms]
Feb 13 23:23:32.312: INFO: Created: latency-svc-m5nr2
Feb 13 23:23:32.322: INFO: Got endpoints: latency-svc-m5nr2 [197.33308ms]
Feb 13 23:23:32.352: INFO: Created: latency-svc-lbz4d
Feb 13 23:23:32.362: INFO: Got endpoints: latency-svc-lbz4d [237.31055ms]
Feb 13 23:23:32.373: INFO: Created: latency-svc-l7whb
Feb 13 23:23:32.384: INFO: Got endpoints: latency-svc-l7whb [259.204088ms]
Feb 13 23:23:32.394: INFO: Created: latency-svc-l7r47
Feb 13 23:23:32.414: INFO: Got endpoints: latency-svc-l7r47 [288.722367ms]
Feb 13 23:23:32.415: INFO: Created: latency-svc-vtjg8
Feb 13 23:23:32.424: INFO: Got endpoints: latency-svc-vtjg8 [299.219852ms]
Feb 13 23:23:32.434: INFO: Created: latency-svc-m9g9h
Feb 13 23:23:32.448: INFO: Got endpoints: latency-svc-m9g9h [323.001867ms]
Feb 13 23:23:32.457: INFO: Created: latency-svc-mvksh
Feb 13 23:23:32.468: INFO: Got endpoints: latency-svc-mvksh [343.139283ms]
Feb 13 23:23:32.478: INFO: Created: latency-svc-4t2hp
Feb 13 23:23:32.489: INFO: Got endpoints: latency-svc-4t2hp [364.309964ms]
Feb 13 23:23:32.499: INFO: Created: latency-svc-xxc8c
Feb 13 23:23:32.510: INFO: Got endpoints: latency-svc-xxc8c [384.969632ms]
Feb 13 23:23:32.540: INFO: Created: latency-svc-sp4mc
Feb 13 23:23:32.542: INFO: Got endpoints: latency-svc-sp4mc [417.191595ms]
Feb 13 23:23:32.555: INFO: Created: latency-svc-9gbj8
Feb 13 23:23:32.566: INFO: Got endpoints: latency-svc-9gbj8 [369.028477ms]
Feb 13 23:23:32.575: INFO: Created: latency-svc-7zt75
Feb 13 23:23:32.585: INFO: Got endpoints: latency-svc-7zt75 [367.704371ms]
Feb 13 23:23:32.624: INFO: Created: latency-svc-pqkk9
Feb 13 23:23:32.668: INFO: Got endpoints: latency-svc-pqkk9 [428.218457ms]
Feb 13 23:23:32.669: INFO: Created: latency-svc-9r6md
Feb 13 23:23:32.672: INFO: Got endpoints: latency-svc-9r6md [405.112703ms]
Feb 13 23:23:32.685: INFO: Created: latency-svc-krtjf
Feb 13 23:23:32.701: INFO: Got endpoints: latency-svc-krtjf [403.290171ms]
Feb 13 23:23:32.718: INFO: Created: latency-svc-cm9rc
Feb 13 23:23:32.729: INFO: Got endpoints: latency-svc-cm9rc [406.870477ms]
Feb 13 23:23:32.738: INFO: Created: latency-svc-6xm9s
Feb 13 23:23:32.749: INFO: Got endpoints: latency-svc-6xm9s [386.589545ms]
Feb 13 23:23:32.759: INFO: Created: latency-svc-nlv49
Feb 13 23:23:32.792: INFO: Got endpoints: latency-svc-nlv49 [407.697174ms]
Feb 13 23:23:32.793: INFO: Created: latency-svc-8j5fh
Feb 13 23:23:32.796: INFO: Got endpoints: latency-svc-8j5fh [381.950597ms]
Feb 13 23:23:32.812: INFO: Created: latency-svc-4zt66
Feb 13 23:23:32.821: INFO: Got endpoints: latency-svc-4zt66 [396.980128ms]
Feb 13 23:23:32.831: INFO: Created: latency-svc-8mzjj
Feb 13 23:23:32.841: INFO: Got endpoints: latency-svc-8mzjj [393.159749ms]
Feb 13 23:23:32.852: INFO: Created: latency-svc-f59cx
Feb 13 23:23:32.863: INFO: Got endpoints: latency-svc-f59cx [394.302742ms]
Feb 13 23:23:32.872: INFO: Created: latency-svc-nbbt2
Feb 13 23:23:32.883: INFO: Got endpoints: latency-svc-nbbt2 [393.658303ms]
Feb 13 23:23:32.933: INFO: Created: latency-svc-zqsh4
Feb 13 23:23:32.935: INFO: Got endpoints: latency-svc-zqsh4 [424.764295ms]
Feb 13 23:23:32.948: INFO: Created: latency-svc-tpbdb
Feb 13 23:23:32.967: INFO: Got endpoints: latency-svc-tpbdb [424.245408ms]
Feb 13 23:23:32.977: INFO: Created: latency-svc-77brg
Feb 13 23:23:32.988: INFO: Got endpoints: latency-svc-77brg [421.840354ms]
Feb 13 23:23:33.000: INFO: Created: latency-svc-qpmb7
Feb 13 23:23:33.011: INFO: Got endpoints: latency-svc-qpmb7 [425.997705ms]
Feb 13 23:23:33.022: INFO: Created: latency-svc-cs7gs
Feb 13 23:23:33.071: INFO: Got endpoints: latency-svc-cs7gs [403.180004ms]
Feb 13 23:23:33.072: INFO: Created: latency-svc-qx44c
Feb 13 23:23:33.075: INFO: Got endpoints: latency-svc-qx44c [403.814845ms]
Feb 13 23:23:33.085: INFO: Created: latency-svc-t2kpb
Feb 13 23:23:33.095: INFO: Got endpoints: latency-svc-t2kpb [394.437973ms]
Feb 13 23:23:33.107: INFO: Created: latency-svc-mshj8
Feb 13 23:23:33.123: INFO: Got endpoints: latency-svc-mshj8 [393.8181ms]
Feb 13 23:23:33.143: INFO: Created: latency-svc-6kdtc
Feb 13 23:23:33.153: INFO: Got endpoints: latency-svc-6kdtc [404.038797ms]
Feb 13 23:23:33.163: INFO: Created: latency-svc-gjtgw
Feb 13 23:23:33.194: INFO: Got endpoints: latency-svc-gjtgw [401.652877ms]
Feb 13 23:23:33.195: INFO: Created: latency-svc-drb74
Feb 13 23:23:33.198: INFO: Got endpoints: latency-svc-drb74 [402.450164ms]
Feb 13 23:23:33.209: INFO: Created: latency-svc-kdrvr
Feb 13 23:23:33.220: INFO: Got endpoints: latency-svc-kdrvr [398.692077ms]
Feb 13 23:23:33.246: INFO: Created: latency-svc-b497w
Feb 13 23:23:33.256: INFO: Got endpoints: latency-svc-b497w [414.963179ms]
Feb 13 23:23:33.266: INFO: Created: latency-svc-vb6ns
Feb 13 23:23:33.276: INFO: Got endpoints: latency-svc-vb6ns [413.502066ms]
Feb 13 23:23:33.319: INFO: Created: latency-svc-zssmr
Feb 13 23:23:33.325: INFO: Got endpoints: latency-svc-zssmr [441.861718ms]
Feb 13 23:23:33.343: INFO: Created: latency-svc-n884r
Feb 13 23:23:33.354: INFO: Got endpoints: latency-svc-n884r [418.556209ms]
Feb 13 23:23:33.362: INFO: Created: latency-svc-ptp2j
Feb 13 23:23:33.373: INFO: Got endpoints: latency-svc-ptp2j [406.357819ms]
Feb 13 23:23:33.382: INFO: Created: latency-svc-fbjgs
Feb 13 23:23:33.394: INFO: Got endpoints: latency-svc-fbjgs [406.014329ms]
Feb 13 23:23:33.403: INFO: Created: latency-svc-pj2kv
Feb 13 23:23:33.413: INFO: Got endpoints: latency-svc-pj2kv [402.001656ms]
Feb 13 23:23:33.475: INFO: Created: latency-svc-94tx9
Feb 13 23:23:33.478: INFO: Got endpoints: latency-svc-94tx9 [406.880634ms]
Feb 13 23:23:33.490: INFO: Created: latency-svc-ccb74
Feb 13 23:23:33.501: INFO: Got endpoints: latency-svc-ccb74 [425.951452ms]
Feb 13 23:23:33.510: INFO: Created: latency-svc-jd8mg
Feb 13 23:23:33.527: INFO: Got endpoints: latency-svc-jd8mg [431.746656ms]
Feb 13 23:23:33.545: INFO: Created: latency-svc-d8cln
Feb 13 23:23:33.556: INFO: Got endpoints: latency-svc-d8cln [432.379795ms]
Feb 13 23:23:33.565: INFO: Created: latency-svc-l8cbd
Feb 13 23:23:33.595: INFO: Got endpoints: latency-svc-l8cbd [442.107511ms]
Feb 13 23:23:33.596: INFO: Created: latency-svc-nq99b
Feb 13 23:23:33.599: INFO: Got endpoints: latency-svc-nq99b [404.703402ms]
Feb 13 23:23:33.625: INFO: Created: latency-svc-r2lqf
Feb 13 23:23:33.643: INFO: Got endpoints: latency-svc-r2lqf [444.962852ms]
Feb 13 23:23:33.653: INFO: Created: latency-svc-blv72
Feb 13 23:23:33.664: INFO: Got endpoints: latency-svc-blv72 [443.380523ms]
Feb 13 23:23:33.673: INFO: Created: latency-svc-sg2gx
Feb 13 23:23:33.724: INFO: Got endpoints: latency-svc-sg2gx [467.708519ms]
Feb 13 23:23:33.726: INFO: Created: latency-svc-52x5l
Feb 13 23:23:33.734: INFO: Got endpoints: latency-svc-52x5l [457.956419ms]
Feb 13 23:23:33.851: INFO: Created: latency-svc-8zh75
Feb 13 23:23:33.857: INFO: Created: latency-svc-678l2
Feb 13 23:23:33.857: INFO: Got endpoints: latency-svc-8zh75 [531.5085ms]
Feb 13 23:23:33.868: INFO: Got endpoints: latency-svc-678l2 [513.813807ms]
Feb 13 23:23:33.877: INFO: Created: latency-svc-mmqbd
Feb 13 23:23:33.887: INFO: Got endpoints: latency-svc-mmqbd [514.052976ms]
Feb 13 23:23:33.897: INFO: Created: latency-svc-s5p2j
Feb 13 23:23:33.907: INFO: Got endpoints: latency-svc-s5p2j [513.785498ms]
Feb 13 23:23:33.918: INFO: Created: latency-svc-pc97l
Feb 13 23:23:33.928: INFO: Got endpoints: latency-svc-pc97l [514.26023ms]
Feb 13 23:23:33.937: INFO: Created: latency-svc-f6fw5
Feb 13 23:23:33.948: INFO: Got endpoints: latency-svc-f6fw5 [470.634954ms]
Feb 13 23:23:33.973: INFO: Created: latency-svc-66k6w
Feb 13 23:23:33.980: INFO: Got endpoints: latency-svc-66k6w [478.706459ms]
Feb 13 23:23:33.997: INFO: Created: latency-svc-b7cfw
Feb 13 23:23:34.008: INFO: Got endpoints: latency-svc-b7cfw [480.749016ms]
Feb 13 23:23:34.017: INFO: Created: latency-svc-hq4p7
Feb 13 23:23:34.028: INFO: Got endpoints: latency-svc-hq4p7 [471.929967ms]
Feb 13 23:23:34.039: INFO: Created: latency-svc-f4xqs
Feb 13 23:23:34.049: INFO: Got endpoints: latency-svc-f4xqs [453.858012ms]
Feb 13 23:23:34.059: INFO: Created: latency-svc-254bx
Feb 13 23:23:34.096: INFO: Created: latency-svc-72d45
Feb 13 23:23:34.096: INFO: Got endpoints: latency-svc-254bx [497.348318ms]
Feb 13 23:23:34.109: INFO: Created: latency-svc-h25tm
Feb 13 23:23:34.139: INFO: Created: latency-svc-gms24
Feb 13 23:23:34.139: INFO: Got endpoints: latency-svc-72d45 [496.05053ms]
Feb 13 23:23:34.194: INFO: Created: latency-svc-dpsc7
Feb 13 23:23:34.194: INFO: Got endpoints: latency-svc-h25tm [530.141952ms]
Feb 13 23:23:34.234: INFO: Created: latency-svc-mdbwc
Feb 13 23:23:34.258: INFO: Got endpoints: latency-svc-gms24 [523.385636ms]
Feb 13 23:23:34.258: INFO: Created: latency-svc-t8mt7
Feb 13 23:23:34.279: INFO: Created: latency-svc-s2kn4
Feb 13 23:23:34.299: INFO: Created: latency-svc-fnk7x
Feb 13 23:23:34.320: INFO: Created: latency-svc-r5tt8
Feb 13 23:23:34.353: INFO: Got endpoints: latency-svc-dpsc7 [628.740533ms]
Feb 13 23:23:34.354: INFO: Created: latency-svc-h9mps
Feb 13 23:23:34.370: INFO: Created: latency-svc-gghz8
Feb 13 23:23:34.404: INFO: Created: latency-svc-f7c28
Feb 13 23:23:34.424: INFO: Created: latency-svc-j49lc
Feb 13 23:23:34.443: INFO: Created: latency-svc-k5hg2
Feb 13 23:23:34.478: INFO: Created: latency-svc-fpzl5
Feb 13 23:23:34.491: INFO: Created: latency-svc-rl5mc
Feb 13 23:23:34.512: INFO: Created: latency-svc-scg8c
Feb 13 23:23:34.529: INFO: Got endpoints: latency-svc-mdbwc [671.852474ms]
Feb 13 23:23:34.529: INFO: Got endpoints: latency-svc-fnk7x [621.15533ms]
Feb 13 23:23:34.546: INFO: Got endpoints: latency-svc-r5tt8 [618.792825ms]
Feb 13 23:23:34.547: INFO: Created: latency-svc-4jm7t
Feb 13 23:23:34.566: INFO: Created: latency-svc-8n5vh
Feb 13 23:23:34.601: INFO: Created: latency-svc-8d66t
Feb 13 23:23:34.615: INFO: Created: latency-svc-rqn6d
Feb 13 23:23:34.632: INFO: Got endpoints: latency-svc-h9mps [683.98792ms]
Feb 13 23:23:34.633: INFO: Got endpoints: latency-svc-s2kn4 [745.141191ms]
Feb 13 23:23:34.650: INFO: Got endpoints: latency-svc-t8mt7 [782.573005ms]
Feb 13 23:23:34.650: INFO: Got endpoints: latency-svc-gghz8 [670.002531ms]
Feb 13 23:23:34.650: INFO: Created: latency-svc-rpklk
Feb 13 23:23:34.673: INFO: Created: latency-svc-rtsj6
Feb 13 23:23:34.693: INFO: Created: latency-svc-4nlh5
Feb 13 23:23:34.693: INFO: Got endpoints: latency-svc-f7c28 [685.198504ms]
Feb 13 23:23:34.732: INFO: Created: latency-svc-gw6rt
Feb 13 23:23:34.743: INFO: Got endpoints: latency-svc-j49lc [715.737611ms]
Feb 13 23:23:34.753: INFO: Created: latency-svc-q7mjg
Feb 13 23:23:34.773: INFO: Created: latency-svc-fl9fl
Feb 13 23:23:34.795: INFO: Got endpoints: latency-svc-fpzl5 [698.611042ms]
Feb 13 23:23:34.798: INFO: Created: latency-svc-zxxwk
Feb 13 23:23:34.865: INFO: Got endpoints: latency-svc-k5hg2 [816.104261ms]
Feb 13 23:23:34.865: INFO: Created: latency-svc-q55gk
Feb 13 23:23:34.889: INFO: Got endpoints: latency-svc-rl5mc [749.289616ms]
Feb 13 23:23:34.904: INFO: Created: latency-svc-qskql
Feb 13 23:23:34.941: INFO: Created: latency-svc-csdwk
Feb 13 23:23:34.941: INFO: Got endpoints: latency-svc-scg8c [746.81338ms]
Feb 13 23:23:34.988: INFO: Got endpoints: latency-svc-4jm7t [730.667147ms]
Feb 13 23:23:34.991: INFO: Created: latency-svc-wkpbf
Feb 13 23:23:35.023: INFO: Created: latency-svc-vgdrx
Feb 13 23:23:35.039: INFO: Got endpoints: latency-svc-8n5vh [686.302578ms]
Feb 13 23:23:35.075: INFO: Created: latency-svc-wgt2q
Feb 13 23:23:35.125: INFO: Got endpoints: latency-svc-8d66t [596.302012ms]
Feb 13 23:23:35.138: INFO: Got endpoints: latency-svc-rqn6d [609.239703ms]
Feb 13 23:23:35.159: INFO: Created: latency-svc-k9k4c
Feb 13 23:23:35.182: INFO: Created: latency-svc-b7k8t
Feb 13 23:23:35.192: INFO: Got endpoints: latency-svc-rpklk [645.972914ms]
Feb 13 23:23:35.259: INFO: Created: latency-svc-h9rdg
Feb 13 23:23:35.259: INFO: Got endpoints: latency-svc-rtsj6 [625.985348ms]
Feb 13 23:23:35.293: INFO: Got endpoints: latency-svc-4nlh5 [660.675118ms]
Feb 13 23:23:35.293: INFO: Created: latency-svc-c4g8d
Feb 13 23:23:35.327: INFO: Created: latency-svc-hbx97
Feb 13 23:23:35.339: INFO: Got endpoints: latency-svc-gw6rt [688.492646ms]
Feb 13 23:23:35.394: INFO: Got endpoints: latency-svc-q7mjg [743.651852ms]
Feb 13 23:23:35.407: INFO: Created: latency-svc-4tvt4
Feb 13 23:23:35.429: INFO: Created: latency-svc-4l6v6
Feb 13 23:23:35.461: INFO: Got endpoints: latency-svc-fl9fl [767.917579ms]
Feb 13 23:23:35.535: INFO: Got endpoints: latency-svc-zxxwk [791.808216ms]
Feb 13 23:23:35.535: INFO: Created: latency-svc-gspn9
Feb 13 23:23:35.538: INFO: Got endpoints: latency-svc-q55gk [742.825291ms]
Feb 13 23:23:35.597: INFO: Created: latency-svc-bchgr
Feb 13 23:23:35.597: INFO: Got endpoints: latency-svc-qskql [731.923659ms]
Feb 13 23:23:35.617: INFO: Created: latency-svc-484x9
Feb 13 23:23:35.667: INFO: Got endpoints: latency-svc-csdwk [777.788498ms]
Feb 13 23:23:35.668: INFO: Created: latency-svc-d9dvn
Feb 13 23:23:35.689: INFO: Got endpoints: latency-svc-wkpbf [747.800028ms]
Feb 13 23:23:35.727: INFO: Created: latency-svc-gzrdh
Feb 13 23:23:35.738: INFO: Got endpoints: latency-svc-vgdrx [749.999191ms]
Feb 13 23:23:35.749: INFO: Created: latency-svc-8lfbw
Feb 13 23:23:35.806: INFO: Got endpoints: latency-svc-wgt2q [766.043118ms]
Feb 13 23:23:35.806: INFO: Created: latency-svc-k86q7
Feb 13 23:23:35.839: INFO: Created: latency-svc-hvq4z
Feb 13 23:23:35.839: INFO: Got endpoints: latency-svc-k9k4c [714.264681ms]
Feb 13 23:23:35.874: INFO: Created: latency-svc-88gbs
Feb 13 23:23:35.891: INFO: Got endpoints: latency-svc-b7k8t [753.237325ms]
Feb 13 23:23:35.940: INFO: Created: latency-svc-qkx8q
Feb 13 23:23:35.940: INFO: Got endpoints: latency-svc-h9rdg [747.134ms]
Feb 13 23:23:35.984: INFO: Created: latency-svc-9sqb9
Feb 13 23:23:35.996: INFO: Got endpoints: latency-svc-c4g8d [737.104296ms]
Feb 13 23:23:36.063: INFO: Created: latency-svc-zxdqp
Feb 13 23:23:36.063: INFO: Got endpoints: latency-svc-hbx97 [770.156964ms]
Feb 13 23:23:36.098: INFO: Created: latency-svc-g8m5x
Feb 13 23:23:36.098: INFO: Got endpoints: latency-svc-4tvt4 [759.668134ms]
Feb 13 23:23:36.143: INFO: Got endpoints: latency-svc-4l6v6 [748.540223ms]
Feb 13 23:23:36.143: INFO: Created: latency-svc-c2dpl
Feb 13 23:23:36.190: INFO: Got endpoints: latency-svc-gspn9 [728.371846ms]
Feb 13 23:23:36.205: INFO: Created: latency-svc-ntrj6
Feb 13 23:23:36.229: INFO: Created: latency-svc-bh969
Feb 13 23:23:36.240: INFO: Got endpoints: latency-svc-bchgr [705.035127ms]
Feb 13 23:23:36.277: INFO: Created: latency-svc-ccrrz
Feb 13 23:23:36.313: INFO: Got endpoints: latency-svc-484x9 [775.546814ms]
Feb 13 23:23:36.349: INFO: Created: latency-svc-lp2ck
Feb 13 23:23:36.349: INFO: Got endpoints: latency-svc-d9dvn [751.098783ms]
Feb 13 23:23:36.383: INFO: Created: latency-svc-fxchh
Feb 13 23:23:36.394: INFO: Got endpoints: latency-svc-gzrdh [727.117382ms]
Feb 13 23:23:36.439: INFO: Got endpoints: latency-svc-8lfbw [750.893269ms]
Feb 13 23:23:36.464: INFO: Created: latency-svc-n8v5f
Feb 13 23:23:36.485: INFO: Created: latency-svc-zhwg4
Feb 13 23:23:36.498: INFO: Got endpoints: latency-svc-k86q7 [759.834704ms]
Feb 13 23:23:36.533: INFO: Created: latency-svc-tmh5c
Feb 13 23:23:36.578: INFO: Got endpoints: latency-svc-hvq4z [772.802536ms]
Feb 13 23:23:36.588: INFO: Got endpoints: latency-svc-88gbs [748.774777ms]
Feb 13 23:23:36.614: INFO: Created: latency-svc-kz9j6
Feb 13 23:23:36.643: INFO: Got endpoints: latency-svc-qkx8q [752.103559ms]
Feb 13 23:23:36.653: INFO: Created: latency-svc-hnxnj
Feb 13 23:23:36.678: INFO: Created: latency-svc-5rqlv
Feb 13 23:23:36.713: INFO: Got endpoints: latency-svc-9sqb9 [772.832341ms]
Feb 13 23:23:36.747: INFO: Created: latency-svc-zwc74
Feb 13 23:23:36.747: INFO: Got endpoints: latency-svc-zxdqp [751.624048ms]
Feb 13 23:23:36.787: INFO: Created: latency-svc-snhqn
Feb 13 23:23:36.797: INFO: Got endpoints: latency-svc-g8m5x [733.709647ms]
Feb 13 23:23:36.842: INFO: Got endpoints: latency-svc-c2dpl [743.707362ms]
Feb 13 23:23:36.854: INFO: Created: latency-svc-mrqs8
Feb 13 23:23:36.880: INFO: Created: latency-svc-n4mqb
Feb 13 23:23:36.891: INFO: Got endpoints: latency-svc-ntrj6 [748.514353ms]
Feb 13 23:23:36.925: INFO: Created: latency-svc-npd75
Feb 13 23:23:36.938: INFO: Got endpoints: latency-svc-bh969 [747.922173ms]
Feb 13 23:23:36.989: INFO: Created: latency-svc-hhf8n
Feb 13 23:23:36.993: INFO: Got endpoints: latency-svc-ccrrz [752.966217ms]
Feb 13 23:23:37.028: INFO: Created: latency-svc-8fx95
Feb 13 23:23:37.038: INFO: Got endpoints: latency-svc-lp2ck [725.082537ms]
Feb 13 23:23:37.087: INFO: Created: latency-svc-5llb9
Feb 13 23:23:37.088: INFO: Got endpoints: latency-svc-fxchh [738.931722ms]
Feb 13 23:23:37.132: INFO: Created: latency-svc-5gmm6
Feb 13 23:23:37.138: INFO: Got endpoints: latency-svc-n8v5f [744.17824ms]
Feb 13 23:23:37.209: INFO: Got endpoints: latency-svc-zhwg4 [769.736023ms]
Feb 13 23:23:37.238: INFO: Got endpoints: latency-svc-tmh5c [739.408216ms]
Feb 13 23:23:37.288: INFO: Got endpoints: latency-svc-kz9j6 [709.43328ms]
Feb 13 23:23:37.339: INFO: Got endpoints: latency-svc-hnxnj [750.53181ms]
Feb 13 23:24:05.867: INFO: Got error: Post https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/e2e-tests-svc-latency-tjpvs/services: read tcp 10.254.0.190:59432->213.199.128.226:443: read: connection reset by peer
Feb 13 23:24:05.867: INFO: Got error: Post https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/e2e-tests-svc-latency-tjpvs/services: read tcp 10.254.0.190:59432->213.199.128.226:443: read: connection reset by peer
Feb 13 23:24:05.867: INFO: Got error: Post https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/e2e-tests-svc-latency-tjpvs/services: read tcp 10.254.0.190:59432->213.199.128.226:443: read: connection reset by peer
Feb 13 23:24:05.867: INFO: Got error: Post https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/e2e-tests-svc-latency-tjpvs/services: read tcp 10.254.0.190:59432->213.199.128.226:443: read: connection reset by peer
Feb 13 23:24:05.867: INFO: Got error: Post https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/e2e-tests-svc-latency-tjpvs/services: read tcp 10.254.0.190:59432->213.199.128.226:443: read: connection reset by peer
Feb 13 23:24:05.992: INFO: Got endpoints: latency-svc-npd75 [29.101208761s]
Feb 13 23:24:05.992: INFO: Got endpoints: latency-svc-snhqn [29.245029249s]
Feb 13 23:24:05.992: INFO: Got endpoints: latency-svc-5rqlv [29.349020256s]
Feb 13 23:24:05.992: INFO: Got endpoints: latency-svc-mrqs8 [29.195114607s]
Feb 13 23:24:05.992: INFO: Got endpoints: latency-svc-zwc74 [29.279780228s]
Feb 13 23:24:05.992: INFO: Got endpoints: latency-svc-n4mqb [29.150206023s]
Feb 13 23:24:05.993: INFO: Got endpoints: latency-svc-5llb9 [28.955058569s]
Feb 13 23:24:05.993: INFO: Got endpoints: latency-svc-8fx95 [29.000126862s]
Feb 13 23:24:05.993: INFO: Got endpoints: latency-svc-hhf8n [29.055907202s]
Feb 13 23:24:05.994: INFO: Got endpoints: latency-svc-5gmm6 [28.90596401s]
Feb 13 23:24:06.016: INFO: Created: latency-svc-zlvmr
Feb 13 23:24:06.029: INFO: Created: latency-svc-wsvrw
Feb 13 23:24:06.050: INFO: Got endpoints: latency-svc-zlvmr [183.219458ms]
Feb 13 23:24:06.050: INFO: Created: latency-svc-75h4p
Feb 13 23:24:06.061: INFO: Got endpoints: latency-svc-75h4p [193.922588ms]
Feb 13 23:24:06.075: INFO: Created: latency-svc-29zcg
Feb 13 23:24:06.075: INFO: Got endpoints: latency-svc-wsvrw [208.266154ms]
Feb 13 23:24:06.104: INFO: Got endpoints: latency-svc-29zcg [237.588459ms]
Feb 13 23:24:06.135: INFO: Created: latency-svc-cswrb
Feb 13 23:24:06.138: INFO: Got endpoints: latency-svc-cswrb [271.032493ms]
Feb 13 23:24:06.153: INFO: Created: latency-svc-hqk8h
Feb 13 23:24:06.167: INFO: Got endpoints: latency-svc-hqk8h [174.67601ms]
Feb 13 23:24:06.272: INFO: Created: latency-svc-gqqc9
Feb 13 23:24:06.287: INFO: Got endpoints: latency-svc-gqqc9 [294.684663ms]
Feb 13 23:24:06.287: INFO: Created: latency-svc-sg2hb
Feb 13 23:24:06.324: INFO: Got endpoints: latency-svc-sg2hb [331.446341ms]
Feb 13 23:24:06.337: INFO: Created: latency-svc-d546j
Feb 13 23:24:06.348: INFO: Got endpoints: latency-svc-d546j [354.848549ms]
Feb 13 23:24:06.357: INFO: Created: latency-svc-tq6hf
Feb 13 23:24:06.402: INFO: Got endpoints: latency-svc-tq6hf [408.997506ms]
Feb 13 23:24:06.404: INFO: Created: latency-svc-hqqmr
Feb 13 23:24:06.407: INFO: Got endpoints: latency-svc-hqqmr [413.435365ms]
Feb 13 23:24:06.416: INFO: Created: latency-svc-4pszd
Feb 13 23:24:06.427: INFO: Got endpoints: latency-svc-4pszd [434.681451ms]
Feb 13 23:24:06.437: INFO: Created: latency-svc-mpccl
Feb 13 23:24:06.448: INFO: Got endpoints: latency-svc-mpccl [454.395836ms]
Feb 13 23:24:06.458: INFO: Created: latency-svc-pv6jb
Feb 13 23:24:06.468: INFO: Got endpoints: latency-svc-pv6jb [473.943207ms]
Feb 13 23:24:06.478: INFO: Created: latency-svc-n86z7
Feb 13 23:24:06.488: INFO: Got endpoints: latency-svc-n86z7 [494.114736ms]
Feb 13 23:24:06.497: INFO: Created: latency-svc-rlzcd
Feb 13 23:24:06.529: INFO: Got endpoints: latency-svc-rlzcd [478.868927ms]
Feb 13 23:24:06.529: INFO: Created: latency-svc-jlhkz
Feb 13 23:24:06.538: INFO: Got endpoints: latency-svc-jlhkz [477.654747ms]
Feb 13 23:24:06.558: INFO: Created: latency-svc-8k4z6
Feb 13 23:24:06.569: INFO: Got endpoints: latency-svc-8k4z6 [493.791076ms]
Feb 13 23:24:06.578: INFO: Created: latency-svc-9x4lp
Feb 13 23:24:06.590: INFO: Got endpoints: latency-svc-9x4lp [485.44636ms]
Feb 13 23:24:06.600: INFO: Created: latency-svc-x6jdj
Feb 13 23:24:06.610: INFO: Got endpoints: latency-svc-x6jdj [472.931666ms]
Feb 13 23:24:06.620: INFO: Created: latency-svc-f9qgs
Feb 13 23:24:06.651: INFO: Got endpoints: latency-svc-f9qgs [483.298223ms]
Feb 13 23:24:06.665: INFO: Created: latency-svc-c2mql
Feb 13 23:24:06.665: INFO: Created: latency-svc-zrq9p
Feb 13 23:24:06.678: INFO: Got endpoints: latency-svc-zrq9p [353.470632ms]
Feb 13 23:24:06.678: INFO: Got endpoints: latency-svc-c2mql [390.315388ms]
Feb 13 23:24:06.687: INFO: Created: latency-svc-dkfn9
Feb 13 23:24:06.704: INFO: Got endpoints: latency-svc-dkfn9 [356.106627ms]
Feb 13 23:24:06.720: INFO: Created: latency-svc-pl6l8
Feb 13 23:24:06.731: INFO: Got endpoints: latency-svc-pl6l8 [328.672535ms]
Feb 13 23:24:06.741: INFO: Created: latency-svc-ss4ns
Feb 13 23:24:06.791: INFO: Got endpoints: latency-svc-ss4ns [383.694095ms]
Feb 13 23:24:06.792: INFO: Created: latency-svc-wxgmh
Feb 13 23:24:06.815: INFO: Got endpoints: latency-svc-wxgmh [388.233714ms]
Feb 13 23:24:06.843: INFO: Created: latency-svc-75r25
Feb 13 23:24:06.855: INFO: Got endpoints: latency-svc-75r25 [407.005503ms]
Feb 13 23:24:06.863: INFO: Created: latency-svc-hpdnt
Feb 13 23:24:06.874: INFO: Got endpoints: latency-svc-hpdnt [406.81504ms]
Feb 13 23:24:06.884: INFO: Created: latency-svc-ktk4w
Feb 13 23:24:06.916: INFO: Got endpoints: latency-svc-ktk4w [428.111827ms]
Feb 13 23:24:06.917: INFO: Created: latency-svc-qd7l4
Feb 13 23:24:06.926: INFO: Got endpoints: latency-svc-qd7l4 [397.212402ms]
Feb 13 23:24:06.947: INFO: Created: latency-svc-j8qdw
Feb 13 23:24:06.958: INFO: Got endpoints: latency-svc-j8qdw [419.313531ms]
Feb 13 23:24:06.968: INFO: Created: latency-svc-cz2td
Feb 13 23:24:06.978: INFO: Got endpoints: latency-svc-cz2td [409.618724ms]
Feb 13 23:24:06.989: INFO: Created: latency-svc-mpnzn
Feb 13 23:24:07.006: INFO: Got endpoints: latency-svc-mpnzn [415.911854ms]
Feb 13 23:24:07.045: INFO: Created: latency-svc-bx9nf
Feb 13 23:24:07.047: INFO: Got endpoints: latency-svc-bx9nf [436.12624ms]
Feb 13 23:24:07.092: INFO: Created: latency-svc-cgm77
Feb 13 23:24:07.095: INFO: Got endpoints: latency-svc-cgm77 [444.781512ms]
Feb 13 23:24:07.106: INFO: Created: latency-svc-7hxk2
Feb 13 23:24:07.123: INFO: Got endpoints: latency-svc-7hxk2 [444.794344ms]
Feb 13 23:24:07.140: INFO: Created: latency-svc-rjz45
Feb 13 23:24:07.170: INFO: Got endpoints: latency-svc-rjz45 [492.0007ms]
Feb 13 23:24:07.171: INFO: Created: latency-svc-p7685
Feb 13 23:24:07.174: INFO: Got endpoints: latency-svc-p7685 [470.297787ms]
Feb 13 23:24:07.183: INFO: Created: latency-svc-r6bzf
Feb 13 23:24:07.201: INFO: Got endpoints: latency-svc-r6bzf [469.538139ms]
Feb 13 23:24:07.219: INFO: Created: latency-svc-j948l
Feb 13 23:24:07.230: INFO: Got endpoints: latency-svc-j948l [438.834312ms]
Feb 13 23:24:07.239: INFO: Created: latency-svc-28tzw
Feb 13 23:24:07.255: INFO: Got endpoints: latency-svc-28tzw [439.226614ms]
Feb 13 23:24:07.266: INFO: Created: latency-svc-v7rp8
Feb 13 23:24:07.295: INFO: Got endpoints: latency-svc-v7rp8 [440.439363ms]
Feb 13 23:24:07.297: INFO: Created: latency-svc-85hvf
Feb 13 23:24:07.300: INFO: Got endpoints: latency-svc-85hvf [425.571558ms]
Feb 13 23:24:07.312: INFO: Created: latency-svc-xg5kt
Feb 13 23:24:07.329: INFO: Got endpoints: latency-svc-xg5kt [413.547646ms]
Feb 13 23:24:07.339: INFO: Created: latency-svc-4wh5j
Feb 13 23:24:07.351: INFO: Got endpoints: latency-svc-4wh5j [424.946447ms]
Feb 13 23:24:07.361: INFO: Created: latency-svc-trgxc
Feb 13 23:24:07.379: INFO: Got endpoints: latency-svc-trgxc [420.841355ms]
Feb 13 23:24:07.430: INFO: Created: latency-svc-fb64w
Feb 13 23:24:07.433: INFO: Got endpoints: latency-svc-fb64w [454.24727ms]
Feb 13 23:24:07.445: INFO: Created: latency-svc-jn54x
Feb 13 23:24:07.457: INFO: Got endpoints: latency-svc-jn54x [450.689681ms]
Feb 13 23:24:07.467: INFO: Created: latency-svc-wd8gz
Feb 13 23:24:07.478: INFO: Got endpoints: latency-svc-wd8gz [431.345116ms]
Feb 13 23:24:07.488: INFO: Created: latency-svc-lsnwg
Feb 13 23:24:07.499: INFO: Got endpoints: latency-svc-lsnwg [403.792683ms]
Feb 13 23:24:07.509: INFO: Created: latency-svc-mz87l
Feb 13 23:24:07.521: INFO: Got endpoints: latency-svc-mz87l [397.960722ms]
Feb 13 23:24:07.521: INFO: Latencies: [71.727024ms 92.722588ms 114.467736ms 141.875494ms 172.530438ms 174.67601ms 183.219458ms 193.922588ms 197.33308ms 208.266154ms 237.31055ms 237.588459ms 259.204088ms 271.032493ms 288.722367ms 294.684663ms 299.219852ms 323.001867ms 328.672535ms 331.446341ms 343.139283ms 353.470632ms 354.848549ms 356.106627ms 364.309964ms 367.704371ms 369.028477ms 381.950597ms 383.694095ms 384.969632ms 386.589545ms 388.233714ms 390.315388ms 393.159749ms 393.658303ms 393.8181ms 394.302742ms 394.437973ms 396.980128ms 397.212402ms 397.960722ms 398.692077ms 401.652877ms 402.001656ms 402.450164ms 403.180004ms 403.290171ms 403.792683ms 403.814845ms 404.038797ms 404.703402ms 405.112703ms 406.014329ms 406.357819ms 406.81504ms 406.870477ms 406.880634ms 407.005503ms 407.697174ms 408.997506ms 409.618724ms 413.435365ms 413.502066ms 413.547646ms 414.963179ms 415.911854ms 417.191595ms 418.556209ms 419.313531ms 420.841355ms 421.840354ms 424.245408ms 424.764295ms 424.946447ms 425.571558ms 425.951452ms 425.997705ms 428.111827ms 428.218457ms 431.345116ms 431.746656ms 432.379795ms 434.681451ms 436.12624ms 438.834312ms 439.226614ms 440.439363ms 441.861718ms 442.107511ms 443.380523ms 444.781512ms 444.794344ms 444.962852ms 450.689681ms 453.858012ms 454.24727ms 454.395836ms 457.956419ms 467.708519ms 469.538139ms 470.297787ms 470.634954ms 471.929967ms 472.931666ms 473.943207ms 477.654747ms 478.706459ms 478.868927ms 480.749016ms 483.298223ms 485.44636ms 492.0007ms 493.791076ms 494.114736ms 496.05053ms 497.348318ms 513.785498ms 513.813807ms 514.052976ms 514.26023ms 523.385636ms 530.141952ms 531.5085ms 596.302012ms 609.239703ms 618.792825ms 621.15533ms 625.985348ms 628.740533ms 645.972914ms 660.675118ms 670.002531ms 671.852474ms 683.98792ms 685.198504ms 686.302578ms 688.492646ms 698.611042ms 705.035127ms 709.43328ms 714.264681ms 715.737611ms 725.082537ms 727.117382ms 728.371846ms 730.667147ms 731.923659ms 733.709647ms 737.104296ms 738.931722ms 739.408216ms 742.825291ms 743.651852ms 743.707362ms 744.17824ms 745.141191ms 746.81338ms 747.134ms 747.800028ms 747.922173ms 748.514353ms 748.540223ms 748.774777ms 749.289616ms 749.999191ms 750.53181ms 750.893269ms 751.098783ms 751.624048ms 752.103559ms 752.966217ms 753.237325ms 759.668134ms 759.834704ms 766.043118ms 767.917579ms 769.736023ms 770.156964ms 772.802536ms 772.832341ms 775.546814ms 777.788498ms 782.573005ms 791.808216ms 816.104261ms 28.90596401s 28.955058569s 29.000126862s 29.055907202s 29.101208761s 29.150206023s 29.195114607s 29.245029249s 29.279780228s 29.349020256s]
Feb 13 23:24:07.521: INFO: 50 %ile: 457.956419ms
Feb 13 23:24:07.521: INFO: 90 %ile: 767.917579ms
Feb 13 23:24:07.521: INFO: 99 %ile: 29.279780228s
Feb 13 23:24:07.521: INFO: Total sample count: 195
Feb 13 23:24:07.521: INFO: Not all RC/pod/service trials succeeded: got 5 errors
50, 90, 99 percentiles: 457.956419ms 767.917579ms 29.279780228s
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-svc-latency-tjpvs".
STEP: Found 5 events.
Feb 13 23:24:07.543: INFO: At 2019-02-13 23:23:28 +0000 UTC - event for svc-latency-rc: {replication-controller } SuccessfulCreate: Created pod: svc-latency-rc-4jtfw
Feb 13 23:24:07.543: INFO: At 2019-02-13 23:23:28 +0000 UTC - event for svc-latency-rc-4jtfw: {default-scheduler } Scheduled: Successfully assigned e2e-tests-svc-latency-tjpvs/svc-latency-rc-4jtfw to shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 13 23:24:07.543: INFO: At 2019-02-13 23:23:30 +0000 UTC - event for svc-latency-rc-4jtfw: {kubelet shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx} Pulled: Container image "k8s.gcr.io/pause:3.1" already present on machine
Feb 13 23:24:07.543: INFO: At 2019-02-13 23:23:30 +0000 UTC - event for svc-latency-rc-4jtfw: {kubelet shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx} Created: Created container
Feb 13 23:24:07.543: INFO: At 2019-02-13 23:23:30 +0000 UTC - event for svc-latency-rc-4jtfw: {kubelet shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx} Started: Started container
Feb 13 23:24:07.607: INFO: POD                                                             NODE                                                 PHASE    GRACE  CONDITIONS
Feb 13 23:24:07.607: INFO: svc-latency-rc-4jtfw                                            shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:23:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:23:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:23:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:23:28 +0000 UTC  }]
Feb 13 23:24:07.607: INFO: addons-kube-lego-69bbdc96b6-6m8wg                               shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 23:24:07.607: INFO: addons-kubernetes-dashboard-6579b646c5-g48vq                    shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: addons-nginx-ingress-controller-d74bfff57-87jx9                 shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-kj5ch  shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: blackbox-exporter-d6c46f9fc-5xt9g                               shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:05:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: calico-node-psbns                                               shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:42 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: calico-node-skgqz                                               shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:50 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: coredns-67df79bbdd-96tpz                                        shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: kube-proxy-6688x                                                shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:42 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: kube-proxy-n2tgw                                                shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:50 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: metrics-server-65cc55bd79-8kjg8                                 shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: node-exporter-clvlv                                             shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:50 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: node-exporter-tmhn5                                             shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:03:42 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: vpn-shoot-c76596df-6nsrb                                        shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:04:02 +0000 UTC  }]
Feb 13 23:24:07.608: INFO: 
Feb 13 23:24:07.630: INFO: 
Logging node info for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 13 23:24:07.651: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,UID:3e65adb9-2fdb-11e9-9f3f-da917295d151,ResourceVersion:12954,Generation:0,CreationTimestamp:2019-02-13 22:03:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: Standard_DS2_v2,beta.kubernetes.io/os: linux,failure-domain.beta.kubernetes.io/region: westeurope,failure-domain.beta.kubernetes.io/zone: 1,kubernetes.io/hostname: shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,kubernetes.io/role: node,node-role.kubernetes.io/node: ,worker.garden.sapcloud.io/group: cpu-worker,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 10.250.0.5/19,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.96.1.0/24,DoNotUse_ExternalID:,ProviderID:azure:///subscriptions/00d2caa5-cd29-46f7-845a-2f8ee0360ef5/resourceGroups/shoot--it--pub-az-7eoqa/providers/Microsoft.Compute/virtualMachines/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-azure-disk: {{8 0} {<nil>} 8 DecimalSI},cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33931702272 0} {<nil>} 33136428Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{7286583296 0} {<nil>} 7115804Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-azure-disk: {{8 0} {<nil>} 8 DecimalSI},cpu: {{1920 -3} {<nil>} 1920m DecimalSI},ephemeral-storage: {{32235117134 0} {<nil>} 32235117134 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{5848512302 0} {<nil>} 5848512302 DecimalSI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-02-13 22:04:14 +0000 UTC 2019-02-13 22:04:14 +0000 UTC RouteCreated RouteController created a route} {MemoryPressure False 2019-02-13 23:24:04 +0000 UTC 2019-02-13 22:03:50 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-02-13 23:24:04 +0000 UTC 2019-02-13 22:03:50 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-02-13 23:24:04 +0000 UTC 2019-02-13 22:03:50 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-02-13 23:24:04 +0000 UTC 2019-02-13 22:04:10 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.250.0.5} {Hostname shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:c6a974bd9e9a471f9985af77c270be63,SystemUUID:485BC509-BC48-AC4D-A484-A78B36A6CFF6,BootID:e730366f-e2e0-4aff-bae0-eb62accbb532,KernelVersion:4.14.96-coreos,OSImage:Container Linux by CoreOS 1967.5.0 (Rhyolite),ContainerRuntimeVersion:docker://18.6.1,KubeletVersion:v1.13.3,KubeProxyVersion:v1.13.3,OperatingSystem:linux,Architecture:amd64,},Images:[{[k8s.gcr.io/hyperkube@sha256:a36f9497dacfac277ca706ef245a04682ec9f5a7afa0ad27036d08e7c108d57b k8s.gcr.io/hyperkube:v1.12.3] 635332234} {[k8s.gcr.io/hyperkube@sha256:388312116707c0420c492accc15ad3ba400911c2c337c8b33049f8d92f9c7bb9 k8s.gcr.io/hyperkube:v1.13.3] 564951434} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[nginx@sha256:dd2d0ac3fff2f007d99e033b64854be0941e19a2ad51f174d9240dda20d9f534 nginx:latest] 109202600} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[quay.io/calico/node@sha256:264d7de573a956f8914de93fbade66bdc7fd86f22a8f4e9a99ebdbb9e45b66f7 quay.io/calico/node:v3.4.0] 75919845} {[quay.io/calico/cni@sha256:526f0c585f66fc70d79062463d78a324f053c406b69a600fc282c2aaa74c4428 quay.io/calico/cni:v3.4.0] 75434816} {[ruby@sha256:83eebc6b30281ff43dc919cdd361803fb296f74f3d4f1cc42036860a49de9caf ruby:2.5.3-alpine] 50827145} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[quay.io/prometheus/node-exporter@sha256:1b129a3801a0440f9c5b2afb20082dfdb31bf6092b561f5f249531130000cb83 quay.io/prometheus/node-exporter:v0.17.0] 20982005} {[nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 nginx:1.14-alpine] 16032888} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 busybox:1.29] 1154361} {[gcr.io/google_containers/pause-amd64@sha256:59eec8837a4d942cc19a52b8c09ea75121acc38114a2c68b98983ce9356b8610 k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea gcr.io/google_containers/pause-amd64:3.1 k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Feb 13 23:24:07.654: INFO: 
Logging kubelet events for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 13 23:24:07.675: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 13 23:24:07.704: INFO: node-exporter-clvlv started at 2019-02-13 22:03:50 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.704: INFO: 	Container node-exporter ready: true, restart count 0
Feb 13 23:24:07.704: INFO: kube-proxy-n2tgw started at 2019-02-13 22:03:50 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.704: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 13 23:24:07.704: INFO: svc-latency-rc-4jtfw started at 2019-02-13 23:23:28 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.704: INFO: 	Container svc-latency-rc ready: true, restart count 0
Feb 13 23:24:07.704: INFO: calico-node-skgqz started at 2019-02-13 22:03:50 +0000 UTC (1+1 container statuses recorded)
Feb 13 23:24:07.704: INFO: 	Init container install-cni ready: true, restart count 0
Feb 13 23:24:07.704: INFO: 	Container calico-node ready: true, restart count 0
W0213 23:24:07.726502   31687 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 23:24:07.856: INFO: 
Latency metrics for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 13 23:24:07.856: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.26888s}
Feb 13 23:24:07.856: INFO: 
Logging node info for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 13 23:24:07.878: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,UID:396631bc-2fdb-11e9-9f3f-da917295d151,ResourceVersion:12950,Generation:0,CreationTimestamp:2019-02-13 22:03:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: Standard_DS2_v2,beta.kubernetes.io/os: linux,failure-domain.beta.kubernetes.io/region: westeurope,failure-domain.beta.kubernetes.io/zone: 0,kubernetes.io/hostname: shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,kubernetes.io/role: node,node-role.kubernetes.io/node: ,worker.garden.sapcloud.io/group: cpu-worker,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 10.250.0.4/19,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.96.0.0/24,DoNotUse_ExternalID:,ProviderID:azure:///subscriptions/00d2caa5-cd29-46f7-845a-2f8ee0360ef5/resourceGroups/shoot--it--pub-az-7eoqa/providers/Microsoft.Compute/virtualMachines/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-azure-disk: {{8 0} {<nil>} 8 DecimalSI},cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33931702272 0} {<nil>} 33136428Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{7286583296 0} {<nil>} 7115804Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{attachable-volumes-azure-disk: {{8 0} {<nil>} 8 DecimalSI},cpu: {{1920 -3} {<nil>} 1920m DecimalSI},ephemeral-storage: {{32235117134 0} {<nil>} 32235117134 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{5848512302 0} {<nil>} 5848512302 DecimalSI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2019-02-13 22:04:31 +0000 UTC 2019-02-13 22:04:31 +0000 UTC RouteCreated RouteController created a route} {MemoryPressure False 2019-02-13 23:24:02 +0000 UTC 2019-02-13 22:03:41 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-02-13 23:24:02 +0000 UTC 2019-02-13 22:03:41 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-02-13 23:24:02 +0000 UTC 2019-02-13 22:03:41 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-02-13 23:24:02 +0000 UTC 2019-02-13 22:04:02 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 10.250.0.4} {Hostname shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:6c8c7e4d4c77480683bae4659ecfe2c8,SystemUUID:ACBA3163-164E-654B-A3CA-359ACDA743E9,BootID:6655f8f9-c370-4c96-b50d-24dd05e8cb6f,KernelVersion:4.14.96-coreos,OSImage:Container Linux by CoreOS 1967.5.0 (Rhyolite),ContainerRuntimeVersion:docker://18.6.1,KubeletVersion:v1.13.3,KubeProxyVersion:v1.13.3,OperatingSystem:linux,Architecture:amd64,},Images:[{[k8s.gcr.io/hyperkube@sha256:a36f9497dacfac277ca706ef245a04682ec9f5a7afa0ad27036d08e7c108d57b k8s.gcr.io/hyperkube:v1.12.3] 635332234} {[quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:617076c3e3d4d0638a4927702530d8456bda64c67194f6daed272a59e93b992f quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0] 568075227} {[k8s.gcr.io/hyperkube@sha256:388312116707c0420c492accc15ad3ba400911c2c337c8b33049f8d92f9c7bb9 k8s.gcr.io/hyperkube:v1.13.3] 564951434} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1] 121711221} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[quay.io/calico/node@sha256:264d7de573a956f8914de93fbade66bdc7fd86f22a8f4e9a99ebdbb9e45b66f7 quay.io/calico/node:v3.4.0] 75919845} {[quay.io/calico/cni@sha256:526f0c585f66fc70d79062463d78a324f053c406b69a600fc282c2aaa74c4428 quay.io/calico/cni:v3.4.0] 75434816} {[eu.gcr.io/gardener-project/gardener/ingress-default-backend@sha256:17b68928ead12cc9df88ee60d9c638d3fd642a7e122c2bb7586da1a21eb2de45 eu.gcr.io/gardener-project/gardener/ingress-default-backend:0.7.0] 69546830} {[ruby@sha256:83eebc6b30281ff43dc919cdd361803fb296f74f3d4f1cc42036860a49de9caf ruby:2.5.3-alpine] 50827145} {[jetstack/kube-lego@sha256:7824e0c58ee4403364227453dda6c2b0bc88ea6159e521973e5d8b2b7a64851b jetstack/kube-lego:0.1.7] 48639481} {[k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b k8s.gcr.io/metrics-server-amd64:v0.3.1] 40767713} {[coredns/coredns@sha256:e030773c7fee285435ed7fc7623532ee54c4c1c4911fb24d95cd0170a8a768bc coredns/coredns:1.3.0] 40415528} {[quay.io/prometheus/node-exporter@sha256:1b129a3801a0440f9c5b2afb20082dfdb31bf6092b561f5f249531130000cb83 quay.io/prometheus/node-exporter:v0.17.0] 20982005} {[quay.io/prometheus/blackbox-exporter@sha256:0958c966dca13db9880a55e03221f507a2cf72ccf401b34642bdb0ba739d056f quay.io/prometheus/blackbox-exporter:v0.13.0] 17490231} {[nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 nginx:1.14-alpine] 16032888} {[eu.gcr.io/gardener-project/gardener/vpn-shoot@sha256:411918b22b7e3dbfee1838bed73b621777cb28896410e06b53ed4cdaa4aa1c1d eu.gcr.io/gardener-project/gardener/vpn-shoot:0.13.0] 13638102} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/google_containers/pause-amd64@sha256:59eec8837a4d942cc19a52b8c09ea75121acc38114a2c68b98983ce9356b8610 gcr.io/google_containers/pause-amd64:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Feb 13 23:24:07.878: INFO: 
Logging kubelet events for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 13 23:24:07.899: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 13 23:24:07.936: INFO: metrics-server-65cc55bd79-8kjg8 started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Container metrics-server ready: true, restart count 0
Feb 13 23:24:07.936: INFO: addons-kubernetes-dashboard-6579b646c5-g48vq started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 13 23:24:07.936: INFO: kube-proxy-6688x started at 2019-02-13 22:03:43 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 13 23:24:07.936: INFO: calico-node-psbns started at 2019-02-13 22:03:43 +0000 UTC (1+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Init container install-cni ready: true, restart count 0
Feb 13 23:24:07.936: INFO: 	Container calico-node ready: true, restart count 0
Feb 13 23:24:07.936: INFO: addons-nginx-ingress-controller-d74bfff57-87jx9 started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 13 23:24:07.936: INFO: blackbox-exporter-d6c46f9fc-5xt9g started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 13 23:24:07.936: INFO: addons-kube-lego-69bbdc96b6-6m8wg started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Container kube-lego ready: true, restart count 0
Feb 13 23:24:07.936: INFO: vpn-shoot-c76596df-6nsrb started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 13 23:24:07.936: INFO: coredns-67df79bbdd-96tpz started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Container coredns ready: true, restart count 0
Feb 13 23:24:07.936: INFO: node-exporter-tmhn5 started at 2019-02-13 22:03:43 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Container node-exporter ready: true, restart count 0
Feb 13 23:24:07.936: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-kj5ch started at 2019-02-13 22:04:02 +0000 UTC (0+1 container statuses recorded)
Feb 13 23:24:07.936: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
W0213 23:24:07.958232   31687 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 23:24:08.107: INFO: 
Latency metrics for node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 13 23:24:08.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-tjpvs" for this suite.
Feb 13 23:24:26.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:24:26.652: INFO: namespace: e2e-tests-svc-latency-tjpvs, resource: bindings, ignored listing per whitelist
Feb 13 23:24:27.069: INFO: namespace e2e-tests-svc-latency-tjpvs deletion completed in 18.939745944s

• Failure [59.165 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 13 23:24:07.521: Not all RC/pod/service trials succeeded: got 5 errors
  50, 90, 99 percentiles: 457.956419ms 767.917579ms 29.279780228s

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service_latency.go:121
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:24:27.069: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-d6l4c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-d6l4c
I0213 23:24:28.026887   31687 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-d6l4c, replica count: 1
I0213 23:24:29.077416   31687 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:24:30.077795   31687 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:24:31.078030   31687 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 13 23:24:31.214: INFO: Created: latency-svc-7dq2t
Feb 13 23:24:31.225: INFO: Got endpoints: latency-svc-7dq2t [46.795144ms]
Feb 13 23:24:31.260: INFO: Created: latency-svc-959qb
Feb 13 23:24:31.271: INFO: Got endpoints: latency-svc-959qb [45.786743ms]
Feb 13 23:24:31.281: INFO: Created: latency-svc-mf6k7
Feb 13 23:24:31.315: INFO: Got endpoints: latency-svc-mf6k7 [89.993896ms]
Feb 13 23:24:31.317: INFO: Created: latency-svc-zh2tt
Feb 13 23:24:31.330: INFO: Got endpoints: latency-svc-zh2tt [104.93261ms]
Feb 13 23:24:31.354: INFO: Created: latency-svc-drqtv
Feb 13 23:24:31.366: INFO: Got endpoints: latency-svc-drqtv [140.811697ms]
Feb 13 23:24:31.377: INFO: Created: latency-svc-b4tr7
Feb 13 23:24:31.387: INFO: Got endpoints: latency-svc-b4tr7 [162.457322ms]
Feb 13 23:24:31.398: INFO: Created: latency-svc-gh949
Feb 13 23:24:31.409: INFO: Got endpoints: latency-svc-gh949 [183.958833ms]
Feb 13 23:24:31.454: INFO: Created: latency-svc-ssh5z
Feb 13 23:24:31.457: INFO: Got endpoints: latency-svc-ssh5z [231.813996ms]
Feb 13 23:24:31.472: INFO: Created: latency-svc-5zsfj
Feb 13 23:24:31.483: INFO: Got endpoints: latency-svc-5zsfj [258.48226ms]
Feb 13 23:24:31.494: INFO: Created: latency-svc-gvxlk
Feb 13 23:24:31.504: INFO: Got endpoints: latency-svc-gvxlk [278.8706ms]
Feb 13 23:24:31.513: INFO: Created: latency-svc-9dd72
Feb 13 23:24:31.524: INFO: Got endpoints: latency-svc-9dd72 [298.996852ms]
Feb 13 23:24:31.534: INFO: Created: latency-svc-srrmk
Feb 13 23:24:31.545: INFO: Got endpoints: latency-svc-srrmk [319.788689ms]
Feb 13 23:24:31.554: INFO: Created: latency-svc-gw67d
Feb 13 23:24:31.591: INFO: Got endpoints: latency-svc-gw67d [365.658421ms]
Feb 13 23:24:31.592: INFO: Created: latency-svc-q59gj
Feb 13 23:24:31.595: INFO: Got endpoints: latency-svc-q59gj [369.884734ms]
Feb 13 23:24:31.608: INFO: Created: latency-svc-mtsbj
Feb 13 23:24:31.620: INFO: Created: latency-svc-ljqts
Feb 13 23:24:31.620: INFO: Got endpoints: latency-svc-mtsbj [394.865795ms]
Feb 13 23:24:31.631: INFO: Got endpoints: latency-svc-ljqts [405.529535ms]
Feb 13 23:24:31.640: INFO: Created: latency-svc-vqpt9
Feb 13 23:24:31.651: INFO: Got endpoints: latency-svc-vqpt9 [379.943057ms]
Feb 13 23:24:31.661: INFO: Created: latency-svc-hj92k
Feb 13 23:24:31.672: INFO: Got endpoints: latency-svc-hj92k [357.142203ms]
Feb 13 23:24:31.682: INFO: Created: latency-svc-l8x69
Feb 13 23:24:31.714: INFO: Got endpoints: latency-svc-l8x69 [383.907337ms]
Feb 13 23:24:31.715: INFO: Created: latency-svc-7gq4n
Feb 13 23:24:31.725: INFO: Got endpoints: latency-svc-7gq4n [359.196847ms]
Feb 13 23:24:31.745: INFO: Created: latency-svc-ls2tm
Feb 13 23:24:31.756: INFO: Got endpoints: latency-svc-ls2tm [368.601008ms]
Feb 13 23:24:31.764: INFO: Created: latency-svc-ghnm7
Feb 13 23:24:31.781: INFO: Got endpoints: latency-svc-ghnm7 [371.871596ms]
Feb 13 23:24:31.790: INFO: Created: latency-svc-8t4dc
Feb 13 23:24:31.802: INFO: Got endpoints: latency-svc-8t4dc [344.564691ms]
Feb 13 23:24:31.812: INFO: Created: latency-svc-ztlx9
Feb 13 23:24:31.843: INFO: Got endpoints: latency-svc-ztlx9 [359.396681ms]
Feb 13 23:24:31.844: INFO: Created: latency-svc-2dlpv
Feb 13 23:24:31.847: INFO: Got endpoints: latency-svc-2dlpv [342.744808ms]
Feb 13 23:24:31.857: INFO: Created: latency-svc-mt44w
Feb 13 23:24:31.872: INFO: Got endpoints: latency-svc-mt44w [347.607657ms]
Feb 13 23:24:31.881: INFO: Created: latency-svc-hgck8
Feb 13 23:24:31.898: INFO: Got endpoints: latency-svc-hgck8 [353.00105ms]
Feb 13 23:24:31.915: INFO: Created: latency-svc-hvkvw
Feb 13 23:24:31.926: INFO: Got endpoints: latency-svc-hvkvw [335.711226ms]
Feb 13 23:24:31.977: INFO: Created: latency-svc-4h2f2
Feb 13 23:24:31.979: INFO: Got endpoints: latency-svc-4h2f2 [384.430531ms]
Feb 13 23:24:31.992: INFO: Created: latency-svc-dl248
Feb 13 23:24:32.003: INFO: Got endpoints: latency-svc-dl248 [383.171963ms]
Feb 13 23:24:32.012: INFO: Created: latency-svc-shssf
Feb 13 23:24:32.023: INFO: Got endpoints: latency-svc-shssf [392.055841ms]
Feb 13 23:24:32.032: INFO: Created: latency-svc-22pbz
Feb 13 23:24:32.043: INFO: Got endpoints: latency-svc-22pbz [391.861098ms]
Feb 13 23:24:32.053: INFO: Created: latency-svc-6cdsx
Feb 13 23:24:32.063: INFO: Got endpoints: latency-svc-6cdsx [391.345996ms]
Feb 13 23:24:32.073: INFO: Created: latency-svc-hf6xq
Feb 13 23:24:32.104: INFO: Got endpoints: latency-svc-hf6xq [389.950121ms]
Feb 13 23:24:32.126: INFO: Created: latency-svc-mgf74
Feb 13 23:24:32.137: INFO: Got endpoints: latency-svc-mgf74 [412.559959ms]
Feb 13 23:24:32.154: INFO: Created: latency-svc-brnn8
Feb 13 23:24:32.166: INFO: Got endpoints: latency-svc-brnn8 [409.415641ms]
Feb 13 23:24:32.174: INFO: Created: latency-svc-ddrxz
Feb 13 23:24:32.186: INFO: Got endpoints: latency-svc-ddrxz [404.607073ms]
Feb 13 23:24:32.197: INFO: Created: latency-svc-wktkl
Feb 13 23:24:32.226: INFO: Got endpoints: latency-svc-wktkl [424.285084ms]
Feb 13 23:24:32.227: INFO: Created: latency-svc-5dthk
Feb 13 23:24:32.231: INFO: Got endpoints: latency-svc-5dthk [387.757088ms]
Feb 13 23:24:32.240: INFO: Created: latency-svc-txkpz
Feb 13 23:24:32.252: INFO: Got endpoints: latency-svc-txkpz [404.859576ms]
Feb 13 23:24:32.262: INFO: Created: latency-svc-vrt72
Feb 13 23:24:32.279: INFO: Got endpoints: latency-svc-vrt72 [407.479617ms]
Feb 13 23:24:32.296: INFO: Created: latency-svc-dbfrp
Feb 13 23:24:32.307: INFO: Got endpoints: latency-svc-dbfrp [409.278176ms]
Feb 13 23:24:32.316: INFO: Created: latency-svc-pbcjz
Feb 13 23:24:32.348: INFO: Got endpoints: latency-svc-pbcjz [421.361629ms]
Feb 13 23:24:32.349: INFO: Created: latency-svc-z456p
Feb 13 23:24:32.352: INFO: Got endpoints: latency-svc-z456p [372.994287ms]
Feb 13 23:24:32.364: INFO: Created: latency-svc-khg4t
Feb 13 23:24:32.375: INFO: Got endpoints: latency-svc-khg4t [372.183466ms]
Feb 13 23:24:32.385: INFO: Created: latency-svc-zhrrh
Feb 13 23:24:32.396: INFO: Got endpoints: latency-svc-zhrrh [372.882895ms]
Feb 13 23:24:32.415: INFO: Created: latency-svc-5fpz5
Feb 13 23:24:32.425: INFO: Got endpoints: latency-svc-5fpz5 [382.657474ms]
Feb 13 23:24:32.435: INFO: Created: latency-svc-k87hz
Feb 13 23:24:32.485: INFO: Got endpoints: latency-svc-k87hz [421.715247ms]
Feb 13 23:24:32.486: INFO: Created: latency-svc-4wj67
Feb 13 23:24:32.490: INFO: Got endpoints: latency-svc-4wj67 [385.492383ms]
Feb 13 23:24:32.500: INFO: Created: latency-svc-kt2kh
Feb 13 23:24:32.521: INFO: Created: latency-svc-88s4g
Feb 13 23:24:32.533: INFO: Got endpoints: latency-svc-kt2kh [395.423373ms]
Feb 13 23:24:32.533: INFO: Got endpoints: latency-svc-88s4g [367.306206ms]
Feb 13 23:24:32.543: INFO: Created: latency-svc-tgp2z
Feb 13 23:24:32.556: INFO: Got endpoints: latency-svc-tgp2z [370.328664ms]
Feb 13 23:24:32.565: INFO: Created: latency-svc-r6nbs
Feb 13 23:24:32.576: INFO: Got endpoints: latency-svc-r6nbs [349.942137ms]
Feb 13 23:24:32.633: INFO: Created: latency-svc-p85jg
Feb 13 23:24:32.635: INFO: Got endpoints: latency-svc-p85jg [404.148864ms]
Feb 13 23:24:32.665: INFO: Created: latency-svc-jxfz8
Feb 13 23:24:32.676: INFO: Got endpoints: latency-svc-jxfz8 [423.771407ms]
Feb 13 23:24:32.686: INFO: Created: latency-svc-jddrp
Feb 13 23:24:32.697: INFO: Got endpoints: latency-svc-jddrp [417.819292ms]
Feb 13 23:24:32.707: INFO: Created: latency-svc-fm9mt
Feb 13 23:24:32.718: INFO: Got endpoints: latency-svc-fm9mt [410.47104ms]
Feb 13 23:24:32.727: INFO: Created: latency-svc-z6z9l
Feb 13 23:24:32.757: INFO: Got endpoints: latency-svc-z6z9l [408.940396ms]
Feb 13 23:24:32.761: INFO: Created: latency-svc-dq4c2
Feb 13 23:24:32.772: INFO: Got endpoints: latency-svc-dq4c2 [419.859827ms]
Feb 13 23:24:32.783: INFO: Created: latency-svc-8p45v
Feb 13 23:24:32.794: INFO: Got endpoints: latency-svc-8p45v [418.66312ms]
Feb 13 23:24:32.806: INFO: Created: latency-svc-htk9k
Feb 13 23:24:32.823: INFO: Got endpoints: latency-svc-htk9k [427.082299ms]
Feb 13 23:24:32.841: INFO: Created: latency-svc-pvd2x
Feb 13 23:24:32.852: INFO: Got endpoints: latency-svc-pvd2x [426.454711ms]
Feb 13 23:24:32.898: INFO: Created: latency-svc-bfzbc
Feb 13 23:24:32.912: INFO: Created: latency-svc-jcgq4
Feb 13 23:24:32.912: INFO: Got endpoints: latency-svc-bfzbc [427.005964ms]
Feb 13 23:24:32.930: INFO: Got endpoints: latency-svc-jcgq4 [439.952623ms]
Feb 13 23:24:32.948: INFO: Created: latency-svc-vj79t
Feb 13 23:24:32.969: INFO: Created: latency-svc-mplww
Feb 13 23:24:32.969: INFO: Got endpoints: latency-svc-vj79t [436.227874ms]
Feb 13 23:24:32.994: INFO: Created: latency-svc-qbrdm
Feb 13 23:24:33.026: INFO: Got endpoints: latency-svc-mplww [493.244721ms]
Feb 13 23:24:33.032: INFO: Created: latency-svc-v4z24
Feb 13 23:24:33.055: INFO: Created: latency-svc-qxwrd
Feb 13 23:24:33.075: INFO: Created: latency-svc-spbhk
Feb 13 23:24:33.075: INFO: Got endpoints: latency-svc-qbrdm [519.014685ms]
Feb 13 23:24:33.096: INFO: Created: latency-svc-87ttl
Feb 13 23:24:33.118: INFO: Created: latency-svc-6ksd9
Feb 13 23:24:33.162: INFO: Got endpoints: latency-svc-v4z24 [585.710835ms]
Feb 13 23:24:33.163: INFO: Created: latency-svc-xngcl
Feb 13 23:24:33.168: INFO: Got endpoints: latency-svc-qxwrd [532.826975ms]
Feb 13 23:24:33.178: INFO: Created: latency-svc-mfccr
Feb 13 23:24:33.200: INFO: Created: latency-svc-8nnxx
Feb 13 23:24:33.220: INFO: Created: latency-svc-p9psw
Feb 13 23:24:33.220: INFO: Got endpoints: latency-svc-spbhk [544.21547ms]
Feb 13 23:24:33.256: INFO: Created: latency-svc-qlw59
Feb 13 23:24:33.286: INFO: Got endpoints: latency-svc-87ttl [588.277094ms]
Feb 13 23:24:33.287: INFO: Created: latency-svc-l7h69
Feb 13 23:24:33.299: INFO: Created: latency-svc-g8bzn
Feb 13 23:24:33.321: INFO: Got endpoints: latency-svc-6ksd9 [602.969346ms]
Feb 13 23:24:33.321: INFO: Created: latency-svc-87lfb
Feb 13 23:24:33.357: INFO: Created: latency-svc-g4lnk
Feb 13 23:24:33.368: INFO: Got endpoints: latency-svc-xngcl [611.31754ms]
Feb 13 23:24:33.378: INFO: Created: latency-svc-px6g6
Feb 13 23:24:33.410: INFO: Created: latency-svc-q7bfg
Feb 13 23:24:33.423: INFO: Created: latency-svc-xhzrn
Feb 13 23:24:33.423: INFO: Got endpoints: latency-svc-mfccr [650.287006ms]
Feb 13 23:24:33.482: INFO: Created: latency-svc-qhfxh
Feb 13 23:24:33.482: INFO: Got endpoints: latency-svc-8nnxx [687.876888ms]
Feb 13 23:24:33.503: INFO: Created: latency-svc-dhg7c
Feb 13 23:24:33.533: INFO: Got endpoints: latency-svc-p9psw [710.434201ms]
Feb 13 23:24:33.535: INFO: Created: latency-svc-429mq
Feb 13 23:24:33.548: INFO: Created: latency-svc-jxcsg
Feb 13 23:24:33.568: INFO: Created: latency-svc-798pp
Feb 13 23:24:33.568: INFO: Got endpoints: latency-svc-qlw59 [716.028426ms]
Feb 13 23:24:33.606: INFO: Created: latency-svc-p9mfs
Feb 13 23:24:33.626: INFO: Created: latency-svc-qxv6r
Feb 13 23:24:33.626: INFO: Got endpoints: latency-svc-l7h69 [713.927918ms]
Feb 13 23:24:33.665: INFO: Created: latency-svc-f2trd
Feb 13 23:24:33.685: INFO: Created: latency-svc-rwnr7
Feb 13 23:24:33.726: INFO: Got endpoints: latency-svc-g8bzn [796.464892ms]
Feb 13 23:24:33.734: INFO: Got endpoints: latency-svc-87lfb [765.215161ms]
Feb 13 23:24:33.794: INFO: Got endpoints: latency-svc-g4lnk [767.376459ms]
Feb 13 23:24:33.794: INFO: Created: latency-svc-xw9qr
Feb 13 23:24:33.806: INFO: Created: latency-svc-kfrrn
Feb 13 23:24:33.817: INFO: Got endpoints: latency-svc-px6g6 [741.670514ms]
Feb 13 23:24:33.830: INFO: Created: latency-svc-gbvf7
Feb 13 23:24:33.853: INFO: Created: latency-svc-tcjw7
Feb 13 23:24:33.870: INFO: Got endpoints: latency-svc-q7bfg [708.72553ms]
Feb 13 23:24:33.919: INFO: Got endpoints: latency-svc-xhzrn [750.934043ms]
Feb 13 23:24:33.920: INFO: Created: latency-svc-sk8cq
Feb 13 23:24:33.952: INFO: Created: latency-svc-9st2v
Feb 13 23:24:33.967: INFO: Got endpoints: latency-svc-qhfxh [747.284389ms]
Feb 13 23:24:34.002: INFO: Created: latency-svc-c54zk
Feb 13 23:24:34.078: INFO: Got endpoints: latency-svc-dhg7c [792.328579ms]
Feb 13 23:24:34.078: INFO: Got endpoints: latency-svc-429mq [756.974464ms]
Feb 13 23:24:34.113: INFO: Created: latency-svc-ftsg5
Feb 13 23:24:34.124: INFO: Got endpoints: latency-svc-jxcsg [755.284143ms]
Feb 13 23:24:34.132: INFO: Created: latency-svc-mvlpm
Feb 13 23:24:34.157: INFO: Created: latency-svc-kkqf9
Feb 13 23:24:34.169: INFO: Got endpoints: latency-svc-798pp [746.183758ms]
Feb 13 23:24:34.229: INFO: Created: latency-svc-4twmx
Feb 13 23:24:34.229: INFO: Got endpoints: latency-svc-p9mfs [747.372172ms]
Feb 13 23:24:34.265: INFO: Created: latency-svc-tptsp
Feb 13 23:24:34.276: INFO: Got endpoints: latency-svc-qxv6r [742.466932ms]
Feb 13 23:24:34.358: INFO: Got endpoints: latency-svc-f2trd [790.042181ms]
Feb 13 23:24:34.358: INFO: Created: latency-svc-8zf8l
Feb 13 23:24:34.367: INFO: Got endpoints: latency-svc-rwnr7 [740.51664ms]
Feb 13 23:24:34.393: INFO: Created: latency-svc-s7m2m
Feb 13 23:24:34.431: INFO: Got endpoints: latency-svc-xw9qr [704.796618ms]
Feb 13 23:24:34.441: INFO: Created: latency-svc-94rn6
Feb 13 23:24:34.495: INFO: Got endpoints: latency-svc-kfrrn [760.093028ms]
Feb 13 23:24:34.508: INFO: Created: latency-svc-5rksj
Feb 13 23:24:34.519: INFO: Got endpoints: latency-svc-gbvf7 [725.280755ms]
Feb 13 23:24:34.533: INFO: Created: latency-svc-vgv2k
Feb 13 23:24:34.557: INFO: Created: latency-svc-g6dwt
Feb 13 23:24:34.568: INFO: Got endpoints: latency-svc-tcjw7 [751.185368ms]
Feb 13 23:24:34.614: INFO: Created: latency-svc-98z5r
Feb 13 23:24:34.621: INFO: Got endpoints: latency-svc-sk8cq [750.382314ms]
Feb 13 23:24:34.656: INFO: Created: latency-svc-v9mx7
Feb 13 23:24:34.667: INFO: Got endpoints: latency-svc-9st2v [748.29759ms]
Feb 13 23:24:34.701: INFO: Created: latency-svc-x92zk
Feb 13 23:24:34.738: INFO: Got endpoints: latency-svc-c54zk [770.544229ms]
Feb 13 23:24:34.787: INFO: Created: latency-svc-884qm
Feb 13 23:24:34.787: INFO: Got endpoints: latency-svc-ftsg5 [709.472861ms]
Feb 13 23:24:34.822: INFO: Got endpoints: latency-svc-mvlpm [743.844516ms]
Feb 13 23:24:34.822: INFO: Created: latency-svc-s5ph9
Feb 13 23:24:34.873: INFO: Created: latency-svc-gxmck
Feb 13 23:24:34.873: INFO: Got endpoints: latency-svc-kkqf9 [749.659922ms]
Feb 13 23:24:34.907: INFO: Created: latency-svc-khw9p
Feb 13 23:24:34.918: INFO: Got endpoints: latency-svc-4twmx [748.863102ms]
Feb 13 23:24:34.952: INFO: Created: latency-svc-f2bdt
Feb 13 23:24:34.988: INFO: Got endpoints: latency-svc-tptsp [758.510848ms]
Feb 13 23:24:35.024: INFO: Got endpoints: latency-svc-8zf8l [747.844351ms]
Feb 13 23:24:35.024: INFO: Created: latency-svc-9mxmd
Feb 13 23:24:35.058: INFO: Created: latency-svc-n6slm
Feb 13 23:24:35.069: INFO: Got endpoints: latency-svc-s7m2m [710.432487ms]
Feb 13 23:24:35.127: INFO: Got endpoints: latency-svc-94rn6 [760.474283ms]
Feb 13 23:24:35.128: INFO: Created: latency-svc-srzcm
Feb 13 23:24:35.175: INFO: Got endpoints: latency-svc-5rksj [744.207669ms]
Feb 13 23:24:35.176: INFO: Created: latency-svc-pd5f6
Feb 13 23:24:35.211: INFO: Created: latency-svc-wfdn4
Feb 13 23:24:35.244: INFO: Got endpoints: latency-svc-vgv2k [749.201379ms]
Feb 13 23:24:35.267: INFO: Got endpoints: latency-svc-g6dwt [748.430512ms]
Feb 13 23:24:35.295: INFO: Created: latency-svc-x6tfb
Feb 13 23:24:35.318: INFO: Created: latency-svc-c87dd
Feb 13 23:24:35.327: INFO: Got endpoints: latency-svc-98z5r [758.734473ms]
Feb 13 23:24:35.367: INFO: Got endpoints: latency-svc-v9mx7 [746.011024ms]
Feb 13 23:24:35.379: INFO: Created: latency-svc-5sn9r
Feb 13 23:24:35.403: INFO: Created: latency-svc-6dsbl
Feb 13 23:24:35.417: INFO: Got endpoints: latency-svc-x92zk [749.595539ms]
Feb 13 23:24:35.450: INFO: Created: latency-svc-qrcd4
Feb 13 23:24:35.490: INFO: Got endpoints: latency-svc-884qm [751.972105ms]
Feb 13 23:24:35.525: INFO: Created: latency-svc-6wvhm
Feb 13 23:24:35.525: INFO: Got endpoints: latency-svc-s5ph9 [737.062635ms]
Feb 13 23:24:35.558: INFO: Created: latency-svc-njb79
Feb 13 23:24:35.576: INFO: Got endpoints: latency-svc-gxmck [753.81798ms]
Feb 13 23:24:35.626: INFO: Created: latency-svc-wq26k
Feb 13 23:24:35.627: INFO: Got endpoints: latency-svc-khw9p [753.178257ms]
Feb 13 23:24:35.661: INFO: Created: latency-svc-d45qd
Feb 13 23:24:35.671: INFO: Got endpoints: latency-svc-f2bdt [753.644666ms]
Feb 13 23:24:35.709: INFO: Created: latency-svc-7ddn6
Feb 13 23:24:35.744: INFO: Got endpoints: latency-svc-9mxmd [756.174907ms]
Feb 13 23:24:35.767: INFO: Got endpoints: latency-svc-n6slm [743.109029ms]
Feb 13 23:24:35.780: INFO: Created: latency-svc-lgf4j
Feb 13 23:24:35.835: INFO: Created: latency-svc-qqlks
Feb 13 23:24:35.835: INFO: Got endpoints: latency-svc-srzcm [766.722538ms]
Feb 13 23:24:35.870: INFO: Got endpoints: latency-svc-pd5f6 [742.988702ms]
Feb 13 23:24:35.891: INFO: Created: latency-svc-5zrrs
Feb 13 23:24:35.915: INFO: Created: latency-svc-mvs78
Feb 13 23:24:35.931: INFO: Got endpoints: latency-svc-wfdn4 [755.381459ms]
Feb 13 23:24:35.969: INFO: Created: latency-svc-f69f8
Feb 13 23:24:35.969: INFO: Got endpoints: latency-svc-x6tfb [724.961489ms]
Feb 13 23:24:36.040: INFO: Got endpoints: latency-svc-c87dd [772.384362ms]
Feb 13 23:24:36.052: INFO: Created: latency-svc-h9ccw
Feb 13 23:24:36.077: INFO: Created: latency-svc-qggxm
Feb 13 23:24:36.077: INFO: Got endpoints: latency-svc-5sn9r [750.146025ms]
Feb 13 23:24:36.113: INFO: Created: latency-svc-79p4d
Feb 13 23:24:36.124: INFO: Got endpoints: latency-svc-6dsbl [756.595087ms]
Feb 13 23:24:36.182: INFO: Created: latency-svc-8wms4
Feb 13 23:24:36.182: INFO: Got endpoints: latency-svc-qrcd4 [765.425076ms]
Feb 13 23:24:36.216: INFO: Created: latency-svc-nwtc2
Feb 13 23:24:36.227: INFO: Got endpoints: latency-svc-6wvhm [736.726392ms]
Feb 13 23:24:36.261: INFO: Created: latency-svc-j792s
Feb 13 23:24:36.286: INFO: Got endpoints: latency-svc-njb79 [761.409909ms]
Feb 13 23:24:36.326: INFO: Created: latency-svc-ws8wl
Feb 13 23:24:36.326: INFO: Got endpoints: latency-svc-wq26k [750.729348ms]
Feb 13 23:24:36.367: INFO: Got endpoints: latency-svc-d45qd [740.648871ms]
Feb 13 23:24:36.426: INFO: Got endpoints: latency-svc-7ddn6 [754.115844ms]
Feb 13 23:24:36.442: INFO: Created: latency-svc-hqk55
Feb 13 23:24:36.462: INFO: Created: latency-svc-d76v9
Feb 13 23:24:36.473: INFO: Got endpoints: latency-svc-lgf4j [729.149866ms]
Feb 13 23:24:36.483: INFO: Created: latency-svc-txkbl
Feb 13 23:24:36.507: INFO: Created: latency-svc-kdk5l
Feb 13 23:24:36.544: INFO: Got endpoints: latency-svc-qqlks [777.118756ms]
Feb 13 23:24:36.567: INFO: Got endpoints: latency-svc-5zrrs [731.343717ms]
Feb 13 23:24:36.595: INFO: Created: latency-svc-dv82k
Feb 13 23:24:36.619: INFO: Got endpoints: latency-svc-mvs78 [748.000403ms]
Feb 13 23:24:36.619: INFO: Created: latency-svc-7wlgh
Feb 13 23:24:36.663: INFO: Created: latency-svc-472v6
Feb 13 23:24:36.667: INFO: Got endpoints: latency-svc-f69f8 [736.414369ms]
Feb 13 23:24:36.701: INFO: Created: latency-svc-hmh25
Feb 13 23:24:36.717: INFO: Got endpoints: latency-svc-h9ccw [748.046927ms]
Feb 13 23:24:36.752: INFO: Created: latency-svc-k8jbz
Feb 13 23:24:36.790: INFO: Got endpoints: latency-svc-qggxm [749.92638ms]
Feb 13 23:24:36.826: INFO: Got endpoints: latency-svc-79p4d [749.424771ms]
Feb 13 23:24:36.827: INFO: Created: latency-svc-j8mwc
Feb 13 23:24:36.861: INFO: Created: latency-svc-ff9lk
Feb 13 23:24:36.873: INFO: Got endpoints: latency-svc-8wms4 [748.851535ms]
Feb 13 23:24:36.927: INFO: Created: latency-svc-v89dp
Feb 13 23:24:36.927: INFO: Got endpoints: latency-svc-nwtc2 [744.331204ms]
Feb 13 23:24:36.978: INFO: Created: latency-svc-9ndnh
Feb 13 23:24:36.989: INFO: Got endpoints: latency-svc-j792s [762.355194ms]
Feb 13 23:24:37.042: INFO: Got endpoints: latency-svc-ws8wl [756.135673ms]
Feb 13 23:24:37.055: INFO: Created: latency-svc-l8tnw
Feb 13 23:24:37.067: INFO: Got endpoints: latency-svc-hqk55 [740.254218ms]
Feb 13 23:24:37.081: INFO: Created: latency-svc-dj9qm
Feb 13 23:24:37.108: INFO: Created: latency-svc-gnxvh
Feb 13 23:24:37.165: INFO: Got endpoints: latency-svc-d76v9 [797.323627ms]
Feb 13 23:24:37.167: INFO: Got endpoints: latency-svc-txkbl [741.004256ms]
Feb 13 23:24:37.200: INFO: Created: latency-svc-svpdm
Feb 13 23:24:37.221: INFO: Created: latency-svc-hfkmw
Feb 13 23:24:37.221: INFO: Got endpoints: latency-svc-kdk5l [747.396182ms]
Feb 13 23:24:37.255: INFO: Created: latency-svc-qwwtd
Feb 13 23:24:37.289: INFO: Got endpoints: latency-svc-dv82k [744.602192ms]
Feb 13 23:24:37.324: INFO: Created: latency-svc-jsfm8
Feb 13 23:24:37.324: INFO: Got endpoints: latency-svc-7wlgh [757.59513ms]
Feb 13 23:24:37.371: INFO: Created: latency-svc-hw66j
Feb 13 23:24:37.371: INFO: Got endpoints: latency-svc-472v6 [752.436705ms]
Feb 13 23:24:37.418: INFO: Got endpoints: latency-svc-hmh25 [750.616358ms]
Feb 13 23:24:37.436: INFO: Created: latency-svc-cp9kw
Feb 13 23:24:37.461: INFO: Created: latency-svc-4lmg4
Feb 13 23:24:37.472: INFO: Got endpoints: latency-svc-k8jbz [754.635289ms]
Feb 13 23:24:37.506: INFO: Created: latency-svc-6xc9s
Feb 13 23:24:37.517: INFO: Got endpoints: latency-svc-j8mwc [727.275922ms]
Feb 13 23:24:37.551: INFO: Created: latency-svc-dkwvk
Feb 13 23:24:37.567: INFO: Got endpoints: latency-svc-ff9lk [740.480134ms]
Feb 13 23:24:37.600: INFO: Created: latency-svc-b9ccs
Feb 13 23:24:37.620: INFO: Got endpoints: latency-svc-v89dp [746.955427ms]
Feb 13 23:24:37.672: INFO: Got endpoints: latency-svc-9ndnh [745.705399ms]
Feb 13 23:24:37.672: INFO: Created: latency-svc-c7thg
Feb 13 23:24:37.706: INFO: Created: latency-svc-wwqbw
Feb 13 23:24:37.717: INFO: Got endpoints: latency-svc-l8tnw [728.253385ms]
Feb 13 23:24:37.750: INFO: Created: latency-svc-6wzvp
Feb 13 23:24:37.784: INFO: Got endpoints: latency-svc-dj9qm [741.51218ms]
Feb 13 23:24:37.819: INFO: Created: latency-svc-hxsbg
Feb 13 23:24:37.819: INFO: Got endpoints: latency-svc-gnxvh [752.319813ms]
Feb 13 23:24:37.854: INFO: Created: latency-svc-hvj6d
Feb 13 23:24:37.870: INFO: Got endpoints: latency-svc-svpdm [705.230105ms]
Feb 13 23:24:37.924: INFO: Created: latency-svc-lwk76
Feb 13 23:24:37.925: INFO: Got endpoints: latency-svc-hfkmw [757.805563ms]
Feb 13 23:24:37.959: INFO: Created: latency-svc-hpp8t
Feb 13 23:24:37.970: INFO: Got endpoints: latency-svc-qwwtd [748.722315ms]
Feb 13 23:24:38.003: INFO: Created: latency-svc-g7swj
Feb 13 23:24:38.039: INFO: Got endpoints: latency-svc-jsfm8 [750.140894ms]
Feb 13 23:24:38.074: INFO: Got endpoints: latency-svc-hw66j [749.206008ms]
Feb 13 23:24:38.076: INFO: Created: latency-svc-h5d8r
Feb 13 23:24:38.108: INFO: Created: latency-svc-ls9bk
Feb 13 23:24:38.119: INFO: Got endpoints: latency-svc-cp9kw [747.45014ms]
Feb 13 23:24:38.172: INFO: Got endpoints: latency-svc-4lmg4 [754.045334ms]
Feb 13 23:24:38.184: INFO: Created: latency-svc-xvp6z
Feb 13 23:24:38.208: INFO: Created: latency-svc-2qpwt
Feb 13 23:24:38.225: INFO: Got endpoints: latency-svc-6xc9s [753.017803ms]
Feb 13 23:24:38.258: INFO: Created: latency-svc-s85qp
Feb 13 23:24:38.269: INFO: Got endpoints: latency-svc-dkwvk [751.213906ms]
Feb 13 23:24:38.313: INFO: Created: latency-svc-pk2tw
Feb 13 23:24:38.326: INFO: Got endpoints: latency-svc-b9ccs [758.936003ms]
Feb 13 23:24:38.391: INFO: Created: latency-svc-cpt4b
Feb 13 23:24:38.391: INFO: Got endpoints: latency-svc-c7thg [771.6758ms]
Feb 13 23:24:38.423: INFO: Got endpoints: latency-svc-wwqbw [750.886434ms]
Feb 13 23:24:38.481: INFO: Got endpoints: latency-svc-6wzvp [763.240184ms]
Feb 13 23:24:38.481: INFO: Created: latency-svc-cv2fn
Feb 13 23:24:38.502: INFO: Created: latency-svc-pdr2m
Feb 13 23:24:38.558: INFO: Created: latency-svc-nksrb
Feb 13 23:24:38.558: INFO: Got endpoints: latency-svc-hxsbg [774.492514ms]
Feb 13 23:24:38.567: INFO: Got endpoints: latency-svc-hvj6d [747.750224ms]
Feb 13 23:24:38.592: INFO: Created: latency-svc-nkg2l
Feb 13 23:24:38.625: INFO: Got endpoints: latency-svc-lwk76 [755.177178ms]
Feb 13 23:24:38.625: INFO: Created: latency-svc-tvfjg
Feb 13 23:24:38.690: INFO: Got endpoints: latency-svc-hpp8t [764.991048ms]
Feb 13 23:24:38.690: INFO: Created: latency-svc-htxvs
Feb 13 23:24:38.725: INFO: Got endpoints: latency-svc-g7swj [755.675568ms]
Feb 13 23:24:38.725: INFO: Created: latency-svc-v8jvc
Feb 13 23:24:38.761: INFO: Created: latency-svc-sv2nr
Feb 13 23:24:38.772: INFO: Got endpoints: latency-svc-h5d8r [732.901407ms]
Feb 13 23:24:38.817: INFO: Got endpoints: latency-svc-ls9bk [743.127457ms]
Feb 13 23:24:38.830: INFO: Created: latency-svc-vs8nh
Feb 13 23:24:38.864: INFO: Created: latency-svc-w56hl
Feb 13 23:24:38.874: INFO: Got endpoints: latency-svc-xvp6z [755.896368ms]
Feb 13 23:24:38.910: INFO: Created: latency-svc-mnfbb
Feb 13 23:24:38.940: INFO: Got endpoints: latency-svc-2qpwt [767.930789ms]
Feb 13 23:24:38.974: INFO: Created: latency-svc-5jzd9
Feb 13 23:24:38.974: INFO: Got endpoints: latency-svc-s85qp [748.736429ms]
Feb 13 23:24:39.011: INFO: Created: latency-svc-lw7c5
Feb 13 23:24:39.022: INFO: Got endpoints: latency-svc-pk2tw [753.043309ms]
Feb 13 23:24:39.067: INFO: Got endpoints: latency-svc-cpt4b [741.015675ms]
Feb 13 23:24:39.080: INFO: Created: latency-svc-hnjhq
Feb 13 23:24:39.117: INFO: Got endpoints: latency-svc-cv2fn [725.484029ms]
Feb 13 23:24:39.167: INFO: Got endpoints: latency-svc-pdr2m [743.632886ms]
Feb 13 23:24:39.217: INFO: Got endpoints: latency-svc-nksrb [736.573548ms]
Feb 13 23:24:39.267: INFO: Got endpoints: latency-svc-nkg2l [708.66681ms]
Feb 13 23:24:39.317: INFO: Got endpoints: latency-svc-tvfjg [750.257222ms]
Feb 13 23:24:39.367: INFO: Got endpoints: latency-svc-htxvs [741.865393ms]
Feb 13 23:24:39.417: INFO: Got endpoints: latency-svc-v8jvc [727.638621ms]
Feb 13 23:24:39.467: INFO: Got endpoints: latency-svc-sv2nr [741.533092ms]
Feb 13 23:24:39.520: INFO: Got endpoints: latency-svc-vs8nh [748.067407ms]
Feb 13 23:24:39.567: INFO: Got endpoints: latency-svc-w56hl [750.170781ms]
Feb 13 23:24:39.617: INFO: Got endpoints: latency-svc-mnfbb [742.471096ms]
Feb 13 23:24:39.668: INFO: Got endpoints: latency-svc-5jzd9 [727.799633ms]
Feb 13 23:24:39.718: INFO: Got endpoints: latency-svc-lw7c5 [743.915807ms]
Feb 13 23:24:39.767: INFO: Got endpoints: latency-svc-hnjhq [745.478933ms]
Feb 13 23:24:39.767: INFO: Latencies: [45.786743ms 89.993896ms 104.93261ms 140.811697ms 162.457322ms 183.958833ms 231.813996ms 258.48226ms 278.8706ms 298.996852ms 319.788689ms 335.711226ms 342.744808ms 344.564691ms 347.607657ms 349.942137ms 353.00105ms 357.142203ms 359.196847ms 359.396681ms 365.658421ms 367.306206ms 368.601008ms 369.884734ms 370.328664ms 371.871596ms 372.183466ms 372.882895ms 372.994287ms 379.943057ms 382.657474ms 383.171963ms 383.907337ms 384.430531ms 385.492383ms 387.757088ms 389.950121ms 391.345996ms 391.861098ms 392.055841ms 394.865795ms 395.423373ms 404.148864ms 404.607073ms 404.859576ms 405.529535ms 407.479617ms 408.940396ms 409.278176ms 409.415641ms 410.47104ms 412.559959ms 417.819292ms 418.66312ms 419.859827ms 421.361629ms 421.715247ms 423.771407ms 424.285084ms 426.454711ms 427.005964ms 427.082299ms 436.227874ms 439.952623ms 493.244721ms 519.014685ms 532.826975ms 544.21547ms 585.710835ms 588.277094ms 602.969346ms 611.31754ms 650.287006ms 687.876888ms 704.796618ms 705.230105ms 708.66681ms 708.72553ms 709.472861ms 710.432487ms 710.434201ms 713.927918ms 716.028426ms 724.961489ms 725.280755ms 725.484029ms 727.275922ms 727.638621ms 727.799633ms 728.253385ms 729.149866ms 731.343717ms 732.901407ms 736.414369ms 736.573548ms 736.726392ms 737.062635ms 740.254218ms 740.480134ms 740.51664ms 740.648871ms 741.004256ms 741.015675ms 741.51218ms 741.533092ms 741.670514ms 741.865393ms 742.466932ms 742.471096ms 742.988702ms 743.109029ms 743.127457ms 743.632886ms 743.844516ms 743.915807ms 744.207669ms 744.331204ms 744.602192ms 745.478933ms 745.705399ms 746.011024ms 746.183758ms 746.955427ms 747.284389ms 747.372172ms 747.396182ms 747.45014ms 747.750224ms 747.844351ms 748.000403ms 748.046927ms 748.067407ms 748.29759ms 748.430512ms 748.722315ms 748.736429ms 748.851535ms 748.863102ms 749.201379ms 749.206008ms 749.424771ms 749.595539ms 749.659922ms 749.92638ms 750.140894ms 750.146025ms 750.170781ms 750.257222ms 750.382314ms 750.616358ms 750.729348ms 750.886434ms 750.934043ms 751.185368ms 751.213906ms 751.972105ms 752.319813ms 752.436705ms 753.017803ms 753.043309ms 753.178257ms 753.644666ms 753.81798ms 754.045334ms 754.115844ms 754.635289ms 755.177178ms 755.284143ms 755.381459ms 755.675568ms 755.896368ms 756.135673ms 756.174907ms 756.595087ms 756.974464ms 757.59513ms 757.805563ms 758.510848ms 758.734473ms 758.936003ms 760.093028ms 760.474283ms 761.409909ms 762.355194ms 763.240184ms 764.991048ms 765.215161ms 765.425076ms 766.722538ms 767.376459ms 767.930789ms 770.544229ms 771.6758ms 772.384362ms 774.492514ms 777.118756ms 790.042181ms 792.328579ms 796.464892ms 797.323627ms]
Feb 13 23:24:39.767: INFO: 50 %ile: 740.648871ms
Feb 13 23:24:39.767: INFO: 90 %ile: 760.093028ms
Feb 13 23:24:39.767: INFO: 99 %ile: 796.464892ms
Feb 13 23:24:39.767: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:24:39.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-d6l4c" for this suite.
Feb 13 23:24:55.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:24:56.168: INFO: namespace: e2e-tests-svc-latency-d6l4c, resource: bindings, ignored listing per whitelist
Feb 13 23:24:56.855: INFO: namespace e2e-tests-svc-latency-d6l4c deletion completed in 17.06482805s

• [SLOW TEST:29.786 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:24:56.855: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-x6vzq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-93bc3156-2fe6-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 13 23:24:57.953: INFO: Waiting up to 5m0s for pod "pod-secrets-93bf6473-2fe6-11e9-9de3-466a6591423c" in namespace "e2e-tests-secrets-x6vzq" to be "success or failure"
Feb 13 23:24:57.974: INFO: Pod "pod-secrets-93bf6473-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.589472ms
Feb 13 23:24:59.995: INFO: Pod "pod-secrets-93bf6473-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042122164s
Feb 13 23:25:02.017: INFO: Pod "pod-secrets-93bf6473-2fe6-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064317786s
STEP: Saw pod success
Feb 13 23:25:02.017: INFO: Pod "pod-secrets-93bf6473-2fe6-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:25:02.038: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-secrets-93bf6473-2fe6-11e9-9de3-466a6591423c container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 23:25:02.095: INFO: Waiting for pod pod-secrets-93bf6473-2fe6-11e9-9de3-466a6591423c to disappear
Feb 13 23:25:02.116: INFO: Pod pod-secrets-93bf6473-2fe6-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:25:02.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-x6vzq" for this suite.
Feb 13 23:25:08.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:25:08.978: INFO: namespace: e2e-tests-secrets-x6vzq, resource: bindings, ignored listing per whitelist
Feb 13 23:25:09.043: INFO: namespace e2e-tests-secrets-x6vzq deletion completed in 6.905371083s

• [SLOW TEST:12.188 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:25:09.043: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hnr55
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:25:10.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9af21083-2fe6-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-hnr55" to be "success or failure"
Feb 13 23:25:10.050: INFO: Pod "downwardapi-volume-9af21083-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.978124ms
Feb 13 23:25:12.072: INFO: Pod "downwardapi-volume-9af21083-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043163658s
Feb 13 23:25:14.095: INFO: Pod "downwardapi-volume-9af21083-2fe6-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06562094s
STEP: Saw pod success
Feb 13 23:25:14.095: INFO: Pod "downwardapi-volume-9af21083-2fe6-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:25:14.116: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-9af21083-2fe6-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 23:25:14.173: INFO: Waiting for pod downwardapi-volume-9af21083-2fe6-11e9-9de3-466a6591423c to disappear
Feb 13 23:25:14.194: INFO: Pod downwardapi-volume-9af21083-2fe6-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:25:14.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hnr55" for this suite.
Feb 13 23:25:22.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:25:23.038: INFO: namespace: e2e-tests-projected-hnr55, resource: bindings, ignored listing per whitelist
Feb 13 23:25:23.080: INFO: namespace e2e-tests-projected-hnr55 deletion completed in 8.864906551s

• [SLOW TEST:14.037 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:25:23.081: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-45nfq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 13 23:25:24.218: INFO: Waiting up to 5m0s for pod "pod-a366fb82-2fe6-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-45nfq" to be "success or failure"
Feb 13 23:25:24.238: INFO: Pod "pod-a366fb82-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.71172ms
Feb 13 23:25:26.260: INFO: Pod "pod-a366fb82-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042708806s
Feb 13 23:25:28.282: INFO: Pod "pod-a366fb82-2fe6-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064280196s
STEP: Saw pod success
Feb 13 23:25:28.282: INFO: Pod "pod-a366fb82-2fe6-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:25:28.303: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-a366fb82-2fe6-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 23:25:28.365: INFO: Waiting for pod pod-a366fb82-2fe6-11e9-9de3-466a6591423c to disappear
Feb 13 23:25:28.386: INFO: Pod pod-a366fb82-2fe6-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:25:28.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-45nfq" for this suite.
Feb 13 23:25:34.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:25:35.069: INFO: namespace: e2e-tests-emptydir-45nfq, resource: bindings, ignored listing per whitelist
Feb 13 23:25:35.303: INFO: namespace e2e-tests-emptydir-45nfq deletion completed in 6.895862477s

• [SLOW TEST:12.223 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:25:35.304: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5lqrr
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-aaa1c87c-2fe6-11e9-9de3-466a6591423c
STEP: Creating configMap with name cm-test-opt-upd-aaa1c8be-2fe6-11e9-9de3-466a6591423c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-aaa1c87c-2fe6-11e9-9de3-466a6591423c
STEP: Updating configmap cm-test-opt-upd-aaa1c8be-2fe6-11e9-9de3-466a6591423c
STEP: Creating configMap with name cm-test-opt-create-aaa1c8d1-2fe6-11e9-9de3-466a6591423c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:25:47.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5lqrr" for this suite.
Feb 13 23:26:09.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:26:09.195: INFO: namespace: e2e-tests-projected-5lqrr, resource: bindings, ignored listing per whitelist
Feb 13 23:26:09.896: INFO: namespace e2e-tests-projected-5lqrr deletion completed in 22.873545956s

• [SLOW TEST:34.593 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:26:09.897: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ndjv5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:26:10.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf3e8e25-2fe6-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-ndjv5" to be "success or failure"
Feb 13 23:26:10.950: INFO: Pod "downwardapi-volume-bf3e8e25-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.851413ms
Feb 13 23:26:12.972: INFO: Pod "downwardapi-volume-bf3e8e25-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043409779s
Feb 13 23:26:15.017: INFO: Pod "downwardapi-volume-bf3e8e25-2fe6-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08832259s
Feb 13 23:26:17.040: INFO: Pod "downwardapi-volume-bf3e8e25-2fe6-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.110634406s
STEP: Saw pod success
Feb 13 23:26:17.040: INFO: Pod "downwardapi-volume-bf3e8e25-2fe6-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:26:17.061: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-bf3e8e25-2fe6-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 23:26:17.124: INFO: Waiting for pod downwardapi-volume-bf3e8e25-2fe6-11e9-9de3-466a6591423c to disappear
Feb 13 23:26:17.145: INFO: Pod downwardapi-volume-bf3e8e25-2fe6-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:26:17.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ndjv5" for this suite.
Feb 13 23:26:23.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:26:23.746: INFO: namespace: e2e-tests-downward-api-ndjv5, resource: bindings, ignored listing per whitelist
Feb 13 23:26:24.085: INFO: namespace e2e-tests-downward-api-ndjv5 deletion completed in 6.917428637s

• [SLOW TEST:14.188 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:26:24.085: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-gzgv2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-gzgv2 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-gzgv2 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-gzgv2.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-gzgv2.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-gzgv2.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-gzgv2.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-gzgv2.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-gzgv2.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-gzgv2.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 236.75.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.75.236_udp@PTR;check="$$(dig +tcp +noall +answer +search 236.75.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.75.236_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-gzgv2 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-gzgv2;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-gzgv2 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-gzgv2.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-gzgv2.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-gzgv2.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-gzgv2.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-gzgv2.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-gzgv2.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-gzgv2.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-gzgv2.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 236.75.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.75.236_udp@PTR;check="$$(dig +tcp +noall +answer +search 236.75.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.75.236_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 13 23:26:45.482: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:45.528: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:45.552: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:45.576: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:45.601: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:45.626: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:45.650: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:45.674: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:46.148: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:46.175: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:46.260: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:46.306: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:46.330: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:46.354: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:46.378: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:46.402: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:46.781: INFO: Lookups using e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2 wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2 wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2.svc wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-gzgv2 jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2 jessie_udp@dns-test-service.e2e-tests-dns-gzgv2.svc jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc]

Feb 13 23:26:51.807: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:51.831: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:51.855: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:51.879: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:51.904: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:51.928: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:51.953: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:51.977: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:52.425: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:52.450: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:52.474: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:52.499: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:52.523: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:52.547: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:52.571: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:52.596: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:52.995: INFO: Lookups using e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2 wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2 wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2.svc wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-gzgv2 jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2 jessie_udp@dns-test-service.e2e-tests-dns-gzgv2.svc jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc]

Feb 13 23:26:56.806: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:56.852: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:56.876: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:56.900: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:56.924: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:56.949: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:56.973: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:56.997: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:57.424: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:57.448: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:57.472: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:57.497: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2 from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:57.525: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:57.549: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:57.576: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:57.600: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc from pod e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c: the server could not find the requested resource (get pods dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c)
Feb 13 23:26:57.982: INFO: Lookups using e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2 wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2 wheezy_udp@dns-test-service.e2e-tests-dns-gzgv2.svc wheezy_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-gzgv2 jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2 jessie_udp@dns-test-service.e2e-tests-dns-gzgv2.svc jessie_tcp@dns-test-service.e2e-tests-dns-gzgv2.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-gzgv2.svc]

Feb 13 23:27:03.595: INFO: DNS probes using e2e-tests-dns-gzgv2/dns-test-c7c8d3fa-2fe6-11e9-9de3-466a6591423c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:27:03.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-gzgv2" for this suite.
Feb 13 23:27:09.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:27:40.973: INFO: namespace: e2e-tests-dns-gzgv2, resource: bindings, ignored listing per whitelist
Feb 13 23:27:41.354: INFO: namespace e2e-tests-dns-gzgv2 deletion completed in 37.62321912s

• [SLOW TEST:77.270 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:27:41.355: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-jfzjt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 13 23:27:42.595: INFO: Number of nodes with available pods: 0
Feb 13 23:27:42.595: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:27:43.639: INFO: Number of nodes with available pods: 0
Feb 13 23:27:43.639: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:27:44.653: INFO: Number of nodes with available pods: 0
Feb 13 23:27:44.653: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:27:45.638: INFO: Number of nodes with available pods: 1
Feb 13 23:27:45.638: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:27:46.656: INFO: Number of nodes with available pods: 1
Feb 13 23:27:46.657: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:27:47.638: INFO: Number of nodes with available pods: 2
Feb 13 23:27:47.638: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 13 23:27:47.756: INFO: Number of nodes with available pods: 1
Feb 13 23:27:47.756: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:27:48.799: INFO: Number of nodes with available pods: 1
Feb 13 23:27:48.799: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:27:49.799: INFO: Number of nodes with available pods: 1
Feb 13 23:27:49.799: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:27:50.798: INFO: Number of nodes with available pods: 2
Feb 13 23:27:50.798: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-jfzjt, will wait for the garbage collector to delete the pods
Feb 13 23:27:50.933: INFO: Deleting DaemonSet.extensions daemon-set took: 23.274599ms
Feb 13 23:27:51.033: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.247561ms
Feb 13 23:28:35.254: INFO: Number of nodes with available pods: 0
Feb 13 23:28:35.254: INFO: Number of running nodes: 0, number of available pods: 0
Feb 13 23:28:35.275: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-jfzjt/daemonsets","resourceVersion":"15693"},"items":null}

Feb 13 23:28:35.295: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-jfzjt/pods","resourceVersion":"15693"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:28:35.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-jfzjt" for this suite.
Feb 13 23:28:41.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:28:41.735: INFO: namespace: e2e-tests-daemonsets-jfzjt, resource: bindings, ignored listing per whitelist
Feb 13 23:28:42.304: INFO: namespace e2e-tests-daemonsets-jfzjt deletion completed in 6.925412537s

• [SLOW TEST:60.950 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:28:42.305: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-n4sl5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 13 23:28:43.427: INFO: Waiting up to 5m0s for pod "var-expansion-1a24337f-2fe7-11e9-9de3-466a6591423c" in namespace "e2e-tests-var-expansion-n4sl5" to be "success or failure"
Feb 13 23:28:43.448: INFO: Pod "var-expansion-1a24337f-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.395299ms
Feb 13 23:28:45.477: INFO: Pod "var-expansion-1a24337f-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049397588s
Feb 13 23:28:47.499: INFO: Pod "var-expansion-1a24337f-2fe7-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071775222s
STEP: Saw pod success
Feb 13 23:28:47.499: INFO: Pod "var-expansion-1a24337f-2fe7-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:28:47.520: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod var-expansion-1a24337f-2fe7-11e9-9de3-466a6591423c container dapi-container: <nil>
STEP: delete the pod
Feb 13 23:28:47.665: INFO: Waiting for pod var-expansion-1a24337f-2fe7-11e9-9de3-466a6591423c to disappear
Feb 13 23:28:47.686: INFO: Pod var-expansion-1a24337f-2fe7-11e9-9de3-466a6591423c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:28:47.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-n4sl5" for this suite.
Feb 13 23:28:53.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:28:54.002: INFO: namespace: e2e-tests-var-expansion-n4sl5, resource: bindings, ignored listing per whitelist
Feb 13 23:28:54.629: INFO: namespace e2e-tests-var-expansion-n4sl5 deletion completed in 6.922364849s

• [SLOW TEST:12.325 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:28:54.630: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-lqwjx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-215abb48-2fe7-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 13 23:28:55.553: INFO: Waiting up to 5m0s for pod "pod-configmaps-215e0321-2fe7-11e9-9de3-466a6591423c" in namespace "e2e-tests-configmap-lqwjx" to be "success or failure"
Feb 13 23:28:55.573: INFO: Pod "pod-configmaps-215e0321-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.508223ms
Feb 13 23:28:57.596: INFO: Pod "pod-configmaps-215e0321-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043704282s
Feb 13 23:28:59.617: INFO: Pod "pod-configmaps-215e0321-2fe7-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064787291s
STEP: Saw pod success
Feb 13 23:28:59.618: INFO: Pod "pod-configmaps-215e0321-2fe7-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:28:59.645: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-configmaps-215e0321-2fe7-11e9-9de3-466a6591423c container configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 23:28:59.699: INFO: Waiting for pod pod-configmaps-215e0321-2fe7-11e9-9de3-466a6591423c to disappear
Feb 13 23:28:59.720: INFO: Pod pod-configmaps-215e0321-2fe7-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:28:59.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-lqwjx" for this suite.
Feb 13 23:29:05.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:29:06.532: INFO: namespace: e2e-tests-configmap-lqwjx, resource: bindings, ignored listing per whitelist
Feb 13 23:29:06.634: INFO: namespace e2e-tests-configmap-lqwjx deletion completed in 6.892318682s

• [SLOW TEST:12.004 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:29:06.634: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-n7pc5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:29:07.649: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28942a78-2fe7-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-n7pc5" to be "success or failure"
Feb 13 23:29:07.669: INFO: Pod "downwardapi-volume-28942a78-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.092538ms
Feb 13 23:29:09.691: INFO: Pod "downwardapi-volume-28942a78-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041622799s
Feb 13 23:29:11.717: INFO: Pod "downwardapi-volume-28942a78-2fe7-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067702062s
STEP: Saw pod success
Feb 13 23:29:11.717: INFO: Pod "downwardapi-volume-28942a78-2fe7-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:29:11.738: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-28942a78-2fe7-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 23:29:11.799: INFO: Waiting for pod downwardapi-volume-28942a78-2fe7-11e9-9de3-466a6591423c to disappear
Feb 13 23:29:11.832: INFO: Pod downwardapi-volume-28942a78-2fe7-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:29:11.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n7pc5" for this suite.
Feb 13 23:29:17.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:29:18.245: INFO: namespace: e2e-tests-projected-n7pc5, resource: bindings, ignored listing per whitelist
Feb 13 23:29:18.709: INFO: namespace e2e-tests-projected-n7pc5 deletion completed in 6.856098155s

• [SLOW TEST:12.075 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:29:18.709: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-cpl6p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:29:19.686: INFO: Creating deployment "test-recreate-deployment"
Feb 13 23:29:19.707: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 13 23:29:19.757: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 13 23:29:19.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685697359, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685697359, loc:(*time.Location)(0x791ea40)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-5dfdcc846d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685697359, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685697359, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:29:21.799: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685697359, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685697359, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685697359, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685697359, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5dfdcc846d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:29:23.801: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 13 23:29:23.844: INFO: Updating deployment test-recreate-deployment
Feb 13 23:29:23.844: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 13 23:29:24.048: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-cpl6p,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cpl6p/deployments/test-recreate-deployment,UID:2fc51a38-2fe7-11e9-9f3f-da917295d151,ResourceVersion:15884,Generation:2,CreationTimestamp:2019-02-13 23:29:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-13 23:29:23 +0000 UTC 2019-02-13 23:29:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-13 23:29:24 +0000 UTC 2019-02-13 23:29:19 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 13 23:29:24.070: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-cpl6p,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cpl6p/replicasets/test-recreate-deployment-697fbf54bf,UID:32497e5b-2fe7-11e9-9f3f-da917295d151,ResourceVersion:15882,Generation:1,CreationTimestamp:2019-02-13 23:29:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 2fc51a38-2fe7-11e9-9f3f-da917295d151 0xc0023f8327 0xc0023f8328}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 13 23:29:24.070: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 13 23:29:24.070: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-cpl6p,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cpl6p/replicasets/test-recreate-deployment-5dfdcc846d,UID:2fc6c60c-2fe7-11e9-9f3f-da917295d151,ResourceVersion:15875,Generation:2,CreationTimestamp:2019-02-13 23:29:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 2fc51a38-2fe7-11e9-9f3f-da917295d151 0xc0023f8267 0xc0023f8268}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 13 23:29:24.094: INFO: Pod "test-recreate-deployment-697fbf54bf-b5svf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-b5svf,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-cpl6p,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cpl6p/pods/test-recreate-deployment-697fbf54bf-b5svf,UID:3249f365-2fe7-11e9-9f3f-da917295d151,ResourceVersion:15883,Generation:0,CreationTimestamp:2019-02-13 23:29:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 32497e5b-2fe7-11e9-9f3f-da917295d151 0xc0023f8c47 0xc0023f8c48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-qc68c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qc68c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qc68c true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023f9480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023f94a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:29:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:29:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:29:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:29:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-13 23:29:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:29:24.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-cpl6p" for this suite.
Feb 13 23:29:32.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:29:32.243: INFO: namespace: e2e-tests-deployment-cpl6p, resource: bindings, ignored listing per whitelist
Feb 13 23:29:32.966: INFO: namespace e2e-tests-deployment-cpl6p deletion completed in 8.849988247s

• [SLOW TEST:14.257 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:29:32.966: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-ltsm6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-384efbbb-2fe7-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 13 23:29:34.062: INFO: Waiting up to 5m0s for pod "pod-configmaps-38522ee0-2fe7-11e9-9de3-466a6591423c" in namespace "e2e-tests-configmap-ltsm6" to be "success or failure"
Feb 13 23:29:34.083: INFO: Pod "pod-configmaps-38522ee0-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.48589ms
Feb 13 23:29:36.104: INFO: Pod "pod-configmaps-38522ee0-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041981182s
Feb 13 23:29:38.126: INFO: Pod "pod-configmaps-38522ee0-2fe7-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063330794s
STEP: Saw pod success
Feb 13 23:29:38.126: INFO: Pod "pod-configmaps-38522ee0-2fe7-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:29:38.147: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-configmaps-38522ee0-2fe7-11e9-9de3-466a6591423c container configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 23:29:38.214: INFO: Waiting for pod pod-configmaps-38522ee0-2fe7-11e9-9de3-466a6591423c to disappear
Feb 13 23:29:38.234: INFO: Pod pod-configmaps-38522ee0-2fe7-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:29:38.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ltsm6" for this suite.
Feb 13 23:29:44.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:29:44.856: INFO: namespace: e2e-tests-configmap-ltsm6, resource: bindings, ignored listing per whitelist
Feb 13 23:29:45.139: INFO: namespace e2e-tests-configmap-ltsm6 deletion completed in 6.883624369s

• [SLOW TEST:12.173 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:29:45.139: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-h8lg6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-h8lg6
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 13 23:29:46.188: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 13 23:30:14.583: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.20:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-h8lg6 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 13 23:30:14.583: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 13 23:30:15.101: INFO: Found all expected endpoints: [netserver-0]
Feb 13 23:30:15.122: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.112:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-h8lg6 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 13 23:30:15.122: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 13 23:30:15.645: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:30:15.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-h8lg6" for this suite.
Feb 13 23:30:39.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:30:40.389: INFO: namespace: e2e-tests-pod-network-test-h8lg6, resource: bindings, ignored listing per whitelist
Feb 13 23:30:40.576: INFO: namespace e2e-tests-pod-network-test-h8lg6 deletion completed in 24.908884385s

• [SLOW TEST:55.437 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:30:40.576: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-7hbxh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 13 23:30:41.615: INFO: Waiting up to 5m0s for pod "downward-api-60964d96-2fe7-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-7hbxh" to be "success or failure"
Feb 13 23:30:41.648: INFO: Pod "downward-api-60964d96-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 32.662064ms
Feb 13 23:30:43.670: INFO: Pod "downward-api-60964d96-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054495534s
Feb 13 23:30:45.692: INFO: Pod "downward-api-60964d96-2fe7-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076864308s
STEP: Saw pod success
Feb 13 23:30:45.692: INFO: Pod "downward-api-60964d96-2fe7-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:30:45.713: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downward-api-60964d96-2fe7-11e9-9de3-466a6591423c container dapi-container: <nil>
STEP: delete the pod
Feb 13 23:30:45.771: INFO: Waiting for pod downward-api-60964d96-2fe7-11e9-9de3-466a6591423c to disappear
Feb 13 23:30:45.791: INFO: Pod downward-api-60964d96-2fe7-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:30:45.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7hbxh" for this suite.
Feb 13 23:30:53.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:30:54.273: INFO: namespace: e2e-tests-downward-api-7hbxh, resource: bindings, ignored listing per whitelist
Feb 13 23:30:54.696: INFO: namespace e2e-tests-downward-api-7hbxh deletion completed in 8.883524642s

• [SLOW TEST:14.120 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:30:54.696: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-75m5z
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-68f3fea1-2fe7-11e9-9de3-466a6591423c
STEP: Creating secret with name s-test-opt-upd-68f3fefa-2fe7-11e9-9de3-466a6591423c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-68f3fea1-2fe7-11e9-9de3-466a6591423c
STEP: Updating secret s-test-opt-upd-68f3fefa-2fe7-11e9-9de3-466a6591423c
STEP: Creating secret with name s-test-opt-create-68f3ff12-2fe7-11e9-9de3-466a6591423c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:32:25.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-75m5z" for this suite.
Feb 13 23:32:47.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:32:48.619: INFO: namespace: e2e-tests-projected-75m5z, resource: bindings, ignored listing per whitelist
Feb 13 23:32:48.791: INFO: namespace e2e-tests-projected-75m5z deletion completed in 22.868938991s

• [SLOW TEST:114.095 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:32:48.791: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-x5lgn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-ad07d4e4-2fe7-11e9-9de3-466a6591423c
Feb 13 23:32:49.895: INFO: Pod name my-hostname-basic-ad07d4e4-2fe7-11e9-9de3-466a6591423c: Found 1 pods out of 1
Feb 13 23:32:49.895: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ad07d4e4-2fe7-11e9-9de3-466a6591423c" are running
Feb 13 23:32:53.937: INFO: Pod "my-hostname-basic-ad07d4e4-2fe7-11e9-9de3-466a6591423c-8k9p4" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-13 23:32:49 +0000 UTC Reason: Message:}])
Feb 13 23:32:53.938: INFO: Trying to dial the pod
Feb 13 23:33:13.844: INFO: Controller my-hostname-basic-ad07d4e4-2fe7-11e9-9de3-466a6591423c: Got expected result from replica 1 [my-hostname-basic-ad07d4e4-2fe7-11e9-9de3-466a6591423c-8k9p4]: "my-hostname-basic-ad07d4e4-2fe7-11e9-9de3-466a6591423c-8k9p4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:33:13.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-x5lgn" for this suite.
Feb 13 23:33:19.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:33:20.319: INFO: namespace: e2e-tests-replication-controller-x5lgn, resource: bindings, ignored listing per whitelist
Feb 13 23:33:20.738: INFO: namespace e2e-tests-replication-controller-x5lgn deletion completed in 6.872843785s

• [SLOW TEST:31.947 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:33:20.738: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-vkk5s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0213 23:33:32.240092   31687 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 23:33:32.240: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:33:32.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-vkk5s" for this suite.
Feb 13 23:33:38.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:33:38.766: INFO: namespace: e2e-tests-gc-vkk5s, resource: bindings, ignored listing per whitelist
Feb 13 23:33:39.182: INFO: namespace e2e-tests-gc-vkk5s deletion completed in 6.92117944s

• [SLOW TEST:18.444 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:33:39.182: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-cnglm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-t4hl
STEP: Creating a pod to test atomic-volume-subpath
Feb 13 23:33:40.171: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-t4hl" in namespace "e2e-tests-subpath-cnglm" to be "success or failure"
Feb 13 23:33:40.192: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Pending", Reason="", readiness=false. Elapsed: 20.34546ms
Feb 13 23:33:42.213: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041993263s
Feb 13 23:33:44.235: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063681021s
Feb 13 23:33:46.256: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084782603s
Feb 13 23:33:48.278: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Running", Reason="", readiness=false. Elapsed: 8.106658376s
Feb 13 23:33:50.299: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Running", Reason="", readiness=false. Elapsed: 10.128057728s
Feb 13 23:33:52.322: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Running", Reason="", readiness=false. Elapsed: 12.150193724s
Feb 13 23:33:54.343: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Running", Reason="", readiness=false. Elapsed: 14.171361286s
Feb 13 23:33:56.365: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Running", Reason="", readiness=false. Elapsed: 16.193245237s
Feb 13 23:33:58.386: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Running", Reason="", readiness=false. Elapsed: 18.214886967s
Feb 13 23:34:00.409: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Running", Reason="", readiness=false. Elapsed: 20.237486198s
Feb 13 23:34:02.431: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Running", Reason="", readiness=false. Elapsed: 22.25931877s
Feb 13 23:34:04.452: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Running", Reason="", readiness=false. Elapsed: 24.281127385s
Feb 13 23:34:35.118: INFO: Pod "pod-subpath-test-configmap-t4hl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 54.946915469s
STEP: Saw pod success
Feb 13 23:34:35.118: INFO: Pod "pod-subpath-test-configmap-t4hl" satisfied condition "success or failure"
Feb 13 23:34:35.139: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-subpath-test-configmap-t4hl container test-container-subpath-configmap-t4hl: <nil>
STEP: delete the pod
Feb 13 23:34:35.232: INFO: Waiting for pod pod-subpath-test-configmap-t4hl to disappear
Feb 13 23:34:35.253: INFO: Pod pod-subpath-test-configmap-t4hl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-t4hl
Feb 13 23:34:35.253: INFO: Deleting pod "pod-subpath-test-configmap-t4hl" in namespace "e2e-tests-subpath-cnglm"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:34:35.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-cnglm" for this suite.
Feb 13 23:34:41.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:34:41.777: INFO: namespace: e2e-tests-subpath-cnglm, resource: bindings, ignored listing per whitelist
Feb 13 23:34:42.194: INFO: namespace e2e-tests-subpath-cnglm deletion completed in 6.898969049s

• [SLOW TEST:63.011 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:34:42.194: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-kwg4s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 13 23:34:43.319: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kwg4s,SelfLink:/api/v1/namespaces/e2e-tests-watch-kwg4s/configmaps/e2e-watch-test-label-changed,UID:f09bbb6d-2fe7-11e9-9f3f-da917295d151,ResourceVersion:16784,Generation:0,CreationTimestamp:2019-02-13 23:34:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 13 23:34:43.319: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kwg4s,SelfLink:/api/v1/namespaces/e2e-tests-watch-kwg4s/configmaps/e2e-watch-test-label-changed,UID:f09bbb6d-2fe7-11e9-9f3f-da917295d151,ResourceVersion:16785,Generation:0,CreationTimestamp:2019-02-13 23:34:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 13 23:34:43.319: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kwg4s,SelfLink:/api/v1/namespaces/e2e-tests-watch-kwg4s/configmaps/e2e-watch-test-label-changed,UID:f09bbb6d-2fe7-11e9-9f3f-da917295d151,ResourceVersion:16786,Generation:0,CreationTimestamp:2019-02-13 23:34:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 13 23:34:53.479: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kwg4s,SelfLink:/api/v1/namespaces/e2e-tests-watch-kwg4s/configmaps/e2e-watch-test-label-changed,UID:f09bbb6d-2fe7-11e9-9f3f-da917295d151,ResourceVersion:16808,Generation:0,CreationTimestamp:2019-02-13 23:34:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 13 23:34:53.479: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kwg4s,SelfLink:/api/v1/namespaces/e2e-tests-watch-kwg4s/configmaps/e2e-watch-test-label-changed,UID:f09bbb6d-2fe7-11e9-9f3f-da917295d151,ResourceVersion:16809,Generation:0,CreationTimestamp:2019-02-13 23:34:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 13 23:34:53.479: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kwg4s,SelfLink:/api/v1/namespaces/e2e-tests-watch-kwg4s/configmaps/e2e-watch-test-label-changed,UID:f09bbb6d-2fe7-11e9-9f3f-da917295d151,ResourceVersion:16810,Generation:0,CreationTimestamp:2019-02-13 23:34:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:34:53.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-kwg4s" for this suite.
Feb 13 23:34:59.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:35:00.333: INFO: namespace: e2e-tests-watch-kwg4s, resource: bindings, ignored listing per whitelist
Feb 13 23:35:00.375: INFO: namespace e2e-tests-watch-kwg4s deletion completed in 6.873796923s

• [SLOW TEST:18.181 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:35:00.375: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-t4sjl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:35:05.560: INFO: Waiting up to 5m0s for pod "client-envvars-fde84cfd-2fe7-11e9-9de3-466a6591423c" in namespace "e2e-tests-pods-t4sjl" to be "success or failure"
Feb 13 23:35:05.583: INFO: Pod "client-envvars-fde84cfd-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.866253ms
Feb 13 23:35:07.605: INFO: Pod "client-envvars-fde84cfd-2fe7-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045480814s
Feb 13 23:35:09.627: INFO: Pod "client-envvars-fde84cfd-2fe7-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066904301s
STEP: Saw pod success
Feb 13 23:35:09.627: INFO: Pod "client-envvars-fde84cfd-2fe7-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:35:09.647: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod client-envvars-fde84cfd-2fe7-11e9-9de3-466a6591423c container env3cont: <nil>
STEP: delete the pod
Feb 13 23:35:09.710: INFO: Waiting for pod client-envvars-fde84cfd-2fe7-11e9-9de3-466a6591423c to disappear
Feb 13 23:35:09.753: INFO: Pod client-envvars-fde84cfd-2fe7-11e9-9de3-466a6591423c no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:35:09.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-t4sjl" for this suite.
Feb 13 23:35:57.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:35:58.153: INFO: namespace: e2e-tests-pods-t4sjl, resource: bindings, ignored listing per whitelist
Feb 13 23:35:58.703: INFO: namespace e2e-tests-pods-t4sjl deletion completed in 48.928848876s

• [SLOW TEST:58.328 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:35:58.703: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-k8jhb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-k8jhb
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-k8jhb to expose endpoints map[]
Feb 13 23:35:59.818: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-k8jhb exposes endpoints map[] (20.060793ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-k8jhb
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-k8jhb to expose endpoints map[pod1:[80]]
Feb 13 23:36:03.022: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-k8jhb exposes endpoints map[pod1:[80]] (3.180230774s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-k8jhb
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-k8jhb to expose endpoints map[pod1:[80] pod2:[80]]
Feb 13 23:36:07.366: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-k8jhb exposes endpoints map[pod2:[80] pod1:[80]] (4.32210569s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-k8jhb
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-k8jhb to expose endpoints map[pod2:[80]]
Feb 13 23:36:07.430: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-k8jhb exposes endpoints map[pod2:[80]] (41.454324ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-k8jhb
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-k8jhb to expose endpoints map[]
Feb 13 23:36:07.473: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-k8jhb exposes endpoints map[] (20.575434ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:36:07.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-k8jhb" for this suite.
Feb 13 23:36:31.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:36:32.275: INFO: namespace: e2e-tests-services-k8jhb, resource: bindings, ignored listing per whitelist
Feb 13 23:36:32.399: INFO: namespace e2e-tests-services-k8jhb deletion completed in 24.865326924s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:33.697 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:36:32.400: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-9pj7f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 13 23:36:33.328: INFO: Waiting up to 5m0s for pod "pod-32395a6a-2fe8-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-9pj7f" to be "success or failure"
Feb 13 23:36:33.349: INFO: Pod "pod-32395a6a-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.595197ms
Feb 13 23:36:35.371: INFO: Pod "pod-32395a6a-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042631827s
Feb 13 23:36:37.392: INFO: Pod "pod-32395a6a-2fe8-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064174814s
STEP: Saw pod success
Feb 13 23:36:37.392: INFO: Pod "pod-32395a6a-2fe8-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:36:37.415: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-32395a6a-2fe8-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 23:36:37.511: INFO: Waiting for pod pod-32395a6a-2fe8-11e9-9de3-466a6591423c to disappear
Feb 13 23:36:37.531: INFO: Pod pod-32395a6a-2fe8-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:36:37.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9pj7f" for this suite.
Feb 13 23:36:43.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:36:44.372: INFO: namespace: e2e-tests-emptydir-9pj7f, resource: bindings, ignored listing per whitelist
Feb 13 23:36:44.433: INFO: namespace e2e-tests-emptydir-9pj7f deletion completed in 6.880686679s

• [SLOW TEST:12.034 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:36:44.433: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-cn4k9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:36:45.399: INFO: Creating deployment "nginx-deployment"
Feb 13 23:36:45.421: INFO: Waiting for observed generation 1
Feb 13 23:36:47.468: INFO: Waiting for all required pods to come up
Feb 13 23:36:47.490: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 13 23:36:55.534: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 13 23:36:55.576: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 13 23:36:55.626: INFO: Updating deployment nginx-deployment
Feb 13 23:36:55.626: INFO: Waiting for observed generation 2
Feb 13 23:36:57.680: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 13 23:36:57.701: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 13 23:36:57.721: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 13 23:36:57.787: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 13 23:36:57.787: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 13 23:36:57.807: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 13 23:36:57.848: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 13 23:36:57.848: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 13 23:36:57.890: INFO: Updating deployment nginx-deployment
Feb 13 23:36:57.890: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 13 23:36:57.940: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 13 23:37:00.049: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 13 23:37:00.092: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cn4k9/deployments/nginx-deployment,UID:396f8347-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17322,Generation:3,CreationTimestamp:2019-02-13 23:36:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-02-13 23:36:57 +0000 UTC 2019-02-13 23:36:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-13 23:36:58 +0000 UTC 2019-02-13 23:36:45 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 13 23:37:00.113: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cn4k9/replicasets/nginx-deployment-65bbdb5f8,UID:3f8621c8-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17319,Generation:3,CreationTimestamp:2019-02-13 23:36:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 396f8347-2fe8-11e9-9f3f-da917295d151 0xc001084767 0xc001084768}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 13 23:37:00.113: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 13 23:37:00.113: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-cn4k9/replicasets/nginx-deployment-555b55d965,UID:3971bf4e-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17315,Generation:3,CreationTimestamp:2019-02-13 23:36:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 396f8347-2fe8-11e9-9f3f-da917295d151 0xc0010846a7 0xc0010846a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 13 23:37:00.137: INFO: Pod "nginx-deployment-555b55d965-2cv6m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2cv6m,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-2cv6m,UID:40e6ba2c-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17302,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001a07f67 0xc001a07f68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a07fd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a07ff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.137: INFO: Pod "nginx-deployment-555b55d965-2g45x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2g45x,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-2g45x,UID:40e0af5f-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17324,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e94060 0xc001e94061}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e940d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e940f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-13 23:36:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.137: INFO: Pod "nginx-deployment-555b55d965-2nz56" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2nz56,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-2nz56,UID:3974c47a-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17194,Generation:0,CreationTimestamp:2019-02-13 23:36:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.28/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e941b0 0xc001e941b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e942d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e942f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.28,StartTime:2019-02-13 23:36:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-13 23:36:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://99d4bae1cc5769cda26b6c49009eb57ab1a4b4c0bde4c0bbaefba4e06b9d9b19}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.137: INFO: Pod "nginx-deployment-555b55d965-4d7tj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4d7tj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-4d7tj,UID:40e3dd87-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17320,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e943b0 0xc001e943b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e94410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e94430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-13 23:36:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.138: INFO: Pod "nginx-deployment-555b55d965-5fm7v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5fm7v,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-5fm7v,UID:40e3f5bc-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17327,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e944e0 0xc001e944e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e94540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e94560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-13 23:36:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.138: INFO: Pod "nginx-deployment-555b55d965-74pb6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-74pb6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-74pb6,UID:397fdb68-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17207,Generation:0,CreationTimestamp:2019-02-13 23:36:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.132/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e94630 0xc001e94631}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e94690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e946b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.132,StartTime:2019-02-13 23:36:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-13 23:36:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://564e32f576cff6fa4c0c772ca6bc0c211d49f3e6079c16b485bc3429a8c4de9c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.138: INFO: Pod "nginx-deployment-555b55d965-b82w6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-b82w6,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-b82w6,UID:397fce1c-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17196,Generation:0,CreationTimestamp:2019-02-13 23:36:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.27/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e94860 0xc001e94861}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e948c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e948e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.27,StartTime:2019-02-13 23:36:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-13 23:36:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://0869f796a4f9ebdbd9d1465067ee4ecac10acb12223d7974f75b0ba2413fb15b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.138: INFO: Pod "nginx-deployment-555b55d965-cwwnk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cwwnk,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-cwwnk,UID:397fdaec-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17210,Generation:0,CreationTimestamp:2019-02-13 23:36:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.129/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e949d0 0xc001e949d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e94a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e94a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.129,StartTime:2019-02-13 23:36:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-13 23:36:52 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://30807a5edd8b961ca7a9e4fcd33d0061ecb18349bd325aea30ee0200f7b47cf5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.138: INFO: Pod "nginx-deployment-555b55d965-dcrsp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dcrsp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-dcrsp,UID:40e6cdc3-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17330,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e94b60 0xc001e94b61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e94bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e94be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-13 23:36:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.138: INFO: Pod "nginx-deployment-555b55d965-dzz8k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dzz8k,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-dzz8k,UID:40df47ea-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17321,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e94cc0 0xc001e94cc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e94d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e94d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-13 23:36:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.138: INFO: Pod "nginx-deployment-555b55d965-jkqkt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jkqkt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-jkqkt,UID:3976311e-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17195,Generation:0,CreationTimestamp:2019-02-13 23:36:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.29/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e94e10 0xc001e94e11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e94e90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e94eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.29,StartTime:2019-02-13 23:36:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-13 23:36:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://e98a55d25e06b884fbfd66ee7c2d08cb3495ec2374e92ed1b7dfcd105fca1fa6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.138: INFO: Pod "nginx-deployment-555b55d965-jqc6x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jqc6x,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-jqc6x,UID:40e6f0e6-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17331,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e94f70 0xc001e94f71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e94fd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e95060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-13 23:36:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.139: INFO: Pod "nginx-deployment-555b55d965-kv4pw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kv4pw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-kv4pw,UID:39740a44-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17213,Generation:0,CreationTimestamp:2019-02-13 23:36:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.128/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e95120 0xc001e95121}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e95180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e951a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.128,StartTime:2019-02-13 23:36:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-13 23:36:52 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://49ac0512adb83fbc0cc21b98050b42a998fc0bcc3420ae02d6c6f04e1cdb9a67}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.139: INFO: Pod "nginx-deployment-555b55d965-mzpfq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mzpfq,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-mzpfq,UID:39763096-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17191,Generation:0,CreationTimestamp:2019-02-13 23:36:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.26/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e95280 0xc001e95281}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e952e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e95300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.26,StartTime:2019-02-13 23:36:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-13 23:36:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://9e42e37ad0f1a0986e05055e4ee5ade400be11d3bee1f7147cf61db963fa8210}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.139: INFO: Pod "nginx-deployment-555b55d965-n5qxv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-n5qxv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-n5qxv,UID:40e0b1a1-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17317,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e953c0 0xc001e953c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e95420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e95440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-13 23:36:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.139: INFO: Pod "nginx-deployment-555b55d965-ncmz2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ncmz2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-ncmz2,UID:39763484-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17216,Generation:0,CreationTimestamp:2019-02-13 23:36:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.133/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e95500 0xc001e95501}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e95560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e95580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:45 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.133,StartTime:2019-02-13 23:36:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-13 23:36:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://f318a764d4d351d3a698514d6427dbf5d0adcf8f4c2ba7bf6269a9a53652e10e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.139: INFO: Pod "nginx-deployment-555b55d965-q4mm8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-q4mm8,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-q4mm8,UID:40e6cfa3-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17309,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e95640 0xc001e95641}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e956a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e956c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.139: INFO: Pod "nginx-deployment-555b55d965-t99lz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-t99lz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-t99lz,UID:40e3e507-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17325,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e95740 0xc001e95741}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e957a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e957c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-13 23:36:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.139: INFO: Pod "nginx-deployment-555b55d965-vcqg9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-vcqg9,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-vcqg9,UID:40e3fd02-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17326,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e95870 0xc001e95871}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e958d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e958f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-13 23:36:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.139: INFO: Pod "nginx-deployment-555b55d965-z8qtn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-z8qtn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-555b55d965-z8qtn,UID:40e6f766-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17332,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 3971bf4e-2fe8-11e9-9f3f-da917295d151 0xc001e959a0 0xc001e959a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e95b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e95b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-13 23:36:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.140: INFO: Pod "nginx-deployment-65bbdb5f8-2k47w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-2k47w,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-2k47w,UID:40e3f2af-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17329,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc001e95c10 0xc001e95c11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e95cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e95cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-13 23:36:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.140: INFO: Pod "nginx-deployment-65bbdb5f8-8zv5s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-8zv5s,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-8zv5s,UID:3f8a8e38-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17246,Generation:0,CreationTimestamp:2019-02-13 23:36:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc001e95db0 0xc001e95db1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e95e90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e95eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-13 23:36:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.140: INFO: Pod "nginx-deployment-65bbdb5f8-bb5hd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-bb5hd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-bb5hd,UID:4102f0b5-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17316,Generation:0,CreationTimestamp:2019-02-13 23:36:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc001e95f70 0xc001e95f71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001480080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0014800a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.140: INFO: Pod "nginx-deployment-65bbdb5f8-btblg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-btblg,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-btblg,UID:3fa53590-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17266,Generation:0,CreationTimestamp:2019-02-13 23:36:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.31/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc001480120 0xc001480121}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001480190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001480290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-13 23:36:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.140: INFO: Pod "nginx-deployment-65bbdb5f8-cdbcs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-cdbcs,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-cdbcs,UID:40e6c700-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17308,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc001480350 0xc001480351}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0014803c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0014803e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.140: INFO: Pod "nginx-deployment-65bbdb5f8-dwl49" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dwl49,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-dwl49,UID:3f8a87a3-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17334,Generation:0,CreationTimestamp:2019-02-13 23:36:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.30/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc0014804e0 0xc0014804e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001480550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001480570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.30,StartTime:2019-02-13 23:36:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.140: INFO: Pod "nginx-deployment-65bbdb5f8-g4692" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-g4692,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-g4692,UID:40f2fc81-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17333,Generation:0,CreationTimestamp:2019-02-13 23:36:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc001480650 0xc001480651}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0014806c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0014806e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-13 23:36:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.141: INFO: Pod "nginx-deployment-65bbdb5f8-hbxxz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-hbxxz,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-hbxxz,UID:3fab0356-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17262,Generation:0,CreationTimestamp:2019-02-13 23:36:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc0014807a0 0xc0014807a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001480850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001480880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-13 23:36:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.141: INFO: Pod "nginx-deployment-65bbdb5f8-hjmqj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-hjmqj,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-hjmqj,UID:3f8a2079-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17242,Generation:0,CreationTimestamp:2019-02-13 23:36:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc001480940 0xc001480941}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001480a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001480a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-13 23:36:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.141: INFO: Pod "nginx-deployment-65bbdb5f8-lwgkp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-lwgkp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-lwgkp,UID:40e704f1-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17307,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc001480b30 0xc001480b31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001480c10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001480c30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.141: INFO: Pod "nginx-deployment-65bbdb5f8-sczqq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-sczqq,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-sczqq,UID:40e6bd51-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17303,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc001480cb0 0xc001480cb1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001480d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001480d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.141: INFO: Pod "nginx-deployment-65bbdb5f8-tff78" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-tff78,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-tff78,UID:40e40500-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17336,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc0014810f0 0xc0014810f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001481160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001481180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:37:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:37:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:37:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-13 23:37:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 13 23:37:00.141: INFO: Pod "nginx-deployment-65bbdb5f8-v2xcp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-v2xcp,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-cn4k9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-cn4k9/pods/nginx-deployment-65bbdb5f8-v2xcp,UID:40e0a33d-2fe8-11e9-9f3f-da917295d151,ResourceVersion:17305,Generation:0,CreationTimestamp:2019-02-13 23:36:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 3f8621c8-2fe8-11e9-9f3f-da917295d151 0xc0014812a0 0xc0014812a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7glmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7glmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7glmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001481310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001481330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:36:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-13 23:36:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:37:00.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-cn4k9" for this suite.
Feb 13 23:37:08.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:37:08.714: INFO: namespace: e2e-tests-deployment-cn4k9, resource: bindings, ignored listing per whitelist
Feb 13 23:37:09.088: INFO: namespace e2e-tests-deployment-cn4k9 deletion completed in 8.925218128s

• [SLOW TEST:24.654 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:37:09.088: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-77vbf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 13 23:37:10.133: INFO: Waiting up to 5m0s for pod "pod-4829505e-2fe8-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-77vbf" to be "success or failure"
Feb 13 23:37:10.165: INFO: Pod "pod-4829505e-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.982519ms
Feb 13 23:37:12.186: INFO: Pod "pod-4829505e-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052210873s
Feb 13 23:37:14.207: INFO: Pod "pod-4829505e-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073310584s
Feb 13 23:37:16.228: INFO: Pod "pod-4829505e-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094579065s
Feb 13 23:37:18.250: INFO: Pod "pod-4829505e-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116405505s
Feb 13 23:37:20.271: INFO: Pod "pod-4829505e-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.13779143s
Feb 13 23:37:22.293: INFO: Pod "pod-4829505e-2fe8-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.159620189s
STEP: Saw pod success
Feb 13 23:37:22.293: INFO: Pod "pod-4829505e-2fe8-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:37:22.315: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-4829505e-2fe8-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 23:37:22.379: INFO: Waiting for pod pod-4829505e-2fe8-11e9-9de3-466a6591423c to disappear
Feb 13 23:37:22.407: INFO: Pod pod-4829505e-2fe8-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:37:22.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-77vbf" for this suite.
Feb 13 23:37:28.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:37:28.592: INFO: namespace: e2e-tests-emptydir-77vbf, resource: bindings, ignored listing per whitelist
Feb 13 23:37:29.308: INFO: namespace e2e-tests-emptydir-77vbf deletion completed in 6.879796827s

• [SLOW TEST:20.221 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:37:29.309: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-m7fn2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 13 23:37:30.316: INFO: Waiting up to 5m0s for pod "pod-5430ea0a-2fe8-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-m7fn2" to be "success or failure"
Feb 13 23:37:30.337: INFO: Pod "pod-5430ea0a-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.054605ms
Feb 13 23:37:32.359: INFO: Pod "pod-5430ea0a-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042280953s
Feb 13 23:37:34.380: INFO: Pod "pod-5430ea0a-2fe8-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063657779s
STEP: Saw pod success
Feb 13 23:37:34.380: INFO: Pod "pod-5430ea0a-2fe8-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:37:34.401: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-5430ea0a-2fe8-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 23:37:34.464: INFO: Waiting for pod pod-5430ea0a-2fe8-11e9-9de3-466a6591423c to disappear
Feb 13 23:37:34.485: INFO: Pod pod-5430ea0a-2fe8-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:37:34.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-m7fn2" for this suite.
Feb 13 23:37:40.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:37:41.380: INFO: namespace: e2e-tests-emptydir-m7fn2, resource: bindings, ignored listing per whitelist
Feb 13 23:37:41.404: INFO: namespace e2e-tests-emptydir-m7fn2 deletion completed in 6.898190291s

• [SLOW TEST:12.096 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:37:41.405: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8h9sj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:37:42.611: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b850363-2fe8-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-8h9sj" to be "success or failure"
Feb 13 23:37:42.631: INFO: Pod "downwardapi-volume-5b850363-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.4333ms
Feb 13 23:37:44.652: INFO: Pod "downwardapi-volume-5b850363-2fe8-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041487206s
Feb 13 23:37:46.674: INFO: Pod "downwardapi-volume-5b850363-2fe8-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062746725s
STEP: Saw pod success
Feb 13 23:37:46.674: INFO: Pod "downwardapi-volume-5b850363-2fe8-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:37:46.694: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-5b850363-2fe8-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 13 23:37:46.752: INFO: Waiting for pod downwardapi-volume-5b850363-2fe8-11e9-9de3-466a6591423c to disappear
Feb 13 23:37:46.774: INFO: Pod downwardapi-volume-5b850363-2fe8-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:37:46.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8h9sj" for this suite.
Feb 13 23:37:52.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:37:53.701: INFO: namespace: e2e-tests-downward-api-8h9sj, resource: bindings, ignored listing per whitelist
Feb 13 23:37:53.742: INFO: namespace e2e-tests-downward-api-8h9sj deletion completed in 6.946788943s

• [SLOW TEST:12.337 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:37:53.742: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-697td
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-697td
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 13 23:37:54.774: INFO: Found 1 stateful pods, waiting for 3
Feb 13 23:38:04.797: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:38:04.797: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:38:04.797: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 13 23:38:14.796: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:38:14.797: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:38:14.797: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 13 23:38:14.912: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 13 23:38:15.007: INFO: Updating stateful set ss2
Feb 13 23:38:15.053: INFO: Waiting for Pod e2e-tests-statefulset-697td/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 13 23:38:25.095: INFO: Waiting for Pod e2e-tests-statefulset-697td/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 13 23:38:35.244: INFO: Found 2 stateful pods, waiting for 3
Feb 13 23:38:45.266: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:38:45.266: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:38:45.266: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 13 23:38:45.361: INFO: Updating stateful set ss2
Feb 13 23:38:45.403: INFO: Waiting for Pod e2e-tests-statefulset-697td/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 13 23:38:55.446: INFO: Waiting for Pod e2e-tests-statefulset-697td/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 13 23:39:05.499: INFO: Updating stateful set ss2
Feb 13 23:39:05.545: INFO: Waiting for StatefulSet e2e-tests-statefulset-697td/ss2 to complete update
Feb 13 23:39:05.545: INFO: Waiting for Pod e2e-tests-statefulset-697td/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 13 23:39:15.588: INFO: Deleting all statefulset in ns e2e-tests-statefulset-697td
Feb 13 23:39:15.609: INFO: Scaling statefulset ss2 to 0
Feb 13 23:39:45.700: INFO: Waiting for statefulset status.replicas updated to 0
Feb 13 23:39:45.720: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:39:45.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-697td" for this suite.
Feb 13 23:39:51.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:39:52.429: INFO: namespace: e2e-tests-statefulset-697td, resource: bindings, ignored listing per whitelist
Feb 13 23:39:52.697: INFO: namespace e2e-tests-statefulset-697td deletion completed in 6.889842991s

• [SLOW TEST:118.955 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:39:52.698: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-cwc9h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 13 23:39:54.024: INFO: Number of nodes with available pods: 0
Feb 13 23:39:54.024: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:39:55.068: INFO: Number of nodes with available pods: 0
Feb 13 23:39:55.068: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:39:56.068: INFO: Number of nodes with available pods: 0
Feb 13 23:39:56.068: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 13 23:39:57.068: INFO: Number of nodes with available pods: 2
Feb 13 23:39:57.068: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 13 23:39:57.176: INFO: Number of nodes with available pods: 1
Feb 13 23:39:57.176: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:39:58.221: INFO: Number of nodes with available pods: 1
Feb 13 23:39:58.221: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:39:59.218: INFO: Number of nodes with available pods: 1
Feb 13 23:39:59.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:00.223: INFO: Number of nodes with available pods: 1
Feb 13 23:40:00.223: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:01.220: INFO: Number of nodes with available pods: 1
Feb 13 23:40:01.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:02.218: INFO: Number of nodes with available pods: 1
Feb 13 23:40:02.218: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:03.220: INFO: Number of nodes with available pods: 1
Feb 13 23:40:03.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:04.218: INFO: Number of nodes with available pods: 1
Feb 13 23:40:04.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:05.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:05.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:06.221: INFO: Number of nodes with available pods: 1
Feb 13 23:40:06.221: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:07.220: INFO: Number of nodes with available pods: 1
Feb 13 23:40:07.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:08.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:08.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:09.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:09.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:10.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:10.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:11.221: INFO: Number of nodes with available pods: 1
Feb 13 23:40:11.221: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:12.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:12.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:13.220: INFO: Number of nodes with available pods: 1
Feb 13 23:40:13.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:14.218: INFO: Number of nodes with available pods: 1
Feb 13 23:40:14.218: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:15.220: INFO: Number of nodes with available pods: 1
Feb 13 23:40:15.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:16.220: INFO: Number of nodes with available pods: 1
Feb 13 23:40:16.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:17.220: INFO: Number of nodes with available pods: 1
Feb 13 23:40:17.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:18.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:18.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:19.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:19.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:20.220: INFO: Number of nodes with available pods: 1
Feb 13 23:40:20.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:21.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:21.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:22.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:22.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:23.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:23.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:24.223: INFO: Number of nodes with available pods: 1
Feb 13 23:40:24.223: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:25.220: INFO: Number of nodes with available pods: 1
Feb 13 23:40:25.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:26.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:26.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:27.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:27.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:28.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:28.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:29.225: INFO: Number of nodes with available pods: 1
Feb 13 23:40:29.225: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:30.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:30.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:31.220: INFO: Number of nodes with available pods: 1
Feb 13 23:40:31.220: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:32.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:32.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:33.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:33.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:34.219: INFO: Number of nodes with available pods: 1
Feb 13 23:40:34.219: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp is running more than one daemon pod
Feb 13 23:40:35.220: INFO: Number of nodes with available pods: 2
Feb 13 23:40:35.220: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-cwc9h, will wait for the garbage collector to delete the pods
Feb 13 23:40:35.336: INFO: Deleting DaemonSet.extensions daemon-set took: 23.28613ms
Feb 13 23:40:35.436: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.336893ms
Feb 13 23:41:15.269: INFO: Number of nodes with available pods: 0
Feb 13 23:41:15.269: INFO: Number of running nodes: 0, number of available pods: 0
Feb 13 23:41:15.290: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-cwc9h/daemonsets","resourceVersion":"18193"},"items":null}

Feb 13 23:41:15.310: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-cwc9h/pods","resourceVersion":"18193"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:41:15.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-cwc9h" for this suite.
Feb 13 23:41:21.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:41:21.908: INFO: namespace: e2e-tests-daemonsets-cwc9h, resource: bindings, ignored listing per whitelist
Feb 13 23:41:22.358: INFO: namespace e2e-tests-daemonsets-cwc9h deletion completed in 6.963499726s

• [SLOW TEST:89.661 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:41:22.359: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-26rh9
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-df18e5ab-2fe8-11e9-9de3-466a6591423c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:41:29.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-26rh9" for this suite.
Feb 13 23:41:51.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:41:52.231: INFO: namespace: e2e-tests-configmap-26rh9, resource: bindings, ignored listing per whitelist
Feb 13 23:41:52.501: INFO: namespace e2e-tests-configmap-26rh9 deletion completed in 22.909800114s

• [SLOW TEST:30.142 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:41:52.501: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-hhkf9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:41:53.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-hhkf9" for this suite.
Feb 13 23:42:15.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:42:15.786: INFO: namespace: e2e-tests-pods-hhkf9, resource: bindings, ignored listing per whitelist
Feb 13 23:42:16.535: INFO: namespace e2e-tests-pods-hhkf9 deletion completed in 22.95859581s

• [SLOW TEST:24.035 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:42:16.536: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-tckw9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 13 23:42:17.587: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-tckw9'
Feb 13 23:42:18.873: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 13 23:42:18.873: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Feb 13 23:42:20.940: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-tckw9'
Feb 13 23:42:21.152: INFO: stderr: ""
Feb 13 23:42:21.152: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:42:21.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tckw9" for this suite.
Feb 13 23:42:45.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:42:45.937: INFO: namespace: e2e-tests-kubectl-tckw9, resource: bindings, ignored listing per whitelist
Feb 13 23:42:46.080: INFO: namespace e2e-tests-kubectl-tckw9 deletion completed in 24.907445127s

• [SLOW TEST:29.545 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:42:46.080: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-zwrpz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 13 23:42:51.211: INFO: Pod pod-hostip-11067786-2fe9-11e9-9de3-466a6591423c has hostIP: 10.250.0.5
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:42:51.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zwrpz" for this suite.
Feb 13 23:43:13.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:43:14.020: INFO: namespace: e2e-tests-pods-zwrpz, resource: bindings, ignored listing per whitelist
Feb 13 23:43:14.142: INFO: namespace e2e-tests-pods-zwrpz deletion completed in 22.909463714s

• [SLOW TEST:28.061 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:43:14.142: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-kl264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0213 23:43:21.251607   31687 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 23:43:21.251: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:43:21.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-kl264" for this suite.
Feb 13 23:43:27.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:43:28.079: INFO: namespace: e2e-tests-gc-kl264, resource: bindings, ignored listing per whitelist
Feb 13 23:43:28.194: INFO: namespace e2e-tests-gc-kl264 deletion completed in 6.922122815s

• [SLOW TEST:14.052 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:43:28.195: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-b8nj5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 13 23:43:33.890: INFO: Successfully updated pod "annotationupdate2a1d414f-2fe9-11e9-9de3-466a6591423c"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:43:35.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-b8nj5" for this suite.
Feb 13 23:43:58.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:43:58.340: INFO: namespace: e2e-tests-projected-b8nj5, resource: bindings, ignored listing per whitelist
Feb 13 23:43:58.951: INFO: namespace e2e-tests-projected-b8nj5 deletion completed in 22.968867711s

• [SLOW TEST:30.756 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:43:58.951: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-tlqw4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-vl59t in namespace e2e-tests-proxy-tlqw4
I0213 23:43:59.960105   31687 runners.go:184] Created replication controller with name: proxy-service-vl59t, namespace: e2e-tests-proxy-tlqw4, replica count: 1
I0213 23:44:01.010612   31687 runners.go:184] proxy-service-vl59t Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:44:02.010868   31687 runners.go:184] proxy-service-vl59t Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:44:03.011176   31687 runners.go:184] proxy-service-vl59t Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:44:04.011464   31687 runners.go:184] proxy-service-vl59t Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:44:05.011700   31687 runners.go:184] proxy-service-vl59t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0213 23:44:06.011995   31687 runners.go:184] proxy-service-vl59t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0213 23:44:07.012256   31687 runners.go:184] proxy-service-vl59t Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 13 23:44:07.033: INFO: setup took 7.128563128s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 13 23:44:07.111: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 78.495507ms)
Feb 13 23:44:07.117: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 83.907388ms)
Feb 13 23:44:07.117: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 83.960599ms)
Feb 13 23:44:07.117: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 84.13097ms)
Feb 13 23:44:07.117: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 84.053116ms)
Feb 13 23:44:07.117: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 84.118508ms)
Feb 13 23:44:07.117: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 84.09367ms)
Feb 13 23:44:07.118: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 84.732262ms)
Feb 13 23:44:07.119: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 86.137908ms)
Feb 13 23:44:07.119: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 86.243741ms)
Feb 13 23:44:07.119: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 86.202602ms)
Feb 13 23:44:07.128: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 95.110161ms)
Feb 13 23:44:07.130: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 96.978026ms)
Feb 13 23:44:07.134: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 100.879937ms)
Feb 13 23:44:07.134: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 101.002856ms)
Feb 13 23:44:07.138: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 105.762116ms)
Feb 13 23:44:07.164: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 25.336311ms)
Feb 13 23:44:07.165: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 25.207798ms)
Feb 13 23:44:07.165: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 25.477693ms)
Feb 13 23:44:07.165: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 26.079797ms)
Feb 13 23:44:07.165: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 25.572065ms)
Feb 13 23:44:07.165: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 25.686152ms)
Feb 13 23:44:07.165: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 25.824985ms)
Feb 13 23:44:07.167: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 27.150545ms)
Feb 13 23:44:07.167: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 27.12672ms)
Feb 13 23:44:07.167: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 27.894567ms)
Feb 13 23:44:07.167: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 27.9737ms)
Feb 13 23:44:07.167: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 27.295738ms)
Feb 13 23:44:07.214: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 75.677444ms)
Feb 13 23:44:07.214: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 75.510516ms)
Feb 13 23:44:07.214: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 75.135308ms)
Feb 13 23:44:07.214: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 75.246522ms)
Feb 13 23:44:07.239: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 23.81949ms)
Feb 13 23:44:07.239: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 23.815065ms)
Feb 13 23:44:07.239: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 23.638982ms)
Feb 13 23:44:07.239: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 23.530066ms)
Feb 13 23:44:07.239: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 24.509785ms)
Feb 13 23:44:07.239: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 24.288203ms)
Feb 13 23:44:07.241: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 25.356254ms)
Feb 13 23:44:07.241: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 25.1858ms)
Feb 13 23:44:07.241: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 25.068594ms)
Feb 13 23:44:07.243: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 28.246949ms)
Feb 13 23:44:07.243: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 27.774832ms)
Feb 13 23:44:07.243: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 28.109354ms)
Feb 13 23:44:07.243: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 27.948497ms)
Feb 13 23:44:07.245: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 30.296773ms)
Feb 13 23:44:07.245: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 30.031235ms)
Feb 13 23:44:07.245: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 29.398707ms)
Feb 13 23:44:07.270: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 23.855916ms)
Feb 13 23:44:07.270: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 24.01521ms)
Feb 13 23:44:07.270: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.12129ms)
Feb 13 23:44:07.270: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 24.365952ms)
Feb 13 23:44:07.270: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 24.642064ms)
Feb 13 23:44:07.271: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 25.347834ms)
Feb 13 23:44:07.271: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 26.488884ms)
Feb 13 23:44:07.271: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 25.511024ms)
Feb 13 23:44:07.271: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 26.083906ms)
Feb 13 23:44:07.274: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 28.489372ms)
Feb 13 23:44:07.274: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 28.604472ms)
Feb 13 23:44:07.274: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 27.663431ms)
Feb 13 23:44:07.274: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 28.350463ms)
Feb 13 23:44:07.274: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 28.076565ms)
Feb 13 23:44:07.275: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 29.368818ms)
Feb 13 23:44:07.277: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 31.553514ms)
Feb 13 23:44:07.303: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 24.220168ms)
Feb 13 23:44:07.303: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 24.508582ms)
Feb 13 23:44:07.303: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 24.223807ms)
Feb 13 23:44:07.303: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.645585ms)
Feb 13 23:44:07.303: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.182964ms)
Feb 13 23:44:07.303: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 25.058785ms)
Feb 13 23:44:07.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 31.187509ms)
Feb 13 23:44:07.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 31.364096ms)
Feb 13 23:44:07.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 31.623867ms)
Feb 13 23:44:07.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 30.67888ms)
Feb 13 23:44:07.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 30.153818ms)
Feb 13 23:44:07.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 30.302534ms)
Feb 13 23:44:07.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 30.961655ms)
Feb 13 23:44:07.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 31.057442ms)
Feb 13 23:44:07.309: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 30.667133ms)
Feb 13 23:44:07.310: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 31.367668ms)
Feb 13 23:44:07.335: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.172293ms)
Feb 13 23:44:07.335: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 23.903661ms)
Feb 13 23:44:07.335: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 23.850009ms)
Feb 13 23:44:07.335: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 24.381573ms)
Feb 13 23:44:07.335: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 24.819797ms)
Feb 13 23:44:07.335: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 24.766036ms)
Feb 13 23:44:07.337: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 26.065553ms)
Feb 13 23:44:07.337: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 25.777873ms)
Feb 13 23:44:07.337: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 25.842403ms)
Feb 13 23:44:07.337: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 26.18498ms)
Feb 13 23:44:07.337: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 25.57802ms)
Feb 13 23:44:07.338: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 27.110451ms)
Feb 13 23:44:07.341: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 29.466921ms)
Feb 13 23:44:07.341: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 29.698206ms)
Feb 13 23:44:07.341: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 30.058733ms)
Feb 13 23:44:07.342: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 31.14297ms)
Feb 13 23:44:07.379: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 36.123001ms)
Feb 13 23:44:07.379: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 35.892334ms)
Feb 13 23:44:07.379: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 35.75332ms)
Feb 13 23:44:07.379: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 35.887935ms)
Feb 13 23:44:07.379: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 36.888818ms)
Feb 13 23:44:07.379: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 36.512245ms)
Feb 13 23:44:07.379: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 35.893136ms)
Feb 13 23:44:07.381: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 37.556185ms)
Feb 13 23:44:07.381: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 38.127215ms)
Feb 13 23:44:07.381: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 38.013428ms)
Feb 13 23:44:07.381: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 37.720777ms)
Feb 13 23:44:07.381: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 38.270917ms)
Feb 13 23:44:07.383: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 40.187097ms)
Feb 13 23:44:07.383: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 39.341044ms)
Feb 13 23:44:07.425: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 82.119662ms)
Feb 13 23:44:07.425: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 82.025456ms)
Feb 13 23:44:07.455: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 29.372016ms)
Feb 13 23:44:07.455: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 29.524053ms)
Feb 13 23:44:07.455: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 29.497203ms)
Feb 13 23:44:07.455: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 29.528665ms)
Feb 13 23:44:07.455: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 29.574531ms)
Feb 13 23:44:07.456: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 30.691647ms)
Feb 13 23:44:07.456: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 30.648851ms)
Feb 13 23:44:07.456: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 30.635877ms)
Feb 13 23:44:07.456: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 30.605361ms)
Feb 13 23:44:07.457: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 32.302473ms)
Feb 13 23:44:07.457: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 32.219888ms)
Feb 13 23:44:07.457: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 32.212782ms)
Feb 13 23:44:07.460: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 34.371382ms)
Feb 13 23:44:07.460: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 34.31377ms)
Feb 13 23:44:07.461: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 36.112089ms)
Feb 13 23:44:07.463: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 38.196976ms)
Feb 13 23:44:07.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 25.670543ms)
Feb 13 23:44:07.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 25.714819ms)
Feb 13 23:44:07.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 25.849497ms)
Feb 13 23:44:07.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 25.773673ms)
Feb 13 23:44:07.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 25.855755ms)
Feb 13 23:44:07.491: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 27.617464ms)
Feb 13 23:44:07.491: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 27.610726ms)
Feb 13 23:44:07.491: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 27.665787ms)
Feb 13 23:44:07.491: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 27.634339ms)
Feb 13 23:44:07.491: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 27.812564ms)
Feb 13 23:44:07.491: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 27.613626ms)
Feb 13 23:44:07.493: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 29.257928ms)
Feb 13 23:44:07.493: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 29.416823ms)
Feb 13 23:44:07.493: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 29.359194ms)
Feb 13 23:44:07.495: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 31.674251ms)
Feb 13 23:44:07.495: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 31.660846ms)
Feb 13 23:44:07.520: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 24.165401ms)
Feb 13 23:44:07.520: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 24.157887ms)
Feb 13 23:44:07.520: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.209528ms)
Feb 13 23:44:07.520: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 24.330946ms)
Feb 13 23:44:07.520: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 24.306969ms)
Feb 13 23:44:07.520: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.282022ms)
Feb 13 23:44:07.520: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 24.377174ms)
Feb 13 23:44:07.522: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 26.175896ms)
Feb 13 23:44:07.522: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 26.114547ms)
Feb 13 23:44:07.522: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 26.062401ms)
Feb 13 23:44:07.522: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 26.259312ms)
Feb 13 23:44:07.523: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 27.99291ms)
Feb 13 23:44:07.523: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 27.918479ms)
Feb 13 23:44:07.523: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 28.01558ms)
Feb 13 23:44:07.525: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 29.893686ms)
Feb 13 23:44:07.527: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 31.655937ms)
Feb 13 23:44:07.552: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 24.203365ms)
Feb 13 23:44:07.552: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 24.928726ms)
Feb 13 23:44:07.552: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 24.796888ms)
Feb 13 23:44:07.552: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.881963ms)
Feb 13 23:44:07.552: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.898872ms)
Feb 13 23:44:07.552: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 24.932707ms)
Feb 13 23:44:07.552: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 24.96431ms)
Feb 13 23:44:07.554: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 26.488261ms)
Feb 13 23:44:07.554: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 26.433671ms)
Feb 13 23:44:07.554: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 26.535139ms)
Feb 13 23:44:07.556: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 28.336102ms)
Feb 13 23:44:07.556: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 28.351439ms)
Feb 13 23:44:07.556: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 28.393508ms)
Feb 13 23:44:07.556: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 28.366207ms)
Feb 13 23:44:07.558: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 30.161081ms)
Feb 13 23:44:07.562: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 35.081751ms)
Feb 13 23:44:07.588: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 25.350197ms)
Feb 13 23:44:07.588: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 25.358229ms)
Feb 13 23:44:07.588: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 25.423713ms)
Feb 13 23:44:07.588: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 25.418477ms)
Feb 13 23:44:07.588: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 25.707699ms)
Feb 13 23:44:07.588: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 25.512972ms)
Feb 13 23:44:07.590: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 27.824207ms)
Feb 13 23:44:07.590: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 27.626086ms)
Feb 13 23:44:07.590: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 27.704845ms)
Feb 13 23:44:07.590: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 27.574596ms)
Feb 13 23:44:07.590: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 27.827846ms)
Feb 13 23:44:07.590: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 27.763221ms)
Feb 13 23:44:07.592: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 29.156703ms)
Feb 13 23:44:07.592: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 29.046261ms)
Feb 13 23:44:07.594: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 31.742019ms)
Feb 13 23:44:07.595: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 31.985917ms)
Feb 13 23:44:07.620: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 24.625859ms)
Feb 13 23:44:07.620: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.689729ms)
Feb 13 23:44:07.620: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 24.859625ms)
Feb 13 23:44:07.620: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 24.948386ms)
Feb 13 23:44:07.620: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.779659ms)
Feb 13 23:44:07.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 27.665062ms)
Feb 13 23:44:07.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 27.02084ms)
Feb 13 23:44:07.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 27.080726ms)
Feb 13 23:44:07.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 27.043766ms)
Feb 13 23:44:07.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 27.130691ms)
Feb 13 23:44:07.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 27.159338ms)
Feb 13 23:44:07.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 27.058724ms)
Feb 13 23:44:07.624: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 28.426817ms)
Feb 13 23:44:07.624: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 29.021114ms)
Feb 13 23:44:07.624: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 28.963001ms)
Feb 13 23:44:07.626: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 30.493706ms)
Feb 13 23:44:07.651: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 25.021125ms)
Feb 13 23:44:07.651: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 25.133601ms)
Feb 13 23:44:07.651: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 25.164185ms)
Feb 13 23:44:07.651: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 25.088392ms)
Feb 13 23:44:07.651: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 25.199017ms)
Feb 13 23:44:07.651: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 25.555313ms)
Feb 13 23:44:07.651: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 25.550987ms)
Feb 13 23:44:07.651: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 25.67508ms)
Feb 13 23:44:07.651: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 25.572795ms)
Feb 13 23:44:07.652: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 25.640451ms)
Feb 13 23:44:07.652: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 25.758711ms)
Feb 13 23:44:07.653: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 26.885465ms)
Feb 13 23:44:07.655: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 29.254118ms)
Feb 13 23:44:07.655: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 29.265442ms)
Feb 13 23:44:07.698: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 72.441027ms)
Feb 13 23:44:07.698: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 72.349503ms)
Feb 13 23:44:07.724: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 25.197949ms)
Feb 13 23:44:07.724: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 25.259926ms)
Feb 13 23:44:07.724: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 25.659128ms)
Feb 13 23:44:07.724: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 25.49265ms)
Feb 13 23:44:07.724: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 25.398935ms)
Feb 13 23:44:07.724: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 25.365526ms)
Feb 13 23:44:07.724: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 25.810262ms)
Feb 13 23:44:07.724: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 25.695977ms)
Feb 13 23:44:07.727: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 28.287461ms)
Feb 13 23:44:07.727: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 28.195922ms)
Feb 13 23:44:07.727: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 28.270463ms)
Feb 13 23:44:07.727: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 28.360839ms)
Feb 13 23:44:07.729: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 30.224694ms)
Feb 13 23:44:07.729: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 29.931254ms)
Feb 13 23:44:07.729: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 30.156174ms)
Feb 13 23:44:07.734: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 35.169194ms)
Feb 13 23:44:07.759: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 24.675351ms)
Feb 13 23:44:07.759: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.760984ms)
Feb 13 23:44:07.759: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 24.362813ms)
Feb 13 23:44:07.759: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 24.584965ms)
Feb 13 23:44:07.759: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 25.150791ms)
Feb 13 23:44:07.759: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 24.299299ms)
Feb 13 23:44:07.761: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 27.141852ms)
Feb 13 23:44:07.761: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 27.480457ms)
Feb 13 23:44:07.761: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 26.748886ms)
Feb 13 23:44:07.761: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 27.024256ms)
Feb 13 23:44:07.763: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 28.186885ms)
Feb 13 23:44:07.763: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 28.255537ms)
Feb 13 23:44:07.765: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 30.262773ms)
Feb 13 23:44:07.765: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 30.222138ms)
Feb 13 23:44:07.767: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 32.785179ms)
Feb 13 23:44:07.767: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 33.056274ms)
Feb 13 23:44:07.797: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 30.023988ms)
Feb 13 23:44:07.797: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 29.930737ms)
Feb 13 23:44:07.797: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 30.093728ms)
Feb 13 23:44:07.797: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 29.991865ms)
Feb 13 23:44:07.797: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 29.970697ms)
Feb 13 23:44:07.797: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 30.267928ms)
Feb 13 23:44:07.799: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 31.382646ms)
Feb 13 23:44:07.799: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 31.46147ms)
Feb 13 23:44:07.799: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 31.538605ms)
Feb 13 23:44:07.799: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 31.688162ms)
Feb 13 23:44:07.799: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 31.767745ms)
Feb 13 23:44:07.799: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 31.659061ms)
Feb 13 23:44:07.803: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 35.233022ms)
Feb 13 23:44:07.803: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 35.18403ms)
Feb 13 23:44:07.803: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 35.296526ms)
Feb 13 23:44:07.805: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 38.060271ms)
Feb 13 23:44:07.831: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 25.121447ms)
Feb 13 23:44:07.831: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 25.136359ms)
Feb 13 23:44:07.831: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 25.084477ms)
Feb 13 23:44:07.831: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 25.192781ms)
Feb 13 23:44:07.831: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 25.132837ms)
Feb 13 23:44:07.831: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 25.234078ms)
Feb 13 23:44:07.833: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 27.08318ms)
Feb 13 23:44:07.833: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 27.032294ms)
Feb 13 23:44:07.833: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 27.162121ms)
Feb 13 23:44:07.833: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 26.988157ms)
Feb 13 23:44:07.833: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 27.102196ms)
Feb 13 23:44:07.833: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 27.088333ms)
Feb 13 23:44:07.879: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 73.002651ms)
Feb 13 23:44:07.879: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 73.069789ms)
Feb 13 23:44:07.879: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 72.981724ms)
Feb 13 23:44:07.879: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 72.927311ms)
Feb 13 23:44:07.904: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 24.848284ms)
Feb 13 23:44:07.904: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 25.281715ms)
Feb 13 23:44:07.904: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 25.189293ms)
Feb 13 23:44:07.904: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 25.122242ms)
Feb 13 23:44:07.904: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 25.272632ms)
Feb 13 23:44:07.904: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 25.316791ms)
Feb 13 23:44:07.904: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 25.427844ms)
Feb 13 23:44:07.905: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 26.357582ms)
Feb 13 23:44:07.905: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 26.557587ms)
Feb 13 23:44:07.908: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 28.88358ms)
Feb 13 23:44:07.908: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 28.768773ms)
Feb 13 23:44:07.908: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 28.750553ms)
Feb 13 23:44:07.909: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 30.406685ms)
Feb 13 23:44:07.911: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 32.105699ms)
Feb 13 23:44:07.911: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 32.218724ms)
Feb 13 23:44:07.913: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 34.234518ms)
Feb 13 23:44:07.938: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:1080/proxy/... (200; 24.737509ms)
Feb 13 23:44:07.938: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:460/proxy/: tls baz (200; 24.678316ms)
Feb 13 23:44:07.938: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:160/proxy/: foo (200; 23.989947ms)
Feb 13 23:44:07.938: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:443/proxy/... (200; 25.240845ms)
Feb 13 23:44:07.938: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:1080/proxy/rewri... (200; 24.995323ms)
Feb 13 23:44:07.938: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:160/proxy/: foo (200; 24.857134ms)
Feb 13 23:44:07.940: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname1/proxy/: tls baz (200; 26.106209ms)
Feb 13 23:44:07.940: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/https:proxy-service-vl59t:tlsportname2/proxy/: tls qux (200; 26.072789ms)
Feb 13 23:44:07.940: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t:162/proxy/: bar (200; 26.264106ms)
Feb 13 23:44:07.940: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/http:proxy-service-vl59t-phb4t:162/proxy/: bar (200; 25.717018ms)
Feb 13 23:44:07.940: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/proxy-service-vl59t-phb4t/proxy/rewriteme"... (200; 26.083729ms)
Feb 13 23:44:07.940: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/pods/https:proxy-service-vl59t-phb4t:462/proxy/: tls qux (200; 26.058703ms)
Feb 13 23:44:07.943: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname2/proxy/: bar (200; 30.025245ms)
Feb 13 23:44:07.986: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname2/proxy/: bar (200; 72.642829ms)
Feb 13 23:44:07.986: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/http:proxy-service-vl59t:portname1/proxy/: foo (200; 72.801586ms)
Feb 13 23:44:07.986: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-tlqw4/services/proxy-service-vl59t:portname1/proxy/: foo (200; 72.305598ms)
STEP: deleting ReplicationController proxy-service-vl59t in namespace e2e-tests-proxy-tlqw4, will wait for the garbage collector to delete the pods
Feb 13 23:44:08.084: INFO: Deleting ReplicationController proxy-service-vl59t took: 27.085778ms
Feb 13 23:44:08.185: INFO: Terminating ReplicationController proxy-service-vl59t pods took: 100.249655ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:44:15.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-tlqw4" for this suite.
Feb 13 23:44:21.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:44:21.811: INFO: namespace: e2e-tests-proxy-tlqw4, resource: bindings, ignored listing per whitelist
Feb 13 23:44:22.202: INFO: namespace e2e-tests-proxy-tlqw4 deletion completed in 6.895668629s

• [SLOW TEST:23.251 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:44:22.203: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-bmtnj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:45:23.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-bmtnj" for this suite.
Feb 13 23:45:43.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:45:43.426: INFO: namespace: e2e-tests-container-probe-bmtnj, resource: bindings, ignored listing per whitelist
Feb 13 23:45:44.159: INFO: namespace e2e-tests-container-probe-bmtnj deletion completed in 20.902491525s

• [SLOW TEST:81.956 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:45:44.159: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-h2b6t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-h2b6t
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-h2b6t
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-h2b6t
Feb 13 23:45:45.182: INFO: Found 0 stateful pods, waiting for 1
Feb 13 23:45:55.204: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 13 23:45:55.225: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-h2b6t ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 13 23:45:56.155: INFO: stderr: ""
Feb 13 23:45:56.155: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 13 23:45:56.155: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 13 23:45:56.177: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 13 23:46:06.199: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 13 23:46:06.199: INFO: Waiting for statefulset status.replicas updated to 0
Feb 13 23:46:06.284: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999397s
Feb 13 23:46:07.306: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.978651041s
Feb 13 23:46:08.328: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.9571518s
Feb 13 23:46:09.349: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.935327939s
Feb 13 23:46:10.371: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.914034503s
Feb 13 23:46:11.393: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.891871183s
Feb 13 23:46:12.415: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.86987384s
Feb 13 23:46:13.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.847550422s
Feb 13 23:46:14.460: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.825546841s
Feb 13 23:46:15.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 803.151639ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-h2b6t
Feb 13 23:46:16.503: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-h2b6t ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:46:17.182: INFO: stderr: ""
Feb 13 23:46:17.182: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 13 23:46:17.182: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 13 23:46:17.203: INFO: Found 1 stateful pods, waiting for 3
Feb 13 23:46:27.225: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:46:27.225: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:46:27.225: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 13 23:46:27.268: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-h2b6t ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 13 23:46:27.978: INFO: stderr: ""
Feb 13 23:46:27.978: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 13 23:46:27.978: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 13 23:46:27.978: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-h2b6t ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 13 23:46:28.759: INFO: stderr: ""
Feb 13 23:46:28.760: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 13 23:46:28.760: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 13 23:46:28.760: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-h2b6t ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 13 23:46:29.494: INFO: stderr: ""
Feb 13 23:46:29.494: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 13 23:46:29.494: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 13 23:46:29.494: INFO: Waiting for statefulset status.replicas updated to 0
Feb 13 23:46:29.515: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 13 23:46:39.557: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 13 23:46:39.557: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 13 23:46:39.557: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 13 23:46:39.631: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999578s
Feb 13 23:46:40.652: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.978763517s
Feb 13 23:46:41.675: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.957084006s
Feb 13 23:46:42.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.933898426s
Feb 13 23:46:43.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.911835091s
Feb 13 23:46:44.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.889841634s
Feb 13 23:46:45.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.867913606s
Feb 13 23:46:46.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.846375287s
Feb 13 23:46:47.817: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.814184931s
Feb 13 23:46:48.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 792.176337ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-h2b6t
Feb 13 23:46:49.862: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-h2b6t ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:46:50.528: INFO: stderr: ""
Feb 13 23:46:50.528: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 13 23:46:50.528: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 13 23:46:50.528: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-h2b6t ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:46:51.229: INFO: stderr: ""
Feb 13 23:46:51.229: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 13 23:46:51.229: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 13 23:46:51.229: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-h2b6t ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 13 23:46:51.889: INFO: stderr: ""
Feb 13 23:46:51.889: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 13 23:46:51.889: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 13 23:46:51.889: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 13 23:47:21.985: INFO: Deleting all statefulset in ns e2e-tests-statefulset-h2b6t
Feb 13 23:47:22.009: INFO: Scaling statefulset ss to 0
Feb 13 23:47:22.071: INFO: Waiting for statefulset status.replicas updated to 0
Feb 13 23:47:22.098: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:47:22.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-h2b6t" for this suite.
Feb 13 23:47:28.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:47:28.393: INFO: namespace: e2e-tests-statefulset-h2b6t, resource: bindings, ignored listing per whitelist
Feb 13 23:47:29.117: INFO: namespace e2e-tests-statefulset-h2b6t deletion completed in 6.933105172s

• [SLOW TEST:104.957 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:47:29.117: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ccdr2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 13 23:47:30.233: INFO: Waiting up to 5m0s for pod "downward-api-b9c4b001-2fe9-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-ccdr2" to be "success or failure"
Feb 13 23:47:30.254: INFO: Pod "downward-api-b9c4b001-2fe9-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.791019ms
Feb 13 23:47:32.276: INFO: Pod "downward-api-b9c4b001-2fe9-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042552498s
Feb 13 23:47:34.298: INFO: Pod "downward-api-b9c4b001-2fe9-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064844141s
Feb 13 23:47:36.319: INFO: Pod "downward-api-b9c4b001-2fe9-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.086475555s
STEP: Saw pod success
Feb 13 23:47:36.320: INFO: Pod "downward-api-b9c4b001-2fe9-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:47:36.340: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downward-api-b9c4b001-2fe9-11e9-9de3-466a6591423c container dapi-container: <nil>
STEP: delete the pod
Feb 13 23:47:36.402: INFO: Waiting for pod downward-api-b9c4b001-2fe9-11e9-9de3-466a6591423c to disappear
Feb 13 23:47:36.444: INFO: Pod downward-api-b9c4b001-2fe9-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:47:36.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ccdr2" for this suite.
Feb 13 23:47:42.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:47:42.634: INFO: namespace: e2e-tests-downward-api-ccdr2, resource: bindings, ignored listing per whitelist
Feb 13 23:47:43.364: INFO: namespace e2e-tests-downward-api-ccdr2 deletion completed in 6.899155151s

• [SLOW TEST:14.247 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:47:43.365: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-bzj25
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 13 23:47:44.336: INFO: Waiting up to 5m0s for pod "pod-c22d1961-2fe9-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-bzj25" to be "success or failure"
Feb 13 23:47:44.357: INFO: Pod "pod-c22d1961-2fe9-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.999359ms
Feb 13 23:47:46.379: INFO: Pod "pod-c22d1961-2fe9-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042670442s
Feb 13 23:47:48.401: INFO: Pod "pod-c22d1961-2fe9-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064627358s
STEP: Saw pod success
Feb 13 23:47:48.401: INFO: Pod "pod-c22d1961-2fe9-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:47:48.422: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-c22d1961-2fe9-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 23:47:48.509: INFO: Waiting for pod pod-c22d1961-2fe9-11e9-9de3-466a6591423c to disappear
Feb 13 23:47:48.529: INFO: Pod pod-c22d1961-2fe9-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:47:48.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bzj25" for this suite.
Feb 13 23:47:54.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:47:55.178: INFO: namespace: e2e-tests-emptydir-bzj25, resource: bindings, ignored listing per whitelist
Feb 13 23:47:55.469: INFO: namespace e2e-tests-emptydir-bzj25 deletion completed in 6.918337633s

• [SLOW TEST:12.104 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:47:55.469: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-9vxbf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-c970f33e-2fe9-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 13 23:47:56.547: INFO: Waiting up to 5m0s for pod "pod-configmaps-c9742eca-2fe9-11e9-9de3-466a6591423c" in namespace "e2e-tests-configmap-9vxbf" to be "success or failure"
Feb 13 23:47:56.567: INFO: Pod "pod-configmaps-c9742eca-2fe9-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.342477ms
Feb 13 23:47:58.589: INFO: Pod "pod-configmaps-c9742eca-2fe9-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042047581s
Feb 13 23:48:00.611: INFO: Pod "pod-configmaps-c9742eca-2fe9-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063895664s
STEP: Saw pod success
Feb 13 23:48:00.611: INFO: Pod "pod-configmaps-c9742eca-2fe9-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:48:00.632: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-configmaps-c9742eca-2fe9-11e9-9de3-466a6591423c container configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 23:48:00.691: INFO: Waiting for pod pod-configmaps-c9742eca-2fe9-11e9-9de3-466a6591423c to disappear
Feb 13 23:48:00.711: INFO: Pod pod-configmaps-c9742eca-2fe9-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:48:00.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9vxbf" for this suite.
Feb 13 23:48:06.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:48:07.488: INFO: namespace: e2e-tests-configmap-9vxbf, resource: bindings, ignored listing per whitelist
Feb 13 23:48:07.633: INFO: namespace e2e-tests-configmap-9vxbf deletion completed in 6.900725485s

• [SLOW TEST:12.164 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:48:07.634: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-6gw49
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 13 23:48:13.371: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d0b72ef0-2fe9-11e9-9de3-466a6591423c"
Feb 13 23:48:13.371: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d0b72ef0-2fe9-11e9-9de3-466a6591423c" in namespace "e2e-tests-pods-6gw49" to be "terminated due to deadline exceeded"
Feb 13 23:48:13.391: INFO: Pod "pod-update-activedeadlineseconds-d0b72ef0-2fe9-11e9-9de3-466a6591423c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 20.623888ms
Feb 13 23:48:13.391: INFO: Pod "pod-update-activedeadlineseconds-d0b72ef0-2fe9-11e9-9de3-466a6591423c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:48:13.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6gw49" for this suite.
Feb 13 23:48:19.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:48:20.116: INFO: namespace: e2e-tests-pods-6gw49, resource: bindings, ignored listing per whitelist
Feb 13 23:48:20.279: INFO: namespace e2e-tests-pods-6gw49 deletion completed in 6.866656061s

• [SLOW TEST:12.646 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:48:20.280: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jsmpq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 13 23:48:21.399: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix560054858/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:48:21.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jsmpq" for this suite.
Feb 13 23:48:27.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:48:28.117: INFO: namespace: e2e-tests-kubectl-jsmpq, resource: bindings, ignored listing per whitelist
Feb 13 23:48:28.419: INFO: namespace e2e-tests-kubectl-jsmpq deletion completed in 6.944223977s

• [SLOW TEST:8.139 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:48:28.419: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-kn9bt
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-dd14cd26-2fe9-11e9-9de3-466a6591423c
STEP: Creating secret with name s-test-opt-upd-dd14cd8a-2fe9-11e9-9de3-466a6591423c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-dd14cd26-2fe9-11e9-9de3-466a6591423c
STEP: Updating secret s-test-opt-upd-dd14cd8a-2fe9-11e9-9de3-466a6591423c
STEP: Creating secret with name s-test-opt-create-dd14cdaf-2fe9-11e9-9de3-466a6591423c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:50:03.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kn9bt" for this suite.
Feb 13 23:50:25.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:50:26.294: INFO: namespace: e2e-tests-secrets-kn9bt, resource: bindings, ignored listing per whitelist
Feb 13 23:50:26.798: INFO: namespace e2e-tests-secrets-kn9bt deletion completed in 22.970324585s

• [SLOW TEST:118.379 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:50:26.798: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-z2ltt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-gdsc
STEP: Creating a pod to test atomic-volume-subpath
Feb 13 23:50:27.962: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-gdsc" in namespace "e2e-tests-subpath-z2ltt" to be "success or failure"
Feb 13 23:50:27.982: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.415417ms
Feb 13 23:50:30.004: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042364523s
Feb 13 23:50:32.026: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063627274s
Feb 13 23:50:34.047: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Running", Reason="", readiness=false. Elapsed: 6.084824202s
Feb 13 23:50:36.069: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Running", Reason="", readiness=false. Elapsed: 8.106518544s
Feb 13 23:50:38.090: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Running", Reason="", readiness=false. Elapsed: 10.128390355s
Feb 13 23:50:40.112: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Running", Reason="", readiness=false. Elapsed: 12.150097142s
Feb 13 23:50:42.134: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Running", Reason="", readiness=false. Elapsed: 14.172346872s
Feb 13 23:50:44.155: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Running", Reason="", readiness=false. Elapsed: 16.193282748s
Feb 13 23:50:46.177: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Running", Reason="", readiness=false. Elapsed: 18.214826856s
Feb 13 23:50:48.199: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Running", Reason="", readiness=false. Elapsed: 20.237245664s
Feb 13 23:50:50.221: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Running", Reason="", readiness=false. Elapsed: 22.259276145s
Feb 13 23:50:52.243: INFO: Pod "pod-subpath-test-projected-gdsc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.280679531s
STEP: Saw pod success
Feb 13 23:50:52.243: INFO: Pod "pod-subpath-test-projected-gdsc" satisfied condition "success or failure"
Feb 13 23:50:52.263: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-subpath-test-projected-gdsc container test-container-subpath-projected-gdsc: <nil>
STEP: delete the pod
Feb 13 23:50:52.332: INFO: Waiting for pod pod-subpath-test-projected-gdsc to disappear
Feb 13 23:50:52.353: INFO: Pod pod-subpath-test-projected-gdsc no longer exists
STEP: Deleting pod pod-subpath-test-projected-gdsc
Feb 13 23:50:52.353: INFO: Deleting pod "pod-subpath-test-projected-gdsc" in namespace "e2e-tests-subpath-z2ltt"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:50:52.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-z2ltt" for this suite.
Feb 13 23:50:58.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:50:58.740: INFO: namespace: e2e-tests-subpath-z2ltt, resource: bindings, ignored listing per whitelist
Feb 13 23:50:59.268: INFO: namespace e2e-tests-subpath-z2ltt deletion completed in 6.873482771s

• [SLOW TEST:32.470 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:50:59.268: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-4kr8f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0213 23:51:10.428904   31687 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 23:51:10.428: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:51:10.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4kr8f" for this suite.
Feb 13 23:51:16.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:51:16.975: INFO: namespace: e2e-tests-gc-4kr8f, resource: bindings, ignored listing per whitelist
Feb 13 23:51:17.342: INFO: namespace e2e-tests-gc-4kr8f deletion completed in 6.89183756s

• [SLOW TEST:18.073 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:51:17.342: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-99rxq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:51:22.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-99rxq" for this suite.
Feb 13 23:51:28.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:51:28.578: INFO: namespace: e2e-tests-kubelet-test-99rxq, resource: bindings, ignored listing per whitelist
Feb 13 23:51:29.369: INFO: namespace e2e-tests-kubelet-test-99rxq deletion completed in 6.957904661s

• [SLOW TEST:12.028 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:51:29.370: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-wcbq2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 13 23:51:34.579: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-48ef1aae-2fea-11e9-9de3-466a6591423c", GenerateName:"", Namespace:"e2e-tests-pods-wcbq2", SelfLink:"/api/v1/namespaces/e2e-tests-pods-wcbq2/pods/pod-submit-remove-48ef1aae-2fea-11e9-9de3-466a6591423c", UID:"48f68f13-2fea-11e9-9f3f-da917295d151", ResourceVersion:"19945", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63685698690, loc:(*time.Location)(0x791ea40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"399305063"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.167/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-fbj58", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0026c8080), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-fbj58", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0023f8b38), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000ece840), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0023f8b70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0023f8b90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0023f8b98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0023f8b9c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698690, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698692, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698692, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698690, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.5", PodIP:"100.96.1.167", StartTime:(*v1.Time)(0xc0022ac0a0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0022ac0c0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://3eb85e169f2c4a268e5a6207c6987272838a14e31416dcdf904d151e0e0bfc1b"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:51:45.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-wcbq2" for this suite.
Feb 13 23:51:51.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:51:51.679: INFO: namespace: e2e-tests-pods-wcbq2, resource: bindings, ignored listing per whitelist
Feb 13 23:51:52.132: INFO: namespace e2e-tests-pods-wcbq2 deletion completed in 6.889227871s

• [SLOW TEST:22.762 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:51:52.132: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-h5bw6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 13 23:51:53.111: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml --namespace=e2e-tests-kubectl-h5bw6 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 13 23:51:56.597: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 13 23:51:56.597: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:51:58.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-h5bw6" for this suite.
Feb 13 23:52:04.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:52:05.370: INFO: namespace: e2e-tests-kubectl-h5bw6, resource: bindings, ignored listing per whitelist
Feb 13 23:52:05.554: INFO: namespace e2e-tests-kubectl-h5bw6 deletion completed in 6.894107213s

• [SLOW TEST:13.423 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:52:05.555: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-4gskp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-v9vf
STEP: Creating a pod to test atomic-volume-subpath
Feb 13 23:52:06.680: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-v9vf" in namespace "e2e-tests-subpath-4gskp" to be "success or failure"
Feb 13 23:52:06.701: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.459964ms
Feb 13 23:52:08.723: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042779179s
Feb 13 23:52:10.745: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064220525s
Feb 13 23:52:12.767: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Running", Reason="", readiness=false. Elapsed: 6.086981733s
Feb 13 23:52:14.789: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Running", Reason="", readiness=false. Elapsed: 8.109063915s
Feb 13 23:52:16.812: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Running", Reason="", readiness=false. Elapsed: 10.131725294s
Feb 13 23:52:18.835: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Running", Reason="", readiness=false. Elapsed: 12.154787701s
Feb 13 23:52:20.857: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Running", Reason="", readiness=false. Elapsed: 14.176584241s
Feb 13 23:52:22.879: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Running", Reason="", readiness=false. Elapsed: 16.198608261s
Feb 13 23:52:24.900: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Running", Reason="", readiness=false. Elapsed: 18.220065673s
Feb 13 23:52:26.922: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Running", Reason="", readiness=false. Elapsed: 20.241895948s
Feb 13 23:52:28.944: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Running", Reason="", readiness=false. Elapsed: 22.26416742s
Feb 13 23:52:30.967: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Running", Reason="", readiness=false. Elapsed: 24.286465078s
Feb 13 23:52:33.005: INFO: Pod "pod-subpath-test-secret-v9vf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.325049493s
STEP: Saw pod success
Feb 13 23:52:33.007: INFO: Pod "pod-subpath-test-secret-v9vf" satisfied condition "success or failure"
Feb 13 23:52:33.029: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-subpath-test-secret-v9vf container test-container-subpath-secret-v9vf: <nil>
STEP: delete the pod
Feb 13 23:52:33.092: INFO: Waiting for pod pod-subpath-test-secret-v9vf to disappear
Feb 13 23:52:33.113: INFO: Pod pod-subpath-test-secret-v9vf no longer exists
STEP: Deleting pod pod-subpath-test-secret-v9vf
Feb 13 23:52:33.113: INFO: Deleting pod "pod-subpath-test-secret-v9vf" in namespace "e2e-tests-subpath-4gskp"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:52:33.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4gskp" for this suite.
Feb 13 23:52:39.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:52:39.309: INFO: namespace: e2e-tests-subpath-4gskp, resource: bindings, ignored listing per whitelist
Feb 13 23:52:40.059: INFO: namespace e2e-tests-subpath-4gskp deletion completed in 6.899014126s

• [SLOW TEST:34.505 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:52:40.060: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-dfmzf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-7w2zn
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb 13 23:52:52.966: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-vqz4s
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:53:10.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-dfmzf" for this suite.
Feb 13 23:53:16.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:53:16.645: INFO: namespace: e2e-tests-namespaces-dfmzf, resource: bindings, ignored listing per whitelist
Feb 13 23:53:17.013: INFO: namespace e2e-tests-namespaces-dfmzf deletion completed in 6.869966412s
STEP: Destroying namespace "e2e-tests-nsdeletetest-7w2zn" for this suite.
Feb 13 23:53:17.033: INFO: Namespace e2e-tests-nsdeletetest-7w2zn was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-vqz4s" for this suite.
Feb 13 23:53:23.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:53:23.613: INFO: namespace: e2e-tests-nsdeletetest-vqz4s, resource: bindings, ignored listing per whitelist
Feb 13 23:53:23.900: INFO: namespace e2e-tests-nsdeletetest-vqz4s deletion completed in 6.86705896s

• [SLOW TEST:43.841 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:53:23.900: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-k5fw8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-k5fw8
Feb 13 23:53:31.057: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-k5fw8
STEP: checking the pod's current state and verifying that restartCount is present
Feb 13 23:53:31.078: INFO: Initial restart count of pod liveness-http is 0
Feb 13 23:53:45.266: INFO: Restart count of pod e2e-tests-container-probe-k5fw8/liveness-http is now 1 (14.187686003s elapsed)
Feb 13 23:54:05.484: INFO: Restart count of pod e2e-tests-container-probe-k5fw8/liveness-http is now 2 (34.406340663s elapsed)
Feb 13 23:54:25.725: INFO: Restart count of pod e2e-tests-container-probe-k5fw8/liveness-http is now 3 (54.647054382s elapsed)
Feb 13 23:54:45.941: INFO: Restart count of pod e2e-tests-container-probe-k5fw8/liveness-http is now 4 (1m14.863030703s elapsed)
Feb 13 23:55:50.649: INFO: Restart count of pod e2e-tests-container-probe-k5fw8/liveness-http is now 5 (2m19.570977684s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:55:50.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-k5fw8" for this suite.
Feb 13 23:55:56.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:55:57.348: INFO: namespace: e2e-tests-container-probe-k5fw8, resource: bindings, ignored listing per whitelist
Feb 13 23:55:57.627: INFO: namespace e2e-tests-container-probe-k5fw8 deletion completed in 6.920740725s

• [SLOW TEST:153.727 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:55:57.628: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-bwjzp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 13 23:55:58.607: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-bwjzp'
Feb 13 23:56:00.159: INFO: stderr: ""
Feb 13 23:56:00.159: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 13 23:56:00.159: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bwjzp'
Feb 13 23:56:00.371: INFO: stderr: ""
Feb 13 23:56:00.371: INFO: stdout: "update-demo-nautilus-hwx2k update-demo-nautilus-p7sjj "
Feb 13 23:56:00.371: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-hwx2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bwjzp'
Feb 13 23:56:00.536: INFO: stderr: ""
Feb 13 23:56:00.536: INFO: stdout: ""
Feb 13 23:56:00.536: INFO: update-demo-nautilus-hwx2k is created but not running
Feb 13 23:56:05.537: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-bwjzp'
Feb 13 23:56:05.702: INFO: stderr: ""
Feb 13 23:56:05.702: INFO: stdout: "update-demo-nautilus-hwx2k update-demo-nautilus-p7sjj "
Feb 13 23:56:05.702: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-hwx2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bwjzp'
Feb 13 23:56:05.858: INFO: stderr: ""
Feb 13 23:56:05.858: INFO: stdout: "true"
Feb 13 23:56:05.858: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-hwx2k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bwjzp'
Feb 13 23:56:06.063: INFO: stderr: ""
Feb 13 23:56:06.063: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 13 23:56:06.063: INFO: validating pod update-demo-nautilus-hwx2k
Feb 13 23:56:06.175: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 13 23:56:06.175: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 13 23:56:06.175: INFO: update-demo-nautilus-hwx2k is verified up and running
Feb 13 23:56:06.175: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-p7sjj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bwjzp'
Feb 13 23:56:06.337: INFO: stderr: ""
Feb 13 23:56:06.337: INFO: stdout: "true"
Feb 13 23:56:06.337: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-p7sjj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-bwjzp'
Feb 13 23:56:06.500: INFO: stderr: ""
Feb 13 23:56:06.500: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 13 23:56:06.500: INFO: validating pod update-demo-nautilus-p7sjj
Feb 13 23:56:06.612: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 13 23:56:06.612: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 13 23:56:06.612: INFO: update-demo-nautilus-p7sjj is verified up and running
STEP: using delete to clean up resources
Feb 13 23:56:06.612: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-bwjzp'
Feb 13 23:56:06.802: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 13 23:56:06.802: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 13 23:56:06.802: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-bwjzp'
Feb 13 23:56:07.006: INFO: stderr: "No resources found.\n"
Feb 13 23:56:07.006: INFO: stdout: ""
Feb 13 23:56:07.006: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-bwjzp -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 13 23:56:07.171: INFO: stderr: ""
Feb 13 23:56:07.171: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:56:07.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bwjzp" for this suite.
Feb 13 23:56:29.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:56:30.053: INFO: namespace: e2e-tests-kubectl-bwjzp, resource: bindings, ignored listing per whitelist
Feb 13 23:56:30.093: INFO: namespace e2e-tests-kubectl-bwjzp deletion completed in 22.898371796s

• [SLOW TEST:32.465 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:56:30.094: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-tmz59
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 13 23:56:31.106: INFO: PodSpec: initContainers in spec.initContainers
Feb 13 23:57:18.216: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-fc2b524a-2fea-11e9-9de3-466a6591423c", GenerateName:"", Namespace:"e2e-tests-init-container-tmz59", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-tmz59/pods/pod-init-fc2b524a-2fea-11e9-9de3-466a6591423c", UID:"fc2c5169-2fea-11e9-9f3f-da917295d151", ResourceVersion:"20791", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63685698991, loc:(*time.Location)(0x791ea40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"106112324"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.173/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-cbd6s", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0013800c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cbd6s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cbd6s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cbd6s", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0004babf8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001fb8420), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0004bac80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0004baca0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0004baca8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0004bacac)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698991, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698991, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698991, loc:(*time.Location)(0x791ea40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698991, loc:(*time.Location)(0x791ea40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.5", PodIP:"100.96.1.173", StartTime:(*v1.Time)(0xc000c5f860), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00135fc70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00135fce0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://07796879b08ea908c4ae51d6b62fb3ec436bc378c768350e8710d686166d2849"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000c5f900), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000c5f8a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:57:18.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-tmz59" for this suite.
Feb 13 23:57:40.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:57:40.922: INFO: namespace: e2e-tests-init-container-tmz59, resource: bindings, ignored listing per whitelist
Feb 13 23:57:41.158: INFO: namespace e2e-tests-init-container-tmz59 deletion completed in 22.920530912s

• [SLOW TEST:71.065 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:57:41.158: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rzszd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Feb 13 23:57:42.111: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-rzszd'
Feb 13 23:57:42.499: INFO: stderr: ""
Feb 13 23:57:42.499: INFO: stdout: "pod/pause created\n"
Feb 13 23:57:42.499: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 13 23:57:42.499: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-rzszd" to be "running and ready"
Feb 13 23:57:42.521: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 21.455623ms
Feb 13 23:57:44.542: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042231388s
Feb 13 23:57:46.563: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.063783146s
Feb 13 23:57:46.563: INFO: Pod "pause" satisfied condition "running and ready"
Feb 13 23:57:46.563: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 13 23:57:46.564: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-rzszd'
Feb 13 23:57:46.842: INFO: stderr: ""
Feb 13 23:57:46.842: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 13 23:57:46.842: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-rzszd'
Feb 13 23:57:47.077: INFO: stderr: ""
Feb 13 23:57:47.077: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 13 23:57:47.077: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml label pods pause testing-label- --namespace=e2e-tests-kubectl-rzszd'
Feb 13 23:57:47.363: INFO: stderr: ""
Feb 13 23:57:47.363: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 13 23:57:47.363: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-rzszd'
Feb 13 23:57:47.550: INFO: stderr: ""
Feb 13 23:57:47.550: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Feb 13 23:57:47.550: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rzszd'
Feb 13 23:57:47.822: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 13 23:57:47.822: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 13 23:57:47.822: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-rzszd'
Feb 13 23:57:48.028: INFO: stderr: "No resources found.\n"
Feb 13 23:57:48.028: INFO: stdout: ""
Feb 13 23:57:48.028: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -l name=pause --namespace=e2e-tests-kubectl-rzszd -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 13 23:57:48.193: INFO: stderr: ""
Feb 13 23:57:48.193: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:57:48.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rzszd" for this suite.
Feb 13 23:57:54.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:57:54.840: INFO: namespace: e2e-tests-kubectl-rzszd, resource: bindings, ignored listing per whitelist
Feb 13 23:57:55.128: INFO: namespace e2e-tests-kubectl-rzszd deletion completed in 6.913414178s

• [SLOW TEST:13.970 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:57:55.129: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-ngmql
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 13 23:58:02.353: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:58:02.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-ngmql" for this suite.
Feb 13 23:58:24.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:58:24.886: INFO: namespace: e2e-tests-replicaset-ngmql, resource: bindings, ignored listing per whitelist
Feb 13 23:58:25.341: INFO: namespace e2e-tests-replicaset-ngmql deletion completed in 22.89699204s

• [SLOW TEST:30.212 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:58:25.341: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-9cnwd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 13 23:58:26.417: INFO: Waiting up to 5m0s for pod "pod-40e2dc95-2feb-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-9cnwd" to be "success or failure"
Feb 13 23:58:26.448: INFO: Pod "pod-40e2dc95-2feb-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.59399ms
Feb 13 23:58:28.469: INFO: Pod "pod-40e2dc95-2feb-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052517219s
Feb 13 23:58:30.491: INFO: Pod "pod-40e2dc95-2feb-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074210294s
STEP: Saw pod success
Feb 13 23:58:30.491: INFO: Pod "pod-40e2dc95-2feb-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:58:30.512: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-40e2dc95-2feb-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 23:58:30.577: INFO: Waiting for pod pod-40e2dc95-2feb-11e9-9de3-466a6591423c to disappear
Feb 13 23:58:30.598: INFO: Pod pod-40e2dc95-2feb-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:58:30.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9cnwd" for this suite.
Feb 13 23:58:36.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:58:37.099: INFO: namespace: e2e-tests-emptydir-9cnwd, resource: bindings, ignored listing per whitelist
Feb 13 23:58:37.481: INFO: namespace e2e-tests-emptydir-9cnwd deletion completed in 6.86202218s

• [SLOW TEST:12.140 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:58:37.481: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-qwsmm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 13 23:58:38.461: INFO: Waiting up to 5m0s for pod "pod-4810a86b-2feb-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-qwsmm" to be "success or failure"
Feb 13 23:58:38.482: INFO: Pod "pod-4810a86b-2feb-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.992734ms
Feb 13 23:58:40.504: INFO: Pod "pod-4810a86b-2feb-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042751128s
Feb 13 23:58:42.525: INFO: Pod "pod-4810a86b-2feb-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063794906s
STEP: Saw pod success
Feb 13 23:58:42.525: INFO: Pod "pod-4810a86b-2feb-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 13 23:58:42.546: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-4810a86b-2feb-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 13 23:58:42.603: INFO: Waiting for pod pod-4810a86b-2feb-11e9-9de3-466a6591423c to disappear
Feb 13 23:58:42.623: INFO: Pod pod-4810a86b-2feb-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:58:42.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qwsmm" for this suite.
Feb 13 23:58:48.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:58:49.084: INFO: namespace: e2e-tests-emptydir-qwsmm, resource: bindings, ignored listing per whitelist
Feb 13 23:58:49.540: INFO: namespace e2e-tests-emptydir-qwsmm deletion completed in 6.895361909s

• [SLOW TEST:12.059 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:58:49.541: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-fg4q6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:59:18.675: INFO: Container started at 2019-02-13 23:58:52 +0000 UTC, pod became ready at 2019-02-13 23:59:16 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 13 23:59:18.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fg4q6" for this suite.
Feb 13 23:59:57.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:59:58.299: INFO: namespace: e2e-tests-container-probe-fg4q6, resource: bindings, ignored listing per whitelist
Feb 13 23:59:58.626: INFO: namespace e2e-tests-container-probe-fg4q6 deletion completed in 39.930078094s

• [SLOW TEST:69.085 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 13 23:59:58.626: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-8f5jc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-787ff58c-2feb-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 13 23:59:59.765: INFO: Waiting up to 5m0s for pod "pod-secrets-788324e7-2feb-11e9-9de3-466a6591423c" in namespace "e2e-tests-secrets-8f5jc" to be "success or failure"
Feb 13 23:59:59.786: INFO: Pod "pod-secrets-788324e7-2feb-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.618442ms
Feb 14 00:00:01.807: INFO: Pod "pod-secrets-788324e7-2feb-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041929344s
Feb 14 00:00:03.829: INFO: Pod "pod-secrets-788324e7-2feb-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063843807s
STEP: Saw pod success
Feb 14 00:00:03.829: INFO: Pod "pod-secrets-788324e7-2feb-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:00:03.850: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-secrets-788324e7-2feb-11e9-9de3-466a6591423c container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 00:00:03.914: INFO: Waiting for pod pod-secrets-788324e7-2feb-11e9-9de3-466a6591423c to disappear
Feb 14 00:00:03.935: INFO: Pod pod-secrets-788324e7-2feb-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:00:03.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8f5jc" for this suite.
Feb 14 00:00:10.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:00:10.491: INFO: namespace: e2e-tests-secrets-8f5jc, resource: bindings, ignored listing per whitelist
Feb 14 00:00:10.872: INFO: namespace e2e-tests-secrets-8f5jc deletion completed in 6.914865342s

• [SLOW TEST:12.246 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:00:10.872: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-v4hvq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:00:16.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-v4hvq" for this suite.
Feb 14 00:00:56.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:00:56.590: INFO: namespace: e2e-tests-kubelet-test-v4hvq, resource: bindings, ignored listing per whitelist
Feb 14 00:00:56.962: INFO: namespace e2e-tests-kubelet-test-v4hvq deletion completed in 40.927507511s

• [SLOW TEST:46.090 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:00:56.962: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bn7dh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:00:58.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b3fdd46-2feb-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-bn7dh" to be "success or failure"
Feb 14 00:00:58.043: INFO: Pod "downwardapi-volume-9b3fdd46-2feb-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.3613ms
Feb 14 00:01:00.065: INFO: Pod "downwardapi-volume-9b3fdd46-2feb-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043263466s
Feb 14 00:01:02.087: INFO: Pod "downwardapi-volume-9b3fdd46-2feb-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065294408s
STEP: Saw pod success
Feb 14 00:01:02.087: INFO: Pod "downwardapi-volume-9b3fdd46-2feb-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:01:02.108: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-9b3fdd46-2feb-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 14 00:01:02.233: INFO: Waiting for pod downwardapi-volume-9b3fdd46-2feb-11e9-9de3-466a6591423c to disappear
Feb 14 00:01:02.301: INFO: Pod downwardapi-volume-9b3fdd46-2feb-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:01:02.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bn7dh" for this suite.
Feb 14 00:01:08.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:01:08.742: INFO: namespace: e2e-tests-projected-bn7dh, resource: bindings, ignored listing per whitelist
Feb 14 00:01:09.213: INFO: namespace e2e-tests-projected-bn7dh deletion completed in 6.890719558s

• [SLOW TEST:12.251 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:01:09.213: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-wz9lq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:01:14.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-wz9lq" for this suite.
Feb 14 00:02:09.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:02:09.753: INFO: namespace: e2e-tests-kubelet-test-wz9lq, resource: bindings, ignored listing per whitelist
Feb 14 00:02:10.257: INFO: namespace e2e-tests-kubelet-test-wz9lq deletion completed in 55.900036162s

• [SLOW TEST:61.044 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:02:10.257: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-xqbb9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:02:11.219: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 14 00:02:11.270: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 14 00:02:15.313: INFO: Creating deployment "test-rolling-update-deployment"
Feb 14 00:02:15.335: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 14 00:02:15.391: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 14 00:02:15.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:0, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699335, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699335, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699335, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699335, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 00:02:17.448: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699335, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699335, loc:(*time.Location)(0x791ea40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699335, loc:(*time.Location)(0x791ea40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699335, loc:(*time.Location)(0x791ea40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 00:02:19.448: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 14 00:02:19.512: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-xqbb9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xqbb9/deployments/test-rolling-update-deployment,UID:c955ebe0-2feb-11e9-9f3f-da917295d151,ResourceVersion:21611,Generation:1,CreationTimestamp:2019-02-14 00:02:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-14 00:02:15 +0000 UTC 2019-02-14 00:02:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-14 00:02:17 +0000 UTC 2019-02-14 00:02:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 14 00:02:19.534: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-xqbb9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xqbb9/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:c957d144-2feb-11e9-9f3f-da917295d151,ResourceVersion:21604,Generation:1,CreationTimestamp:2019-02-14 00:02:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment c955ebe0-2feb-11e9-9f3f-da917295d151 0xc000ef2157 0xc000ef2158}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 14 00:02:19.534: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 14 00:02:19.534: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-xqbb9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xqbb9/replicasets/test-rolling-update-controller,UID:c6e545a0-2feb-11e9-9f3f-da917295d151,ResourceVersion:21610,Generation:2,CreationTimestamp:2019-02-14 00:02:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment c955ebe0-2feb-11e9-9f3f-da917295d151 0xc000ef2097 0xc000ef2098}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 00:02:19.555: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-vlwtm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-vlwtm,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-xqbb9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xqbb9/pods/test-rolling-update-deployment-68b55d7bc6-vlwtm,UID:c9587358-2feb-11e9-9f3f-da917295d151,ResourceVersion:21603,Generation:0,CreationTimestamp:2019-02-14 00:02:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.185/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 c957d144-2feb-11e9-9f3f-da917295d151 0xc000ef2e07 0xc000ef2e08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8tj7k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8tj7k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8tj7k true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ef2ed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ef2ef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:02:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:02:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:02:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:02:15 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.185,StartTime:2019-02-14 00:02:15 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-14 00:02:17 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://4374c4439ac0a425a4b6ca4c462c30b7f7271f1baf2fdc5660c0bffadb928f80}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:02:19.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-xqbb9" for this suite.
Feb 14 00:02:25.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:02:25.907: INFO: namespace: e2e-tests-deployment-xqbb9, resource: bindings, ignored listing per whitelist
Feb 14 00:02:26.531: INFO: namespace e2e-tests-deployment-xqbb9 deletion completed in 6.952615357s

• [SLOW TEST:16.274 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:02:26.531: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-lqlbl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 14 00:02:32.096: INFO: Successfully updated pod "annotationupdated08b0574-2feb-11e9-9de3-466a6591423c"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:02:34.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lqlbl" for this suite.
Feb 14 00:03:21.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:03:21.253: INFO: namespace: e2e-tests-downward-api-lqlbl, resource: bindings, ignored listing per whitelist
Feb 14 00:03:21.990: INFO: namespace e2e-tests-downward-api-lqlbl deletion completed in 47.812099188s

• [SLOW TEST:55.459 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:03:21.991: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-cvjwq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-cvjwq
Feb 14 00:03:26.980: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-cvjwq
STEP: checking the pod's current state and verifying that restartCount is present
Feb 14 00:03:27.002: INFO: Initial restart count of pod liveness-exec is 0
Feb 14 00:04:11.495: INFO: Restart count of pod e2e-tests-container-probe-cvjwq/liveness-exec is now 1 (44.492842511s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:04:11.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cvjwq" for this suite.
Feb 14 00:04:17.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:04:18.013: INFO: namespace: e2e-tests-container-probe-cvjwq, resource: bindings, ignored listing per whitelist
Feb 14 00:04:18.487: INFO: namespace e2e-tests-container-probe-cvjwq deletion completed in 6.927936257s

• [SLOW TEST:56.497 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:04:18.488: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-qlrg9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-qlrg9
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 14 00:04:19.487: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 14 00:04:39.860: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.96.1.188 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-qlrg9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:04:39.860: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:04:41.467: INFO: Found all expected endpoints: [netserver-0]
Feb 14 00:04:41.488: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.96.0.47 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-qlrg9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:04:41.488: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:04:42.983: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:04:42.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-qlrg9" for this suite.
Feb 14 00:05:07.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:05:07.204: INFO: namespace: e2e-tests-pod-network-test-qlrg9, resource: bindings, ignored listing per whitelist
Feb 14 00:05:07.910: INFO: namespace e2e-tests-pod-network-test-qlrg9 deletion completed in 24.905348356s

• [SLOW TEST:49.422 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:05:07.910: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-v9gm6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:05:08.921: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30cbf154-2fec-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-v9gm6" to be "success or failure"
Feb 14 00:05:08.950: INFO: Pod "downwardapi-volume-30cbf154-2fec-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 29.77616ms
Feb 14 00:05:10.974: INFO: Pod "downwardapi-volume-30cbf154-2fec-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053646326s
Feb 14 00:05:12.997: INFO: Pod "downwardapi-volume-30cbf154-2fec-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075938428s
STEP: Saw pod success
Feb 14 00:05:12.997: INFO: Pod "downwardapi-volume-30cbf154-2fec-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:05:13.019: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-30cbf154-2fec-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 14 00:05:13.097: INFO: Waiting for pod downwardapi-volume-30cbf154-2fec-11e9-9de3-466a6591423c to disappear
Feb 14 00:05:13.117: INFO: Pod downwardapi-volume-30cbf154-2fec-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:05:13.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v9gm6" for this suite.
Feb 14 00:05:19.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:05:19.680: INFO: namespace: e2e-tests-projected-v9gm6, resource: bindings, ignored listing per whitelist
Feb 14 00:05:20.034: INFO: namespace e2e-tests-projected-v9gm6 deletion completed in 6.896269396s

• [SLOW TEST:12.124 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:05:20.034: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-b4wf4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-38024f76-2fec-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 14 00:05:21.041: INFO: Waiting up to 5m0s for pod "pod-configmaps-38058473-2fec-11e9-9de3-466a6591423c" in namespace "e2e-tests-configmap-b4wf4" to be "success or failure"
Feb 14 00:05:21.062: INFO: Pod "pod-configmaps-38058473-2fec-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.663909ms
Feb 14 00:05:23.084: INFO: Pod "pod-configmaps-38058473-2fec-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042167691s
Feb 14 00:05:25.105: INFO: Pod "pod-configmaps-38058473-2fec-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06356465s
STEP: Saw pod success
Feb 14 00:05:25.105: INFO: Pod "pod-configmaps-38058473-2fec-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:05:25.125: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-configmaps-38058473-2fec-11e9-9de3-466a6591423c container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:05:25.217: INFO: Waiting for pod pod-configmaps-38058473-2fec-11e9-9de3-466a6591423c to disappear
Feb 14 00:05:25.237: INFO: Pod pod-configmaps-38058473-2fec-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:05:25.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-b4wf4" for this suite.
Feb 14 00:05:31.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:05:31.947: INFO: namespace: e2e-tests-configmap-b4wf4, resource: bindings, ignored listing per whitelist
Feb 14 00:05:32.165: INFO: namespace e2e-tests-configmap-b4wf4 deletion completed in 6.907214601s

• [SLOW TEST:12.131 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:05:32.166: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-b48z4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:05:33.393: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3f5fffca-2fec-11e9-9f3f-da917295d151", Controller:(*bool)(0xc00263c2c2), BlockOwnerDeletion:(*bool)(0xc00263c2c3)}}
Feb 14 00:05:33.423: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3f5925ad-2fec-11e9-9f3f-da917295d151", Controller:(*bool)(0xc002ab3c6e), BlockOwnerDeletion:(*bool)(0xc002ab3c6f)}}
Feb 14 00:05:33.445: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3f5c7071-2fec-11e9-9f3f-da917295d151", Controller:(*bool)(0xc0025b3006), BlockOwnerDeletion:(*bool)(0xc0025b3007)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:05:38.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-b48z4" for this suite.
Feb 14 00:05:44.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:05:45.312: INFO: namespace: e2e-tests-gc-b48z4, resource: bindings, ignored listing per whitelist
Feb 14 00:05:45.375: INFO: namespace e2e-tests-gc-b48z4 deletion completed in 6.858350061s

• [SLOW TEST:13.210 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:05:45.376: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-hhrt8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-hhrt8
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-hhrt8 to expose endpoints map[]
Feb 14 00:05:46.414: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-hhrt8 exposes endpoints map[] (20.182034ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-hhrt8
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-hhrt8 to expose endpoints map[pod1:[100]]
Feb 14 00:05:49.610: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-hhrt8 exposes endpoints map[pod1:[100]] (3.167854286s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-hhrt8
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-hhrt8 to expose endpoints map[pod2:[101] pod1:[100]]
Feb 14 00:05:52.880: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-hhrt8 exposes endpoints map[pod1:[100] pod2:[101]] (3.247415s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-hhrt8
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-hhrt8 to expose endpoints map[pod2:[101]]
Feb 14 00:05:52.943: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-hhrt8 exposes endpoints map[pod2:[101]] (40.75373ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-hhrt8
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-hhrt8 to expose endpoints map[]
Feb 14 00:05:53.029: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-hhrt8 exposes endpoints map[] (20.032598ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:05:53.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-hhrt8" for this suite.
Feb 14 00:06:15.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:06:15.865: INFO: namespace: e2e-tests-services-hhrt8, resource: bindings, ignored listing per whitelist
Feb 14 00:06:15.989: INFO: namespace e2e-tests-services-hhrt8 deletion completed in 22.898502211s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:30.613 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:06:15.989: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-rhbwq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 14 00:06:17.609: INFO: created pod pod-service-account-defaultsa
Feb 14 00:06:17.609: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 14 00:06:17.644: INFO: created pod pod-service-account-mountsa
Feb 14 00:06:17.644: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 14 00:06:17.666: INFO: created pod pod-service-account-nomountsa
Feb 14 00:06:17.666: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 14 00:06:17.701: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 14 00:06:17.702: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 14 00:06:17.725: INFO: created pod pod-service-account-mountsa-mountspec
Feb 14 00:06:17.725: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 14 00:06:17.747: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 14 00:06:17.747: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 14 00:06:17.770: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 14 00:06:17.770: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 14 00:06:17.793: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 14 00:06:17.796: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 14 00:06:17.824: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 14 00:06:17.824: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:06:17.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-rhbwq" for this suite.
Feb 14 00:06:41.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:06:42.168: INFO: namespace: e2e-tests-svcaccounts-rhbwq, resource: bindings, ignored listing per whitelist
Feb 14 00:06:42.789: INFO: namespace e2e-tests-svcaccounts-rhbwq deletion completed in 24.943413045s

• [SLOW TEST:26.800 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:06:42.790: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-hct68
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 14 00:06:43.791: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 14 00:06:43.832: INFO: Waiting for terminating namespaces to be deleted...
Feb 14 00:06:43.852: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx before test
Feb 14 00:06:43.880: INFO: kube-proxy-n2tgw from kube-system started at 2019-02-13 22:03:50 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.880: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 00:06:43.880: INFO: node-exporter-clvlv from kube-system started at 2019-02-13 22:03:50 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.880: INFO: 	Container node-exporter ready: true, restart count 0
Feb 14 00:06:43.880: INFO: calico-node-skgqz from kube-system started at 2019-02-13 22:03:50 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.880: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 00:06:43.880: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp before test
Feb 14 00:06:43.917: INFO: metrics-server-65cc55bd79-8kjg8 from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container metrics-server ready: true, restart count 0
Feb 14 00:06:43.917: INFO: addons-kubernetes-dashboard-6579b646c5-g48vq from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 14 00:06:43.917: INFO: addons-kube-lego-69bbdc96b6-6m8wg from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container kube-lego ready: true, restart count 0
Feb 14 00:06:43.917: INFO: vpn-shoot-c76596df-6nsrb from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 14 00:06:43.917: INFO: node-exporter-tmhn5 from kube-system started at 2019-02-13 22:03:43 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container node-exporter ready: true, restart count 0
Feb 14 00:06:43.917: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-kj5ch from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 14 00:06:43.917: INFO: kube-proxy-6688x from kube-system started at 2019-02-13 22:03:43 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 00:06:43.917: INFO: calico-node-psbns from kube-system started at 2019-02-13 22:03:43 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 00:06:43.917: INFO: addons-nginx-ingress-controller-d74bfff57-87jx9 from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 14 00:06:43.917: INFO: blackbox-exporter-d6c46f9fc-5xt9g from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 14 00:06:43.917: INFO: coredns-67df79bbdd-96tpz from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:06:43.917: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6be34698-2fec-11e9-9de3-466a6591423c 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-6be34698-2fec-11e9-9de3-466a6591423c off the node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6be34698-2fec-11e9-9de3-466a6591423c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:06:52.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-hct68" for this suite.
Feb 14 00:07:08.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:07:08.857: INFO: namespace: e2e-tests-sched-pred-hct68, resource: bindings, ignored listing per whitelist
Feb 14 00:07:09.197: INFO: namespace e2e-tests-sched-pred-hct68 deletion completed in 16.888061186s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:26.407 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:07:09.197: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-wmss7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-wmss7
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 14 00:07:10.259: INFO: Found 1 stateful pods, waiting for 3
Feb 14 00:07:20.287: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:07:20.287: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:07:20.287: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Feb 14 00:07:30.281: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:07:30.282: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:07:30.282: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:07:30.351: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wmss7 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:07:31.122: INFO: stderr: ""
Feb 14 00:07:31.122: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:07:31.122: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 14 00:07:41.259: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 14 00:07:41.329: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wmss7 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:07:42.000: INFO: stderr: ""
Feb 14 00:07:42.000: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 00:07:42.000: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 00:07:52.127: INFO: Waiting for StatefulSet e2e-tests-statefulset-wmss7/ss2 to complete update
Feb 14 00:07:52.127: INFO: Waiting for Pod e2e-tests-statefulset-wmss7/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 14 00:07:52.127: INFO: Waiting for Pod e2e-tests-statefulset-wmss7/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 14 00:07:52.127: INFO: Waiting for Pod e2e-tests-statefulset-wmss7/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 14 00:08:02.170: INFO: Waiting for StatefulSet e2e-tests-statefulset-wmss7/ss2 to complete update
Feb 14 00:08:02.170: INFO: Waiting for Pod e2e-tests-statefulset-wmss7/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 14 00:08:02.170: INFO: Waiting for Pod e2e-tests-statefulset-wmss7/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 14 00:08:12.170: INFO: Waiting for StatefulSet e2e-tests-statefulset-wmss7/ss2 to complete update
STEP: Rolling back to a previous revision
Feb 14 00:08:22.170: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wmss7 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:08:22.947: INFO: stderr: ""
Feb 14 00:08:22.947: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:08:22.947: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 00:08:33.084: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 14 00:08:33.163: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wmss7 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:08:33.898: INFO: stderr: ""
Feb 14 00:08:33.898: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 00:08:33.898: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 00:08:44.024: INFO: Waiting for StatefulSet e2e-tests-statefulset-wmss7/ss2 to complete update
Feb 14 00:08:44.024: INFO: Waiting for Pod e2e-tests-statefulset-wmss7/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 14 00:08:44.024: INFO: Waiting for Pod e2e-tests-statefulset-wmss7/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 14 00:08:44.024: INFO: Waiting for Pod e2e-tests-statefulset-wmss7/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 14 00:08:54.067: INFO: Waiting for StatefulSet e2e-tests-statefulset-wmss7/ss2 to complete update
Feb 14 00:08:54.067: INFO: Waiting for Pod e2e-tests-statefulset-wmss7/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 14 00:08:54.067: INFO: Waiting for Pod e2e-tests-statefulset-wmss7/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 14 00:09:04.067: INFO: Waiting for StatefulSet e2e-tests-statefulset-wmss7/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 00:09:14.069: INFO: Deleting all statefulset in ns e2e-tests-statefulset-wmss7
Feb 14 00:09:14.089: INFO: Scaling statefulset ss2 to 0
Feb 14 00:09:44.177: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 00:09:44.197: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:09:44.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-wmss7" for this suite.
Feb 14 00:09:50.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:09:50.503: INFO: namespace: e2e-tests-statefulset-wmss7, resource: bindings, ignored listing per whitelist
Feb 14 00:09:51.212: INFO: namespace e2e-tests-statefulset-wmss7 deletion completed in 6.880779144s

• [SLOW TEST:162.015 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:09:51.212: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-66hxp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:09:52.316: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9b6b097-2fec-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-66hxp" to be "success or failure"
Feb 14 00:09:52.337: INFO: Pod "downwardapi-volume-d9b6b097-2fec-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.850557ms
Feb 14 00:09:54.358: INFO: Pod "downwardapi-volume-d9b6b097-2fec-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04222412s
Feb 14 00:09:56.379: INFO: Pod "downwardapi-volume-d9b6b097-2fec-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06319601s
STEP: Saw pod success
Feb 14 00:09:56.379: INFO: Pod "downwardapi-volume-d9b6b097-2fec-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:09:56.401: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-d9b6b097-2fec-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 14 00:09:56.492: INFO: Waiting for pod downwardapi-volume-d9b6b097-2fec-11e9-9de3-466a6591423c to disappear
Feb 14 00:09:56.526: INFO: Pod downwardapi-volume-d9b6b097-2fec-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:09:56.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-66hxp" for this suite.
Feb 14 00:10:02.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:10:03.085: INFO: namespace: e2e-tests-downward-api-66hxp, resource: bindings, ignored listing per whitelist
Feb 14 00:10:03.464: INFO: namespace e2e-tests-downward-api-66hxp deletion completed in 6.917347477s

• [SLOW TEST:12.252 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:10:03.464: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6h98p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 14 00:10:04.487: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:10:04.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6h98p" for this suite.
Feb 14 00:10:10.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:10:11.045: INFO: namespace: e2e-tests-kubectl-6h98p, resource: bindings, ignored listing per whitelist
Feb 14 00:10:11.657: INFO: namespace e2e-tests-kubectl-6h98p deletion completed in 6.938744479s

• [SLOW TEST:8.193 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:10:11.658: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-6jmf2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-8btw
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 00:10:12.674: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8btw" in namespace "e2e-tests-subpath-6jmf2" to be "success or failure"
Feb 14 00:10:12.694: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Pending", Reason="", readiness=false. Elapsed: 20.328307ms
Feb 14 00:10:14.716: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041471785s
Feb 14 00:10:16.737: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062946319s
Feb 14 00:10:18.759: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084990559s
Feb 14 00:10:20.782: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Running", Reason="", readiness=false. Elapsed: 8.107579174s
Feb 14 00:10:22.804: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Running", Reason="", readiness=false. Elapsed: 10.129464573s
Feb 14 00:10:24.825: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Running", Reason="", readiness=false. Elapsed: 12.151059337s
Feb 14 00:10:26.847: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Running", Reason="", readiness=false. Elapsed: 14.172737866s
Feb 14 00:10:28.868: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Running", Reason="", readiness=false. Elapsed: 16.194081771s
Feb 14 00:10:30.890: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Running", Reason="", readiness=false. Elapsed: 18.215728552s
Feb 14 00:10:32.911: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Running", Reason="", readiness=false. Elapsed: 20.237324041s
Feb 14 00:10:34.934: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Running", Reason="", readiness=false. Elapsed: 22.259343559s
Feb 14 00:10:36.955: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Running", Reason="", readiness=false. Elapsed: 24.280626105s
Feb 14 00:10:38.976: INFO: Pod "pod-subpath-test-configmap-8btw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.302186101s
STEP: Saw pod success
Feb 14 00:10:38.976: INFO: Pod "pod-subpath-test-configmap-8btw" satisfied condition "success or failure"
Feb 14 00:10:38.997: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-subpath-test-configmap-8btw container test-container-subpath-configmap-8btw: <nil>
STEP: delete the pod
Feb 14 00:10:39.056: INFO: Waiting for pod pod-subpath-test-configmap-8btw to disappear
Feb 14 00:10:39.085: INFO: Pod pod-subpath-test-configmap-8btw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8btw
Feb 14 00:10:39.086: INFO: Deleting pod "pod-subpath-test-configmap-8btw" in namespace "e2e-tests-subpath-6jmf2"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:10:39.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-6jmf2" for this suite.
Feb 14 00:10:45.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:10:45.922: INFO: namespace: e2e-tests-subpath-6jmf2, resource: bindings, ignored listing per whitelist
Feb 14 00:10:46.025: INFO: namespace e2e-tests-subpath-6jmf2 deletion completed in 6.897783474s

• [SLOW TEST:34.368 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:10:46.025: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-z9n42
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 14 00:10:47.179: INFO: Waiting up to 5m0s for pod "pod-fa6a245b-2fec-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-z9n42" to be "success or failure"
Feb 14 00:10:47.199: INFO: Pod "pod-fa6a245b-2fec-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.387782ms
Feb 14 00:10:49.221: INFO: Pod "pod-fa6a245b-2fec-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041577904s
Feb 14 00:10:51.242: INFO: Pod "pod-fa6a245b-2fec-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063247035s
STEP: Saw pod success
Feb 14 00:10:51.242: INFO: Pod "pod-fa6a245b-2fec-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:10:51.263: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-fa6a245b-2fec-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 14 00:10:51.322: INFO: Waiting for pod pod-fa6a245b-2fec-11e9-9de3-466a6591423c to disappear
Feb 14 00:10:51.342: INFO: Pod pod-fa6a245b-2fec-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:10:51.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-z9n42" for this suite.
Feb 14 00:10:57.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:10:57.899: INFO: namespace: e2e-tests-emptydir-z9n42, resource: bindings, ignored listing per whitelist
Feb 14 00:10:58.230: INFO: namespace e2e-tests-emptydir-z9n42 deletion completed in 6.866317684s

• [SLOW TEST:12.205 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:10:58.231: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-nj7mw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:10:59.249: INFO: Waiting up to 5m0s for pod "downwardapi-volume-019bc09f-2fed-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-nj7mw" to be "success or failure"
Feb 14 00:10:59.269: INFO: Pod "downwardapi-volume-019bc09f-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.368017ms
Feb 14 00:11:01.290: INFO: Pod "downwardapi-volume-019bc09f-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041799337s
Feb 14 00:11:03.313: INFO: Pod "downwardapi-volume-019bc09f-2fed-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063941166s
STEP: Saw pod success
Feb 14 00:11:03.313: INFO: Pod "downwardapi-volume-019bc09f-2fed-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:11:03.333: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-019bc09f-2fed-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 14 00:11:03.397: INFO: Waiting for pod downwardapi-volume-019bc09f-2fed-11e9-9de3-466a6591423c to disappear
Feb 14 00:11:03.418: INFO: Pod downwardapi-volume-019bc09f-2fed-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:11:03.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nj7mw" for this suite.
Feb 14 00:11:09.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:11:10.156: INFO: namespace: e2e-tests-projected-nj7mw, resource: bindings, ignored listing per whitelist
Feb 14 00:11:10.378: INFO: namespace e2e-tests-projected-nj7mw deletion completed in 6.937103403s

• [SLOW TEST:12.147 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:11:10.378: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-v8nb8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 14 00:11:11.400: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-v8nb8'
Feb 14 00:11:12.856: INFO: stderr: ""
Feb 14 00:11:12.856: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 14 00:11:13.879: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:11:13.879: INFO: Found 0 / 1
Feb 14 00:11:14.878: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:11:14.878: INFO: Found 0 / 1
Feb 14 00:11:15.884: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:11:15.885: INFO: Found 1 / 1
Feb 14 00:11:15.885: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 14 00:11:15.906: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:11:15.906: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 14 00:11:15.906: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml patch pod redis-master-72gc5 --namespace=e2e-tests-kubectl-v8nb8 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 14 00:11:16.128: INFO: stderr: ""
Feb 14 00:11:16.128: INFO: stdout: "pod/redis-master-72gc5 patched\n"
STEP: checking annotations
Feb 14 00:11:16.161: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:11:16.161: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:11:16.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v8nb8" for this suite.
Feb 14 00:11:38.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:11:38.376: INFO: namespace: e2e-tests-kubectl-v8nb8, resource: bindings, ignored listing per whitelist
Feb 14 00:11:39.100: INFO: namespace e2e-tests-kubectl-v8nb8 deletion completed in 22.913591041s

• [SLOW TEST:28.722 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:11:39.100: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-9s95x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 14 00:11:40.028: INFO: Waiting up to 5m0s for pod "pod-19ea5503-2fed-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-9s95x" to be "success or failure"
Feb 14 00:11:40.049: INFO: Pod "pod-19ea5503-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.695187ms
Feb 14 00:11:42.071: INFO: Pod "pod-19ea5503-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042453203s
Feb 14 00:11:44.093: INFO: Pod "pod-19ea5503-2fed-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064998317s
STEP: Saw pod success
Feb 14 00:11:44.093: INFO: Pod "pod-19ea5503-2fed-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:11:44.114: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-19ea5503-2fed-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 14 00:11:44.168: INFO: Waiting for pod pod-19ea5503-2fed-11e9-9de3-466a6591423c to disappear
Feb 14 00:11:44.188: INFO: Pod pod-19ea5503-2fed-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:11:44.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9s95x" for this suite.
Feb 14 00:11:50.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:11:50.643: INFO: namespace: e2e-tests-emptydir-9s95x, resource: bindings, ignored listing per whitelist
Feb 14 00:11:51.116: INFO: namespace e2e-tests-emptydir-9s95x deletion completed in 6.906267594s

• [SLOW TEST:12.016 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:11:51.116: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-r6n4c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 00:11:52.093: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-r6n4c'
Feb 14 00:11:52.312: INFO: stderr: ""
Feb 14 00:11:52.312: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Feb 14 00:11:52.333: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-r6n4c'
Feb 14 00:11:56.683: INFO: stderr: ""
Feb 14 00:11:56.683: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:11:56.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-r6n4c" for this suite.
Feb 14 00:12:02.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:12:03.015: INFO: namespace: e2e-tests-kubectl-r6n4c, resource: bindings, ignored listing per whitelist
Feb 14 00:12:03.637: INFO: namespace e2e-tests-kubectl-r6n4c deletion completed in 6.929059604s

• [SLOW TEST:12.521 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:12:03.637: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-2f6lg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-2f6lg/configmap-test-2893bcbe-2fed-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 14 00:12:04.648: INFO: Waiting up to 5m0s for pod "pod-configmaps-2896f1e8-2fed-11e9-9de3-466a6591423c" in namespace "e2e-tests-configmap-2f6lg" to be "success or failure"
Feb 14 00:12:04.681: INFO: Pod "pod-configmaps-2896f1e8-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 32.036605ms
Feb 14 00:12:06.702: INFO: Pod "pod-configmaps-2896f1e8-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053490062s
Feb 14 00:12:08.724: INFO: Pod "pod-configmaps-2896f1e8-2fed-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075069033s
STEP: Saw pod success
Feb 14 00:12:08.724: INFO: Pod "pod-configmaps-2896f1e8-2fed-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:12:08.744: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-configmaps-2896f1e8-2fed-11e9-9de3-466a6591423c container env-test: <nil>
STEP: delete the pod
Feb 14 00:12:08.828: INFO: Waiting for pod pod-configmaps-2896f1e8-2fed-11e9-9de3-466a6591423c to disappear
Feb 14 00:12:08.849: INFO: Pod pod-configmaps-2896f1e8-2fed-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:12:08.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-2f6lg" for this suite.
Feb 14 00:12:14.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:12:15.184: INFO: namespace: e2e-tests-configmap-2f6lg, resource: bindings, ignored listing per whitelist
Feb 14 00:12:15.747: INFO: namespace e2e-tests-configmap-2f6lg deletion completed in 6.875400678s

• [SLOW TEST:12.109 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:12:15.749: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-65sgg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 14 00:12:16.737: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml api-versions'
Feb 14 00:12:16.966: INFO: stderr: ""
Feb 14 00:12:16.966: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:12:16.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-65sgg" for this suite.
Feb 14 00:12:23.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:12:23.151: INFO: namespace: e2e-tests-kubectl-65sgg, resource: bindings, ignored listing per whitelist
Feb 14 00:12:23.855: INFO: namespace e2e-tests-kubectl-65sgg deletion completed in 6.867579528s

• [SLOW TEST:8.106 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:12:23.855: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-smlq8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 14 00:12:35.064: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-smlq8 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:12:35.064: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:12:35.540: INFO: Exec stderr: ""
Feb 14 00:12:35.540: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-smlq8 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:12:35.540: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:12:36.107: INFO: Exec stderr: ""
Feb 14 00:12:36.107: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-smlq8 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:12:36.107: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:12:36.611: INFO: Exec stderr: ""
Feb 14 00:12:36.611: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-smlq8 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:12:36.611: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:12:37.129: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 14 00:12:37.129: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-smlq8 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:12:37.129: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:12:37.659: INFO: Exec stderr: ""
Feb 14 00:12:37.660: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-smlq8 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:12:37.660: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:12:39.181: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 14 00:12:39.181: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-smlq8 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:12:39.181: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:12:39.805: INFO: Exec stderr: ""
Feb 14 00:12:39.805: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-smlq8 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:12:39.805: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:12:40.381: INFO: Exec stderr: ""
Feb 14 00:12:40.382: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-smlq8 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:12:40.382: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:12:40.932: INFO: Exec stderr: ""
Feb 14 00:12:40.932: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-smlq8 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:12:40.932: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:12:41.464: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:12:41.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-smlq8" for this suite.
Feb 14 00:13:21.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:13:22.334: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-smlq8, resource: bindings, ignored listing per whitelist
Feb 14 00:13:22.354: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-smlq8 deletion completed in 40.869465619s

• [SLOW TEST:58.499 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:13:22.355: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lsvv2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 14 00:13:23.386: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml cluster-info'
Feb 14 00:13:23.636: INFO: stderr: ""
Feb 14 00:13:23.636: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:13:23.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lsvv2" for this suite.
Feb 14 00:13:29.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:13:30.210: INFO: namespace: e2e-tests-kubectl-lsvv2, resource: bindings, ignored listing per whitelist
Feb 14 00:13:30.605: INFO: namespace e2e-tests-kubectl-lsvv2 deletion completed in 6.948134209s

• [SLOW TEST:8.250 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:13:30.605: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-7vlmq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 14 00:13:31.621: INFO: Waiting up to 5m0s for pod "client-containers-5c6dc6ae-2fed-11e9-9de3-466a6591423c" in namespace "e2e-tests-containers-7vlmq" to be "success or failure"
Feb 14 00:13:31.645: INFO: Pod "client-containers-5c6dc6ae-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.824633ms
Feb 14 00:13:33.666: INFO: Pod "client-containers-5c6dc6ae-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044982775s
Feb 14 00:13:35.728: INFO: Pod "client-containers-5c6dc6ae-2fed-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.107119697s
STEP: Saw pod success
Feb 14 00:13:35.728: INFO: Pod "client-containers-5c6dc6ae-2fed-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:13:35.749: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod client-containers-5c6dc6ae-2fed-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 14 00:13:35.807: INFO: Waiting for pod client-containers-5c6dc6ae-2fed-11e9-9de3-466a6591423c to disappear
Feb 14 00:13:35.827: INFO: Pod client-containers-5c6dc6ae-2fed-11e9-9de3-466a6591423c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:13:35.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-7vlmq" for this suite.
Feb 14 00:13:41.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:13:42.093: INFO: namespace: e2e-tests-containers-7vlmq, resource: bindings, ignored listing per whitelist
Feb 14 00:13:42.755: INFO: namespace e2e-tests-containers-7vlmq deletion completed in 6.906483044s

• [SLOW TEST:12.150 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:13:42.755: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-zw42v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 14 00:13:43.815: INFO: Waiting up to 5m0s for pod "client-containers-63b2998e-2fed-11e9-9de3-466a6591423c" in namespace "e2e-tests-containers-zw42v" to be "success or failure"
Feb 14 00:13:43.835: INFO: Pod "client-containers-63b2998e-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.376427ms
Feb 14 00:13:45.857: INFO: Pod "client-containers-63b2998e-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042044466s
Feb 14 00:13:47.878: INFO: Pod "client-containers-63b2998e-2fed-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063472584s
STEP: Saw pod success
Feb 14 00:13:47.878: INFO: Pod "client-containers-63b2998e-2fed-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:13:47.899: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod client-containers-63b2998e-2fed-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 14 00:13:47.969: INFO: Waiting for pod client-containers-63b2998e-2fed-11e9-9de3-466a6591423c to disappear
Feb 14 00:13:47.989: INFO: Pod client-containers-63b2998e-2fed-11e9-9de3-466a6591423c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:13:47.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-zw42v" for this suite.
Feb 14 00:13:54.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:13:54.819: INFO: namespace: e2e-tests-containers-zw42v, resource: bindings, ignored listing per whitelist
Feb 14 00:13:54.902: INFO: namespace e2e-tests-containers-zw42v deletion completed in 6.88997781s

• [SLOW TEST:12.146 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:13:54.902: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qpqhx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-6aea1bf9-2fed-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 14 00:13:55.945: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6aed58ab-2fed-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-qpqhx" to be "success or failure"
Feb 14 00:13:55.966: INFO: Pod "pod-projected-configmaps-6aed58ab-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.567493ms
Feb 14 00:13:57.987: INFO: Pod "pod-projected-configmaps-6aed58ab-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042162591s
Feb 14 00:14:00.009: INFO: Pod "pod-projected-configmaps-6aed58ab-2fed-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063699456s
STEP: Saw pod success
Feb 14 00:14:00.009: INFO: Pod "pod-projected-configmaps-6aed58ab-2fed-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:14:00.030: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-configmaps-6aed58ab-2fed-11e9-9de3-466a6591423c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:14:00.134: INFO: Waiting for pod pod-projected-configmaps-6aed58ab-2fed-11e9-9de3-466a6591423c to disappear
Feb 14 00:14:00.155: INFO: Pod pod-projected-configmaps-6aed58ab-2fed-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:14:00.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qpqhx" for this suite.
Feb 14 00:14:06.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:14:06.540: INFO: namespace: e2e-tests-projected-qpqhx, resource: bindings, ignored listing per whitelist
Feb 14 00:14:07.143: INFO: namespace e2e-tests-projected-qpqhx deletion completed in 6.966750872s

• [SLOW TEST:12.241 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:14:07.143: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-mhxnq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-mhxnq
Feb 14 00:14:12.173: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-mhxnq
STEP: checking the pod's current state and verifying that restartCount is present
Feb 14 00:14:12.194: INFO: Initial restart count of pod liveness-http is 0
Feb 14 00:14:28.388: INFO: Restart count of pod e2e-tests-container-probe-mhxnq/liveness-http is now 1 (16.193827072s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:14:28.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-mhxnq" for this suite.
Feb 14 00:14:34.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:14:35.020: INFO: namespace: e2e-tests-container-probe-mhxnq, resource: bindings, ignored listing per whitelist
Feb 14 00:14:35.306: INFO: namespace e2e-tests-container-probe-mhxnq deletion completed in 6.850938859s

• [SLOW TEST:28.162 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:14:35.306: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-9ppdf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:14:36.302: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml version'
Feb 14 00:14:36.479: INFO: stderr: ""
Feb 14 00:14:36.479: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"archive\", BuildDate:\"2019-02-13T22:27:45Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:14:36.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9ppdf" for this suite.
Feb 14 00:14:42.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:14:43.005: INFO: namespace: e2e-tests-kubectl-9ppdf, resource: bindings, ignored listing per whitelist
Feb 14 00:14:43.406: INFO: namespace e2e-tests-kubectl-9ppdf deletion completed in 6.905275321s

• [SLOW TEST:8.100 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:14:43.406: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-l7bst
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 14 00:14:44.573: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-a,UID:87ea9a0e-2fed-11e9-9f3f-da917295d151,ResourceVersion:23911,Generation:0,CreationTimestamp:2019-02-14 00:14:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 00:14:44.573: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-a,UID:87ea9a0e-2fed-11e9-9f3f-da917295d151,ResourceVersion:23911,Generation:0,CreationTimestamp:2019-02-14 00:14:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 14 00:14:54.616: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-a,UID:87ea9a0e-2fed-11e9-9f3f-da917295d151,ResourceVersion:23935,Generation:0,CreationTimestamp:2019-02-14 00:14:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 14 00:14:54.616: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-a,UID:87ea9a0e-2fed-11e9-9f3f-da917295d151,ResourceVersion:23935,Generation:0,CreationTimestamp:2019-02-14 00:14:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 14 00:15:04.659: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-a,UID:87ea9a0e-2fed-11e9-9f3f-da917295d151,ResourceVersion:23955,Generation:0,CreationTimestamp:2019-02-14 00:14:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 00:15:04.659: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-a,UID:87ea9a0e-2fed-11e9-9f3f-da917295d151,ResourceVersion:23955,Generation:0,CreationTimestamp:2019-02-14 00:14:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 14 00:15:14.683: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-a,UID:87ea9a0e-2fed-11e9-9f3f-da917295d151,ResourceVersion:23975,Generation:0,CreationTimestamp:2019-02-14 00:14:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 00:15:14.683: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-a,UID:87ea9a0e-2fed-11e9-9f3f-da917295d151,ResourceVersion:23975,Generation:0,CreationTimestamp:2019-02-14 00:14:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 14 00:15:24.709: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-b,UID:9fd67da5-2fed-11e9-9f3f-da917295d151,ResourceVersion:23996,Generation:0,CreationTimestamp:2019-02-14 00:15:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 00:15:24.709: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-b,UID:9fd67da5-2fed-11e9-9f3f-da917295d151,ResourceVersion:23996,Generation:0,CreationTimestamp:2019-02-14 00:15:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 14 00:15:34.733: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-b,UID:9fd67da5-2fed-11e9-9f3f-da917295d151,ResourceVersion:24016,Generation:0,CreationTimestamp:2019-02-14 00:15:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 00:15:34.733: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-l7bst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7bst/configmaps/e2e-watch-test-configmap-b,UID:9fd67da5-2fed-11e9-9f3f-da917295d151,ResourceVersion:24016,Generation:0,CreationTimestamp:2019-02-14 00:15:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:15:44.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-l7bst" for this suite.
Feb 14 00:15:50.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:15:51.506: INFO: namespace: e2e-tests-watch-l7bst, resource: bindings, ignored listing per whitelist
Feb 14 00:15:51.608: INFO: namespace e2e-tests-watch-l7bst deletion completed in 6.852733547s

• [SLOW TEST:68.202 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:15:51.608: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8kjnj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-b079cb9d-2fed-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 14 00:15:52.649: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b07d055f-2fed-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-8kjnj" to be "success or failure"
Feb 14 00:15:52.669: INFO: Pod "pod-projected-secrets-b07d055f-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.327285ms
Feb 14 00:15:54.693: INFO: Pod "pod-projected-secrets-b07d055f-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044290562s
Feb 14 00:16:25.452: INFO: Pod "pod-projected-secrets-b07d055f-2fed-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.802802966s
STEP: Saw pod success
Feb 14 00:16:25.452: INFO: Pod "pod-projected-secrets-b07d055f-2fed-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:16:25.472: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-secrets-b07d055f-2fed-11e9-9de3-466a6591423c container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 00:16:25.557: INFO: Waiting for pod pod-projected-secrets-b07d055f-2fed-11e9-9de3-466a6591423c to disappear
Feb 14 00:16:25.578: INFO: Pod pod-projected-secrets-b07d055f-2fed-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:16:25.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8kjnj" for this suite.
Feb 14 00:16:31.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:16:31.741: INFO: namespace: e2e-tests-projected-8kjnj, resource: bindings, ignored listing per whitelist
Feb 14 00:16:32.505: INFO: namespace e2e-tests-projected-8kjnj deletion completed in 6.90684632s

• [SLOW TEST:40.898 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:16:32.506: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubelet-test-9n7mn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:16:33.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-9n7mn" for this suite.
Feb 14 00:16:55.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:16:55.865: INFO: namespace: e2e-tests-kubelet-test-9n7mn, resource: bindings, ignored listing per whitelist
Feb 14 00:16:56.475: INFO: namespace e2e-tests-kubelet-test-9n7mn deletion completed in 22.902326308s

• [SLOW TEST:23.969 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:16:56.476: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-lfqr4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-d73754df-2fed-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 14 00:16:57.643: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d73a7d91-2fed-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-lfqr4" to be "success or failure"
Feb 14 00:16:57.664: INFO: Pod "pod-projected-secrets-d73a7d91-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.185599ms
Feb 14 00:16:59.685: INFO: Pod "pod-projected-secrets-d73a7d91-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041480874s
Feb 14 00:17:01.707: INFO: Pod "pod-projected-secrets-d73a7d91-2fed-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06349264s
STEP: Saw pod success
Feb 14 00:17:01.707: INFO: Pod "pod-projected-secrets-d73a7d91-2fed-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:17:01.727: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-secrets-d73a7d91-2fed-11e9-9de3-466a6591423c container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 00:17:01.790: INFO: Waiting for pod pod-projected-secrets-d73a7d91-2fed-11e9-9de3-466a6591423c to disappear
Feb 14 00:17:01.810: INFO: Pod pod-projected-secrets-d73a7d91-2fed-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:17:01.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lfqr4" for this suite.
Feb 14 00:17:07.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:17:08.729: INFO: namespace: e2e-tests-projected-lfqr4, resource: bindings, ignored listing per whitelist
Feb 14 00:17:08.790: INFO: namespace e2e-tests-projected-lfqr4 deletion completed in 6.958718173s

• [SLOW TEST:12.315 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:17:08.790: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-5s89h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 14 00:17:09.727: INFO: Waiting up to 5m0s for pod "var-expansion-de6e36a6-2fed-11e9-9de3-466a6591423c" in namespace "e2e-tests-var-expansion-5s89h" to be "success or failure"
Feb 14 00:17:09.747: INFO: Pod "var-expansion-de6e36a6-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.380249ms
Feb 14 00:17:11.769: INFO: Pod "var-expansion-de6e36a6-2fed-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041889779s
Feb 14 00:17:13.791: INFO: Pod "var-expansion-de6e36a6-2fed-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064497945s
STEP: Saw pod success
Feb 14 00:17:13.791: INFO: Pod "var-expansion-de6e36a6-2fed-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:17:13.812: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod var-expansion-de6e36a6-2fed-11e9-9de3-466a6591423c container dapi-container: <nil>
STEP: delete the pod
Feb 14 00:17:13.875: INFO: Waiting for pod var-expansion-de6e36a6-2fed-11e9-9de3-466a6591423c to disappear
Feb 14 00:17:13.895: INFO: Pod var-expansion-de6e36a6-2fed-11e9-9de3-466a6591423c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:17:13.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-5s89h" for this suite.
Feb 14 00:17:32.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:17:33.050: INFO: namespace: e2e-tests-var-expansion-5s89h, resource: bindings, ignored listing per whitelist
Feb 14 00:17:33.476: INFO: namespace e2e-tests-var-expansion-5s89h deletion completed in 19.559940793s

• [SLOW TEST:24.686 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:17:33.477: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-92d4l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:17:40.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-92d4l" for this suite.
Feb 14 00:18:04.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:18:04.790: INFO: namespace: e2e-tests-replication-controller-92d4l, resource: bindings, ignored listing per whitelist
Feb 14 00:18:05.478: INFO: namespace e2e-tests-replication-controller-92d4l deletion completed in 24.880711359s

• [SLOW TEST:32.001 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:18:05.478: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-p5dqf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:18:06.505: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:18:13.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-p5dqf" for this suite.
Feb 14 00:18:57.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:18:57.390: INFO: namespace: e2e-tests-pods-p5dqf, resource: bindings, ignored listing per whitelist
Feb 14 00:18:57.888: INFO: namespace e2e-tests-pods-p5dqf deletion completed in 44.86566055s

• [SLOW TEST:52.411 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:18:57.889: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-lxr6m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 14 00:19:03.626: INFO: Successfully updated pod "labelsupdate1f899692-2fee-11e9-9de3-466a6591423c"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:19:05.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lxr6m" for this suite.
Feb 14 00:19:29.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:19:30.375: INFO: namespace: e2e-tests-downward-api-lxr6m, resource: bindings, ignored listing per whitelist
Feb 14 00:19:30.620: INFO: namespace e2e-tests-downward-api-lxr6m deletion completed in 24.913680905s

• [SLOW TEST:32.732 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:19:30.621: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2l5jt
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-331474cc-2fee-11e9-9de3-466a6591423c
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-331474cc-2fee-11e9-9de3-466a6591423c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:20:39.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2l5jt" for this suite.
Feb 14 00:21:03.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:21:03.593: INFO: namespace: e2e-tests-projected-2l5jt, resource: bindings, ignored listing per whitelist
Feb 14 00:21:03.941: INFO: namespace e2e-tests-projected-2l5jt deletion completed in 24.876943208s

• [SLOW TEST:93.321 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:21:03.942: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-nqmcj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-6a9f929d-2fee-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 14 00:21:04.953: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6aa2c4d0-2fee-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-nqmcj" to be "success or failure"
Feb 14 00:21:04.973: INFO: Pod "pod-projected-configmaps-6aa2c4d0-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.188062ms
Feb 14 00:21:06.995: INFO: Pod "pod-projected-configmaps-6aa2c4d0-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042000852s
Feb 14 00:21:09.021: INFO: Pod "pod-projected-configmaps-6aa2c4d0-2fee-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067955998s
STEP: Saw pod success
Feb 14 00:21:09.021: INFO: Pod "pod-projected-configmaps-6aa2c4d0-2fee-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:21:09.042: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-configmaps-6aa2c4d0-2fee-11e9-9de3-466a6591423c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:21:09.105: INFO: Waiting for pod pod-projected-configmaps-6aa2c4d0-2fee-11e9-9de3-466a6591423c to disappear
Feb 14 00:21:09.133: INFO: Pod pod-projected-configmaps-6aa2c4d0-2fee-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:21:09.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nqmcj" for this suite.
Feb 14 00:21:15.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:21:15.423: INFO: namespace: e2e-tests-projected-nqmcj, resource: bindings, ignored listing per whitelist
Feb 14 00:21:16.044: INFO: namespace e2e-tests-projected-nqmcj deletion completed in 6.890667883s

• [SLOW TEST:12.103 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:21:16.045: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-9tr7s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 00:21:17.028: INFO: Waiting up to 5m0s for pod "downward-api-71d577a7-2fee-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-9tr7s" to be "success or failure"
Feb 14 00:21:17.061: INFO: Pod "downward-api-71d577a7-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 32.319898ms
Feb 14 00:21:19.082: INFO: Pod "downward-api-71d577a7-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053825143s
Feb 14 00:21:21.104: INFO: Pod "downward-api-71d577a7-2fee-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075378333s
STEP: Saw pod success
Feb 14 00:21:21.104: INFO: Pod "downward-api-71d577a7-2fee-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:21:21.124: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downward-api-71d577a7-2fee-11e9-9de3-466a6591423c container dapi-container: <nil>
STEP: delete the pod
Feb 14 00:21:21.193: INFO: Waiting for pod downward-api-71d577a7-2fee-11e9-9de3-466a6591423c to disappear
Feb 14 00:21:21.213: INFO: Pod downward-api-71d577a7-2fee-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:21:21.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9tr7s" for this suite.
Feb 14 00:21:27.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:21:28.031: INFO: namespace: e2e-tests-downward-api-9tr7s, resource: bindings, ignored listing per whitelist
Feb 14 00:21:28.137: INFO: namespace e2e-tests-downward-api-9tr7s deletion completed in 6.903147196s

• [SLOW TEST:12.093 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:21:28.138: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5twk7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 14 00:21:29.198: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:30.853: INFO: stderr: ""
Feb 14 00:21:30.853: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 00:21:30.853: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:31.022: INFO: stderr: ""
Feb 14 00:21:31.022: INFO: stdout: "update-demo-nautilus-48pbl update-demo-nautilus-gpmss "
Feb 14 00:21:31.023: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-48pbl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:31.198: INFO: stderr: ""
Feb 14 00:21:31.198: INFO: stdout: ""
Feb 14 00:21:31.198: INFO: update-demo-nautilus-48pbl is created but not running
Feb 14 00:21:36.199: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:36.364: INFO: stderr: ""
Feb 14 00:21:36.364: INFO: stdout: "update-demo-nautilus-48pbl update-demo-nautilus-gpmss "
Feb 14 00:21:36.364: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-48pbl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:36.564: INFO: stderr: ""
Feb 14 00:21:36.564: INFO: stdout: "true"
Feb 14 00:21:36.564: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-48pbl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:36.723: INFO: stderr: ""
Feb 14 00:21:36.723: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:21:36.723: INFO: validating pod update-demo-nautilus-48pbl
Feb 14 00:21:36.834: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:21:36.835: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:21:36.835: INFO: update-demo-nautilus-48pbl is verified up and running
Feb 14 00:21:36.835: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-gpmss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:36.996: INFO: stderr: ""
Feb 14 00:21:36.996: INFO: stdout: "true"
Feb 14 00:21:36.996: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-gpmss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:37.161: INFO: stderr: ""
Feb 14 00:21:37.161: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:21:37.161: INFO: validating pod update-demo-nautilus-gpmss
Feb 14 00:21:37.273: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:21:37.274: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:21:37.274: INFO: update-demo-nautilus-gpmss is verified up and running
STEP: scaling down the replication controller
Feb 14 00:21:37.288: INFO: scanned /root for discovery docs: <nil>
Feb 14 00:21:37.288: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:37.530: INFO: stderr: ""
Feb 14 00:21:37.530: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 00:21:37.530: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:37.695: INFO: stderr: ""
Feb 14 00:21:37.695: INFO: stdout: "update-demo-nautilus-48pbl update-demo-nautilus-gpmss "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 14 00:21:42.695: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:42.859: INFO: stderr: ""
Feb 14 00:21:42.859: INFO: stdout: "update-demo-nautilus-48pbl update-demo-nautilus-gpmss "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 14 00:21:47.860: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:48.069: INFO: stderr: ""
Feb 14 00:21:48.069: INFO: stdout: "update-demo-nautilus-gpmss "
Feb 14 00:21:48.069: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-gpmss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:48.228: INFO: stderr: ""
Feb 14 00:21:48.229: INFO: stdout: "true"
Feb 14 00:21:48.229: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-gpmss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:48.401: INFO: stderr: ""
Feb 14 00:21:48.401: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:21:48.401: INFO: validating pod update-demo-nautilus-gpmss
Feb 14 00:21:48.427: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:21:48.428: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:21:48.428: INFO: update-demo-nautilus-gpmss is verified up and running
STEP: scaling up the replication controller
Feb 14 00:21:48.432: INFO: scanned /root for discovery docs: <nil>
Feb 14 00:21:48.432: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:48.682: INFO: stderr: ""
Feb 14 00:21:48.682: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 00:21:48.682: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:48.840: INFO: stderr: ""
Feb 14 00:21:48.840: INFO: stdout: "update-demo-nautilus-gpmss update-demo-nautilus-pq467 "
Feb 14 00:21:48.840: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-gpmss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:48.996: INFO: stderr: ""
Feb 14 00:21:48.996: INFO: stdout: "true"
Feb 14 00:21:48.996: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-gpmss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:49.157: INFO: stderr: ""
Feb 14 00:21:49.157: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:21:49.157: INFO: validating pod update-demo-nautilus-gpmss
Feb 14 00:21:49.184: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:21:49.184: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:21:49.184: INFO: update-demo-nautilus-gpmss is verified up and running
Feb 14 00:21:49.184: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-pq467 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:49.341: INFO: stderr: ""
Feb 14 00:21:49.341: INFO: stdout: ""
Feb 14 00:21:49.341: INFO: update-demo-nautilus-pq467 is created but not running
Feb 14 00:21:54.341: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:54.508: INFO: stderr: ""
Feb 14 00:21:54.509: INFO: stdout: "update-demo-nautilus-gpmss update-demo-nautilus-pq467 "
Feb 14 00:21:54.509: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-gpmss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:54.664: INFO: stderr: ""
Feb 14 00:21:54.664: INFO: stdout: "true"
Feb 14 00:21:54.664: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-gpmss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:54.819: INFO: stderr: ""
Feb 14 00:21:54.819: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:21:54.819: INFO: validating pod update-demo-nautilus-gpmss
Feb 14 00:21:54.846: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:21:54.846: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:21:54.846: INFO: update-demo-nautilus-gpmss is verified up and running
Feb 14 00:21:54.846: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-pq467 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:55.013: INFO: stderr: ""
Feb 14 00:21:55.013: INFO: stdout: "true"
Feb 14 00:21:55.013: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-pq467 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:55.173: INFO: stderr: ""
Feb 14 00:21:55.173: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:21:55.173: INFO: validating pod update-demo-nautilus-pq467
Feb 14 00:21:55.283: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:21:55.283: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:21:55.283: INFO: update-demo-nautilus-pq467 is verified up and running
STEP: using delete to clean up resources
Feb 14 00:21:55.283: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:55.462: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 00:21:55.463: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 14 00:21:55.463: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-5twk7'
Feb 14 00:21:55.684: INFO: stderr: "No resources found.\n"
Feb 14 00:21:55.684: INFO: stdout: ""
Feb 14 00:21:55.684: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-5twk7 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 14 00:21:55.848: INFO: stderr: ""
Feb 14 00:21:55.848: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:21:55.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5twk7" for this suite.
Feb 14 00:22:17.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:22:18.652: INFO: namespace: e2e-tests-kubectl-5twk7, resource: bindings, ignored listing per whitelist
Feb 14 00:22:18.780: INFO: namespace e2e-tests-kubectl-5twk7 deletion completed in 22.909799681s

• [SLOW TEST:50.642 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:22:18.780: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-jw2rm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:22:19.802: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 14 00:22:19.882: INFO: Number of nodes with available pods: 0
Feb 14 00:22:19.882: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 14 00:22:20.925: INFO: Number of nodes with available pods: 0
Feb 14 00:22:20.925: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 14 00:22:21.929: INFO: Number of nodes with available pods: 0
Feb 14 00:22:21.930: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 14 00:22:22.925: INFO: Number of nodes with available pods: 1
Feb 14 00:22:22.925: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 14 00:22:23.926: INFO: Number of nodes with available pods: 2
Feb 14 00:22:23.926: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 14 00:22:24.057: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:24.057: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:25.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:25.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:26.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:26.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:27.112: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:27.112: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:28.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:28.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:29.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:29.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:30.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:30.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:31.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:31.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:32.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:32.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:33.102: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:33.102: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:34.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:34.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:35.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:35.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:36.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:36.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:37.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:37.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:38.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:38.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:39.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:39.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:40.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:40.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:41.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:41.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:42.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:42.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:43.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:43.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:44.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:44.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:45.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:45.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:46.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:46.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:47.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:47.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:48.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:48.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:49.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:49.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:50.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:50.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:51.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:51.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:52.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:52.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:53.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:53.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:54.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:54.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:55.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:55.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:56.101: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:56.101: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:56.101: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:22:57.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:57.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:57.100: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:22:58.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:58.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:58.100: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:22:59.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:59.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:22:59.099: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:23:00.101: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:00.101: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:00.101: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:23:01.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:01.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:01.100: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:23:02.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:02.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:02.100: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:23:03.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:03.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:03.100: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:23:04.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:04.099: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:04.099: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:23:05.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:05.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:05.100: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:23:06.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:06.100: INFO: Wrong image for pod: daemon-set-t4wmv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:06.100: INFO: Pod daemon-set-t4wmv is not available
Feb 14 00:23:07.104: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:07.104: INFO: Pod daemon-set-kk4mz is not available
Feb 14 00:23:08.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:08.100: INFO: Pod daemon-set-kk4mz is not available
Feb 14 00:23:09.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:09.100: INFO: Pod daemon-set-kk4mz is not available
Feb 14 00:23:10.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:10.099: INFO: Pod daemon-set-kk4mz is not available
Feb 14 00:23:11.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:11.100: INFO: Pod daemon-set-kk4mz is not available
Feb 14 00:23:12.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:13.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:14.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:15.101: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:16.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:17.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:18.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:19.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:20.101: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:21.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:22.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:23.105: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:24.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:25.101: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:26.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:27.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:28.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:29.105: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:30.101: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:31.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:32.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:33.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:34.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:35.101: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:36.101: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:37.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:38.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:39.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:40.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:41.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:42.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:43.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:44.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:44.100: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:45.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:45.100: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:46.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:46.099: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:47.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:47.100: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:48.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:48.100: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:49.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:49.100: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:50.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:50.100: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:51.101: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:51.102: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:52.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:52.100: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:53.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:53.100: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:54.100: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:54.100: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:55.099: INFO: Wrong image for pod: daemon-set-fhc79. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 14 00:23:55.099: INFO: Pod daemon-set-fhc79 is not available
Feb 14 00:23:56.105: INFO: Pod daemon-set-r6jzj is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 14 00:23:56.169: INFO: Number of nodes with available pods: 1
Feb 14 00:23:56.169: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 14 00:23:57.212: INFO: Number of nodes with available pods: 1
Feb 14 00:23:57.213: INFO: Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx is running more than one daemon pod
Feb 14 00:23:58.213: INFO: Number of nodes with available pods: 2
Feb 14 00:23:58.213: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-jw2rm, will wait for the garbage collector to delete the pods
Feb 14 00:23:58.415: INFO: Deleting DaemonSet.extensions daemon-set took: 23.599038ms
Feb 14 00:23:58.515: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.241965ms
Feb 14 00:24:06.836: INFO: Number of nodes with available pods: 0
Feb 14 00:24:06.836: INFO: Number of running nodes: 0, number of available pods: 0
Feb 14 00:24:06.857: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-jw2rm/daemonsets","resourceVersion":"25341"},"items":null}

Feb 14 00:24:06.877: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-jw2rm/pods","resourceVersion":"25341"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:24:06.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-jw2rm" for this suite.
Feb 14 00:24:13.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:24:13.459: INFO: namespace: e2e-tests-daemonsets-jw2rm, resource: bindings, ignored listing per whitelist
Feb 14 00:24:13.833: INFO: namespace e2e-tests-daemonsets-jw2rm deletion completed in 6.871122837s

• [SLOW TEST:115.054 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:24:13.834: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hn7r8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-dbc19a0b-2fee-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 14 00:24:14.758: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dbc4d2a2-2fee-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-hn7r8" to be "success or failure"
Feb 14 00:24:14.789: INFO: Pod "pod-projected-secrets-dbc4d2a2-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 31.566107ms
Feb 14 00:24:16.811: INFO: Pod "pod-projected-secrets-dbc4d2a2-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053088798s
Feb 14 00:24:18.832: INFO: Pod "pod-projected-secrets-dbc4d2a2-2fee-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074679321s
STEP: Saw pod success
Feb 14 00:24:18.833: INFO: Pod "pod-projected-secrets-dbc4d2a2-2fee-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:24:18.853: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-secrets-dbc4d2a2-2fee-11e9-9de3-466a6591423c container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 00:24:18.920: INFO: Waiting for pod pod-projected-secrets-dbc4d2a2-2fee-11e9-9de3-466a6591423c to disappear
Feb 14 00:24:18.955: INFO: Pod pod-projected-secrets-dbc4d2a2-2fee-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:24:18.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hn7r8" for this suite.
Feb 14 00:24:25.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:24:25.801: INFO: namespace: e2e-tests-projected-hn7r8, resource: bindings, ignored listing per whitelist
Feb 14 00:24:25.905: INFO: namespace e2e-tests-projected-hn7r8 deletion completed in 6.929016369s

• [SLOW TEST:12.071 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:24:25.905: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-hh5cr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 14 00:24:26.937: INFO: Waiting up to 5m0s for pod "client-containers-e306f286-2fee-11e9-9de3-466a6591423c" in namespace "e2e-tests-containers-hh5cr" to be "success or failure"
Feb 14 00:24:26.958: INFO: Pod "client-containers-e306f286-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.79361ms
Feb 14 00:24:28.981: INFO: Pod "client-containers-e306f286-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043355379s
Feb 14 00:24:31.003: INFO: Pod "client-containers-e306f286-2fee-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065877252s
STEP: Saw pod success
Feb 14 00:24:31.004: INFO: Pod "client-containers-e306f286-2fee-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:24:31.024: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod client-containers-e306f286-2fee-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 14 00:24:31.082: INFO: Waiting for pod client-containers-e306f286-2fee-11e9-9de3-466a6591423c to disappear
Feb 14 00:24:31.103: INFO: Pod client-containers-e306f286-2fee-11e9-9de3-466a6591423c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:24:31.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-hh5cr" for this suite.
Feb 14 00:24:37.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:24:38.028: INFO: namespace: e2e-tests-containers-hh5cr, resource: bindings, ignored listing per whitelist
Feb 14 00:24:38.049: INFO: namespace e2e-tests-containers-hh5cr deletion completed in 6.923963005s

• [SLOW TEST:12.143 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:24:38.049: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-jffb5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-ea49a61a-2fee-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 14 00:24:39.137: INFO: Waiting up to 5m0s for pod "pod-configmaps-ea4cd4c5-2fee-11e9-9de3-466a6591423c" in namespace "e2e-tests-configmap-jffb5" to be "success or failure"
Feb 14 00:24:39.158: INFO: Pod "pod-configmaps-ea4cd4c5-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.407427ms
Feb 14 00:24:41.189: INFO: Pod "pod-configmaps-ea4cd4c5-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051174466s
Feb 14 00:24:43.215: INFO: Pod "pod-configmaps-ea4cd4c5-2fee-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077396512s
STEP: Saw pod success
Feb 14 00:24:43.215: INFO: Pod "pod-configmaps-ea4cd4c5-2fee-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:24:43.236: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-configmaps-ea4cd4c5-2fee-11e9-9de3-466a6591423c container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:24:43.339: INFO: Waiting for pod pod-configmaps-ea4cd4c5-2fee-11e9-9de3-466a6591423c to disappear
Feb 14 00:24:43.360: INFO: Pod pod-configmaps-ea4cd4c5-2fee-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:24:43.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jffb5" for this suite.
Feb 14 00:24:49.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:24:50.242: INFO: namespace: e2e-tests-configmap-jffb5, resource: bindings, ignored listing per whitelist
Feb 14 00:24:50.347: INFO: namespace e2e-tests-configmap-jffb5 deletion completed in 6.966033739s

• [SLOW TEST:12.298 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:24:50.347: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bl8zx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-f19dc61a-2fee-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 14 00:24:51.433: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1a0fd2f-2fee-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-bl8zx" to be "success or failure"
Feb 14 00:24:51.454: INFO: Pod "pod-projected-configmaps-f1a0fd2f-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.477368ms
Feb 14 00:24:53.475: INFO: Pod "pod-projected-configmaps-f1a0fd2f-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041465252s
Feb 14 00:24:55.500: INFO: Pod "pod-projected-configmaps-f1a0fd2f-2fee-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066379947s
STEP: Saw pod success
Feb 14 00:24:55.500: INFO: Pod "pod-projected-configmaps-f1a0fd2f-2fee-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:24:55.527: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-configmaps-f1a0fd2f-2fee-11e9-9de3-466a6591423c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:24:55.584: INFO: Waiting for pod pod-projected-configmaps-f1a0fd2f-2fee-11e9-9de3-466a6591423c to disappear
Feb 14 00:24:55.605: INFO: Pod pod-projected-configmaps-f1a0fd2f-2fee-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:24:55.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bl8zx" for this suite.
Feb 14 00:25:01.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:25:02.240: INFO: namespace: e2e-tests-projected-bl8zx, resource: bindings, ignored listing per whitelist
Feb 14 00:25:02.547: INFO: namespace e2e-tests-projected-bl8zx deletion completed in 6.920800176s

• [SLOW TEST:12.200 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:25:02.548: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-g8558
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-f8e6b88a-2fee-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 14 00:25:03.656: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f8ea049a-2fee-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-g8558" to be "success or failure"
Feb 14 00:25:03.676: INFO: Pod "pod-projected-secrets-f8ea049a-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.738788ms
Feb 14 00:25:05.698: INFO: Pod "pod-projected-secrets-f8ea049a-2fee-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04194395s
Feb 14 00:25:07.719: INFO: Pod "pod-projected-secrets-f8ea049a-2fee-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063089658s
STEP: Saw pod success
Feb 14 00:25:07.719: INFO: Pod "pod-projected-secrets-f8ea049a-2fee-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:25:07.740: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-secrets-f8ea049a-2fee-11e9-9de3-466a6591423c container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 00:25:07.803: INFO: Waiting for pod pod-projected-secrets-f8ea049a-2fee-11e9-9de3-466a6591423c to disappear
Feb 14 00:25:07.833: INFO: Pod pod-projected-secrets-f8ea049a-2fee-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:25:07.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-g8558" for this suite.
Feb 14 00:25:13.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:25:14.640: INFO: namespace: e2e-tests-projected-g8558, resource: bindings, ignored listing per whitelist
Feb 14 00:25:14.747: INFO: namespace e2e-tests-projected-g8558 deletion completed in 6.892310498s

• [SLOW TEST:12.199 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:25:14.747: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-f76h7
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-0021c56e-2fef-11e9-9de3-466a6591423c
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-0021c56e-2fef-11e9-9de3-466a6591423c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:25:21.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-f76h7" for this suite.
Feb 14 00:25:42.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:25:42.243: INFO: namespace: e2e-tests-configmap-f76h7, resource: bindings, ignored listing per whitelist
Feb 14 00:25:42.877: INFO: namespace e2e-tests-configmap-f76h7 deletion completed in 20.885853422s

• [SLOW TEST:28.130 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:25:42.877: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-s2s89
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-10ea55f1-2fef-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 14 00:25:43.944: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-10ed7f63-2fef-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-s2s89" to be "success or failure"
Feb 14 00:25:43.968: INFO: Pod "pod-projected-configmaps-10ed7f63-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 24.047945ms
Feb 14 00:25:45.989: INFO: Pod "pod-projected-configmaps-10ed7f63-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045679377s
Feb 14 00:25:48.011: INFO: Pod "pod-projected-configmaps-10ed7f63-2fef-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067174903s
STEP: Saw pod success
Feb 14 00:25:48.011: INFO: Pod "pod-projected-configmaps-10ed7f63-2fef-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:25:48.039: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-configmaps-10ed7f63-2fef-11e9-9de3-466a6591423c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:25:48.105: INFO: Waiting for pod pod-projected-configmaps-10ed7f63-2fef-11e9-9de3-466a6591423c to disappear
Feb 14 00:25:48.125: INFO: Pod pod-projected-configmaps-10ed7f63-2fef-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:25:48.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s2s89" for this suite.
Feb 14 00:25:54.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:25:54.289: INFO: namespace: e2e-tests-projected-s2s89, resource: bindings, ignored listing per whitelist
Feb 14 00:25:55.051: INFO: namespace e2e-tests-projected-s2s89 deletion completed in 6.902748873s

• [SLOW TEST:12.174 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:25:55.052: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-l6gmn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 14 00:26:06.247: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:26:06.268: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 00:26:08.268: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:26:08.291: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 00:26:10.268: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:26:10.290: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:26:10.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-l6gmn" for this suite.
Feb 14 00:26:34.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:26:34.978: INFO: namespace: e2e-tests-container-lifecycle-hook-l6gmn, resource: bindings, ignored listing per whitelist
Feb 14 00:26:35.165: INFO: namespace e2e-tests-container-lifecycle-hook-l6gmn deletion completed in 24.853700866s

• [SLOW TEST:40.114 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:26:35.165: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-545l9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 14 00:26:36.227: INFO: Waiting up to 5m0s for pod "pod-30172229-2fef-11e9-9de3-466a6591423c" in namespace "e2e-tests-emptydir-545l9" to be "success or failure"
Feb 14 00:26:36.247: INFO: Pod "pod-30172229-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.630343ms
Feb 14 00:26:38.274: INFO: Pod "pod-30172229-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047620314s
Feb 14 00:26:40.295: INFO: Pod "pod-30172229-2fef-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068596041s
STEP: Saw pod success
Feb 14 00:26:40.295: INFO: Pod "pod-30172229-2fef-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:26:40.316: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-30172229-2fef-11e9-9de3-466a6591423c container test-container: <nil>
STEP: delete the pod
Feb 14 00:26:40.390: INFO: Waiting for pod pod-30172229-2fef-11e9-9de3-466a6591423c to disappear
Feb 14 00:26:40.410: INFO: Pod pod-30172229-2fef-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:26:40.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-545l9" for this suite.
Feb 14 00:26:46.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:26:46.912: INFO: namespace: e2e-tests-emptydir-545l9, resource: bindings, ignored listing per whitelist
Feb 14 00:26:47.327: INFO: namespace e2e-tests-emptydir-545l9 deletion completed in 6.894175762s

• [SLOW TEST:12.161 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:26:47.327: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-5wdjm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Feb 14 00:26:48.224: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 14 00:26:48.273: INFO: Waiting for terminating namespaces to be deleted...
Feb 14 00:26:48.294: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx before test
Feb 14 00:26:48.321: INFO: kube-proxy-n2tgw from kube-system started at 2019-02-13 22:03:50 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.321: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 00:26:48.321: INFO: node-exporter-clvlv from kube-system started at 2019-02-13 22:03:50 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.321: INFO: 	Container node-exporter ready: true, restart count 0
Feb 14 00:26:48.321: INFO: calico-node-skgqz from kube-system started at 2019-02-13 22:03:50 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.321: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 00:26:48.321: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp before test
Feb 14 00:26:48.358: INFO: node-exporter-tmhn5 from kube-system started at 2019-02-13 22:03:43 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container node-exporter ready: true, restart count 0
Feb 14 00:26:48.358: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-kj5ch from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 14 00:26:48.358: INFO: kube-proxy-6688x from kube-system started at 2019-02-13 22:03:43 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 00:26:48.358: INFO: calico-node-psbns from kube-system started at 2019-02-13 22:03:43 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 00:26:48.358: INFO: addons-nginx-ingress-controller-d74bfff57-87jx9 from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 14 00:26:48.358: INFO: blackbox-exporter-d6c46f9fc-5xt9g from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 14 00:26:48.358: INFO: coredns-67df79bbdd-96tpz from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container coredns ready: true, restart count 0
Feb 14 00:26:48.358: INFO: metrics-server-65cc55bd79-8kjg8 from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container metrics-server ready: true, restart count 0
Feb 14 00:26:48.358: INFO: addons-kubernetes-dashboard-6579b646c5-g48vq from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 14 00:26:48.358: INFO: addons-kube-lego-69bbdc96b6-6m8wg from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container kube-lego ready: true, restart count 0
Feb 14 00:26:48.358: INFO: vpn-shoot-c76596df-6nsrb from kube-system started at 2019-02-13 22:04:02 +0000 UTC (1 container statuses recorded)
Feb 14 00:26:48.358: INFO: 	Container vpn-shoot ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
STEP: verifying the node has the label node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod addons-kube-lego-69bbdc96b6-6m8wg requesting resource cpu=20m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod addons-kubernetes-dashboard-6579b646c5-g48vq requesting resource cpu=50m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod addons-nginx-ingress-controller-d74bfff57-87jx9 requesting resource cpu=100m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-74b5975d7-kj5ch requesting resource cpu=0m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod blackbox-exporter-d6c46f9fc-5xt9g requesting resource cpu=5m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod calico-node-psbns requesting resource cpu=100m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod calico-node-skgqz requesting resource cpu=100m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 14 00:26:48.498: INFO: Pod coredns-67df79bbdd-96tpz requesting resource cpu=50m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod kube-proxy-6688x requesting resource cpu=20m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod kube-proxy-n2tgw requesting resource cpu=20m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 14 00:26:48.498: INFO: Pod metrics-server-65cc55bd79-8kjg8 requesting resource cpu=20m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod node-exporter-clvlv requesting resource cpu=5m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
Feb 14 00:26:48.498: INFO: Pod node-exporter-tmhn5 requesting resource cpu=5m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
Feb 14 00:26:48.498: INFO: Pod vpn-shoot-c76596df-6nsrb requesting resource cpu=50m on Node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-376b71b4-2fef-11e9-9de3-466a6591423c.1583136208b08b7f], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-5wdjm/filler-pod-376b71b4-2fef-11e9-9de3-466a6591423c to shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-376b71b4-2fef-11e9-9de3-466a6591423c.1583136251585fcf], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-376b71b4-2fef-11e9-9de3-466a6591423c.1583136271017630], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-376b71b4-2fef-11e9-9de3-466a6591423c.158313627c539cba], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-376f3279-2fef-11e9-9de3-466a6591423c.15831362098bf2b1], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-5wdjm/filler-pod-376f3279-2fef-11e9-9de3-466a6591423c to shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-376f3279-2fef-11e9-9de3-466a6591423c.158313625c2fa57d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-376f3279-2fef-11e9-9de3-466a6591423c.1583136275f718c1], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-376f3279-2fef-11e9-9de3-466a6591423c.1583136280ff03e6], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15831362ff5adbd6], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-z8sbp
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:26:53.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-5wdjm" for this suite.
Feb 14 00:26:59.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:27:00.153: INFO: namespace: e2e-tests-sched-pred-5wdjm, resource: bindings, ignored listing per whitelist
Feb 14 00:27:00.827: INFO: namespace e2e-tests-sched-pred-5wdjm deletion completed in 6.947991213s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:13.500 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:27:00.827: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rhdkk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 14 00:27:01.806: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:02.390: INFO: stderr: ""
Feb 14 00:27:02.390: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 00:27:02.390: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:02.675: INFO: stderr: ""
Feb 14 00:27:02.675: INFO: stdout: "update-demo-nautilus-p4qg9 update-demo-nautilus-ww64l "
Feb 14 00:27:02.675: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-p4qg9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:02.848: INFO: stderr: ""
Feb 14 00:27:02.848: INFO: stdout: ""
Feb 14 00:27:02.848: INFO: update-demo-nautilus-p4qg9 is created but not running
Feb 14 00:27:07.850: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:08.087: INFO: stderr: ""
Feb 14 00:27:08.087: INFO: stdout: "update-demo-nautilus-p4qg9 update-demo-nautilus-ww64l "
Feb 14 00:27:08.087: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-p4qg9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:08.268: INFO: stderr: ""
Feb 14 00:27:08.268: INFO: stdout: "true"
Feb 14 00:27:08.268: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-p4qg9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:08.448: INFO: stderr: ""
Feb 14 00:27:08.448: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:27:08.448: INFO: validating pod update-demo-nautilus-p4qg9
Feb 14 00:27:08.561: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:27:08.561: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:27:08.561: INFO: update-demo-nautilus-p4qg9 is verified up and running
Feb 14 00:27:08.561: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-ww64l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:08.730: INFO: stderr: ""
Feb 14 00:27:08.730: INFO: stdout: "true"
Feb 14 00:27:08.730: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-ww64l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:08.965: INFO: stderr: ""
Feb 14 00:27:08.965: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:27:08.965: INFO: validating pod update-demo-nautilus-ww64l
Feb 14 00:27:09.075: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:27:09.075: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:27:09.075: INFO: update-demo-nautilus-ww64l is verified up and running
STEP: rolling-update to new replication controller
Feb 14 00:27:09.079: INFO: scanned /root for discovery docs: <nil>
Feb 14 00:27:09.079: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:34.991: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 14 00:27:34.992: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 00:27:34.992: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:35.303: INFO: stderr: ""
Feb 14 00:27:35.303: INFO: stdout: "update-demo-kitten-q465s update-demo-kitten-v558g "
Feb 14 00:27:35.304: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-kitten-q465s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:35.511: INFO: stderr: ""
Feb 14 00:27:35.511: INFO: stdout: "true"
Feb 14 00:27:35.511: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-kitten-q465s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:35.717: INFO: stderr: ""
Feb 14 00:27:35.717: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 14 00:27:35.717: INFO: validating pod update-demo-kitten-q465s
Feb 14 00:27:35.828: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 14 00:27:35.828: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 14 00:27:35.829: INFO: update-demo-kitten-q465s is verified up and running
Feb 14 00:27:35.829: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-kitten-v558g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:36.033: INFO: stderr: ""
Feb 14 00:27:36.033: INFO: stdout: "true"
Feb 14 00:27:36.033: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-kitten-v558g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rhdkk'
Feb 14 00:27:36.267: INFO: stderr: ""
Feb 14 00:27:36.267: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 14 00:27:36.267: INFO: validating pod update-demo-kitten-v558g
Feb 14 00:27:36.381: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 14 00:27:36.381: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 14 00:27:36.381: INFO: update-demo-kitten-v558g is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:27:36.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rhdkk" for this suite.
Feb 14 00:28:00.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:28:00.841: INFO: namespace: e2e-tests-kubectl-rhdkk, resource: bindings, ignored listing per whitelist
Feb 14 00:28:01.277: INFO: namespace e2e-tests-kubectl-rhdkk deletion completed in 24.869688608s

• [SLOW TEST:60.451 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:28:01.278: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-4t8gk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-636e1122-2fef-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 14 00:28:02.380: INFO: Waiting up to 5m0s for pod "pod-secrets-63713e9c-2fef-11e9-9de3-466a6591423c" in namespace "e2e-tests-secrets-4t8gk" to be "success or failure"
Feb 14 00:28:02.400: INFO: Pod "pod-secrets-63713e9c-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.332192ms
Feb 14 00:28:04.421: INFO: Pod "pod-secrets-63713e9c-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041418926s
Feb 14 00:28:06.443: INFO: Pod "pod-secrets-63713e9c-2fef-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063068717s
STEP: Saw pod success
Feb 14 00:28:06.443: INFO: Pod "pod-secrets-63713e9c-2fef-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:28:06.477: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-secrets-63713e9c-2fef-11e9-9de3-466a6591423c container secret-env-test: <nil>
STEP: delete the pod
Feb 14 00:28:06.540: INFO: Waiting for pod pod-secrets-63713e9c-2fef-11e9-9de3-466a6591423c to disappear
Feb 14 00:28:06.564: INFO: Pod pod-secrets-63713e9c-2fef-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:28:06.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4t8gk" for this suite.
Feb 14 00:28:12.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:28:12.757: INFO: namespace: e2e-tests-secrets-4t8gk, resource: bindings, ignored listing per whitelist
Feb 14 00:28:13.441: INFO: namespace e2e-tests-secrets-4t8gk deletion completed in 6.852504626s

• [SLOW TEST:12.164 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:28:13.442: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-jjk2c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 14 00:28:14.910: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-jjk2c,SelfLink:/api/v1/namespaces/e2e-tests-watch-jjk2c/configmaps/e2e-watch-test-resource-version,UID:6ad75ff1-2fef-11e9-9f3f-da917295d151,ResourceVersion:26171,Generation:0,CreationTimestamp:2019-02-14 00:28:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 00:28:14.911: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-jjk2c,SelfLink:/api/v1/namespaces/e2e-tests-watch-jjk2c/configmaps/e2e-watch-test-resource-version,UID:6ad75ff1-2fef-11e9-9f3f-da917295d151,ResourceVersion:26172,Generation:0,CreationTimestamp:2019-02-14 00:28:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:28:14.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-jjk2c" for this suite.
Feb 14 00:28:20.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:28:21.205: INFO: namespace: e2e-tests-watch-jjk2c, resource: bindings, ignored listing per whitelist
Feb 14 00:28:21.795: INFO: namespace e2e-tests-watch-jjk2c deletion completed in 6.862873633s

• [SLOW TEST:8.354 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:28:21.796: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-sk7gk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 00:28:22.804: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-sk7gk'
Feb 14 00:28:23.053: INFO: stderr: ""
Feb 14 00:28:23.053: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 14 00:28:28.104: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-sk7gk -o json'
Feb 14 00:28:28.365: INFO: stderr: ""
Feb 14 00:28:28.365: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.246/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-02-14T00:28:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-sk7gk\",\n        \"resourceVersion\": \"26210\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-sk7gk/pods/e2e-test-nginx-pod\",\n        \"uid\": \"6fc30134-2fef-11e9-9f3f-da917295d151\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-gtp2p\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-gtp2p\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-gtp2p\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T00:28:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T00:28:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T00:28:25Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T00:28:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://93c94cb4fffff2024c7a29d1b289e8fe7abc68cad2a2322fa90b604d99ce1e91\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-14T00:28:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.246\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-14T00:28:23Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 14 00:28:28.365: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml replace -f - --namespace=e2e-tests-kubectl-sk7gk'
Feb 14 00:28:28.797: INFO: stderr: ""
Feb 14 00:28:28.797: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Feb 14 00:28:28.818: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-sk7gk'
Feb 14 00:28:31.895: INFO: stderr: ""
Feb 14 00:28:31.895: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:28:31.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sk7gk" for this suite.
Feb 14 00:28:37.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:28:38.424: INFO: namespace: e2e-tests-kubectl-sk7gk, resource: bindings, ignored listing per whitelist
Feb 14 00:28:38.856: INFO: namespace e2e-tests-kubectl-sk7gk deletion completed in 6.939154151s

• [SLOW TEST:17.060 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:28:38.856: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-6bnb6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:28:39.933: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79d3458e-2fef-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-6bnb6" to be "success or failure"
Feb 14 00:28:39.954: INFO: Pod "downwardapi-volume-79d3458e-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.421598ms
Feb 14 00:28:41.975: INFO: Pod "downwardapi-volume-79d3458e-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041712245s
Feb 14 00:28:43.996: INFO: Pod "downwardapi-volume-79d3458e-2fef-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062645683s
STEP: Saw pod success
Feb 14 00:28:43.996: INFO: Pod "downwardapi-volume-79d3458e-2fef-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:28:44.017: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-79d3458e-2fef-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 14 00:28:44.185: INFO: Waiting for pod downwardapi-volume-79d3458e-2fef-11e9-9de3-466a6591423c to disappear
Feb 14 00:28:44.212: INFO: Pod downwardapi-volume-79d3458e-2fef-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:28:44.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6bnb6" for this suite.
Feb 14 00:28:50.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:28:51.056: INFO: namespace: e2e-tests-downward-api-6bnb6, resource: bindings, ignored listing per whitelist
Feb 14 00:28:51.181: INFO: namespace e2e-tests-downward-api-6bnb6 deletion completed in 6.947051226s

• [SLOW TEST:12.324 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:28:51.181: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-cz6pz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0214 00:28:52.810207   31687 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 00:28:52.810: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:28:52.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-cz6pz" for this suite.
Feb 14 00:28:58.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:28:59.434: INFO: namespace: e2e-tests-gc-cz6pz, resource: bindings, ignored listing per whitelist
Feb 14 00:28:59.768: INFO: namespace e2e-tests-gc-cz6pz deletion completed in 6.936724257s

• [SLOW TEST:8.587 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:28:59.768: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-swbwn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 14 00:29:00.729: INFO: Waiting up to 5m0s for pod "var-expansion-8638a204-2fef-11e9-9de3-466a6591423c" in namespace "e2e-tests-var-expansion-swbwn" to be "success or failure"
Feb 14 00:29:00.751: INFO: Pod "var-expansion-8638a204-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.93438ms
Feb 14 00:29:02.773: INFO: Pod "var-expansion-8638a204-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043802533s
Feb 14 00:29:04.795: INFO: Pod "var-expansion-8638a204-2fef-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065098139s
STEP: Saw pod success
Feb 14 00:29:04.795: INFO: Pod "var-expansion-8638a204-2fef-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:29:04.816: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod var-expansion-8638a204-2fef-11e9-9de3-466a6591423c container dapi-container: <nil>
STEP: delete the pod
Feb 14 00:29:04.931: INFO: Waiting for pod var-expansion-8638a204-2fef-11e9-9de3-466a6591423c to disappear
Feb 14 00:29:04.952: INFO: Pod var-expansion-8638a204-2fef-11e9-9de3-466a6591423c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:29:04.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-swbwn" for this suite.
Feb 14 00:29:11.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:29:11.422: INFO: namespace: e2e-tests-var-expansion-swbwn, resource: bindings, ignored listing per whitelist
Feb 14 00:29:11.883: INFO: namespace e2e-tests-var-expansion-swbwn deletion completed in 6.910282364s

• [SLOW TEST:12.115 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:29:11.883: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-mtvv6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-mtvv6/secret-test-8d812c41-2fef-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume secrets
Feb 14 00:29:12.977: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d846603-2fef-11e9-9de3-466a6591423c" in namespace "e2e-tests-secrets-mtvv6" to be "success or failure"
Feb 14 00:29:12.997: INFO: Pod "pod-configmaps-8d846603-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.391637ms
Feb 14 00:29:15.018: INFO: Pod "pod-configmaps-8d846603-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041659211s
Feb 14 00:29:17.040: INFO: Pod "pod-configmaps-8d846603-2fef-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062986055s
STEP: Saw pod success
Feb 14 00:29:17.040: INFO: Pod "pod-configmaps-8d846603-2fef-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:29:17.061: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-configmaps-8d846603-2fef-11e9-9de3-466a6591423c container env-test: <nil>
STEP: delete the pod
Feb 14 00:29:17.130: INFO: Waiting for pod pod-configmaps-8d846603-2fef-11e9-9de3-466a6591423c to disappear
Feb 14 00:29:17.150: INFO: Pod pod-configmaps-8d846603-2fef-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:29:17.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mtvv6" for this suite.
Feb 14 00:29:23.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:29:23.507: INFO: namespace: e2e-tests-secrets-mtvv6, resource: bindings, ignored listing per whitelist
Feb 14 00:29:24.035: INFO: namespace e2e-tests-secrets-mtvv6 deletion completed in 6.863161627s

• [SLOW TEST:12.151 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:29:24.035: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-bqk48
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 00:29:25.028: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-bqk48'
Feb 14 00:29:25.321: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 14 00:29:25.321: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Feb 14 00:29:27.372: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-bqk48'
Feb 14 00:29:27.693: INFO: stderr: ""
Feb 14 00:29:27.693: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:29:27.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bqk48" for this suite.
Feb 14 00:29:33.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:29:34.433: INFO: namespace: e2e-tests-kubectl-bqk48, resource: bindings, ignored listing per whitelist
Feb 14 00:29:34.578: INFO: namespace e2e-tests-kubectl-bqk48 deletion completed in 6.862999586s

• [SLOW TEST:10.543 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:29:34.578: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-dl2w7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 00:29:35.635: INFO: Waiting up to 5m0s for pod "downward-api-9b067057-2fef-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-dl2w7" to be "success or failure"
Feb 14 00:29:35.656: INFO: Pod "downward-api-9b067057-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.627119ms
Feb 14 00:29:37.678: INFO: Pod "downward-api-9b067057-2fef-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043368294s
Feb 14 00:29:39.700: INFO: Pod "downward-api-9b067057-2fef-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065221856s
STEP: Saw pod success
Feb 14 00:29:39.700: INFO: Pod "downward-api-9b067057-2fef-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:29:39.721: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downward-api-9b067057-2fef-11e9-9de3-466a6591423c container dapi-container: <nil>
STEP: delete the pod
Feb 14 00:29:39.790: INFO: Waiting for pod downward-api-9b067057-2fef-11e9-9de3-466a6591423c to disappear
Feb 14 00:29:39.815: INFO: Pod downward-api-9b067057-2fef-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:29:39.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dl2w7" for this suite.
Feb 14 00:29:45.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:29:46.174: INFO: namespace: e2e-tests-downward-api-dl2w7, resource: bindings, ignored listing per whitelist
Feb 14 00:29:46.764: INFO: namespace e2e-tests-downward-api-dl2w7 deletion completed in 6.927641454s

• [SLOW TEST:12.186 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:29:46.764: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-xwth5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-xwth5
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-xwth5
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-xwth5
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-xwth5
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-xwth5
Feb 14 00:29:51.946: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-xwth5, name: ss-0, uid: a26bd766-2fef-11e9-9f3f-da917295d151, status phase: Pending. Waiting for statefulset controller to delete.
Feb 14 00:29:55.177: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-xwth5, name: ss-0, uid: a26bd766-2fef-11e9-9f3f-da917295d151, status phase: Failed. Waiting for statefulset controller to delete.
Feb 14 00:29:55.182: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-xwth5, name: ss-0, uid: a26bd766-2fef-11e9-9f3f-da917295d151, status phase: Failed. Waiting for statefulset controller to delete.
Feb 14 00:29:55.186: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-xwth5
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-xwth5
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-xwth5 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 00:30:05.441: INFO: Deleting all statefulset in ns e2e-tests-statefulset-xwth5
Feb 14 00:30:05.463: INFO: Scaling statefulset ss to 0
Feb 14 00:30:15.589: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 00:30:15.610: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:30:15.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-xwth5" for this suite.
Feb 14 00:30:21.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:30:22.112: INFO: namespace: e2e-tests-statefulset-xwth5, resource: bindings, ignored listing per whitelist
Feb 14 00:30:22.580: INFO: namespace e2e-tests-statefulset-xwth5 deletion completed in 6.882831297s

• [SLOW TEST:35.816 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:30:22.580: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-swsbb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 14 00:30:23.715: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-swsbb" to be "success or failure"
Feb 14 00:30:23.735: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 20.339055ms
Feb 14 00:30:25.757: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042070585s
Feb 14 00:30:27.779: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063791568s
Feb 14 00:30:29.800: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.085372707s
STEP: Saw pod success
Feb 14 00:30:29.800: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 14 00:30:29.821: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 14 00:30:29.909: INFO: Waiting for pod pod-host-path-test to disappear
Feb 14 00:30:29.929: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:30:29.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-swsbb" for this suite.
Feb 14 00:30:36.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:30:36.448: INFO: namespace: e2e-tests-hostpath-swsbb, resource: bindings, ignored listing per whitelist
Feb 14 00:30:36.846: INFO: namespace e2e-tests-hostpath-swsbb deletion completed in 6.895593538s

• [SLOW TEST:14.266 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:30:36.846: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-8jkkc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-8jkkc.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-8jkkc.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8jkkc.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-8jkkc.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-8jkkc.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8jkkc.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 14 00:30:46.244: INFO: DNS probes using e2e-tests-dns-8jkkc/dns-test-c027cb34-2fef-11e9-9de3-466a6591423c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:30:46.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-8jkkc" for this suite.
Feb 14 00:30:52.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:30:53.164: INFO: namespace: e2e-tests-dns-8jkkc, resource: bindings, ignored listing per whitelist
Feb 14 00:30:53.268: INFO: namespace e2e-tests-dns-8jkkc deletion completed in 6.950164715s

• [SLOW TEST:16.422 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:30:53.268: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-4r9tp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:30:54.348: INFO: (0) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 27.367703ms)
Feb 14 00:30:54.394: INFO: (1) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 45.923179ms)
Feb 14 00:30:54.418: INFO: (2) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.045753ms)
Feb 14 00:30:54.443: INFO: (3) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.78085ms)
Feb 14 00:30:54.468: INFO: (4) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.331718ms)
Feb 14 00:30:54.491: INFO: (5) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 23.743852ms)
Feb 14 00:30:54.516: INFO: (6) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.422289ms)
Feb 14 00:30:54.540: INFO: (7) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.280476ms)
Feb 14 00:30:54.564: INFO: (8) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 23.661823ms)
Feb 14 00:30:54.588: INFO: (9) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.22922ms)
Feb 14 00:30:54.613: INFO: (10) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.198204ms)
Feb 14 00:30:54.637: INFO: (11) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.298606ms)
Feb 14 00:30:54.661: INFO: (12) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.025287ms)
Feb 14 00:30:54.695: INFO: (13) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 34.039805ms)
Feb 14 00:30:54.719: INFO: (14) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.012984ms)
Feb 14 00:30:54.744: INFO: (15) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.159649ms)
Feb 14 00:30:54.768: INFO: (16) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 23.797445ms)
Feb 14 00:30:54.798: INFO: (17) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 29.70564ms)
Feb 14 00:30:54.822: INFO: (18) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.042831ms)
Feb 14 00:30:54.846: INFO: (19) /api/v1/nodes/shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 23.744886ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:30:54.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-4r9tp" for this suite.
Feb 14 00:31:00.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:31:01.596: INFO: namespace: e2e-tests-proxy-4r9tp, resource: bindings, ignored listing per whitelist
Feb 14 00:31:01.784: INFO: namespace e2e-tests-proxy-4r9tp deletion completed in 6.917214107s

• [SLOW TEST:8.516 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:31:01.785: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pxnb9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 14 00:31:07.456: INFO: Successfully updated pod "labelsupdatecefe1f8e-2fef-11e9-9de3-466a6591423c"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:31:09.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pxnb9" for this suite.
Feb 14 00:31:33.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:31:34.343: INFO: namespace: e2e-tests-projected-pxnb9, resource: bindings, ignored listing per whitelist
Feb 14 00:31:34.426: INFO: namespace e2e-tests-projected-pxnb9 deletion completed in 24.865713252s

• [SLOW TEST:32.641 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:31:34.426: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-hbhcr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-hbhcr
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 14 00:31:35.404: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 14 00:32:01.804: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.5:8080/dial?request=hostName&protocol=http&host=100.96.0.62&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-hbhcr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:32:01.804: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:32:02.377: INFO: Waiting for endpoints: map[]
Feb 14 00:32:02.399: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.5:8080/dial?request=hostName&protocol=http&host=100.96.1.4&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-hbhcr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:32:02.399: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:32:02.968: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:32:02.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-hbhcr" for this suite.
Feb 14 00:32:25.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:32:25.428: INFO: namespace: e2e-tests-pod-network-test-hbhcr, resource: bindings, ignored listing per whitelist
Feb 14 00:32:25.845: INFO: namespace e2e-tests-pod-network-test-hbhcr deletion completed in 22.855261184s

• [SLOW TEST:51.420 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:32:25.845: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-wzgrw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0214 00:32:57.098171   31687 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 00:32:57.098: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:32:57.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-wzgrw" for this suite.
Feb 14 00:33:03.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:33:03.494: INFO: namespace: e2e-tests-gc-wzgrw, resource: bindings, ignored listing per whitelist
Feb 14 00:33:04.064: INFO: namespace e2e-tests-gc-wzgrw deletion completed in 6.944627185s

• [SLOW TEST:38.218 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:33:04.064: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-t254m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Feb 14 00:33:05.111: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-t254m'
Feb 14 00:33:06.456: INFO: stderr: ""
Feb 14 00:33:06.456: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 14 00:33:07.478: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:33:07.478: INFO: Found 0 / 1
Feb 14 00:33:08.478: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:33:08.478: INFO: Found 0 / 1
Feb 14 00:33:09.478: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:33:09.478: INFO: Found 0 / 1
Feb 14 00:33:10.478: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:33:10.478: INFO: Found 1 / 1
Feb 14 00:33:10.478: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 14 00:33:10.500: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:33:10.500: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 14 00:33:10.500: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml logs redis-master-dh78f redis-master --namespace=e2e-tests-kubectl-t254m'
Feb 14 00:33:10.695: INFO: stderr: ""
Feb 14 00:33:10.695: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Feb 00:33:08.536 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Feb 00:33:08.536 # Server started, Redis version 3.2.12\n1:M 14 Feb 00:33:08.536 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Feb 00:33:08.536 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 14 00:33:10.695: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml log redis-master-dh78f redis-master --namespace=e2e-tests-kubectl-t254m --tail=1'
Feb 14 00:33:10.900: INFO: stderr: ""
Feb 14 00:33:10.900: INFO: stdout: "1:M 14 Feb 00:33:08.536 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 14 00:33:10.900: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml log redis-master-dh78f redis-master --namespace=e2e-tests-kubectl-t254m --limit-bytes=1'
Feb 14 00:33:11.093: INFO: stderr: ""
Feb 14 00:33:11.093: INFO: stdout: " "
STEP: exposing timestamps
Feb 14 00:33:11.093: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml log redis-master-dh78f redis-master --namespace=e2e-tests-kubectl-t254m --tail=1 --timestamps'
Feb 14 00:33:11.286: INFO: stderr: ""
Feb 14 00:33:11.286: INFO: stdout: "2019-02-14T00:33:08.536800551Z 1:M 14 Feb 00:33:08.536 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 14 00:33:13.786: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml log redis-master-dh78f redis-master --namespace=e2e-tests-kubectl-t254m --since=1s'
Feb 14 00:33:13.980: INFO: stderr: ""
Feb 14 00:33:13.980: INFO: stdout: ""
Feb 14 00:33:13.980: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml log redis-master-dh78f redis-master --namespace=e2e-tests-kubectl-t254m --since=24h'
Feb 14 00:33:14.167: INFO: stderr: ""
Feb 14 00:33:14.167: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Feb 00:33:08.536 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Feb 00:33:08.536 # Server started, Redis version 3.2.12\n1:M 14 Feb 00:33:08.536 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Feb 00:33:08.536 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Feb 14 00:33:14.167: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-t254m'
Feb 14 00:33:14.357: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 00:33:14.357: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 14 00:33:14.357: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-t254m'
Feb 14 00:33:14.549: INFO: stderr: "No resources found.\n"
Feb 14 00:33:14.549: INFO: stdout: ""
Feb 14 00:33:14.549: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -l name=nginx --namespace=e2e-tests-kubectl-t254m -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 14 00:33:14.709: INFO: stderr: ""
Feb 14 00:33:14.709: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:33:14.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-t254m" for this suite.
Feb 14 00:33:36.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:33:37.631: INFO: namespace: e2e-tests-kubectl-t254m, resource: bindings, ignored listing per whitelist
Feb 14 00:33:37.714: INFO: namespace e2e-tests-kubectl-t254m deletion completed in 22.982752672s

• [SLOW TEST:33.649 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:33:37.714: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-m2bs9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 00:33:38.689: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-m2bs9'
Feb 14 00:33:38.892: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 14 00:33:38.892: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 14 00:33:38.933: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 14 00:33:38.950: INFO: scanned /root for discovery docs: <nil>
Feb 14 00:33:38.950: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-m2bs9'
Feb 14 00:33:52.166: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 14 00:33:52.167: INFO: stdout: "Created e2e-test-nginx-rc-d348144e08f879739737163abd287601\nScaling up e2e-test-nginx-rc-d348144e08f879739737163abd287601 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-d348144e08f879739737163abd287601 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-d348144e08f879739737163abd287601 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 14 00:33:52.167: INFO: stdout: "Created e2e-test-nginx-rc-d348144e08f879739737163abd287601\nScaling up e2e-test-nginx-rc-d348144e08f879739737163abd287601 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-d348144e08f879739737163abd287601 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-d348144e08f879739737163abd287601 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 14 00:33:52.167: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-m2bs9'
Feb 14 00:33:52.323: INFO: stderr: ""
Feb 14 00:33:52.323: INFO: stdout: "e2e-test-nginx-rc-bkc6c e2e-test-nginx-rc-d348144e08f879739737163abd287601-bp455 "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb 14 00:33:57.324: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-m2bs9'
Feb 14 00:33:57.481: INFO: stderr: ""
Feb 14 00:33:57.481: INFO: stdout: "e2e-test-nginx-rc-d348144e08f879739737163abd287601-bp455 "
Feb 14 00:33:57.481: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods e2e-test-nginx-rc-d348144e08f879739737163abd287601-bp455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-m2bs9'
Feb 14 00:33:57.668: INFO: stderr: ""
Feb 14 00:33:57.668: INFO: stdout: "true"
Feb 14 00:33:57.668: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods e2e-test-nginx-rc-d348144e08f879739737163abd287601-bp455 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-m2bs9'
Feb 14 00:33:57.829: INFO: stderr: ""
Feb 14 00:33:57.829: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 14 00:33:57.829: INFO: e2e-test-nginx-rc-d348144e08f879739737163abd287601-bp455 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Feb 14 00:33:57.829: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-7eoqa.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-m2bs9'
Feb 14 00:33:58.096: INFO: stderr: ""
Feb 14 00:33:58.096: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:33:58.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-m2bs9" for this suite.
Feb 14 00:34:20.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:34:20.350: INFO: namespace: e2e-tests-kubectl-m2bs9, resource: bindings, ignored listing per whitelist
Feb 14 00:34:21.009: INFO: namespace e2e-tests-kubectl-m2bs9 deletion completed in 22.891946198s

• [SLOW TEST:43.295 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:34:21.009: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8zlgh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:34:22.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45bbc8a3-2ff0-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-8zlgh" to be "success or failure"
Feb 14 00:34:22.059: INFO: Pod "downwardapi-volume-45bbc8a3-2ff0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 24.749935ms
Feb 14 00:34:24.081: INFO: Pod "downwardapi-volume-45bbc8a3-2ff0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046234809s
Feb 14 00:34:26.103: INFO: Pod "downwardapi-volume-45bbc8a3-2ff0-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068363006s
STEP: Saw pod success
Feb 14 00:34:26.103: INFO: Pod "downwardapi-volume-45bbc8a3-2ff0-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:34:26.124: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downwardapi-volume-45bbc8a3-2ff0-11e9-9de3-466a6591423c container client-container: <nil>
STEP: delete the pod
Feb 14 00:34:26.206: INFO: Waiting for pod downwardapi-volume-45bbc8a3-2ff0-11e9-9de3-466a6591423c to disappear
Feb 14 00:34:26.235: INFO: Pod downwardapi-volume-45bbc8a3-2ff0-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:34:26.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8zlgh" for this suite.
Feb 14 00:34:32.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:34:32.886: INFO: namespace: e2e-tests-projected-8zlgh, resource: bindings, ignored listing per whitelist
Feb 14 00:34:33.113: INFO: namespace e2e-tests-projected-8zlgh deletion completed in 6.85708936s

• [SLOW TEST:12.104 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:34:33.114: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5d4zh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4cee71a0-2ff0-11e9-9de3-466a6591423c
STEP: Creating a pod to test consume configMaps
Feb 14 00:34:34.137: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4cf2b017-2ff0-11e9-9de3-466a6591423c" in namespace "e2e-tests-projected-5d4zh" to be "success or failure"
Feb 14 00:34:34.158: INFO: Pod "pod-projected-configmaps-4cf2b017-2ff0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.347253ms
Feb 14 00:34:36.191: INFO: Pod "pod-projected-configmaps-4cf2b017-2ff0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053572525s
Feb 14 00:34:38.215: INFO: Pod "pod-projected-configmaps-4cf2b017-2ff0-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077186152s
STEP: Saw pod success
Feb 14 00:34:38.215: INFO: Pod "pod-projected-configmaps-4cf2b017-2ff0-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:34:38.235: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod pod-projected-configmaps-4cf2b017-2ff0-11e9-9de3-466a6591423c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:34:38.300: INFO: Waiting for pod pod-projected-configmaps-4cf2b017-2ff0-11e9-9de3-466a6591423c to disappear
Feb 14 00:34:38.320: INFO: Pod pod-projected-configmaps-4cf2b017-2ff0-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:34:38.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5d4zh" for this suite.
Feb 14 00:34:44.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:34:44.585: INFO: namespace: e2e-tests-projected-5d4zh, resource: bindings, ignored listing per whitelist
Feb 14 00:34:45.325: INFO: namespace e2e-tests-projected-5d4zh deletion completed in 6.984614109s

• [SLOW TEST:12.212 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Feb 14 00:34:45.326: INFO: >>> kubeConfig: /tmp/build/3ab43379/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-xj7bb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 00:34:46.326: INFO: Waiting up to 5m0s for pod "downward-api-543685d0-2ff0-11e9-9de3-466a6591423c" in namespace "e2e-tests-downward-api-xj7bb" to be "success or failure"
Feb 14 00:34:46.369: INFO: Pod "downward-api-543685d0-2ff0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 42.814041ms
Feb 14 00:34:48.392: INFO: Pod "downward-api-543685d0-2ff0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065180309s
Feb 14 00:34:50.414: INFO: Pod "downward-api-543685d0-2ff0-11e9-9de3-466a6591423c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087351634s
Feb 14 00:34:52.435: INFO: Pod "downward-api-543685d0-2ff0-11e9-9de3-466a6591423c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.108852654s
STEP: Saw pod success
Feb 14 00:34:52.435: INFO: Pod "downward-api-543685d0-2ff0-11e9-9de3-466a6591423c" satisfied condition "success or failure"
Feb 14 00:34:52.456: INFO: Trying to get logs from node shoot--it--pub-az-7eoqa-cpu-worker-7f75855789-nrfjx pod downward-api-543685d0-2ff0-11e9-9de3-466a6591423c container dapi-container: <nil>
STEP: delete the pod
Feb 14 00:34:52.518: INFO: Waiting for pod downward-api-543685d0-2ff0-11e9-9de3-466a6591423c to disappear
Feb 14 00:34:52.538: INFO: Pod downward-api-543685d0-2ff0-11e9-9de3-466a6591423c no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Feb 14 00:34:52.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xj7bb" for this suite.
Feb 14 00:34:58.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:34:58.939: INFO: namespace: e2e-tests-downward-api-xj7bb, resource: bindings, ignored listing per whitelist
Feb 14 00:34:59.636: INFO: namespace e2e-tests-downward-api-xj7bb deletion completed in 7.077147153s

• [SLOW TEST:14.311 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
Feb 14 00:34:59.638: INFO: Running AfterSuite actions on all nodes
Feb 14 00:34:59.638: INFO: Running AfterSuite actions on node 1
Feb 14 00:34:59.638: INFO: Skipping dumping logs from cluster


Summarizing 2 Failures:

[Fail] [k8s.io] Kubelet when scheduling a read only busybox container [It] should not write to root filesystem [NodeConformance] [Conformance] 
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/pods.go:110

[Fail] [sig-network] Service endpoints latency [It] should not be very high  [Conformance] 
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service_latency.go:121

Ran 200 of 2161 Specs in 7426.454 seconds
SUCCESS! -- 200 Passed | 0 Failed | 2 Flaked | 0 Pending | 1961 Skipped PASS

Ginkgo ran 1 suite in 2h3m47.501208882s
Test Suite Passed
